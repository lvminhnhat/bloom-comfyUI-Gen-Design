

<!-- BEGIN Built_In_Node/built-in-nodes/api-node/image/bfl/flux-pro-ultra-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Introduction

# Introduction

Official documentation for ComfyUI. Contribute [here](https://github.com/Comfy-Org/docs).

## [​](http://docs.comfy.org#comfyui) [ComfyUI](https://github.com/comfyanonymous/ComfyUI)

The most powerful and modular stable diffusion GUI and backend. Written by [comfyanonymous](https://github.com/comfyanonymous) and other [contributors](https://github.com/comfyanonymous/ComfyUI/graphs/contributors).

- **ComfyUI** is a node-based interface and inference engine for generative AI
- Users can combine various AI models and operations through nodes to achieve highly customizable and controllable content generation
- ComfyUI is completely open source and can run on your local device

## [​](http://docs.comfy.org#getting-started-with-comfyui) Getting Started with ComfyUI

### [​](http://docs.comfy.org#comfyui-installation) ComfyUI Installation

ComfyUI currently offers multiple installation methods, supporting Windows, MacOS, and Linux systems:

ComfyUI Desktop (Recommended)

ComfyUI Desktop currently supports standalone installation for **Windows and MacOS (ARM)**, currently in Beta

- Code is open source on [Github](https://github.com/Comfy-Org/desktop)

You can choose the appropriate installation for your system and hardware below

- Windows
- MacOS(Apple Silicon)
- Linux

[**ComfyUI Desktop (Windows) Installation Guide**  
\
Suitable for **Windows** version with **Nvidia** GPU](http://docs.comfy.org/installation/desktop/windows)

[**ComfyUI Desktop (Windows) Installation Guide**  
\
Suitable for **Windows** version with **Nvidia** GPU](http://docs.comfy.org/installation/desktop/windows)

[**ComfyUI Desktop (MacOS) Installation Guide**  
\
Suitable for MacOS with **Apple Silicon**](http://docs.comfy.org/installation/desktop/macos)

ComfyUI Desktop **currently has no Linux prebuilds**, please visit the [Manual Installation](http://docs.comfy.org/installation/manual_install) section to install ComfyUI

ComfyUI Portable (Windows)

[**ComfyUI Portable (Windows) Installation Guide**  
\
Supports **Windows** ComfyUI version running on **Nvidia GPUs** or **CPU-only**, always use the latest commits and completely portable.](http://docs.comfy.org/installation/comfyui_portable_windows)

Manual Installation

[**ComfyUI Manual Installation Guide**  
\
Supports all system types and GPU types (Nvidia, AMD, Intel, Apple Silicon, Ascend NPU, Cambricon MLU)](http://docs.comfy.org/installation/manual_install)

## [​](http://docs.comfy.org#contributing-to-comfyui-ecosystem) Contributing to ComfyUI Ecosystem

If you’re planning to develop ComfyUI custom nodes (plugins), please read the following section.

[**Custom Node Development Guide**  
\
Learn how to build a custom node (plugin) for ComfyUI](http://docs.comfy.org/custom-nodes/overview)

## [​](http://docs.comfy.org#contributing-to-documentation) Contributing to Documentation

Fork the documentation [repo](https://github.com/comfyanonymous/ComfyUI) on Github and submit a PR to us

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/get_started/introduction.mdx)

[System RequirementsThis guide introduces some system requirements for ComfyUI, including hardware and software requirements  
\
Next](http://docs.comfy.org/installation/system_requirements)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [ComfyUI](http://docs.comfy.org#comfyui)
- [Getting Started with ComfyUI](http://docs.comfy.org#getting-started-with-comfyui)
- [ComfyUI Installation](http://docs.comfy.org#comfyui-installation)
- [Contributing to ComfyUI Ecosystem](http://docs.comfy.org#contributing-to-comfyui-ecosystem)
- [Contributing to Documentation](http://docs.comfy.org#contributing-to-documentation)

<!-- END Built_In_Node/built-in-nodes/api-node/image/bfl/flux-pro-ultra-image.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/image/ideogram/ideogram-v1.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
  - Ideogram
    
    - [Ideogram V2](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v2)
    - [Ideogram V3](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3)
    - [Ideogram V1](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v1)
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Ideogram V1 - ComfyUI Native Node Documentation

# Ideogram V1 - ComfyUI Native Node Documentation

Node for creating precise text rendering images using Ideogram API

The Ideogram V1 node allows you to generate images with high-quality text rendering capabilities using Ideogram’s text-to-image API.

## [​](http://docs.comfy.org#parameter-description) Parameter Description

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing the content to generateturbobooleanFalseWhether to use turbo mode (faster but possibly lower quality)aspect\_ratioselect”1:1”Image aspect ratiomagic\_prompt\_optionselect”AUTO”Determines whether to use MagicPrompt in generation, options: AUTO, ON, OFFseedinteger0Random seed value (0-2147483647)negative\_promptstring""Specifies elements you don’t want in the imagenum\_imagesinteger1Number of images to generate (1-8)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image result

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated on 2025-05-03)]

```python
class IdeogramV1(ComfyNodeABC):
    """
    Generates images synchronously using the Ideogram V1 model.

    Images links are available for a limited period of time; if you would like to keep the image, you must download it.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation",
                    },
                ),
                "turbo": (
                    IO.BOOLEAN,
                    {
                        "default": False,
                        "tooltip": "Whether to use turbo mode (faster generation, potentially lower quality)",
                    }
                ),
            },
            "optional": {
                "aspect_ratio": (
                    IO.COMBO,
                    {
                        "options": list(V1_V2_RATIO_MAP.keys()),
                        "default": "1:1",
                        "tooltip": "The aspect ratio for image generation.",
                    },
                ),
                "magic_prompt_option": (
                    IO.COMBO,
                    {
                        "options": ["AUTO", "ON", "OFF"],
                        "default": "AUTO",
                        "tooltip": "Determine if MagicPrompt should be used in generation",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2147483647,
                        "step": 1,
                        "control_after_generate": True,
                        "display": "number",
                    },
                ),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Description of what to exclude from the image",
                    },
                ),
                "num_images": (
                    IO.INT,
                    {"default": 1, "min": 1, "max": 8, "step": 1, "display": "number"},
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = (IO.IMAGE,)
    FUNCTION = "api_call"
    CATEGORY = "api node/image/ideogram/v1"
    DESCRIPTION = cleandoc(__doc__ or "")
    API_NODE = True

    def api_call(
        self,
        prompt,
        turbo=False,
        aspect_ratio="1:1",
        magic_prompt_option="AUTO",
        seed=0,
        negative_prompt="",
        num_images=1,
        auth_token=None,
    ):
        # Determine the model based on turbo setting
        aspect_ratio = V1_V2_RATIO_MAP.get(aspect_ratio, None)
        model = "V_1_TURBO" if turbo else "V_1"

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/ideogram/generate",
                method=HttpMethod.POST,
                request_model=IdeogramGenerateRequest,
                response_model=IdeogramGenerateResponse,
            ),
            request=IdeogramGenerateRequest(
                image_request=ImageRequest(
                    prompt=prompt,
                    model=model,
                    num_images=num_images,
                    seed=seed,
                    aspect_ratio=aspect_ratio if aspect_ratio != "ASPECT_1_1" else None,
                    magic_prompt_option=(
                        magic_prompt_option if magic_prompt_option != "AUTO" else None
                    ),
                    negative_prompt=negative_prompt if negative_prompt else None,
                )
            ),
            auth_token=auth_token,
        )

        response = operation.execute()

        if not response.data or len(response.data) == 0:
            raise Exception("No images were generated in the response")

        image_urls = [image_data.url for image_data in response.data if image_data.url]

        if not image_urls:
            raise Exception("No image URLs were generated in the response")

        return (download_and_process_images(image_urls),)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/ideogram/ideogram-v1.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3)

[Stability Stable Image UltraA node that generates high-quality images using Stability AI's ultra stable diffusion model  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameter Description](http://docs.comfy.org#parameter-description)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/image/ideogram/ideogram-v1.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/image/ideogram/ideogram-v2.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
  - Ideogram
    
    - [Ideogram V2](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v2)
    - [Ideogram V3](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3)
    - [Ideogram V1](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v1)
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Ideogram V2 - ComfyUI Built-in Node Documentation

# Ideogram V2 - ComfyUI Built-in Node Documentation

Node for creating high-quality images and text rendering using Ideogram V2 API

The Ideogram V2 node allows you to generate more refined images using Ideogram’s second-generation AI model, with significant improvements in text rendering, image quality, and overall aesthetics.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing the content to generateturbobooleanFalseWhether to use turbo mode (faster generation, possibly lower quality)

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDefaultDescriptionaspect\_ratiodropdown”1:1”Image aspect ratio, effective when resolution is set to “Auto”resolutiondropdown”Auto”Output image resolution, if not set to “Auto”, it will override the aspect\_ratio settingmagic\_prompt\_optiondropdown”AUTO”Determines whether to use MagicPrompt feature during generation, options are \[“AUTO”, “ON”, “OFF”]seedinteger0Random seed value, range 0-2147483647style\_typedropdown”NONE”Generation style type (V2 only), options are \[“AUTO”, “GENERAL”, “REALISTIC”, “DESIGN”, “RENDER\_3D”, “ANIME”]negative\_promptstring""Specifies elements you don’t want to appear in the imagenum\_imagesinteger1Number of images to generate, range 1-8

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image(s)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated on 2025-05-03)]

```python

class IdeogramV2(ComfyNodeABC):
    """
    Generates images synchronously using the Ideogram V2 model.

    Images links are available for a limited period of time; if you would like to keep the image, you must download it.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation",
                    },
                ),
                "turbo": (
                    IO.BOOLEAN,
                    {
                        "default": False,
                        "tooltip": "Whether to use turbo mode (faster generation, potentially lower quality)",
                    }
                ),
            },
            "optional": {
                "aspect_ratio": (
                    IO.COMBO,
                    {
                        "options": list(V1_V2_RATIO_MAP.keys()),
                        "default": "1:1",
                        "tooltip": "The aspect ratio for image generation. Ignored if resolution is not set to AUTO.",
                    },
                ),
                "resolution": (
                    IO.COMBO,
                    {
                        "options": list(V1_V1_RES_MAP.keys()),
                        "default": "Auto",
                        "tooltip": "The resolution for image generation. If not set to AUTO, this overrides the aspect_ratio setting.",
                    },
                ),
                "magic_prompt_option": (
                    IO.COMBO,
                    {
                        "options": ["AUTO", "ON", "OFF"],
                        "default": "AUTO",
                        "tooltip": "Determine if MagicPrompt should be used in generation",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2147483647,
                        "step": 1,
                        "control_after_generate": True,
                        "display": "number",
                    },
                ),
                "style_type": (
                    IO.COMBO,
                    {
                        "options": ["AUTO", "GENERAL", "REALISTIC", "DESIGN", "RENDER_3D", "ANIME"],
                        "default": "NONE",
                        "tooltip": "Style type for generation (V2 only)",
                    },
                ),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Description of what to exclude from the image",
                    },
                ),
                "num_images": (
                    IO.INT,
                    {"default": 1, "min": 1, "max": 8, "step": 1, "display": "number"},
                ),
                #"color_palette": (
                #    IO.STRING,
                #    {
                #        "multiline": False,
                #        "default": "",
                #        "tooltip": "Color palette preset name or hex colors with weights",
                #    },
                #),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = (IO.IMAGE,)
    FUNCTION = "api_call"
    CATEGORY = "api node/image/ideogram/v2"
    DESCRIPTION = cleandoc(__doc__ or "")
    API_NODE = True

    def api_call(
        self,
        prompt,
        turbo=False,
        aspect_ratio="1:1",
        resolution="Auto",
        magic_prompt_option="AUTO",
        seed=0,
        style_type="NONE",
        negative_prompt="",
        num_images=1,
        color_palette="",
        auth_token=None,
    ):
        aspect_ratio = V1_V2_RATIO_MAP.get(aspect_ratio, None)
        resolution = V1_V1_RES_MAP.get(resolution, None)
        # Determine the model based on turbo setting
        model = "V_2_TURBO" if turbo else "V_2"

        # Handle resolution vs aspect_ratio logic
        # If resolution is not AUTO, it overrides aspect_ratio
        final_resolution = None
        final_aspect_ratio = None

        if resolution != "AUTO":
            final_resolution = resolution
        else:
            final_aspect_ratio = aspect_ratio if aspect_ratio != "ASPECT_1_1" else None

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/ideogram/generate",
                method=HttpMethod.POST,
                request_model=IdeogramGenerateRequest,
                response_model=IdeogramGenerateResponse,
            ),
            request=IdeogramGenerateRequest(
                image_request=ImageRequest(
                    prompt=prompt,
                    model=model,
                    num_images=num_images,
                    seed=seed,
                    aspect_ratio=final_aspect_ratio,
                    resolution=final_resolution,
                    magic_prompt_option=(
                        magic_prompt_option if magic_prompt_option != "AUTO" else None
                    ),
                    style_type=style_type if style_type != "NONE" else None,
                    negative_prompt=negative_prompt if negative_prompt else None,
                    color_palette=color_palette if color_palette else None,
                )
            ),
            auth_token=auth_token,
        )

        response = operation.execute()

        if not response.data or len(response.data) == 0:
            raise Exception("No images were generated in the response")

        image_urls = [image_data.url for image_data in response.data if image_data.url]

        if not image_urls:
            raise Exception("No image URLs were generated in the response")

        return (download_and_process_images(image_urls),)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/ideogram/ideogram-v2.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)

[Ideogram V3Node for creating top-quality images and text rendering using Ideogram's latest V3 API  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/image/ideogram/ideogram-v2.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/image/ideogram/ideogram-v3.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
  - Ideogram
    
    - [Ideogram V2](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v2)
    - [Ideogram V3](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3)
    - [Ideogram V1](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v1)
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Ideogram V3 - ComfyUI Native Node Documentation

# Ideogram V3 - ComfyUI Native Node Documentation

Node for creating top-quality images and text rendering using Ideogram’s latest V3 API

This node connects to the Ideogram V3 API to perform image generation tasks.

Currently, this node supports two image generation modes:

- **Text-to-Image Mode** - Generate new images from text prompts
- **Inpainting Mode** - Regenerate specific areas by providing an original image and mask

### [​](http://docs.comfy.org#text-to-image-mode) Text-to-Image Mode

This is the default mode, activated when no image or mask inputs are provided. Simply provide a prompt and the desired parameters:

1. Describe the image you want in the prompt field
2. Select an appropriate aspect ratio or resolution
3. Adjust other parameters like magic prompt, seed, and rendering quality
4. Run the node to generate the image

### [​](http://docs.comfy.org#inpainting-mode) Inpainting Mode

**Important Note**: This mode requires both image and mask inputs. If only one is provided, the node will throw an error.

1. Connect the original image to the `image` input port
2. Create a mask with the same dimensions as the original image, where white areas represent parts to be regenerated
3. Connect the mask to the `mask` input port
4. Describe what you want to generate in the masked area in the prompt
5. Run the node to perform local editing

## [​](http://docs.comfy.org#parameter-descriptions) Parameter Descriptions

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing the content to generateaspect\_ratiocombo”1:1”Image aspect ratio (text-to-image mode only)resolutioncombo”Auto”Image resolution, overrides aspect ratio when setmagic\_prompt\_optioncombo”AUTO”Magic prompt enhancement: AUTO, ON, or OFFseedint0Random seed value, 0 for random generationnum\_imagesint1Number of images to generate (1-8)rendering\_speedcombo”BALANCED”Rendering speed: BALANCED, TURBO, or QUALITY

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionimageimageInput image for inpainting mode (**must be provided with mask**)maskmaskMask for inpainting, white areas will be replaced (**must be provided with image**)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image

## [​](http://docs.comfy.org#how-it-works) How It Works

The Ideogram V3 node uses state-of-the-art AI models to process user input, capable of understanding complex design intentions and text layout requirements. It supports two main modes:

1. **Generation Mode**: Creates new images from text prompts
2. **Edit Mode**: Uses original image + mask combination, replacing only the areas specified by the mask

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class IdeogramV3(ComfyNodeABC):
    """
    Generates images synchronously using the Ideogram V3 model.

    Supports both regular image generation from text prompts and image editing with mask.
    Images links are available for a limited period of time; if you would like to keep the image, you must download it.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation or editing",
                    },
                ),
            },
            "optional": {
                "image": (
                    IO.IMAGE,
                    {
                        "default": None,
                        "tooltip": "Optional reference image for image editing.",
                    },
                ),
                "mask": (
                    IO.MASK,
                    {
                        "default": None,
                        "tooltip": "Optional mask for inpainting (white areas will be replaced)",
                    },
                ),
                "aspect_ratio": (
                    IO.COMBO,
                    {
                        "options": list(V3_RATIO_MAP.keys()),
                        "default": "1:1",
                        "tooltip": "The aspect ratio for image generation. Ignored if resolution is not set to Auto.",
                    },
                ),
                "resolution": (
                    IO.COMBO,
                    {
                        "options": V3_RESOLUTIONS,
                        "default": "Auto",
                        "tooltip": "The resolution for image generation. If not set to Auto, this overrides the aspect_ratio setting.",
                    },
                ),
                "magic_prompt_option": (
                    IO.COMBO,
                    {
                        "options": ["AUTO", "ON", "OFF"],
                        "default": "AUTO",
                        "tooltip": "Determine if MagicPrompt should be used in generation",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2147483647,
                        "step": 1,
                        "control_after_generate": True,
                        "display": "number",
                    },
                ),
                "num_images": (
                    IO.INT,
                    {"default": 1, "min": 1, "max": 8, "step": 1, "display": "number"},
                ),
                "rendering_speed": (
                    IO.COMBO,
                    {
                        "options": ["BALANCED", "TURBO", "QUALITY"],
                        "default": "BALANCED",
                        "tooltip": "Controls the trade-off between generation speed and quality",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = (IO.IMAGE,)
    FUNCTION = "api_call"
    CATEGORY = "api node/image/ideogram/v3"
    DESCRIPTION = cleandoc(__doc__ or "")
    API_NODE = True

    def api_call(
        self,
        prompt,
        image=None,
        mask=None,
        resolution="Auto",
        aspect_ratio="1:1",
        magic_prompt_option="AUTO",
        seed=0,
        num_images=1,
        rendering_speed="BALANCED",
        auth_token=None,
    ):
        # Check if both image and mask are provided for editing mode
        if image is not None and mask is not None:
            # Edit mode
            path = "/proxy/ideogram/ideogram-v3/edit"

            # Process image and mask
            input_tensor = image.squeeze().cpu()

            # Validate mask dimensions match image
            if mask.shape[1:] != image.shape[1:-1]:
                raise Exception("Mask and Image must be the same size")

            # Process image
            img_np = (input_tensor.numpy() * 255).astype(np.uint8)
            img = Image.fromarray(img_np)
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format="PNG")
            img_byte_arr.seek(0)
            img_binary = img_byte_arr
            img_binary.name = "image.png"

            # Process mask - white areas will be replaced
            mask_np = (mask.squeeze().cpu().numpy() * 255).astype(np.uint8)
            mask_img = Image.fromarray(mask_np)
            mask_byte_arr = io.BytesIO()
            mask_img.save(mask_byte_arr, format="PNG")
            mask_byte_arr.seek(0)
            mask_binary = mask_byte_arr
            mask_binary.name = "mask.png"

            # Create edit request
            edit_request = IdeogramV3EditRequest(
                prompt=prompt,
                rendering_speed=rendering_speed,
            )

            # Add optional parameters
            if magic_prompt_option != "AUTO":
                edit_request.magic_prompt = magic_prompt_option
            if seed != 0:
                edit_request.seed = seed
            if num_images > 1:
                edit_request.num_images = num_images

            # Execute the operation for edit mode
            operation = SynchronousOperation(
                endpoint=ApiEndpoint(
                    path=path,
                    method=HttpMethod.POST,
                    request_model=IdeogramV3EditRequest,
                    response_model=IdeogramGenerateResponse,
                ),
                request=edit_request,
                files={
                    "image": img_binary,
                    "mask": mask_binary,
                },
                content_type="multipart/form-data",
                auth_token=auth_token,
            )

        elif image is not None or mask is not None:
            # If only one of image or mask is provided, raise an error
            raise Exception("Ideogram V3 image editing requires both an image AND a mask")
        else:
            # Generation mode
            path = "/proxy/ideogram/ideogram-v3/generate"

            # Create generation request
            gen_request = IdeogramV3Request(
                prompt=prompt,
                rendering_speed=rendering_speed,
            )

            # Handle resolution vs aspect ratio
            if resolution != "Auto":
                gen_request.resolution = resolution
            elif aspect_ratio != "1:1":
                v3_aspect = V3_RATIO_MAP.get(aspect_ratio)
                if v3_aspect:
                    gen_request.aspect_ratio = v3_aspect

            # Add optional parameters
            if magic_prompt_option != "AUTO":
                gen_request.magic_prompt = magic_prompt_option
            if seed != 0:
                gen_request.seed = seed
            if num_images > 1:
                gen_request.num_images = num_images

            # Execute the operation for generation mode
            operation = SynchronousOperation(
                endpoint=ApiEndpoint(
                    path=path,
                    method=HttpMethod.POST,
                    request_model=IdeogramV3Request,
                    response_model=IdeogramGenerateResponse,
                ),
                request=gen_request,
                auth_token=auth_token,
            )

        # Execute the operation and process response
        response = operation.execute()

        if not response.data or len(response.data) == 0:
            raise Exception("No images were generated in the response")

        image_urls = [image_data.url for image_data in response.data if image_data.url]

        if not image_urls:
            raise Exception("No image URLs were generated in the response")

        return (download_and_process_images(image_urls),)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/ideogram/ideogram-v3.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v2)

[Ideogram V1Node for creating precise text rendering images using Ideogram API  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v1)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Text-to-Image Mode](http://docs.comfy.org#text-to-image-mode)
- [Inpainting Mode](http://docs.comfy.org#inpainting-mode)
- [Parameter Descriptions](http://docs.comfy.org#parameter-descriptions)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/image/ideogram/ideogram-v3.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/image/luma/luma-image-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
    
    - [Luma Reference](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-reference)
    - [Luma Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image)
    - [Luma Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image)
  - Recraft
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Image to Image - ComfyUI Built-in Node Documentation

# Luma Image to Image - ComfyUI Built-in Node Documentation

Node for modifying images using Luma AI

The Luma Image to Image node allows you to modify existing images using Luma AI technology based on text prompts, while preserving certain features and structure of the original image.

## [​](http://docs.comfy.org#node-function) Node Function

This node connects to Luma AI’s text-to-image API, enabling users to generate images through detailed text prompts. Luma AI is known for its excellent realism and detail, particularly excelling at generating photorealistic content and artistic style images.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing the content to generatemodelselect-Select which generation model to useaspect\_ratioselect16:9Set the aspect ratio of the output imageseedinteger0Seed value to determine if node should rerun, but actual results don’t depend on seedstyle\_image\_weightfloat1.0Weight of the style image, range 0.02-1.0, only effective when style\_image is provided

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

Without the following parameter inputs, the node functions in text-to-image mode

ParameterTypeDescriptionimage\_luma\_refLUMA\_REFLuma reference node connection, influences generation results through input images, can consider up to 4 imagesstyle\_imageimageStyle reference image, uses only 1 image, influences the style of generated images, adjusted through `style_image_weight`character\_imageimageAdds character features to the generated results, can be a batch of multiple images, up to 4 images

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageThe generated image

## [​](http://docs.comfy.org#usage-examples) Usage Examples

## [​](http://docs.comfy.org#how-it-works) How It Works

The Luma Image to Image node analyzes the input image and combines it with text prompts to guide the modification process. It uses Luma AI’s generation models to make creative changes to images based on prompts.

Node process:

1. First uploads the input image to ComfyAPI
2. Then sends the image URL with the prompt to Luma API
3. Waits for Luma AI to complete processing
4. Downloads and returns the generated image

The image\_weight parameter controls the degree of influence from the original image - values closer to 0 will preserve more of the original image features, while values closer to 1 allow for more substantial modifications.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated on 2025-05-05)]

```python

class LumaImageModifyNode(ComfyNodeABC):
    """
    Modifies images synchronously based on prompt and aspect ratio.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE,),
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation",
                    },
                ),
                "image_weight": (
                    IO.FLOAT,
                    {
                        "default": 1.0,
                        "min": 0.02,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Weight of the image; the closer to 0.0, the less the image will be modified.",
                    },
                ),
                "model": ([model.value for model in LumaImageModel],),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {},
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        model: str,
        image: torch.Tensor,
        image_weight: float,
        seed,
        auth_token=None,
        **kwargs,
    ):
        # first, upload image
        download_urls = upload_images_to_comfyapi(
            image, max_images=1, auth_token=auth_token
        )
        image_url = download_urls[0]
        # next, make Luma call with download url provided
        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/luma/generations/image",
                method=HttpMethod.POST,
                request_model=LumaImageGenerationRequest,
                response_model=LumaGeneration,
            ),
            request=LumaImageGenerationRequest(
                prompt=prompt,
                model=model,
                modify_image_ref=LumaModifyImageRef(
                    url=image_url, weight=round(image_weight, 2)
                ),
            ),
            auth_token=auth_token,
        )
        response_api: LumaGeneration = operation.execute()

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/luma/generations/{response_api.id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=LumaGeneration,
            ),
            completed_statuses=[LumaState.completed],
            failed_statuses=[LumaState.failed],
            status_extractor=lambda x: x.state,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        img_response = requests.get(response_poll.assets.image)
        img = process_image_response(img_response)
        return (img,)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/luma/luma-image-to-image.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image)

[Save SVGA utility node for saving SVG vector graphics to files  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Examples](http://docs.comfy.org#usage-examples)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/image/luma/luma-image-to-image.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/image/luma/luma-reference.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
    
    - [Luma Reference](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-reference)
    - [Luma Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image)
    - [Luma Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image)
  - Recraft
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Reference - ComfyUI Built-in Node Documentation

# Luma Reference - ComfyUI Built-in Node Documentation

Helper node providing reference images for Luma image generation

The Luma Reference node allows you to set reference images and weights to guide the creation process of Luma image generation nodes, making the generated images closer to specific features of the reference images.

## [​](http://docs.comfy.org#node-function) Node Function

This node works as a helper tool for Luma generation nodes, allowing users to provide reference images to influence generation results. It enables users to set the weight of reference images to control how much they affect the final result. Multiple Luma Reference nodes can be chained together, with a maximum of 4 working simultaneously according to API requirements.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageImage-Input image used as referenceweightFloat1.0Controls the strength of the reference image’s influence (0-1)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionluma\_refLUMA\_REFReference object containing image and weight

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Luma Text to Image Workflow Example**  
\
Luma Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image)

## [​](http://docs.comfy.org#how-it-works) How It Works

The Luma Reference node receives image input and allows setting a weight value. The node doesn’t directly generate or modify images but creates a reference object containing image data and weight information, which is then passed to Luma generation nodes.

During the generation process, Luma AI analyzes the features of the reference image and incorporates these features into the generation results based on the set weight. Higher weight values mean the generated image will be closer to the reference image’s features, while lower weight values indicate the reference image will only slightly influence the final result.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated on 2025-05-03)]

```python

class LumaReferenceNode(ComfyNodeABC):
    """
    Holds an image and weight for use with Luma Generate Image node.
    """

    RETURN_TYPES = (LumaIO.LUMA_REF,)
    RETURN_NAMES = ("luma_ref",)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "create_luma_reference"
    CATEGORY = "api node/image/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (
                    IO.IMAGE,
                    {
                        "tooltip": "Image to use as reference.",
                    },
                ),
                "weight": (
                    IO.FLOAT,
                    {
                        "default": 1.0,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Weight of image reference.",
                    },
                ),
            },
            "optional": {"luma_ref": (LumaIO.LUMA_REF,)},
        }

    def create_luma_reference(
        self, image: torch.Tensor, weight: float, luma_ref: LumaReferenceChain = None
    ):
        if luma_ref is not None:
            luma_ref = luma_ref.clone()
        else:
            luma_ref = LumaReferenceChain()
        luma_ref.add(LumaReference(image=image, weight=round(weight, 2)))
        return (luma_ref,)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/luma/luma-reference.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/bfl/flux-pro-ultra-image)

[Luma Text to ImageA node that converts text descriptions into high-quality images using Luma AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/image/luma/luma-reference.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/image/luma/luma-text-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
    
    - [Luma Reference](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-reference)
    - [Luma Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image)
    - [Luma Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image)
  - Recraft
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Text to Image - ComfyUI Native Node Documentation

# Luma Text to Image - ComfyUI Native Node Documentation

A node that converts text descriptions into high-quality images using Luma AI

The Luma Text to Image node allows you to create highly realistic and artistic images from text descriptions using Luma AI’s advanced image generation capabilities.

## [​](http://docs.comfy.org#node-function) Node Function

This node connects to Luma AI’s text-to-image API, enabling users to generate images through detailed text prompts. Luma AI is known for its excellent realism and detail representation, particularly excelling at producing photorealistic content and artistic style images.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptString""Text prompt describing the content to generatemodelSelect-Choose which generation model to useaspect\_ratioSelect16:9Set the output image’s aspect ratioseedInteger0Seed value to determine if the node should re-run, but actual results are independent of the seedstyle\_image\_weightFloat1.0Style image weight, range 0.0-1.0, only applies when style\_image is provided, higher means stronger style reference

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionimage\_luma\_refLUMA\_REFLuma reference node connection to influence generation with input images; up to 4 imagesstyle\_imageImageStyle reference image; only 1 image will be usedcharacter\_imageImageCharacter reference images; can be a batch of multiple, up to 4 images

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEImageGenerated image result

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Luma Text to Image Usage Example**  
\
Detailed guide for Luma Text to Image workflow](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image)

## [​](http://docs.comfy.org#how-it-works) How It Works

The Luma Text to Image node analyzes the text prompt provided by the user and creates corresponding images through Luma AI’s generation models. This process uses deep learning technology to understand text descriptions and convert them into visual representations. Users can fine-tune the generation process by adjusting various parameters, including resolution, guidance scale, and negative prompts.

Additionally, the node supports using reference images and concept guidance to further influence the generation results, allowing creators to more precisely achieve their creative vision.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated on 2025-05-03)]

```python

class LumaImageGenerationNode(ComfyNodeABC):
    """
    Generates images synchronously based on prompt and aspect ratio.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation",
                    },
                ),
                "model": ([model.value for model in LumaImageModel],),
                "aspect_ratio": (
                    [ratio.value for ratio in LumaAspectRatio],
                    {
                        "default": LumaAspectRatio.ratio_16_9,
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
                "style_image_weight": (
                    IO.FLOAT,
                    {
                        "default": 1.0,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Weight of style image. Ignored if no style_image provided.",
                    },
                ),
            },
            "optional": {
                "image_luma_ref": (
                    LumaIO.LUMA_REF,
                    {
                        "tooltip": "Luma Reference node connection to influence generation with input images; up to 4 images can be considered."
                    },
                ),
                "style_image": (
                    IO.IMAGE,
                    {"tooltip": "Style reference image; only 1 image will be used."},
                ),
                "character_image": (
                    IO.IMAGE,
                    {
                        "tooltip": "Character reference images; can be a batch of multiple, up to 4 images can be considered."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        model: str,
        aspect_ratio: str,
        seed,
        style_image_weight: float,
        image_luma_ref: LumaReferenceChain = None,
        style_image: torch.Tensor = None,
        character_image: torch.Tensor = None,
        auth_token=None,
        **kwargs,
    ):
        # handle image_luma_ref
        api_image_ref = None
        if image_luma_ref is not None:
            api_image_ref = self._convert_luma_refs(
                image_luma_ref, max_refs=4, auth_token=auth_token
            )
        # handle style_luma_ref
        api_style_ref = None
        if style_image is not None:
            api_style_ref = self._convert_style_image(
                style_image, weight=style_image_weight, auth_token=auth_token
            )
        # handle character_ref images
        character_ref = None
        if character_image is not None:
            download_urls = upload_images_to_comfyapi(
                character_image, max_images=4, auth_token=auth_token
            )
            character_ref = LumaCharacterRef(
                identity0=LumaImageIdentity(images=download_urls)
            )

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/luma/generations/image",
                method=HttpMethod.POST,
                request_model=LumaImageGenerationRequest,
                response_model=LumaGeneration,
            ),
            request=LumaImageGenerationRequest(
                prompt=prompt,
                model=model,
                aspect_ratio=aspect_ratio,
                image_ref=api_image_ref,
                style_ref=api_style_ref,
                character_ref=character_ref,
            ),
            auth_token=auth_token,
        )
        response_api: LumaGeneration = operation.execute()

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/luma/generations/{response_api.id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=LumaGeneration,
            ),
            completed_statuses=[LumaState.completed],
            failed_statuses=[LumaState.failed],
            status_extractor=lambda x: x.state,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        img_response = requests.get(response_poll.assets.image)
        img = process_image_response(img_response)
        return (img,)

    def _convert_luma_refs(
        self, luma_ref: LumaReferenceChain, max_refs: int, auth_token=None
    ):
        luma_urls = []
        ref_count = 0
        for ref in luma_ref.refs:
            download_urls = upload_images_to_comfyapi(
                ref.image, max_images=1, auth_token=auth_token
            )
            luma_urls.append(download_urls[0])
            ref_count += 1
            if ref_count >= max_refs:
                break
        return luma_ref.create_api_model(download_urls=luma_urls, max_refs=max_refs)

    def _convert_style_image(
        self, style_image: torch.Tensor, weight: float, auth_token=None
    ):
        chain = LumaReferenceChain(
            first_ref=LumaReference(image=style_image, weight=weight)
        )
        return self._convert_luma_refs(chain, max_refs=1, auth_token=auth_token)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/luma/luma-text-to-image.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-reference)

[Luma Image to ImageNode for modifying images using Luma AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/image/luma/luma-text-to-image.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-color-rgb.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Color RGB - ComfyUI Native Node Documentation

# Recraft Color RGB - ComfyUI Native Node Documentation

Helper node for defining color controls in Recraft image generation

The Recraft Color RGB node lets you define precise RGB color values to control colors in Recraft image generation.

## [​](http://docs.comfy.org#node-function) Node Function

This node creates a color configuration object that connects to the Recraft Controls node to specify colors used in generated images.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionrinteger0Red channel (0-255)ginteger0Green channel (0-255)binteger0Blue channel (0-255)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionrecraft\_colorRecraft ColorColor config object to connect to Recraft Controls

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Recraft Text to Image Workflow Example**  
\
Recraft Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftColorRGBNode:
    """
    Create Recraft Color by choosing specific RGB values.
    """

    RETURN_TYPES = (RecraftIO.COLOR,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    RETURN_NAMES = ("recraft_color",)
    FUNCTION = "create_color"
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "r": (IO.INT, {
                    "default": 0,
                    "min": 0,
                    "max": 255,
                    "tooltip": "Red value of color."
                }),
                "g": (IO.INT, {
                    "default": 0,
                    "min": 0,
                    "max": 255,
                    "tooltip": "Green value of color."
                }),
                "b": (IO.INT, {
                    "default": 0,
                    "min": 0,
                    "max": 255,
                    "tooltip": "Blue value of color."
                }),
            },
            "optional": {
                "recraft_color": (RecraftIO.COLOR,),
            }
        }

    def create_color(self, r: int, g: int, b: int, recraft_color: RecraftColorChain=None):
        recraft_color = recraft_color.clone() if recraft_color else RecraftColorChain()
        recraft_color.add(RecraftColor(r, g, b))
        return (recraft_color, )

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-color-rgb.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)

[Recraft Text to ImageA Recraft API node that generates high-quality images from text descriptions  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-color-rgb.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-controls.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Controls - ComfyUI Native Node Documentation

# Recraft Controls - ComfyUI Native Node Documentation

Node providing advanced control parameters for Recraft image generation

The Recraft Controls node lets you define control parameters (like colors and background colors) to guide Recraft’s image generation process. This node combines multiple control inputs into a unified control object.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptioncolorsRecraft ColorColor controls for image generationbackground\_colorRecraft ColorBackground color control

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionrecraft\_controlsRecraft ControlsControl config object for Recraft generation nodes

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Recraft Text to Image Workflow Example**  
\
Recraft Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

## [​](http://docs.comfy.org#how-it-works) How It Works

Node process:

1. Collects input control parameters (colors and background\_color)
2. Combines these parameters into a structured control object
3. Outputs this control object for connecting to Recraft generation nodes

When connected to Recraft generation nodes, these control parameters influence the AI generation process. The AI considers multiple factors beyond just the text prompt’s semantic content. If color inputs are configured, the AI will try to use these colors appropriately in the generated image.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftControlsNode:
    """
    Create Recraft Controls for customizing Recraft generation.
    """

    RETURN_TYPES = (RecraftIO.CONTROLS,)
    RETURN_NAMES = ("recraft_controls",)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "create_controls"
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
            },
            "optional": {
                "colors": (RecraftIO.COLOR,),
                "background_color": (RecraftIO.COLOR,),
            }
        }

    def create_controls(self, colors: RecraftColorChain=None, background_color: RecraftColorChain=None):
        return (RecraftControls(colors=colors, background_color=background_color), )

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-controls.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)

[Recraft Replace BackgroundA Recraft API node that automatically detects foreground subjects and replaces backgrounds  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-controls.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-creative-upscale.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Creative Upscale - ComfyUI Native Node Documentation

# Recraft Creative Upscale - ComfyUI Native Node Documentation

A Recraft API node that uses AI to creatively enhance image details and resolution

The Recraft Creative Upscale node uses Recraft’s API to increase image resolution while creatively enhancing and enriching image details.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageimage-Input image to be creatively upscaled

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageHigh-resolution image after creative upscaling

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftCreativeUpscaleNode(RecraftCrispUpscaleNode):
    """
    Upscale image synchronously.
    Enhances a given raster image using ‘creative upscale’ tool, boosting resolution with a focus on refining small details and faces.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    RECRAFT_PATH = "/proxy/recraft/images/creativeUpscale"
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-creative-upscale.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)

[Recraft Image to ImageA Recraft API node that generates new images based on text prompts and reference images  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-creative-upscale.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Crisp Upscale - ComfyUI Native Node Documentation

# Recraft Crisp Upscale - ComfyUI Native Node Documentation

A Recraft API node that enhances image clarity and resolution using AI

The Recraft Crisp Upscale node uses Recraft’s API to improve image resolution and clarity.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageimage-Input image to be upscaled

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageUpscaled and enhanced output image

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftCrispUpscaleNode:
    """
    Upscale image synchronously.
    Enhances a given raster image using ‘crisp upscale’ tool, increasing image resolution, making the image sharper and cleaner.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    RECRAFT_PATH = "/proxy/recraft/images/crispUpscale"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
            },
            "optional": {
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        auth_token=None,
        **kwargs,
    ):
        images = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                path=self.RECRAFT_PATH,
                auth_token=auth_token,
            )
            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))
            pbar.update(1)

        images_tensor = torch.cat(images, dim=0)
        return (images_tensor,)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)

[Recraft Color RGBHelper node for defining color controls in Recraft image generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-image-inpainting.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Image Inpainting - ComfyUI Native Node Documentation

# Recraft Image Inpainting - ComfyUI Native Node Documentation

Selectively modify image regions using Recraft API

The Recraft Image Inpainting node lets you modify specific areas of an image while keeping the rest unchanged. By providing an image, mask and text prompt, you can generate new content to fill the selected areas.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageimage-Input image to modifymaskmask-Black and white mask defining areas to changepromptstring""Text describing what to generate in masked areaninteger1Number of results to generate (1-6)seedinteger0Random seed value

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionrecraft\_styleRecraft StyleStyle settings for generated contentnegative\_promptstringElements to avoid in generated contentrecraft\_controlsRecraft ControlsAdditional controls like colors

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageModified image result

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftImageInpaintingNode:
    """
    Modify image based on prompt and mask.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
                "mask": (IO.MASK, ),
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation.",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 6,
                        "tooltip": "The number of images to generate.",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "recraft_style": (RecraftIO.STYLEV3,),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        mask: torch.Tensor,
        prompt: str,
        n: int,
        seed,
        auth_token=None,
        recraft_style: RecraftStyle = None,
        negative_prompt: str = None,
        **kwargs,
    ):
        default_style = RecraftStyle(RecraftStyleV3.realistic_image)
        if recraft_style is None:
            recraft_style = default_style

        if not negative_prompt:
            negative_prompt = None

        request = RecraftImageGenerationRequest(
            prompt=prompt,
            negative_prompt=negative_prompt,
            model=RecraftModel.recraftv3,
            n=n,
            style=recraft_style.style,
            substyle=recraft_style.substyle,
            style_id=recraft_style.style_id,
            random_seed=seed,
        )

        # prepare mask tensor
        _, H, W, _ = image.shape
        mask = mask.unsqueeze(-1)
        mask = mask.movedim(-1,1)
        mask = common_upscale(mask, width=W, height=H, upscale_method="nearest-exact", crop="disabled")
        mask = mask.movedim(1,-1)
        mask = (mask > 0.5).float()

        images = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                mask=mask[i:i+1],
                path="/proxy/recraft/images/inpaint",
                request=request,
                auth_token=auth_token,
            )
            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))
            pbar.update(1)

        images_tensor = torch.cat(images, dim=0)
        return (images_tensor, )
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-image-inpainting.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)

[Recraft Vectorize ImageA Recraft API node that converts raster images to vector SVG format  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-image-inpainting.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-image-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Image to Image - ComfyUI Native Node Documentation

# Recraft Image to Image - ComfyUI Native Node Documentation

A Recraft API node that generates new images based on text prompts and reference images

The Recraft Image to Image node uses Recraft’s API to generate new images based on a reference image and text prompts.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageimage-Reference image inputpromptstring""Text description for the generated imageninteger1Number of images to generate (1-6)seedinteger0Random seed value

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionrecraft\_styleRecraft StyleStyle settings for generated imagesnegative\_promptstringElements to avoid in generated imagesrecraft\_controlsRecraft ControlsAdditional controls like colors

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image result

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class RecraftImageToImageNode:
    """
    Modify image based on prompt and strength.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation.",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 6,
                        "tooltip": "The number of images to generate.",
                    },
                ),
                "strength": (
                    IO.FLOAT,
                    {
                        "default": 0.5,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Defines the difference with the original image, should lie in [0, 1], where 0 means almost identical, and 1 means miserable similarity."
                    }
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "recraft_style": (RecraftIO.STYLEV3,),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
                "recraft_controls": (
                    RecraftIO.CONTROLS,
                    {
                        "tooltip": "Optional additional controls over the generation via the Recraft Controls node."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        prompt: str,
        n: int,
        strength: float,
        seed,
        auth_token=None,
        recraft_style: RecraftStyle = None,
        negative_prompt: str = None,
        recraft_controls: RecraftControls = None,
        **kwargs,
    ):
        default_style = RecraftStyle(RecraftStyleV3.realistic_image)
        if recraft_style is None:
            recraft_style = default_style

        controls_api = None
        if recraft_controls:
            controls_api = recraft_controls.create_api_model()

        if not negative_prompt:
            negative_prompt = None

        request = RecraftImageGenerationRequest(
            prompt=prompt,
            negative_prompt=negative_prompt,
            model=RecraftModel.recraftv3,
            n=n,
            strength=round(strength, 2),
            style=recraft_style.style,
            substyle=recraft_style.substyle,
            style_id=recraft_style.style_id,
            controls=controls_api,
            random_seed=seed,
        )

        images = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                path="/proxy/recraft/images/imageToImage",
                request=request,
                auth_token=auth_token,
            )
            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))
            pbar.update(1)

        images_tensor = torch.cat(images, dim=0)
        return (images_tensor, )
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-image-to-image.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)

[Recraft Crisp UpscaleA Recraft API node that enhances image clarity and resolution using AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-image-to-image.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-remove-background.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Remove Background - ComfyUI Native Node Documentation

# Recraft Remove Background - ComfyUI Native Node Documentation

A Recraft API node that automatically removes image backgrounds and creates transparent alpha channels

The Recraft Remove Background node uses Recraft’s API to intelligently detect and remove image backgrounds, creating images with transparent backgrounds and corresponding alpha masks.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageimage-Input image to remove background from

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageImage with background removed (with alpha channel)MASKmaskMask of the main subject (white areas are preserved)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftRemoveBackgroundNode:
    """
    Remove background from image, and return processed image and mask.
    """

    RETURN_TYPES = (IO.IMAGE, IO.MASK)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
            },
            "optional": {
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        auth_token=None,
        **kwargs,
    ):
        images = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                path="/proxy/recraft/images/removeBackground",
                auth_token=auth_token,
            )
            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))
            pbar.update(1)

        images_tensor = torch.cat(images, dim=0)
        # use alpha channel as masks, in B,H,W format
        masks_tensor = images_tensor[:,:,:,-1:].squeeze(-1)
        return (images_tensor, masks_tensor)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-remove-background.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)

[Recraft Style - Logo RasterHelper node for setting logo raster style in Recraft image generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-remove-background.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-replace-background.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Replace Background - ComfyUI Native Node Documentation

# Recraft Replace Background - ComfyUI Native Node Documentation

A Recraft API node that automatically detects foreground subjects and replaces backgrounds

The Recraft Replace Background node uses Recraft’s API to intelligently detect subjects in images and generate new backgrounds based on text prompts.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageimage-Input image with subject to preservepromptstring""Text prompt for background generationninteger1Number of images to generate (1-6)seedinteger0Random seed value for node re-runs

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionrecraft\_styleRecraft StyleStyle settings for background generationnegative\_promptstringText describing elements to avoid

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageFinal image with replaced background

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class RecraftReplaceBackgroundNode:
    """
    Replace background on image, based on provided prompt.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation.",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 6,
                        "tooltip": "The number of images to generate.",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "recraft_style": (RecraftIO.STYLEV3,),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        prompt: str,
        n: int,
        seed,
        auth_token=None,
        recraft_style: RecraftStyle = None,
        negative_prompt: str = None,
        **kwargs,
    ):
        default_style = RecraftStyle(RecraftStyleV3.realistic_image)
        if recraft_style is None:
            recraft_style = default_style

        if not negative_prompt:
            negative_prompt = None

        request = RecraftImageGenerationRequest(
            prompt=prompt,
            negative_prompt=negative_prompt,
            model=RecraftModel.recraftv3,
            n=n,
            style=recraft_style.style,
            substyle=recraft_style.substyle,
            style_id=recraft_style.style_id,
        )

        images = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                path="/proxy/recraft/images/replaceBackground",
                request=request,
                auth_token=auth_token,
            )
            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))
            pbar.update(1)

        images_tensor = torch.cat(images, dim=0)
        return (images_tensor, )

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-replace-background.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)

[Ideogram V2Node for creating high-quality images and text rendering using Ideogram V2 API  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v2)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-replace-background.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Style - Digital Illustration - ComfyUI Native Node Documentation

# Recraft Style - Digital Illustration - ComfyUI Native Node Documentation

A helper node for setting digital illustration style in Recraft image generation

This node creates a style configuration object that guides Recraft’s image generation process towards a digital illustration look.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionsubstyleselectNoneSpecific substyle of digital illustration

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionrecraft\_styleRecraft StyleStyle config object to connect to Recraft generation nodes

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Recraft Text to Image Workflow Example**  
\
Recraft Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftStyleV3DigitalIllustrationNode(RecraftStyleV3RealisticImageNode):
    """
    Select digital_illustration style and optional substyle.
    """

    RECRAFT_STYLE = RecraftStyleV3.digital_illustration

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)

[Recraft Remove BackgroundA Recraft API node that automatically removes image backgrounds and creates transparent alpha channels  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Style - Logo Raster - ComfyUI Built-in Node Documentation

# Recraft Style - Logo Raster - ComfyUI Built-in Node Documentation

Helper node for setting logo raster style in Recraft image generation

This node creates a style configuration object that guides Recraft’s image generation process toward professional logo design effects. By selecting different substyles, you can define the design style, complexity and use cases of the generated logo.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionsubstyleSelection-Specific substyle for logo raster (Required)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionrecraft\_styleRecraft StyleStyle configuration object, connects to Recraft generation node

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Recraft Text to Image Workflow Example**  
\
Recraft Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python
class RecraftStyleV3LogoRasterNode(RecraftStyleV3RealisticImageNode):
    """
    Select vector_illustration style and optional substyle.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "substyle": (get_v3_substyles(s.RECRAFT_STYLE, include_none=False),),
            }
        }

    RECRAFT_STYLE = RecraftStyleV3.logo_raster
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)

[Recraft ControlsNode providing advanced control parameters for Recraft image generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Style - Realistic Image - ComfyUI Native Node Documentation

# Recraft Style - Realistic Image - ComfyUI Native Node Documentation

A helper node for setting realistic photo style in Recraft image generation

The Recraft Style - Realistic Image node helps set up realistic photo styles for Recraft image generation, with various substyle options to control the visual characteristics of generated images.

## [​](http://docs.comfy.org#node-function) Node Function

This node creates a style configuration object that guides Recraft’s image generation process towards realistic photo effects.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionsubstyleselectNoneSpecific substyle of realistic photo (Required)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionrecraft\_styleRecraft StyleStyle config object to connect to Recraft generation nodes

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Recraft Text to Image Workflow Example**  
\
Recraft Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class RecraftStyleV3RealisticImageNode:
    """
    Select realistic_image style and optional substyle.
    """

    RETURN_TYPES = (RecraftIO.STYLEV3,)
    RETURN_NAMES = ("recraft_style",)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "create_style"
    CATEGORY = "api node/image/Recraft"

    RECRAFT_STYLE = RecraftStyleV3.realistic_image

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "substyle": (get_v3_substyles(s.RECRAFT_STYLE),),
            }
        }

    def create_style(self, substyle: str):
        if substyle == "None":
            substyle = None
        return (RecraftStyle(self.RECRAFT_STYLE, substyle),)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)

[Recraft Text to VectorA Recraft API node that generates scalable vector graphics from text descriptions  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-text-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Text to Image - ComfyUI Built-in Node Documentation

# Recraft Text to Image - ComfyUI Built-in Node Documentation

A Recraft API node that generates high-quality images from text descriptions

The Recraft Text to Image node lets you generate high-quality images from text prompts by directly connecting to Recraft AI’s image generation API to create images in various styles.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text description for the imagesizeselect1024x1024Output image sizenint1Number of images (1-6)seedint0Random seed value

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionrecraft\_styleRecraft StyleImage style setting, default is “realistic photo”negative\_promptstringElements to exclude from generationrecraft\_controlsRecraft ControlsAdditional control parameters (colors, etc.)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image(s)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftTextToImageNode:
    """
    Generates images synchronously based on prompt and resolution.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation.",
                    },
                ),
                "size": (
                    [res.value for res in RecraftImageSize],
                    {
                        "default": RecraftImageSize.res_1024x1024,
                        "tooltip": "The size of the generated image.",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 6,
                        "tooltip": "The number of images to generate.",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "recraft_style": (RecraftIO.STYLEV3,),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
                "recraft_controls": (
                    RecraftIO.CONTROLS,
                    {
                        "tooltip": "Optional additional controls over the generation via the Recraft Controls node."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        size: str,
        n: int,
        seed,
        recraft_style: RecraftStyle = None,
        negative_prompt: str = None,
        recraft_controls: RecraftControls = None,
        auth_token=None,
        **kwargs,
    ):
        default_style = RecraftStyle(RecraftStyleV3.realistic_image)
        if recraft_style is None:
            recraft_style = default_style

        controls_api = None
        if recraft_controls:
            controls_api = recraft_controls.create_api_model()

        if not negative_prompt:
            negative_prompt = None

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/recraft/image_generation",
                method=HttpMethod.POST,
                request_model=RecraftImageGenerationRequest,
                response_model=RecraftImageGenerationResponse,
            ),
            request=RecraftImageGenerationRequest(
                prompt=prompt,
                negative_prompt=negative_prompt,
                model=RecraftModel.recraftv3,
                size=size,
                n=n,
                style=recraft_style.style,
                substyle=recraft_style.substyle,
                style_id=recraft_style.style_id,
                controls=controls_api,
            ),
            auth_token=auth_token,
        )
        response: RecraftImageGenerationResponse = operation.execute()
        images = []
        for data in response.data:
            image = bytesio_to_image_tensor(
                download_url_to_bytesio(data.url, timeout=1024)
            )
            if len(image.shape) < 4:
                image = image.unsqueeze(0)
            images.append(image)
        output_image = torch.cat(images, dim=0)

        return (output_image,)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-text-to-image.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)

[Recraft Image InpaintingSelectively modify image regions using Recraft API  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-text-to-image.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-text-to-vector.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Text to Vector - ComfyUI Native Node Documentation

# Recraft Text to Vector - ComfyUI Native Node Documentation

A Recraft API node that generates scalable vector graphics from text descriptions

The Recraft Text to Vector node lets you create high-quality vector graphics (SVG format) from text descriptions using Recraft’s API. It’s perfect for making logos, icons, and infinitely scalable illustrations.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text description of the vector graphic to generatesubstyleselect-Vector style subtypesizeselect1024x1024Canvas size for the vector outputninteger1Number of results to generate (1-6)seedinteger0Random seed value

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionnegative\_promptstringElements to exclude from generationrecraft\_controlsRecraft ControlsAdditional control parameters (colors, etc.)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionSVGvectorGenerated SVG vector graphic, connect to SaveSVG node to save

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class RecraftTextToVectorNode:
    """
    Generates SVG synchronously based on prompt and resolution.
    """

    RETURN_TYPES = (RecraftIO.SVG,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation.",
                    },
                ),
                "substyle": (get_v3_substyles(RecraftStyleV3.vector_illustration),),
                "size": (
                    [res.value for res in RecraftImageSize],
                    {
                        "default": RecraftImageSize.res_1024x1024,
                        "tooltip": "The size of the generated image.",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 6,
                        "tooltip": "The number of images to generate.",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
                "recraft_controls": (
                    RecraftIO.CONTROLS,
                    {
                        "tooltip": "Optional additional controls over the generation via the Recraft Controls node."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        substyle: str,
        size: str,
        n: int,
        seed,
        negative_prompt: str = None,
        recraft_controls: RecraftControls = None,
        auth_token=None,
        **kwargs,
    ):
        # create RecraftStyle so strings will be formatted properly (i.e. "None" will become None)
        recraft_style = RecraftStyle(RecraftStyleV3.vector_illustration, substyle=substyle)

        controls_api = None
        if recraft_controls:
            controls_api = recraft_controls.create_api_model()

        if not negative_prompt:
            negative_prompt = None

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/recraft/image_generation",
                method=HttpMethod.POST,
                request_model=RecraftImageGenerationRequest,
                response_model=RecraftImageGenerationResponse,
            ),
            request=RecraftImageGenerationRequest(
                prompt=prompt,
                negative_prompt=negative_prompt,
                model=RecraftModel.recraftv3,
                size=size,
                n=n,
                style=recraft_style.style,
                substyle=recraft_style.substyle,
                controls=controls_api,
            ),
            auth_token=auth_token,
        )
        response: RecraftImageGenerationResponse = operation.execute()
        svg_data = []
        for data in response.data:
            svg_data.append(download_url_to_bytesio(data.url, timeout=1024))

        return (SVG(svg_data),)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-text-to-vector.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)

[Recraft Creative UpscaleA Recraft API node that uses AI to creatively enhance image details and resolution  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-text-to-vector.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-vectorize-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Vectorize Image - ComfyUI Built-in Node Documentation

# Recraft Vectorize Image - ComfyUI Built-in Node Documentation

A Recraft API node that converts raster images to vector SVG format

The Recraft Vectorize Image node uses Recraft’s API to convert raster images (like photos, PNGs or JPEGs) into vector SVG format.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageImage-Input image to be converted to vector

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionSVGVectorConverted SVG vector graphic, needs to be connected to SaveSVG node to save

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Recraft Text to Image Workflow Example**  
\
Recraft Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class RecraftVectorizeImageNode:
    """
    Generates SVG synchronously from an input image.
    """

    RETURN_TYPES = (RecraftIO.SVG,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
            },
            "optional": {
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        auth_token=None,
        **kwargs,
    ):
        svgs = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                path="/proxy/recraft/images/vectorize",
                auth_token=auth_token,
            )
            svgs.append(SVG(sub_bytes))
            pbar.update(1)

        return (SVG.combine_all(svgs), )

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-vectorize-image.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)

[Recraft Style - Digital IllustrationA helper node for setting digital illustration style in Recraft image generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/image/recraft/recraft-vectorize-image.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/image/recraft/save-svg.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Save SVG - ComfyUI Built-in Node Documentation

# Save SVG - ComfyUI Built-in Node Documentation

A utility node for saving SVG vector graphics to files

The Save SVG node allows you to save SVG data from Recraft vector generation nodes as usable files in the filesystem. This is an essential component for handling and exporting vector graphics.

## [​](http://docs.comfy.org#node-function) Node Function

This node receives SVG vector data and saves it as standard SVG files in the filesystem. It supports automatic file naming and save path specification, allowing vector graphics to be opened and edited in other software.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionsvgSVG-SVG vector data to savefilename\_prefixstring”recraft”Prefix for the filenameoutput\_dirstring-Output directory, defaults to ComfyUI output folder at `ComfyUI/output/svg/`indexinteger-1Save index, -1 means save all generated SVGs

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionSVGSVGPasses through the input SVG data

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Recraft Text to Image Workflow Example**  
\
Recraft Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python
class SaveSVGNode:
    """
    Save SVG files on disk.
    """

    def __init__(self):
        self.output_dir = folder_paths.get_output_directory()
        self.type = "output"
        self.prefix_append = ""

    RETURN_TYPES = ()
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "save_svg"
    CATEGORY = "api node/image/Recraft"
    OUTPUT_NODE = True

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "svg": (RecraftIO.SVG,),
                "filename_prefix": ("STRING", {"default": "svg/ComfyUI", "tooltip": "The prefix for the file to save. This may include formatting information such as %date:yyyy-MM-dd% or %Empty Latent Image.width% to include values from nodes."})
            },
            "hidden": {
                "prompt": "PROMPT",
                "extra_pnginfo": "EXTRA_PNGINFO"
            }
        }

    def save_svg(self, svg: SVG, filename_prefix="svg/ComfyUI", prompt=None, extra_pnginfo=None):
        filename_prefix += self.prefix_append
        full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(filename_prefix, self.output_dir)
        results = list()

        # Prepare metadata JSON
        metadata_dict = {}
        if prompt is not None:
            metadata_dict["prompt"] = prompt
        if extra_pnginfo is not None:
            metadata_dict.update(extra_pnginfo)

        # Convert metadata to JSON string
        metadata_json = json.dumps(metadata_dict, indent=2) if metadata_dict else None

        for batch_number, svg_bytes in enumerate(svg.data):
            filename_with_batch_num = filename.replace("%batch_num%", str(batch_number))
            file = f"{filename_with_batch_num}_{counter:05}_.svg"

            # Read SVG content
            svg_bytes.seek(0)
            svg_content = svg_bytes.read().decode('utf-8')

            # Inject metadata if available
            if metadata_json:
                # Create metadata element with CDATA section
                metadata_element = f"""  <metadata>
    <![CDATA[
{metadata_json}
    ]]>
  </metadata>
"""
                # Insert metadata after opening svg tag using regex
                import re
                svg_content = re.sub(r'(<svg[^>]*>)', r'\1\n' + metadata_element, svg_content)

            # Write the modified SVG to file
            with open(os.path.join(full_output_folder, file), 'wb') as svg_file:
                svg_file.write(svg_content.encode('utf-8'))

            results.append({
                "filename": file,
                "subfolder": subfolder,
                "type": self.type
            })
            counter += 1
        return { "ui": { "images": results } }

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/save-svg.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image)

[Recraft Style - Realistic ImageA helper node for setting realistic photo style in Recraft image generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/image/recraft/save-svg.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/video/google/google-veo2-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
    
    - [Google Veo2 Video](http://docs.comfy.org/built-in-nodes/api-node/video/google/google-veo2-video)
  - Kling
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Google Veo2 Video - ComfyUI Native Node Documentation

# Google Veo2 Video - ComfyUI Native Node Documentation

A node that generates videos from text descriptions using Google’s Veo2 technology

The Google Veo2 Video node generates high-quality videos from text descriptions using Google’s Veo2 API technology, converting text prompts into dynamic video content.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text description of the video content to generateaspect\_ratioselect”16:9”Output video aspect ratio, “16:9” or “9:16”negative\_promptstring""Text describing what to avoid in the videoduration\_secondsinteger5Video duration, 5-8 secondsenhance\_promptbooleanTrueWhether to use AI to enhance the promptperson\_generationselect”ALLOW”Allow or block person generation, “ALLOW” or “BLOCK”seedinteger0Random seed, 0 means randomly generated

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDefaultDescriptionimageimageNoneOptional reference image to guide video creation

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOvideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class VeoVideoGenerationNode(ComfyNodeABC):
    """
    Generates videos from text prompts using Google's Veo API.

    This node can create videos from text descriptions and optional image inputs,
    with control over parameters like aspect ratio, duration, and more.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Text description of the video",
                    },
                ),
                "aspect_ratio": (
                    IO.COMBO,
                    {
                        "options": ["16:9", "9:16"],
                        "default": "16:9",
                        "tooltip": "Aspect ratio of the output video",
                    },
                ),
            },
            "optional": {
                "negative_prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Negative text prompt to guide what to avoid in the video",
                    },
                ),
                "duration_seconds": (
                    IO.INT,
                    {
                        "default": 5,
                        "min": 5,
                        "max": 8,
                        "step": 1,
                        "display": "number",
                        "tooltip": "Duration of the output video in seconds",
                    },
                ),
                "enhance_prompt": (
                    IO.BOOLEAN,
                    {
                        "default": True,
                        "tooltip": "Whether to enhance the prompt with AI assistance",
                    }
                ),
                "person_generation": (
                    IO.COMBO,
                    {
                        "options": ["ALLOW", "BLOCK"],
                        "default": "ALLOW",
                        "tooltip": "Whether to allow generating people in the video",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFF,
                        "step": 1,
                        "display": "number",
                        "control_after_generate": True,
                        "tooltip": "Seed for video generation (0 for random)",
                    },
                ),
                "image": (IO.IMAGE, {
                    "default": None,
                    "tooltip": "Optional reference image to guide video generation",
                }),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    RETURN_TYPES = (IO.VIDEO,)
    FUNCTION = "generate_video"
    CATEGORY = "api node/video/Veo"
    DESCRIPTION = "Generates videos from text prompts using Google's Veo API"
    API_NODE = True

    def generate_video(
        self,
        prompt,
        aspect_ratio="16:9",
        negative_prompt="",
        duration_seconds=5,
        enhance_prompt=True,
        person_generation="ALLOW",
        seed=0,
        image=None,
        auth_token=None,
    ):
        # Prepare the instances for the request
        instances = []

        instance = {
            "prompt": prompt
        }

        # Add image if provided
        if image is not None:
            image_base64 = convert_image_to_base64(image)
            if image_base64:
                instance["image"] = {
                    "bytesBase64Encoded": image_base64,
                    "mimeType": "image/png"
                }

        instances.append(instance)

        # Create parameters dictionary
        parameters = {
            "aspectRatio": aspect_ratio,
            "personGeneration": person_generation,
            "durationSeconds": duration_seconds,
            "enhancePrompt": enhance_prompt,
        }

        # Add optional parameters if provided
        if negative_prompt:
            parameters["negativePrompt"] = negative_prompt
        if seed > 0:
            parameters["seed"] = seed

        # Initial request to start video generation
        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/veo/generate",
                method=HttpMethod.POST,
                request_model=Veo2GenVidRequest,
                response_model=Veo2GenVidResponse
            ),
            request=Veo2GenVidRequest(
                instances=instances,
                parameters=parameters
            ),
            auth_token=auth_token
        )

        initial_response = initial_operation.execute()
        operation_name = initial_response.name

        logging.info(f"Veo generation started with operation name: {operation_name}")

        # Define status extractor function
        def status_extractor(response):
            # Only return "completed" if the operation is done, regardless of success or failure
            # We'll check for errors after polling completes
            return "completed" if response.done else "pending"

        # Define progress extractor function
        def progress_extractor(response):
            # Could be enhanced if the API provides progress information
            return None

        # Define the polling operation
        poll_operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path="/proxy/veo/poll",
                method=HttpMethod.POST,
                request_model=Veo2GenVidPollRequest,
                response_model=Veo2GenVidPollResponse
            ),
            completed_statuses=["completed"],
            failed_statuses=[],  # No failed statuses, we'll handle errors after polling
            status_extractor=status_extractor,
            progress_extractor=progress_extractor,
            request=Veo2GenVidPollRequest(
                operationName=operation_name
            ),
            auth_token=auth_token,
            poll_interval=5.0
        )

        # Execute the polling operation
        poll_response = poll_operation.execute()

        # Now check for errors in the final response
        # Check for error in poll response
        if hasattr(poll_response, 'error') and poll_response.error:
            error_message = f"Veo API error: {poll_response.error.message} (code: {poll_response.error.code})"
            logging.error(error_message)
            raise Exception(error_message)

        # Check for RAI filtered content
        if (hasattr(poll_response.response, 'raiMediaFilteredCount') and
            poll_response.response.raiMediaFilteredCount > 0):

            # Extract reason message if available
            if (hasattr(poll_response.response, 'raiMediaFilteredReasons') and
                poll_response.response.raiMediaFilteredReasons):
                reason = poll_response.response.raiMediaFilteredReasons[0]
                error_message = f"Content filtered by Google's Responsible AI practices: {reason} ({poll_response.response.raiMediaFilteredCount} videos filtered.)"
            else:
                error_message = f"Content filtered by Google's Responsible AI practices ({poll_response.response.raiMediaFilteredCount} videos filtered.)"

            logging.error(error_message)
            raise Exception(error_message)

        # Extract video data
        video_data = None
        if poll_response.response and hasattr(poll_response.response, 'videos') and poll_response.response.videos and len(poll_response.response.videos) > 0:
            video = poll_response.response.videos[0]

            # Check if video is provided as base64 or URL
            if hasattr(video, 'bytesBase64Encoded') and video.bytesBase64Encoded:
                # Decode base64 string to bytes
                video_data = base64.b64decode(video.bytesBase64Encoded)
            elif hasattr(video, 'gcsUri') and video.gcsUri:
                # Download from URL
                video_url = video.gcsUri
                video_response = requests.get(video_url)
                video_data = video_response.content
            else:
                raise Exception("Video returned but no data or URL was provided")
        else:
            raise Exception("Video generation completed but no video was returned")

        if not video_data:
            raise Exception("No video data was returned")

        logging.info("Video generation completed successfully")

        # Convert video data to BytesIO object
        video_io = io.BytesIO(video_data)

        # Return VideoFromFile object
        return (VideoFromFile(video_io),)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/google/google-veo2-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-text-to-video)

[Kling Camera ControlsA node that provides camera control parameters for Kling video generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/video/google/google-veo2-video.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
    
    - [Kling Camera Controls](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)
    - [Kling Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)
    - [Kling Image to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)
    - [Kling Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)
    - [Kling Start-End Frame to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)
    - [Kling Text to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Kling Image to Video (Camera Control) - ComfyUI Built-in Node

# Kling Image to Video (Camera Control) - ComfyUI Built-in Node

Image to video conversion node with camera control features

The Kling Image to Video (Camera Control) node converts static images into videos with professional camera movements. It supports camera controls like zoom, rotation, pan, tilt and first-person view while maintaining focus on the original image content.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionstart\_frameImage-Input image to convert to videopromptString""Text prompt describing video action and contentnegative\_promptString""Elements to avoid in the videocfg\_scaleFloat7.0Controls how closely to follow the promptaspect\_ratioSelect16:9Output video aspect ratio

### [​](http://docs.comfy.org#camera-control-parameters) Camera Control Parameters

ParameterTypeDescriptioncamera\_controlCameraControlCamera control config from Kling Camera Controls node

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class KlingCameraControlI2VNode(KlingImage2VideoNode):
    """
    Kling Image to Video Camera Control Node. This node is a image to video node, but it supports controlling the camera.
    Duration, mode, and model_name request fields are hard-coded because camera control is only supported in pro mode with the kling-v1-5 model at 5s duration as of 2025-05-02.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "start_frame": model_field_to_node_input(
                    IO.IMAGE, KlingImage2VideoRequest, "image"
                ),
                "prompt": model_field_to_node_input(
                    IO.STRING, KlingImage2VideoRequest, "prompt", multiline=True
                ),
                "negative_prompt": model_field_to_node_input(
                    IO.STRING,
                    KlingImage2VideoRequest,
                    "negative_prompt",
                    multiline=True,
                ),
                "cfg_scale": model_field_to_node_input(
                    IO.FLOAT, KlingImage2VideoRequest, "cfg_scale"
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "aspect_ratio",
                    enum_type=AspectRatio,
                ),
                "camera_control": (
                    "CAMERA_CONTROL",
                    {
                        "tooltip": "Can be created using the Kling Camera Controls node. Controls the camera movement and motion during the video generation.",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    DESCRIPTION = "Transform still images into cinematic videos with professional camera movements that simulate real-world cinematography. Control virtual camera actions including zoom, rotation, pan, tilt, and first-person view, while maintaining focus on your original image."

    def api_call(
        self,
        start_frame: torch.Tensor,
        prompt: str,
        negative_prompt: str,
        cfg_scale: float,
        aspect_ratio: str,
        camera_control: CameraControl,
        auth_token: Optional[str] = None,
    ):
        return super().api_call(
            model_name="kling-v1-5",
            start_frame=start_frame,
            cfg_scale=cfg_scale,
            mode="pro",
            aspect_ratio=aspect_ratio,
            duration="5",
            prompt=prompt,
            negative_prompt=negative_prompt,
            camera_control=camera_control,
            auth_token=auth_token,
        )



```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)

[Kling Image to VideoA node that converts static images to dynamic videos using Kling's AI technology  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Camera Control Parameters](http://docs.comfy.org#camera-control-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
    
    - [Kling Camera Controls](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)
    - [Kling Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)
    - [Kling Image to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)
    - [Kling Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)
    - [Kling Start-End Frame to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)
    - [Kling Text to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Kling Text to Video (Camera Control) - ComfyUI Built-in Node

# Kling Text to Video (Camera Control) - ComfyUI Built-in Node

A text to video generation node with camera control features

The Kling Text to Video (Camera Control) node converts text into videos with professional camera movements. It extends the standard Kling Text to Video node by adding camera control capabilities.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptString""Text prompt describing video contentnegative\_promptString""Elements to avoid in the videocfg\_scaleFloat7.0Controls how closely to follow the promptaspect\_ratioSelect”16:9”Output video aspect ratiocamera\_controlCAMERA\_CONTROL-Camera settings from Kling Camera Controls node

### [​](http://docs.comfy.org#fixed-parameters) Fixed Parameters

Note: The following parameters are fixed and cannot be changed:

- Model: kling-v1-5
- Mode: pro
- Duration: 5 seconds

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class KlingCameraControlT2VNode(KlingTextToVideoNode):
    """
    Kling Text to Video Camera Control Node. This node is a text to video node, but it supports controlling the camera.
    Duration, mode, and model_name request fields are hard-coded because camera control is only supported in pro mode with the kling-v1-5 model at 5s duration as of 2025-05-02.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": model_field_to_node_input(
                    IO.STRING, KlingText2VideoRequest, "prompt", multiline=True
                ),
                "negative_prompt": model_field_to_node_input(
                    IO.STRING,
                    KlingText2VideoRequest,
                    "negative_prompt",
                    multiline=True,
                ),
                "cfg_scale": model_field_to_node_input(
                    IO.FLOAT, KlingText2VideoRequest, "cfg_scale"
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.COMBO,
                    KlingText2VideoRequest,
                    "aspect_ratio",
                    enum_type=AspectRatio,
                ),
                "camera_control": (
                    "CAMERA_CONTROL",
                    {
                        "tooltip": "Can be created using the Kling Camera Controls node. Controls the camera movement and motion during the video generation.",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    DESCRIPTION = "Transform text into cinematic videos with professional camera movements that simulate real-world cinematography. Control virtual camera actions including zoom, rotation, pan, tilt, and first-person view, while maintaining focus on your original text."

    def api_call(
        self,
        prompt: str,
        negative_prompt: str,
        cfg_scale: float,
        aspect_ratio: str,
        camera_control: Optional[CameraControl] = None,
        auth_token: Optional[str] = None,
    ):
        return super().api_call(
            model_name="kling-v1-5",
            cfg_scale=cfg_scale,
            mode="pro",
            aspect_ratio=aspect_ratio,
            duration="5",
            prompt=prompt,
            negative_prompt=negative_prompt,
            camera_control=camera_control,
            auth_token=auth_token,
        )



```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)

[Luma Text to VideoA node that converts text descriptions to videos using Luma AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Fixed Parameters](http://docs.comfy.org#fixed-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
    
    - [Kling Camera Controls](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)
    - [Kling Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)
    - [Kling Image to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)
    - [Kling Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)
    - [Kling Start-End Frame to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)
    - [Kling Text to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Kling Camera Controls - ComfyUI Built-in Node Documentation

# Kling Camera Controls - ComfyUI Built-in Node Documentation

A node that provides camera control parameters for Kling video generation

The Kling Camera Controls node defines virtual camera behavior parameters to control camera movement and view changes during Kling video generation.

## [​](http://docs.comfy.org#parameters) Parameters

ParameterTypeDefaultDescriptioncamera\_control\_typeSelect”simple”Preset camera motion types. simple: Custom camera movement; down\_back: Camera moves down and back; forward\_up: Camera moves forward and up; right\_turn\_forward: Rotate right and move forward; left\_turn\_forward: Rotate left and move forwardhorizontal\_movementFloat0Controls camera movement on horizontal axis (x-axis). Negative values move left, positive values move rightvertical\_movementFloat0Controls camera movement on vertical axis (y-axis). Negative values move down, positive values move uppanFloat0.5Controls camera rotation in vertical plane (x-axis). Negative values rotate down, positive values rotate uptiltFloat0Controls camera rotation in horizontal plane (y-axis). Negative values rotate left, positive values rotate rightrollFloat0Controls camera roll amount (z-axis). Negative values rotate counterclockwise, positive values rotate clockwisezoomFloat0Controls camera focal length. Negative values narrow field of view, positive values widen it

**Note**: At least one non-zero camera control parameter is required for the effect to work.

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptioncamera\_controlCAMERA\_CONTROLConfiguration object with camera settings

**Note**: Not all model and mode combinations support camera control. Please check the Kling API documentation for details.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class KlingCameraControls(KlingNodeBase):
    """Kling Camera Controls Node"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "camera_control_type": (
                    IO.COMBO,
                    {
                        "options": [
                            camera_control_type.value
                            for camera_control_type in CameraType
                        ],
                        "default": "simple",
                        "tooltip": "Predefined camera movements type. simple: Customizable camera movement. down_back: Camera descends and moves backward. forward_up: Camera moves forward and tilts up. right_turn_forward: Rotate right and move forward. left_turn_forward: Rotate left and move forward.",
                    },
                ),
                "horizontal_movement": get_camera_control_input_config(
                    "Controls camera's movement along horizontal axis (x-axis). Negative indicates left, positive indicates right"
                ),
                "vertical_movement": get_camera_control_input_config(
                    "Controls camera's movement along vertical axis (y-axis). Negative indicates downward, positive indicates upward."
                ),
                "pan": get_camera_control_input_config(
                    "Controls camera's rotation in vertical plane (x-axis). Negative indicates downward rotation, positive indicates upward rotation.",
                    default=0.5,
                ),
                "tilt": get_camera_control_input_config(
                    "Controls camera's rotation in horizontal plane (y-axis). Negative indicates left rotation, positive indicates right rotation.",
                ),
                "roll": get_camera_control_input_config(
                    "Controls camera's rolling amount (z-axis). Negative indicates counterclockwise, positive indicates clockwise.",
                ),
                "zoom": get_camera_control_input_config(
                    "Controls change in camera's focal length. Negative indicates narrower field of view, positive indicates wider field of view.",
                ),
            }
        }

    DESCRIPTION = "Kling Camera Controls Node. Not all model and mode combinations support camera control. Please refer to the Kling API documentation for more information."
    RETURN_TYPES = ("CAMERA_CONTROL",)
    RETURN_NAMES = ("camera_control",)
    FUNCTION = "main"

    @classmethod
    def VALIDATE_INPUTS(
        cls,
        horizontal_movement: float,
        vertical_movement: float,
        pan: float,
        tilt: float,
        roll: float,
        zoom: float,
    ) -> bool | str:
        if not is_valid_camera_control_configs(
            [
                horizontal_movement,
                vertical_movement,
                pan,
                tilt,
                roll,
                zoom,
            ]
        ):
            return "Invalid camera control configs: at least one of the values must be non-zero"
        return True

    def main(
        self,
        camera_control_type: str,
        horizontal_movement: float,
        vertical_movement: float,
        pan: float,
        tilt: float,
        roll: float,
        zoom: float,
    ) -> tuple[CameraControl]:
        return (
            CameraControl(
                type=CameraType(camera_control_type),
                config=CameraConfig(
                    horizontal=horizontal_movement,
                    vertical=vertical_movement,
                    pan=pan,
                    roll=roll,
                    tilt=tilt,
                    zoom=zoom,
                ),
            ),
        )
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/google/google-veo2-video)

[Kling Text to VideoA node that converts text descriptions into videos using Kling's AI technology  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
    
    - [Kling Camera Controls](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)
    - [Kling Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)
    - [Kling Image to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)
    - [Kling Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)
    - [Kling Start-End Frame to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)
    - [Kling Text to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Kling Image to Video - ComfyUI Built-in Node

# Kling Image to Video - ComfyUI Built-in Node

A node that converts static images to dynamic videos using Kling’s AI technology

The Kling Image to Video node converts static images into dynamic video content using Kling’s image-to-video API.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

All parameters below are required:

ParameterTypeDefaultDescriptionstart\_frameImage-Input source imagepromptString""Text prompt describing video action and contentnegative\_promptString""Elements to avoid in the videocfg\_scaleFloat7.0Controls how closely to follow the promptmodel\_nameSelect”kling-v1-5”Model type to useaspect\_ratioSelect”16:9”Output video aspect ratiodurationSelect”5s”Generated video durationmodeSelect”pro”Video generation mode

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated videovideo\_idStringUnique video identifierdurationStringActual video duration

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class KlingImage2VideoNode(KlingNodeBase):
    """Kling Image to Video Node"""

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "start_frame": model_field_to_node_input(
                    IO.IMAGE, KlingImage2VideoRequest, "image"
                ),
                "prompt": model_field_to_node_input(
                    IO.STRING, KlingImage2VideoRequest, "prompt", multiline=True
                ),
                "negative_prompt": model_field_to_node_input(
                    IO.STRING,
                    KlingImage2VideoRequest,
                    "negative_prompt",
                    multiline=True,
                ),
                "model_name": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "model_name",
                    enum_type=KlingVideoGenModelName,
                ),
                "cfg_scale": model_field_to_node_input(
                    IO.FLOAT, KlingImage2VideoRequest, "cfg_scale"
                ),
                "mode": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "mode",
                    enum_type=KlingVideoGenMode,
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "aspect_ratio",
                    enum_type=KlingVideoGenAspectRatio,
                ),
                "duration": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "duration",
                    enum_type=KlingVideoGenDuration,
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = ("VIDEO", "STRING", "STRING")
    RETURN_NAMES = ("VIDEO", "video_id", "duration")
    DESCRIPTION = "Kling Image to Video Node"

    def get_response(self, task_id: str, auth_token: str) -> KlingImage2VideoResponse:
        return poll_until_finished(
            auth_token,
            ApiEndpoint(
                path=f"{PATH_IMAGE_TO_VIDEO}/{task_id}",
                method=HttpMethod.GET,
                request_model=KlingImage2VideoRequest,
                response_model=KlingImage2VideoResponse,
            ),
        )

    def api_call(
        self,
        start_frame: torch.Tensor,
        prompt: str,
        negative_prompt: str,
        model_name: str,
        cfg_scale: float,
        mode: str,
        aspect_ratio: str,
        duration: str,
        camera_control: Optional[KlingCameraControl] = None,
        end_frame: Optional[torch.Tensor] = None,
        auth_token: Optional[str] = None,
    ) -> tuple[VideoFromFile]:
        validate_prompts(prompt, negative_prompt, MAX_PROMPT_LENGTH_I2V)
        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=PATH_IMAGE_TO_VIDEO,
                method=HttpMethod.POST,
                request_model=KlingImage2VideoRequest,
                response_model=KlingImage2VideoResponse,
            ),
            request=KlingImage2VideoRequest(
                model_name=KlingVideoGenModelName(model_name),
                image=tensor_to_base64_string(start_frame),
                image_tail=(
                    tensor_to_base64_string(end_frame)
                    if end_frame is not None
                    else None
                ),
                prompt=prompt,
                negative_prompt=negative_prompt if negative_prompt else None,
                cfg_scale=cfg_scale,
                mode=KlingVideoGenMode(mode),
                aspect_ratio=KlingVideoGenAspectRatio(aspect_ratio),
                duration=KlingVideoGenDuration(duration),
                camera_control=camera_control,
            ),
            auth_token=auth_token,
        )

        task_creation_response = initial_operation.execute()
        validate_task_creation_response(task_creation_response)
        task_id = task_creation_response.data.task_id

        final_response = self.get_response(task_id, auth_token)
        validate_video_result_response(final_response)

        video = get_video_from_response(final_response)
        return video_result_to_node_output(video)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)

[Kling Start-End Frame to VideoA node that creates smooth video transitions between start and end frames using Kling's AI technology  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
    
    - [Kling Camera Controls](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)
    - [Kling Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)
    - [Kling Image to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)
    - [Kling Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)
    - [Kling Start-End Frame to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)
    - [Kling Text to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Kling Start-End Frame to Video - ComfyUI Built-in Node

# Kling Start-End Frame to Video - ComfyUI Built-in Node

A node that creates smooth video transitions between start and end frames using Kling’s AI technology

The Kling Start-End Frame to Video node lets you create smooth video transitions between two images. It automatically generates all the intermediate frames to produce a fluid transformation.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDescriptionstart\_frameImageStarting image for the videoend\_frameImageEnding image for the videopromptStringText describing video content and transitionnegative\_promptStringElements to avoid in the videocfg\_scaleFloatControls how closely to follow the promptaspect\_ratioSelectOutput video aspect ratiomodeSelectVideo generation settings (mode/duration/model)

### [​](http://docs.comfy.org#mode-options) Mode Options

Available mode combinations:

- standard mode / 5s duration / kling-v1
- standard mode / 5s duration / kling-v1-5
- pro mode / 5s duration / kling-v1
- pro mode / 5s duration / kling-v1-5
- pro mode / 5s duration / kling-v1-6
- pro mode / 10s duration / kling-v1-5
- pro mode / 10s duration / kling-v1-6

Default: “pro mode / 5s duration / kling-v1”

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#how-it-works) How It Works

The node analyzes the start and end frames to create a smooth transition sequence between them. It sends the images and parameters to Kling’s API server, which generates all necessary intermediate frames for a fluid transformation.

The transition style and content can be guided using prompts, while negative prompts help avoid unwanted elements.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python


class KlingStartEndFrameNode(KlingImage2VideoNode):
    """
    Kling First Last Frame Node. This node allows creation of a video from a first and last frame. It calls the normal image to video endpoint, but only allows the subset of input options that support the `image_tail` request field.
    """

    @staticmethod
    def get_mode_string_mapping() -> dict[str, tuple[str, str, str]]:
        """
        Returns a mapping of mode strings to their corresponding (mode, duration, model_name) tuples.
        Only includes config combos that support the `image_tail` request field.
        """
        return {
            "standard mode / 5s duration / kling-v1": ("std", "5", "kling-v1"),
            "standard mode / 5s duration / kling-v1-5": ("std", "5", "kling-v1-5"),
            "pro mode / 5s duration / kling-v1": ("pro", "5", "kling-v1"),
            "pro mode / 5s duration / kling-v1-5": ("pro", "5", "kling-v1-5"),
            "pro mode / 5s duration / kling-v1-6": ("pro", "5", "kling-v1-6"),
            "pro mode / 10s duration / kling-v1-5": ("pro", "10", "kling-v1-5"),
            "pro mode / 10s duration / kling-v1-6": ("pro", "10", "kling-v1-6"),
        }

    @classmethod
    def INPUT_TYPES(s):
        modes = list(KlingStartEndFrameNode.get_mode_string_mapping().keys())
        return {
            "required": {
                "start_frame": model_field_to_node_input(
                    IO.IMAGE, KlingImage2VideoRequest, "image"
                ),
                "end_frame": model_field_to_node_input(
                    IO.IMAGE, KlingImage2VideoRequest, "image_tail"
                ),
                "prompt": model_field_to_node_input(
                    IO.STRING, KlingImage2VideoRequest, "prompt", multiline=True
                ),
                "negative_prompt": model_field_to_node_input(
                    IO.STRING,
                    KlingImage2VideoRequest,
                    "negative_prompt",
                    multiline=True,
                ),
                "cfg_scale": model_field_to_node_input(
                    IO.FLOAT, KlingImage2VideoRequest, "cfg_scale"
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "aspect_ratio",
                    enum_type=AspectRatio,
                ),
                "mode": (
                    modes,
                    {
                        "default": modes[2],
                        "tooltip": "The configuration to use for the video generation following the format: mode / duration / model_name.",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    DESCRIPTION = "Generate a video sequence that transitions between your provided start and end images. The node creates all frames in between, producing a smooth transformation from the first frame to the last."

    def parse_inputs_from_mode(self, mode: str) -> tuple[str, str, str]:
        """Parses the mode input into a tuple of (model_name, duration, mode)."""
        return KlingStartEndFrameNode.get_mode_string_mapping()[mode]

    def api_call(
        self,
        start_frame: torch.Tensor,
        end_frame: torch.Tensor,
        prompt: str,
        negative_prompt: str,
        cfg_scale: float,
        aspect_ratio: str,
        mode: str,
        auth_token: Optional[str] = None,
    ):
        mode, duration, model_name = self.parse_inputs_from_mode(mode)
        return super().api_call(
            prompt=prompt,
            negative_prompt=negative_prompt,
            model_name=model_name,
            start_frame=start_frame,
            cfg_scale=cfg_scale,
            mode=mode,
            aspect_ratio=aspect_ratio,
            duration=duration,
            end_frame=end_frame,
            auth_token=auth_token,
        )


```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)

[Kling Text to Video (Camera Control)A text to video generation node with camera control features  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Mode Options](http://docs.comfy.org#mode-options)
- [Output](http://docs.comfy.org#output)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
    
    - [Kling Camera Controls](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)
    - [Kling Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)
    - [Kling Image to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)
    - [Kling Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)
    - [Kling Start-End Frame to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)
    - [Kling Text to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Kling Text to Video - ComfyUI Built-in Node

# Kling Text to Video - ComfyUI Built-in Node

A node that converts text descriptions into videos using Kling’s AI technology

The Kling Text to Video node connects to Kling’s API service to generate videos from text descriptions. Users simply provide descriptive text to create corresponding video content.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionpromptString""Text prompt describing desired video contentnegative\_promptString""Elements to avoid in the videocfg\_scaleFloat7.0Controls how closely to follow the promptmodel\_nameSelect”kling-v2-master”Video generation model to useaspect\_ratioSelectAspectRatio enumOutput video aspect ratiodurationSelectDuration enumLength of generated videomodeSelectMode enumVideo generation mode

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated videoKling IDStringTask identifierDuration (sec)StringVideo length in seconds

## [​](http://docs.comfy.org#how-it-works) How It Works

The node sends text prompts to Kling’s API server, which processes and returns the generated video. The process includes initial request and status polling. When complete, the node downloads and outputs the video.

Users can control the generation by adjusting parameters like negative prompts, configuration scale, and video properties. The system validates prompt length to ensure API compliance.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class KlingTextToVideoNode(KlingNodeBase):
    """Kling Text to Video Node"""

    @staticmethod
    def poll_for_task_status(task_id: str, auth_token: str) -> KlingText2VideoResponse:
        """Polls the Kling API endpoint until the task reaches a terminal state."""
        polling_operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"{PATH_TEXT_TO_VIDEO}/{task_id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=KlingText2VideoResponse,
            ),
            completed_statuses=[
                TaskStatus.succeed.value,
            ],
            failed_statuses=[TaskStatus.failed.value],
            status_extractor=lambda response: (
                response.data.task_status.value
                if response.data and response.data.task_status
                else None
            ),
            auth_token=auth_token,
        )
        return polling_operation.execute()

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": model_field_to_node_input(
                    IO.STRING, KlingText2VideoRequest, "prompt", multiline=True
                ),
                "negative_prompt": model_field_to_node_input(
                    IO.STRING, KlingText2VideoRequest, "negative_prompt", multiline=True
                ),
                "model_name": model_field_to_node_input(
                    IO.COMBO,
                    KlingText2VideoRequest,
                    "model_name",
                    enum_type=ModelName,
                    default="kling-v2-master",
                ),
                "cfg_scale": model_field_to_node_input(
                    IO.FLOAT, KlingText2VideoRequest, "cfg_scale"
                ),
                "mode": model_field_to_node_input(
                    IO.COMBO, KlingText2VideoRequest, "mode", enum_type=Mode
                ),
                "duration": model_field_to_node_input(
                    IO.COMBO, KlingText2VideoRequest, "duration", enum_type=Duration
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.COMBO,
                    KlingText2VideoRequest,
                    "aspect_ratio",
                    enum_type=AspectRatio,
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = ("VIDEO", "STRING", "STRING")
    RETURN_NAMES = ("VIDEO", "Kling ID", "Duration (sec)")
    DESCRIPTION = "Kling Text to Video Node"

    def api_call(
        self,
        prompt: str,
        negative_prompt: str,
        model_name: str,
        cfg_scale: float,
        mode: str,
        duration: int,
        aspect_ratio: str,
        camera_control: Optional[CameraControl] = None,
        auth_token: Optional[str] = None,
    ) -> tuple[VideoFromFile, str, str]:
        validate_prompts(prompt, negative_prompt, MAX_PROMPT_LENGTH_T2V)
        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=PATH_TEXT_TO_VIDEO,
                method=HttpMethod.POST,
                request_model=KlingText2VideoRequest,
                response_model=KlingText2VideoResponse,
            ),
            request=KlingText2VideoRequest(
                prompt=prompt if prompt else None,
                negative_prompt=negative_prompt if negative_prompt else None,
                duration=Duration(duration),
                mode=Mode(mode),
                model_name=ModelName(model_name),
                cfg_scale=cfg_scale,
                aspect_ratio=AspectRatio(aspect_ratio),
                camera_control=camera_control,
            ),
            auth_token=auth_token,
        )

        initial_response = initial_operation.execute()
        if not is_valid_initial_response(initial_response):
            error_msg = f"Kling initial request failed. Code: {initial_response.code}, Message: {initial_response.message}, Data: {initial_response.data}"
            logging.error(error_msg)
            raise KlingApiError(error_msg)

        task_id = initial_response.data.task_id
        final_response = self.poll_for_task_status(task_id, auth_token)
        if not is_valid_video_response(final_response):
            error_msg = (
                f"Kling task {task_id} succeeded but no video data found in response."
            )
            logging.error(error_msg)
            raise KlingApiError(error_msg)

        video = final_response.data.task_result.videos[0]
        logging.debug("Kling task %s succeeded. Video URL: %s", task_id, video.url)
        return (
            download_url_to_video_output(video.url),
            str(video.id),
            str(video.duration),
        )

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)

[Kling Image to Video (Camera Control)Image to video conversion node with camera control features  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Output](http://docs.comfy.org#output)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/video/luma/luma-concepts.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
    
    - [Luma Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video)
    - [Luma Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video)
    - [Luma Concepts](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Concepts - ComfyUI Native Node Documentation

# Luma Concepts - ComfyUI Native Node Documentation

A helper node that provides concept guidance for Luma image generation

The Luma Concepts node allows you to apply predefined camera concepts to the Luma generation process, providing precise control over camera angles and perspectives without complex prompt descriptions.

## [​](http://docs.comfy.org#node-function) Node Function

This node serves as a helper tool for Luma generation nodes, enabling users to select and apply predefined camera concepts. These concepts include different shooting angles (like overhead or low angle), camera distances (like close-up or long shot), and movement styles (like push-in or follow). It simplifies the creative workflow by providing an intuitive way to control camera effects in the generated output.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDescriptionconcept1selectFirst camera concept choice, includes various presets and “none”concept2selectSecond camera concept choice, includes various presets and “none”concept3selectThird camera concept choice, includes various presets and “none”concept4selectFourth camera concept choice, includes various presets and “none”

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionluma\_conceptsLUMA\_CONCEPTSOptional Camera Concepts to merge with selected concepts

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionluma\_conceptsLUMA\_CONCEPTCombined object containing all selected concepts

## [​](http://docs.comfy.org#usage-examples) Usage Examples

[**Luma Text to Video Workflow Example**  
\
Luma Text to Video Workflow Example](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video)

[**Luma Image to Video Workflow Example**  
\
Luma Image to Video Workflow Example](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video)

## [​](http://docs.comfy.org#how-it-works) How It Works

The Luma Concepts node offers a variety of predefined camera concepts including:

- Camera distances (close-up, medium shot, long shot)
- View angles (eye level, overhead, low angle)
- Movement types (push-in, follow, orbit)
- Special effects (handheld, stabilized, floating)

Users can select up to 4 concepts to use together. The node creates an object containing the selected camera concepts, which is then passed to Luma generation nodes. During generation, Luma AI uses these camera concepts to influence the viewpoint and composition of the output, ensuring the results reflect the chosen photographic effects.

By combining multiple camera concepts, users can create complex camera guidance without writing detailed prompt descriptions. This is particularly useful when specific camera angles or compositions are needed.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class LumaConceptsNode(ComfyNodeABC):
    """
    Holds one or more Camera Concepts for use with Luma Text to Video and Luma Image to Video nodes.
    """

    RETURN_TYPES = (LumaIO.LUMA_CONCEPTS,)
    RETURN_NAMES = ("luma_concepts",)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "create_concepts"
    CATEGORY = "api node/image/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "concept1": (get_luma_concepts(include_none=True),),
                "concept2": (get_luma_concepts(include_none=True),),
                "concept3": (get_luma_concepts(include_none=True),),
                "concept4": (get_luma_concepts(include_none=True),),
            },
            "optional": {
                "luma_concepts": (
                    LumaIO.LUMA_CONCEPTS,
                    {
                        "tooltip": "Optional Camera Concepts to add to the ones chosen here."
                    },
                ),
            },
        }

    def create_concepts(
        self,
        concept1: str,
        concept2: str,
        concept3: str,
        concept4: str,
        luma_concepts: LumaConceptChain = None,
    ):
        chain = LumaConceptChain(str_list=[concept1, concept2, concept3, concept4])
        if luma_concepts is not None:
            chain = luma_concepts.clone_and_merge(chain)
        return (chain,)


```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/luma/luma-concepts.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video)

[Pika 2.2 Text to VideoA node that converts text descriptions into videos using Pika AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-text-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Examples](http://docs.comfy.org#usage-examples)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/video/luma/luma-concepts.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/video/luma/luma-image-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
    
    - [Luma Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video)
    - [Luma Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video)
    - [Luma Concepts](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Image to Video - ComfyUI Native API Node Documentation

# Luma Image to Video - ComfyUI Native API Node Documentation

A node that converts static images to dynamic videos using Luma AI

The Luma Image to Video node uses Luma AI’s technology to transform static images into smooth, dynamic videos, bringing your images to life.

## [​](http://docs.comfy.org#node-function) Node Function

This node connects to Luma AI’s image-to-video API, allowing users to create dynamic videos from input images. It understands the image content and generates natural, coherent motion while maintaining the original visual style. Combined with text prompts, users can precisely control the video’s dynamic effects.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing video motion and contentmodelselect-Video generation model to useresolutionselect”540p”Output video resolutiondurationselect-Video length optionsloopbooleanFalseWhether to loop the videoseedinteger0Seed value for node rerun, results are nondeterministic

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionfirst\_imageimageFirst frame of video (required if no last\_image)last\_imageimageLast frame of video (required if no first\_image)luma\_conceptsLUMA\_CONCEPTSConcepts for controlling camera motion and shot style

### [​](http://docs.comfy.org#requirements) Requirements

- Either **first\_image** or **last\_image** must be provided
- Each image input (first\_image and last\_image) accepts only 1 image

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOvideoGenerated video

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Luma Image to Video Workflow Example**  
\
Luma Image to Video Workflow Tutorial](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class LumaImageToVideoGenerationNode(ComfyNodeABC):
    """
    Generates videos synchronously based on prompt, input images, and output_size.
    """

    RETURN_TYPES = (IO.VIDEO,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/video/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the video generation",
                    },
                ),
                "model": ([model.value for model in LumaVideoModel],),
                # "aspect_ratio": ([ratio.value for ratio in LumaAspectRatio], {
                #     "default": LumaAspectRatio.ratio_16_9,
                # }),
                "resolution": (
                    [resolution.value for resolution in LumaVideoOutputResolution],
                    {
                        "default": LumaVideoOutputResolution.res_540p,
                    },
                ),
                "duration": ([dur.value for dur in LumaVideoModelOutputDuration],),
                "loop": (
                    IO.BOOLEAN,
                    {
                        "default": False,
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "first_image": (
                    IO.IMAGE,
                    {"tooltip": "First frame of generated video."},
                ),
                "last_image": (IO.IMAGE, {"tooltip": "Last frame of generated video."}),
                "luma_concepts": (
                    LumaIO.LUMA_CONCEPTS,
                    {
                        "tooltip": "Optional Camera Concepts to dictate camera motion via the Luma Concepts node."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        model: str,
        resolution: str,
        duration: str,
        loop: bool,
        seed,
        first_image: torch.Tensor = None,
        last_image: torch.Tensor = None,
        luma_concepts: LumaConceptChain = None,
        auth_token=None,
        **kwargs,
    ):
        if first_image is None and last_image is None:
            raise Exception(
                "At least one of first_image and last_image requires an input."
            )
        keyframes = self._convert_to_keyframes(first_image, last_image, auth_token)
        duration = duration if model != LumaVideoModel.ray_1_6 else None
        resolution = resolution if model != LumaVideoModel.ray_1_6 else None

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/luma/generations",
                method=HttpMethod.POST,
                request_model=LumaGenerationRequest,
                response_model=LumaGeneration,
            ),
            request=LumaGenerationRequest(
                prompt=prompt,
                model=model,
                aspect_ratio=LumaAspectRatio.ratio_16_9,  # ignored, but still needed by the API for some reason
                resolution=resolution,
                duration=duration,
                loop=loop,
                keyframes=keyframes,
                concepts=luma_concepts.create_api_model() if luma_concepts else None,
            ),
            auth_token=auth_token,
        )
        response_api: LumaGeneration = operation.execute()

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/luma/generations/{response_api.id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=LumaGeneration,
            ),
            completed_statuses=[LumaState.completed],
            failed_statuses=[LumaState.failed],
            status_extractor=lambda x: x.state,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        vid_response = requests.get(response_poll.assets.video)
        return (VideoFromFile(BytesIO(vid_response.content)),)

    def _convert_to_keyframes(
        self,
        first_image: torch.Tensor = None,
        last_image: torch.Tensor = None,
        auth_token=None,
    ):
        if first_image is None and last_image is None:
            return None
        frame0 = None
        frame1 = None
        if first_image is not None:
            download_urls = upload_images_to_comfyapi(
                first_image, max_images=1, auth_token=auth_token
            )
            frame0 = LumaImageReference(type="image", url=download_urls[0])
        if last_image is not None:
            download_urls = upload_images_to_comfyapi(
                last_image, max_images=1, auth_token=auth_token
            )
            frame1 = LumaImageReference(type="image", url=download_urls[0])
        return LumaKeyframes(frame0=frame0, frame1=frame1)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/luma/luma-image-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video)

[Luma ConceptsA helper node that provides concept guidance for Luma image generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Requirements](http://docs.comfy.org#requirements)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/video/luma/luma-image-to-video.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/video/luma/luma-text-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
    
    - [Luma Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video)
    - [Luma Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video)
    - [Luma Concepts](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Text to Video - ComfyUI Native Node Documentation

# Luma Text to Video - ComfyUI Native Node Documentation

A node that converts text descriptions to videos using Luma AI

The Luma Text to Video node lets you create high-quality, smooth videos from text descriptions using Luma AI’s video generation technology.

## [​](http://docs.comfy.org#node-function) Node Function

This node connects to Luma AI’s text-to-video API, allowing users to generate dynamic video content from detailed text prompts.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing the video content to generatemodelselect-Video generation model to useaspect\_ratioselect”ratio\_16\_9”Video aspect ratioresolutionselect”res\_540p”Video resolutiondurationselect-Video length optionsloopbooleanFalseWhether to loop the videoseedinteger0Seed value for node rerun, results are nondeterministic

When using Ray 1.6 model, duration and resolution parameters will not take effect.

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionluma\_conceptsLUMA\_CONCEPTSCamera concepts to control motion via Luma Concepts node

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOvideoGenerated video

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Luma Text to Video Workflow Example**  
\
Luma Text to Video Workflow Example](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class LumaTextToVideoGenerationNode(ComfyNodeABC):
    """
    Generates videos synchronously based on prompt and output_size.
    """

    RETURN_TYPES = (IO.VIDEO,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/video/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the video generation",
                    },
                ),
                "model": ([model.value for model in LumaVideoModel],),
                "aspect_ratio": (
                    [ratio.value for ratio in LumaAspectRatio],
                    {
                        "default": LumaAspectRatio.ratio_16_9,
                    },
                ),
                "resolution": (
                    [resolution.value for resolution in LumaVideoOutputResolution],
                    {
                        "default": LumaVideoOutputResolution.res_540p,
                    },
                ),
                "duration": ([dur.value for dur in LumaVideoModelOutputDuration],),
                "loop": (
                    IO.BOOLEAN,
                    {
                        "default": False,
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "luma_concepts": (
                    LumaIO.LUMA_CONCEPTS,
                    {
                        "tooltip": "Optional Camera Concepts to dictate camera motion via the Luma Concepts node."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        model: str,
        aspect_ratio: str,
        resolution: str,
        duration: str,
        loop: bool,
        seed,
        luma_concepts: LumaConceptChain = None,
        auth_token=None,
        **kwargs,
    ):
        duration = duration if model != LumaVideoModel.ray_1_6 else None
        resolution = resolution if model != LumaVideoModel.ray_1_6 else None

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/luma/generations",
                method=HttpMethod.POST,
                request_model=LumaGenerationRequest,
                response_model=LumaGeneration,
            ),
            request=LumaGenerationRequest(
                prompt=prompt,
                model=model,
                resolution=resolution,
                aspect_ratio=aspect_ratio,
                duration=duration,
                loop=loop,
                concepts=luma_concepts.create_api_model() if luma_concepts else None,
            ),
            auth_token=auth_token,
        )
        response_api: LumaGeneration = operation.execute()

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/luma/generations/{response_api.id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=LumaGeneration,
            ),
            completed_statuses=[LumaState.completed],
            failed_statuses=[LumaState.failed],
            status_extractor=lambda x: x.state,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        vid_response = requests.get(response_poll.assets.video)
        return (VideoFromFile(BytesIO(vid_response.content)),)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/luma/luma-text-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)

[Luma Image to VideoA node that converts static images to dynamic videos using Luma AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/video/luma/luma-text-to-video.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/video/minimax/minimax-image-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
    
    - [MiniMax Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-image-to-video)
    - [MiniMax Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-text-to-video)
  - Google
  - Kling
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

MiniMax Image to Video - ComfyUI Native Node Documentation

# MiniMax Image to Video - ComfyUI Native Node Documentation

A node that converts static images to dynamic videos using MiniMax AI

The MiniMax Image to Video node uses MiniMax’s API to generate videos from input images and text prompts.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionimageimage-Input image used as the first frame of videoprompt\_textstring""Text prompt to guide video generationmodelselect”I2V-01”Available models: “I2V-01-Director”, “I2V-01”, “I2V-01-live”

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionseedintegerRandom seed for noise generation

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOvideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class MinimaxImageToVideoNode(MinimaxTextToVideoNode):
    """
    Generates videos synchronously based on an image and prompt, and optional parameters using Minimax's API.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (
                    IO.IMAGE,
                    {
                        "tooltip": "Image to use as first frame of video generation"
                    },
                ),
                "prompt_text": (
                    "STRING",
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Text prompt to guide the video generation",
                    },
                ),
                "model": (
                    [
                        "I2V-01-Director",
                        "I2V-01",
                        "I2V-01-live",
                    ],
                    {
                        "default": "I2V-01",
                        "tooltip": "Model to use for video generation",
                    },
                ),
            },
            "optional": {
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "The random seed used for creating the noise.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    RETURN_TYPES = ("VIDEO",)
    DESCRIPTION = "Generates videos from an image and prompts using Minimax's API"
    FUNCTION = "generate_video"
    CATEGORY = "api node/video/Minimax"
    API_NODE = True
    OUTPUT_NODE = True
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/minimax/minimax-image-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle3)

[MiniMax Text to VideoA node that converts text descriptions into videos using MiniMax AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-text-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/video/minimax/minimax-image-to-video.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/video/minimax/minimax-text-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
    
    - [MiniMax Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-image-to-video)
    - [MiniMax Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-text-to-video)
  - Google
  - Kling
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

MiniMax Text to Video - ComfyUI Native Node Documentation

# MiniMax Text to Video - ComfyUI Native Node Documentation

A node that converts text descriptions into videos using MiniMax AI

The MiniMax Text to Video node connects to MiniMax’s API to generate high-quality, smooth videos from text prompts. It supports different video generation models to create short video clips in various styles.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionprompt\_textString""Text prompt that guides the video generationmodelSelect”T2V-01”Video model to use, options are “T2V-01” and “T2V-01-Director”seedInteger0Random seed for noise generation, defaults to 0

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class MinimaxTextToVideoNode:
    """
    Generates videos synchronously based on a prompt, and optional parameters using Minimax's API.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt_text": (
                    "STRING",
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Text prompt to guide the video generation",
                    },
                ),
                "model": (
                    [
                        "T2V-01",
                        "T2V-01-Director",
                    ],
                    {
                        "default": "T2V-01",
                        "tooltip": "Model to use for video generation",
                    },
                ),
            },
            "optional": {
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "The random seed used for creating the noise.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    RETURN_TYPES = ("VIDEO",)
    DESCRIPTION = "Generates videos from prompts using Minimax's API"
    FUNCTION = "generate_video"
    CATEGORY = "api node/video/Minimax"
    API_NODE = True
    OUTPUT_NODE = True

    def generate_video(
        self,
        prompt_text,
        seed=0,
        model="T2V-01",
        image: torch.Tensor=None, # used for ImageToVideo
        subject: torch.Tensor=None, # used for SubjectToVideo
        auth_token=None,
    ):
        '''
        Function used between Minimax nodes - supports T2V, I2V, and S2V, based on provided arguments.
        '''
        # upload image, if passed in
        image_url = None
        if image is not None:
            image_url = upload_images_to_comfyapi(image, max_images=1, auth_token=auth_token)[0]

        # TODO: figure out how to deal with subject properly, API returns invalid params when using S2V-01 model
        subject_reference = None
        if subject is not None:
            subject_url = upload_images_to_comfyapi(subject, max_images=1, auth_token=auth_token)[0]
            subject_reference = [SubjectReferenceItem(image=subject_url)]


        video_generate_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/minimax/video_generation",
                method=HttpMethod.POST,
                request_model=MinimaxVideoGenerationRequest,
                response_model=MinimaxVideoGenerationResponse,
            ),
            request=MinimaxVideoGenerationRequest(
                model=Model(model),
                prompt=prompt_text,
                callback_url=None,
                first_frame_image=image_url,
                subject_reference=subject_reference,
                prompt_optimizer=None,
            ),
            auth_token=auth_token,
        )
        response = video_generate_operation.execute()

        task_id = response.task_id
        if not task_id:
            raise Exception(f"Minimax generation failed: {response.base_resp}")

        video_generate_operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path="/proxy/minimax/query/video_generation",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=MinimaxTaskResultResponse,
                query_params={"task_id": task_id},
            ),
            completed_statuses=["Success"],
            failed_statuses=["Fail"],
            status_extractor=lambda x: x.status.value,
            auth_token=auth_token,
        )
        task_result = video_generate_operation.execute()

        file_id = task_result.file_id
        if file_id is None:
            raise Exception("Request was not successful. Missing file ID.")
        file_retrieve_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/minimax/files/retrieve",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=MinimaxFileRetrieveResponse,
                query_params={"file_id": int(file_id)},
            ),
            request=EmptyRequest(),
            auth_token=auth_token,
        )
        file_result = file_retrieve_operation.execute()

        file_url = file_result.file.download_url
        if file_url is None:
            raise Exception(
                f"No video was found in the response. Full response: {file_result.model_dump()}"
            )
        logging.info(f"Generated video URL: {file_url}")

        video_io = download_url_to_bytesio(file_url)
        if video_io is None:
            error_msg = f"Failed to download video from {file_url}"
            logging.error(error_msg)
            raise Exception(error_msg)
        return (VideoFromFile(video_io),)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/minimax/minimax-text-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-image-to-video)

[Google Veo2 VideoA node that generates videos from text descriptions using Google's Veo2 technology  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/google/google-veo2-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/video/minimax/minimax-text-to-video.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/video/pika/pika-image-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
  - Pika
    
    - [Pika 2.2 Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-text-to-video)
    - [Pika 2.2 Scenes](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-scenes)
    - [Pika 2.2 Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-image-to-video)
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Pika 2.2 Image to Video - ComfyUI Native Node Documentation

# Pika 2.2 Image to Video - ComfyUI Native Node Documentation

A node that converts static images to dynamic videos using Pika AI

The Pika 2.2 Image to Video node connects to Pika’s latest 2.2 API to transform static images into dynamic videos. It preserves the visual features of the original image while adding natural motion based on text prompts.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionimageImage-Input image to convert to videoprompt\_textString""Text prompt describing video motion and contentnegative\_promptString""Elements to avoid in the videoseedInteger0Random seed for generationresolutionSelect”1080p”Output video resolutiondurationSelect”5s”Length of generated video

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#how-it-works) How It Works

The node sends the input image and parameters (prompts, resolution, duration, etc.) to Pika’s API server as multipart form data. The API processes this and returns the generated video. Users can control the output by adjusting the prompts, negative prompts, random seed and other parameters.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-05)]

```python

class PikaImageToVideoV2_2(PikaNodeBase):
    """Pika 2.2 Image to Video Node."""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": (
                    IO.IMAGE,
                    {"tooltip": "The image to convert to video"},
                ),
                **cls.get_base_inputs_types(PikaBodyGenerate22I2vGenerate22I2vPost),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    DESCRIPTION = "Sends an image and prompt to the Pika API v2.2 to generate a video."
    RETURN_TYPES = ("VIDEO",)

    def api_call(
        self,
        image: torch.Tensor,
        prompt_text: str,
        negative_prompt: str,
        seed: int,
        resolution: str,
        duration: int,
        auth_token: Optional[str] = None,
    ) -> tuple[VideoFromFile]:
        """API call for Pika 2.2 Image to Video."""
        # Convert image to BytesIO
        image_bytes_io = tensor_to_bytesio(image)
        image_bytes_io.seek(0)  # Reset stream position

        # Prepare file data for multipart upload
        pika_files = {"image": ("image.png", image_bytes_io, "image/png")}

        # Prepare non-file data using the Pydantic model
        pika_request_data = PikaBodyGenerate22I2vGenerate22I2vPost(
            promptText=prompt_text,
            negativePrompt=negative_prompt,
            seed=seed,
            resolution=resolution,
            duration=duration,
        )

        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=PATH_IMAGE_TO_VIDEO,
                method=HttpMethod.POST,
                request_model=PikaBodyGenerate22I2vGenerate22I2vPost,
                response_model=PikaGenerateResponse,
            ),
            request=pika_request_data,
            files=pika_files,
            content_type="multipart/form-data",
            auth_token=auth_token,
        )

        return self.execute_task(initial_operation, auth_token)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/pika/pika-image-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-scenes)

[PixVerse TemplateA helper node that provides preset templates for PixVerse video generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-template)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Output](http://docs.comfy.org#output)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/video/pika/pika-image-to-video.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/video/pika/pika-scenes.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
  - Pika
    
    - [Pika 2.2 Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-text-to-video)
    - [Pika 2.2 Scenes](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-scenes)
    - [Pika 2.2 Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-image-to-video)
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Pika 2.2 Scenes - ComfyUI Built-in Node Documentation

# Pika 2.2 Scenes - ComfyUI Built-in Node Documentation

A node that creates coherent scene videos from multiple images using Pika AI

The Pika 2.2 Scenes node allows you to upload multiple images and generate a high-quality video incorporating these elements. It uses Pika’s 2.2 API to create smooth scene transitions between the images.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionprompt\_textstring""Text prompt describing video content and scenesnegative\_promptstring""Elements to exclude from the videoseedinteger0Random seed for generationingredients\_modeselect”creative”Image combination moderesolutionselectAPI defaultOutput video resolutiondurationselectAPI defaultOutput video lengthaspect\_ratiofloat1.7777777777777777 (16:9)Video aspect ratio, range 0.4-2.5

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionimage\_ingredient\_1imageFirst scene imageimage\_ingredient\_2imageSecond scene imageimage\_ingredient\_3imageThird scene imageimage\_ingredient\_4imageFourth scene imageimage\_ingredient\_5imageFifth scene image

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOvideoGenerated video

## [​](http://docs.comfy.org#how-it-works) How It Works

The Pika 2.2 Scenes node analyzes all input images and creates a video containing these image elements. The node sends the images and parameters to Pika’s API server, which processes them and returns the generated video.

Users can guide the video style and content through prompts, and exclude unwanted elements using negative prompts. The node supports up to 5 input images as ingredients and generates the final video based on the specified combination mode, resolution, duration, and aspect ratio.

## [​](http://docs.comfy.org#source-code) Source Code

```python

class PikaScenesV2_2(PikaNodeBase):
    """Pika 2.2 Scenes Node."""

    @classmethod
    def INPUT_TYPES(cls):
        image_ingredient_input = (
            IO.IMAGE,
            {"tooltip": "Image that will be used as ingredient to create a video."},
        )
        return {
            "required": {
                **cls.get_base_inputs_types(
                    PikaBodyGenerate22C2vGenerate22PikascenesPost,
                ),
                "ingredients_mode": model_field_to_node_input(
                    IO.COMBO,
                    PikaBodyGenerate22C2vGenerate22PikascenesPost,
                    "ingredientsMode",
                    enum_type=IngredientsMode,
                    default="creative",
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.FLOAT,
                    PikaBodyGenerate22C2vGenerate22PikascenesPost,
                    "aspectRatio",
                    step=0.001,
                    min=0.4,
                    max=2.5,
                    default=1.7777777777777777,
                ),
            },
            "optional": {
                "image_ingredient_1": image_ingredient_input,
                "image_ingredient_2": image_ingredient_input,
                "image_ingredient_3": image_ingredient_input,
                "image_ingredient_4": image_ingredient_input,
                "image_ingredient_5": image_ingredient_input,
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    DESCRIPTION = "Combine your images to create a video with the objects in them. Upload multiple images as ingredients and generate a high-quality video that incorporates all of them."
    RETURN_TYPES = ("VIDEO",)

    def api_call(
        self,
        prompt_text: str,
        negative_prompt: str,
        seed: int,
        resolution: str,
        duration: int,
        ingredients_mode: str,
        aspect_ratio: float,
        image_ingredient_1: Optional[torch.Tensor] = None,
        image_ingredient_2: Optional[torch.Tensor] = None,
        image_ingredient_3: Optional[torch.Tensor] = None,
        image_ingredient_4: Optional[torch.Tensor] = None,
        image_ingredient_5: Optional[torch.Tensor] = None,
        auth_token: Optional[str] = None,
    ) -> tuple[VideoFromFile]:
        """API call for Pika Scenes 2.2."""
        all_image_bytes_io = []
        for image in [
            image_ingredient_1,
            image_ingredient_2,
            image_ingredient_3,
            image_ingredient_4,
            image_ingredient_5,
        ]:
            if image is not None:
                image_bytes_io = tensor_to_bytesio(image)
                image_bytes_io.seek(0)
                all_image_bytes_io.append(image_bytes_io)

        # Prepare files data for multipart upload
        pika_files = [
            ("images", (f"image_{i}.png", image_bytes_io, "image/png"))
            for i, image_bytes_io in enumerate(all_image_bytes_io)
        ]

        # Prepare non-file data using the Pydantic model
        pika_request_data = PikaBodyGenerate22C2vGenerate22PikascenesPost(
            ingredientsMode=ingredients_mode,
            promptText=prompt_text,
            negativePrompt=negative_prompt,
            seed=seed,
            resolution=resolution,
            duration=duration,
            aspectRatio=aspect_ratio,
        )

        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=PATH_PIKASCENES,
                method=HttpMethod.POST,
                request_model=PikaBodyGenerate22C2vGenerate22PikascenesPost,
                response_model=PikaGenerateResponse,
            ),
            request=pika_request_data,
            files=pika_files,
            content_type="multipart/form-data",
            auth_token=auth_token,
        )

        return self.execute_task(initial_operation, auth_token)


```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/pika/pika-scenes.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-text-to-video)

[Pika 2.2 Image to VideoA node that converts static images to dynamic videos using Pika AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-image-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/video/pika/pika-scenes.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/video/pika/pika-text-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
  - Pika
    
    - [Pika 2.2 Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-text-to-video)
    - [Pika 2.2 Scenes](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-scenes)
    - [Pika 2.2 Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-image-to-video)
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Pika 2.2 Text to Video - ComfyUI Native Node Documentation

# Pika 2.2 Text to Video - ComfyUI Native Node Documentation

A node that converts text descriptions into videos using Pika AI

The Pika 2.2 Text to Video node uses Pika’s 2.2 API to create videos from text descriptions. It connects to Pika’s text-to-video API, allowing users to generate videos using text prompts with various control parameters.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionprompt\_textString""Text prompt describing the video contentnegative\_promptString""Elements to exclude from the videoseedInteger0Random seed for generationresolutionSelect”1080p”Output video resolutiondurationSelect”5s”Length of generated videoaspect\_ratioFloat1.7777777777777777Video aspect ratio, range 0.4-2.5, step 0.001

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-05)]

```python

class PikaTextToVideoNodeV2_2(PikaNodeBase):
    """Pika 2.2 Text to Video Node."""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                **cls.get_base_inputs_types(PikaBodyGenerate22T2vGenerate22T2vPost),
                "aspect_ratio": model_field_to_node_input(
                    IO.FLOAT,
                    PikaBodyGenerate22T2vGenerate22T2vPost,
                    "aspectRatio",
                    step=0.001,
                    min=0.4,
                    max=2.5,
                    default=1.7777777777777777,
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    RETURN_TYPES = ("VIDEO",)
    DESCRIPTION = "Sends a text prompt to the Pika API v2.2 to generate a video."

    def api_call(
        self,
        prompt_text: str,
        negative_prompt: str,
        seed: int,
        resolution: str,
        duration: int,
        aspect_ratio: float,
        auth_token: Optional[str] = None,
    ) -> tuple[VideoFromFile]:
        """API call for Pika 2.2 Text to Video."""
        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=PATH_TEXT_TO_VIDEO,
                method=HttpMethod.POST,
                request_model=PikaBodyGenerate22T2vGenerate22T2vPost,
                response_model=PikaGenerateResponse,
            ),
            request=PikaBodyGenerate22T2vGenerate22T2vPost(
                promptText=prompt_text,
                negativePrompt=negative_prompt,
                seed=seed,
                resolution=resolution,
                duration=duration,
                aspectRatio=aspect_ratio,
            ),
            auth_token=auth_token,
            content_type="application/x-www-form-urlencoded",
        )

        return self.execute_task(initial_operation, auth_token)


```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/pika/pika-text-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)

[Pika 2.2 ScenesA node that creates coherent scene videos from multiple images using Pika AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-scenes)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/video/pika/pika-text-to-video.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
  - Pika
  - PixVerse
    
    - [PixVerse Template](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-template)
    - [PixVerse Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video)
    - [PixVerse Transition Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-transition-video)
    - [PixVerse Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

PixVerse Image to Video - ComfyUI Native Node Documentation

# PixVerse Image to Video - ComfyUI Native Node Documentation

A node that converts static images to dynamic videos using PixVerse AI

The PixVerse Image to Video node uses PixVerse’s API to transform static images into dynamic videos. It preserves the visual features of the original image while adding natural motion based on text prompts.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionimageImage-Input image to convert to videopromptString""Text prompt describing video motion/contentnegative\_promptString""Elements to avoid in the videoseedInteger-1Random seed (-1 for random)qualitySelect”high”Output video quality levelaspect\_ratioSelect”r16\_9”Output video aspect ratiodurationSelect”seconds\_4”Length of generated videomotion\_modeSelect”standard”Video motion style

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDefaultDescriptionpixverse\_templatePIXVERSE\_TEMPLATENoneOptional PixVerse template

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-05)]

```python
class PixverseImageToVideoNode(ComfyNodeABC):
    """
    Pixverse Image to Video

    Generates videos from an image and prompts.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": ("IMAGE",),
                "prompt": ("STRING", {"multiline": True, "default": ""}),
                "negative_prompt": ("STRING", {"multiline": True, "default": ""}),
                "seed": ("INT", {"default": -1, "min": -1, "max": 0xffffffffffffffff}),
                "quality": (list(PixverseQuality.__members__.keys()), {"default": "high"}),
                "aspect_ratio": (list(PixverseAspectRatio.__members__.keys()), {"default": "r16_9"}),
                "duration": (list(PixverseDuration.__members__.keys()), {"default": "seconds_4"}),
                "motion_mode": (list(PixverseMotionMode.__members__.keys()), {"default": "standard"}),
            },
            "optional": {
                "pixverse_template": ("PIXVERSE_TEMPLATE",),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    RETURN_TYPES = ("VIDEO",)
    DESCRIPTION = "Generates videos from an image and prompts using Pixverse's API"
    FUNCTION = "generate_video"
    CATEGORY = "api node/video/Pixverse"
    API_NODE = True
    OUTPUT_NODE = True
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video.mdx)

[Previous  
\
PixVerse Transition VideoCreate smooth transition videos between start and end frames using PixVerse AI](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-transition-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/video/pixverse/pixverse-template.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
  - Pika
  - PixVerse
    
    - [PixVerse Template](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-template)
    - [PixVerse Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video)
    - [PixVerse Transition Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-transition-video)
    - [PixVerse Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

PixVerse Template - ComfyUI Native Node Documentation

# PixVerse Template - ComfyUI Native Node Documentation

A helper node that provides preset templates for PixVerse video generation

The PixVerse Template node lets you choose from predefined video generation templates to control the style and effects of PixVerse video generation nodes. This helper node connects to PixVerse video generation nodes, allowing users to quickly apply preset video styles without manually adjusting complex parameter combinations.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDescriptiontemplateSelectChoose a template from available video presets

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionpixverse\_templatePixverseIO.TEMPLATEConfiguration object containing the selected template ID

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-05)]

```python

class PixverseTemplateNode:
    """
    Select template for Pixverse Video generation.
    """

    RETURN_TYPES = (PixverseIO.TEMPLATE,)
    RETURN_NAMES = ("pixverse_template",)
    FUNCTION = "create_template"
    CATEGORY = "api node/video/Pixverse"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "template": (list(pixverse_templates.keys()), ),
            }
        }

    def create_template(self, template: str):
        template_id = pixverse_templates.get(template, None)
        if template_id is None:
            raise Exception(f"Template '{template}' is not recognized.")
        # just return the integer
        return (template_id,)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/pixverse/pixverse-template.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-image-to-video)

[PixVerse Text to VideoA node that converts text descriptions into videos using PixVerse AI technology  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/video/pixverse/pixverse-template.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
  - Pika
  - PixVerse
    
    - [PixVerse Template](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-template)
    - [PixVerse Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video)
    - [PixVerse Transition Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-transition-video)
    - [PixVerse Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

PixVerse Text to Video - ComfyUI Built-in Node Documentation

# PixVerse Text to Video - ComfyUI Built-in Node Documentation

A node that converts text descriptions into videos using PixVerse AI technology

The PixVerse Text to Video node connects to PixVerse’s text-to-video API, allowing users to generate high-quality videos from text descriptions. Users can customize their creations by adjusting various parameters like video quality, duration, and motion mode.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing the video contentaspect\_ratioselect-Output video aspect ratioqualityselectPixverseQuality.res\_540pVideo quality levelduration\_secondsselect-Video durationmotion\_modeselect-Video motion modeseedinteger0Random seed for consistent generation results

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDefaultDescriptionnegative\_promptstring""Elements to exclude from the videopixverse\_templatePIXVERSE\_TEMPLATENoneOptional template for style settings

### [​](http://docs.comfy.org#limitations) Limitations

- 1080p quality only supports normal motion mode with 5-second duration
- Non 5-second durations only support normal motion mode

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOvideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-05)]

```python

class PixverseTextToVideoNode(ComfyNodeABC):
    """
    Generates videos synchronously based on prompt and output_size.
    """

    RETURN_TYPES = (IO.VIDEO,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/video/Pixverse"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the video generation",
                    },
                ),
                "aspect_ratio": (
                    [ratio.value for ratio in PixverseAspectRatio],
                ),
                "quality": (
                    [resolution.value for resolution in PixverseQuality],
                    {
                        "default": PixverseQuality.res_540p,
                    },
                ),
                "duration_seconds": ([dur.value for dur in PixverseDuration],),
                "motion_mode": ([mode.value for mode in PixverseMotionMode],),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2147483647,
                        "control_after_generate": True,
                        "tooltip": "Seed for video generation.",
                    },
                ),
            },
            "optional": {
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
                "pixverse_template": (
                    PixverseIO.TEMPLATE,
                    {
                        "tooltip": "An optional template to influence style of generation, created by the Pixverse Template node."
                    }
                )
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        aspect_ratio: str,
        quality: str,
        duration_seconds: int,
        motion_mode: str,
        seed,
        negative_prompt: str=None,
        pixverse_template: int=None,
        auth_token=None,
        **kwargs,
    ):
        # 1080p is limited to 5 seconds duration
        # only normal motion_mode supported for 1080p or for non-5 second duration
        if quality == PixverseQuality.res_1080p:
            motion_mode = PixverseMotionMode.normal
            duration_seconds = PixverseDuration.dur_5
        elif duration_seconds != PixverseDuration.dur_5:
            motion_mode = PixverseMotionMode.normal

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/pixverse/video/text/generate",
                method=HttpMethod.POST,
                request_model=PixverseTextVideoRequest,
                response_model=PixverseVideoResponse,
            ),
            request=PixverseTextVideoRequest(
                prompt=prompt,
                aspect_ratio=aspect_ratio,
                quality=quality,
                duration=duration_seconds,
                motion_mode=motion_mode,
                negative_prompt=negative_prompt if negative_prompt else None,
                template_id=pixverse_template,
                seed=seed,
            ),
            auth_token=auth_token,
        )
        response_api = operation.execute()

        if response_api.Resp is None:
            raise Exception(f"Pixverse request failed: '{response_api.ErrMsg}'")

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/pixverse/video/result/{response_api.Resp.video_id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=PixverseGenerationStatusResponse,
            ),
            completed_statuses=[PixverseStatus.successful],
            failed_statuses=[PixverseStatus.contents_moderation, PixverseStatus.failed, PixverseStatus.deleted],
            status_extractor=lambda x: x.Resp.status,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        vid_response = requests.get(response_poll.Resp.url)
        return (VideoFromFile(BytesIO(vid_response.content)),)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-template)

[PixVerse Transition VideoCreate smooth transition videos between start and end frames using PixVerse AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-transition-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Limitations](http://docs.comfy.org#limitations)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/api-node/video/pixverse/pixverse-transition-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
  - Pika
  - PixVerse
    
    - [PixVerse Template](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-template)
    - [PixVerse Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video)
    - [PixVerse Transition Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-transition-video)
    - [PixVerse Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

PixVerse Transition Video - ComfyUI Native Node Documentation

# PixVerse Transition Video - ComfyUI Native Node Documentation

Create smooth transition videos between start and end frames using PixVerse AI

The Pixverse Transition Video node connects to PixVerse’s API to generate smooth video transitions between two images. It automatically creates all intermediate frames to produce fluid transformations, perfect for morphing effects, scene transitions, and object evolution.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionfirst\_frameImage-Starting frame imagelast\_frameImage-Ending frame imagepromptString""Text prompt describing video and transitionqualitySelect”PixverseQuality.res\_540p”Output video qualityduration\_secondsSelect-Length of generated videomotion\_modeSelect-Video motion styleseedInteger0Random seed (range: 0-2147483647)

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDefaultDescriptionnegative\_promptString""Elements to avoid in videopixverse\_templatePIXVERSE\_TEMPLATENoneOptional style preset

### [​](http://docs.comfy.org#parameter-constraints) Parameter Constraints

- When quality is set to 1080p, motion\_mode is forced to normal and duration\_seconds to 5 seconds
- When duration\_seconds is not 5 seconds, motion\_mode is forced to normal

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

```python

class PixverseTransitionVideoNode(ComfyNodeABC):
    """
    Generates videos synchronously based on prompt and output_size.
    """

    RETURN_TYPES = (IO.VIDEO,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/video/Pixverse"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "first_frame": (
                    IO.IMAGE,
                ),
                "last_frame": (
                    IO.IMAGE,
                ),
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the video generation",
                    },
                ),
                "quality": (
                    [resolution.value for resolution in PixverseQuality],
                    {
                        "default": PixverseQuality.res_540p,
                    },
                ),
                "duration_seconds": ([dur.value for dur in PixverseDuration],),
                "motion_mode": ([mode.value for mode in PixverseMotionMode],),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2147483647,
                        "control_after_generate": True,
                        "tooltip": "Seed for video generation.",
                    },
                ),
            },
            "optional": {
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
                "pixverse_template": (
                    PixverseIO.TEMPLATE,
                    {
                        "tooltip": "An optional template to influence style of generation, created by the Pixverse Template node."
                    }
                )
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        first_frame: torch.Tensor,
        last_frame: torch.Tensor,
        prompt: str,
        quality: str,
        duration_seconds: int,
        motion_mode: str,
        seed,
        negative_prompt: str=None,
        pixverse_template: int=None,
        auth_token=None,
        **kwargs,
    ):
        first_frame_id = upload_image_to_pixverse(first_frame, auth_token=auth_token)
        last_frame_id = upload_image_to_pixverse(last_frame, auth_token=auth_token)

        # 1080p is limited to 5 seconds duration
        # only normal motion_mode supported for 1080p or for non-5 second duration
        if quality == PixverseQuality.res_1080p:
            motion_mode = PixverseMotionMode.normal
            duration_seconds = PixverseDuration.dur_5
        elif duration_seconds != PixverseDuration.dur_5:
            motion_mode = PixverseMotionMode.normal

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/pixverse/video/transition/generate",
                method=HttpMethod.POST,
                request_model=PixverseTransitionVideoRequest,
                response_model=PixverseVideoResponse,
            ),
            request=PixverseTransitionVideoRequest(
                first_frame_img=first_frame_id,
                last_frame_img=last_frame_id,
                prompt=prompt,
                quality=quality,
                duration=duration_seconds,
                motion_mode=motion_mode,
                negative_prompt=negative_prompt if negative_prompt else None,
                template_id=pixverse_template,
                seed=seed,
            ),
            auth_token=auth_token,
        )
        response_api = operation.execute()

        if response_api.Resp is None:
            raise Exception(f"Pixverse request failed: '{response_api.ErrMsg}'")

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/pixverse/video/result/{response_api.Resp.video_id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=PixverseGenerationStatusResponse,
            ),
            completed_statuses=[PixverseStatus.successful],
            failed_statuses=[PixverseStatus.contents_moderation, PixverseStatus.failed, PixverseStatus.deleted],
            status_extractor=lambda x: x.Resp.status,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        vid_response = requests.get(response_poll.Resp.url)
        return (VideoFromFile(BytesIO(vid_response.content)),)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/pixverse/pixverse-transition-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video)

[PixVerse Image to VideoA node that converts static images to dynamic videos using PixVerse AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Parameter Constraints](http://docs.comfy.org#parameter-constraints)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Built_In_Node/built-in-nodes/api-node/video/pixverse/pixverse-transition-video.md -->


<!-- BEGIN Built_In_Node/built-in-nodes/overview.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Built-in Nodes

# ComfyUI Built-in Nodes

Introduction to ComfyUI Built-in Nodes

Built-in nodes are ComfyUI’s default nodes. They are core functionalities of ComfyUI that you can use without installing any third-party custom node packages.

As we have just started updating this section, the content is not yet complete. We will gradually add more content in the future.

If you find any errors in the content, you can submit an issue or PR in this [repo](https://github.com/Comfy-Org/docs) to help us improve.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/overview.mdx)

[Flux pro ultra image  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/bfl/flux-pro-ultra-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

<!-- END Built_In_Node/built-in-nodes/overview.md -->


<!-- BEGIN Built_In_Node/essentials/core-concepts/dependencies.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Dependencies

# Dependencies

Understand dependencies in ComfyUI

## [​](http://docs.comfy.org#a-workflow-file-depends-on-other-files) A workflow file depends on other files

We often obtain various workflow files from the community, but frequently find that the workflow cannot run directly after loading. This is because a workflow file depends on other files besides the workflow itself, such as media asset inputs, models, custom nodes, related Python dependencies, etc. ComfyUI workflows can only run normally when all relevant dependencies are satisfied.

ComfyUI workflow dependencies mainly fall into the following categories:

- Assets (media files including audio, video, images, and other inputs)
- Custom nodes
- Python dependencies
- Models (such as Stable Diffusion models, etc.)

## [​](http://docs.comfy.org#assets) Assets

An AI model is an example of an ***asset***. In media production, an asset is some media file that supplies input data. For example, a video editing program operates on movie files stored on disk. The editing program’s project file holds links to these movie file assets, allowing non-destructive editing that doesn’t alter the original movie files.

ComfyUI works the same way. A workflow can only run if all of the required assets are found and loaded. Generative AI models, images, movies, and sounds are some examples of assets that a workflow might depend upon. These are therefore known as ***dependent assets*** or ***asset dependencies***.

## [​](http://docs.comfy.org#custom-nodes) Custom Nodes

Custom nodes are an important component of ComfyUI that extend its functionality. They are created by the community and can be installed to add new capabilities to your workflows.

## [​](http://docs.comfy.org#python-dependencies) Python Dependencies

ComfyUI is a Python-based project. We build a standalone Python environment to run ComfyUI, and all related dependencies are installed in this isolated Python environment.

### [​](http://docs.comfy.org#comfyui-dependencies) ComfyUI Dependencies

You can view ComfyUI’s current dependencies in the [requirements.txt](https://github.com/comfyanonymous/ComfyUI/blob/master/requirements.txt) file:

```text
comfyui-frontend-package==1.14.5
torch
torchsde
torchvision
torchaudio
numpy>=1.25.0
einops
transformers>=4.28.1
tokenizers>=0.13.3
sentencepiece
safetensors>=0.4.2
aiohttp>=3.11.8
yarl>=1.18.0
pyyaml
Pillow
scipy
tqdm
psutil

#non essential dependencies:
kornia>=0.7.1
spandrel
soundfile
av
```

As ComfyUI evolves, we may adjust dependencies accordingly, such as adding new dependencies or removing ones that are no longer needed. So if you use Git to update ComfyUI, you need to run the following command in the corresponding environment after pulling the latest updates:

```bash
pip install -r requirements.txt
```

This ensures that ComfyUI’s dependencies are up to date for proper operation. You can also modify specific package dependency versions to upgrade or downgrade certain dependencies.

Additionally, ComfyUI’s frontend [ComfyUI\_frontend](https://github.com/Comfy-Org/ComfyUI_frontend) is currently maintained as a separate project. We update the `comfyui-frontend-package` dependency version after the corresponding version stabilizes. If you need to switch to a different frontend version, you can check the version information [here](https://pypi.org/project/comfyui-frontend-package/#history).

### [​](http://docs.comfy.org#custom-node-dependencies) Custom Node Dependencies

Thanks to the efforts of many authors in the ComfyUI community, we can extend ComfyUI’s functionality by using different custom nodes, enabling impressive creativity.

Typically, each custom node has its own dependencies and a separate `requirements.txt` file. If you use [ComfyUI Manager](https://github.com/ltdrdata/ComfyUI-Manager) to install custom nodes, ComfyUI Manager will usually automatically install the corresponding dependencies.

There are also cases where you need to install dependencies manually. Currently, all custom nodes are installed in the `ComfyUI/custom_nodes` directory.

You need to navigate to the corresponding plugin directory in your ComfyUI Python environment and run `pip install -r requirements.txt` to install the dependencies.

If you’re using the [Windows Portable version](http://docs.comfy.org/installation/comfyui_portable_windows), you can use the following command in the `ComfyUI_windows_portable` directory:

```plaintext
python_embeded\python.exe -m pip install -r ComfyUI\custom_nodes\<custom_node_name>\requirements.txt
```

to install the dependencies for the corresponding node.

### [​](http://docs.comfy.org#dependency-conflicts) Dependency Conflicts

Dependency conflicts are a common issue when using ComfyUI. You might find that after installing or updating a custom node, previously installed custom nodes can no longer be found in ComfyUI’s node library, or error pop-ups appear. One possible reason is dependency conflicts.

There can be many reasons for dependency conflicts, such as:

1. Custom node version locking

Some plugins may fix the exact version of a dependency library (e.g., `open_clip_torch==2.26.1`), while other plugins may require a higher version (e.g., `open_clip_torch>=2.29.0`), making it impossible to satisfy both version requirements simultaneously.

**Solution**: You can try changing the fixed version dependency to a range constraint, such as `open_clip_torch>=2.26.1`, and then reinstall the dependencies to resolve these issues.

2. Environment pollution

During the installation of custom node dependencies, it may overwrite versions of libraries already installed by other plugins. For example, multiple plugins may depend on `PyTorch` but require different CUDA versions, and the later installed plugin will break the existing environment.

**Solutions**:

- You can try manually installing specific versions of dependencies in the Python virtual environment to resolve such issues.
- Or create different Python virtual environments for different plugins to resolve these issues.
- Try installing plugins one by one, restarting ComfyUI after each installation to observe if dependency conflicts occur.

<!--THE END-->

3. Custom node dependency versions incompatible with ComfyUI dependency versions

These types of dependency conflicts may be more difficult to resolve, and you may need to upgrade/downgrade ComfyUI or change the dependency versions of custom nodes to resolve these issues.

**Solution**: These types of dependency conflicts may be more difficult to resolve, and you may need to upgrade/downgrade ComfyUI or change the dependency versions of custom nodes to resolve these issues.

## [​](http://docs.comfy.org#models) Models

Models are a significant asset dependency for ComfyUI. Various custom nodes and workflows are built around specific models, such as the Stable Diffusion series, Flux series, Ltxv, and others. These models are an essential foundation for creation with ComfyUI, so we need to ensure that the models we use are properly available. Typically, our models are saved in the corresponding directory under `ComfyUI/models/`. Of course, you can also create an [extra\_model\_paths.yaml](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) by modifying the template to make additional model paths recognized by ComfyUI. This allows multiple ComfyUI instances to share the same model library, reducing disk usage.

## [​](http://docs.comfy.org#software) Software

An advanced application like ComfyUI also has ***software dependencies***. These are libraries of programming code and data that are required for the application to run. Custom nodes are examples of software dependencies. On an even more fundamental level, the Python programming environment is the ultimate dependency for ComfyUI. The correct version of Python is required to run a particular version of ComfyUI. Updates to Python, ComfyUI, and custom nodes can all be handled from the **ComfyUI Manager** window.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/core-concepts/dependencies.mdx)

[Previous](http://docs.comfy.org/essentials/core-concepts/models)

[ShortcutsKeyboard and mouse shortcuts for ComfyUI and related settings  
\
Next](http://docs.comfy.org/interface/shortcuts)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [A workflow file depends on other files](http://docs.comfy.org#a-workflow-file-depends-on-other-files)
- [Assets](http://docs.comfy.org#assets)
- [Custom Nodes](http://docs.comfy.org#custom-nodes)
- [Python Dependencies](http://docs.comfy.org#python-dependencies)
- [ComfyUI Dependencies](http://docs.comfy.org#comfyui-dependencies)
- [Custom Node Dependencies](http://docs.comfy.org#custom-node-dependencies)
- [Dependency Conflicts](http://docs.comfy.org#dependency-conflicts)
- [Models](http://docs.comfy.org#models)
- [Software](http://docs.comfy.org#software)

<!-- END Built_In_Node/essentials/core-concepts/dependencies.md -->


<!-- BEGIN Built_In_Node/essentials/core-concepts/links.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Links

# Links

Understand connection links in ComfyUI

As ComfyUI is still in rapid iteration and development, we are continuously improving it every day. Therefore, some operations mentioned in this article may change or be omitted. Please refer to the actual interface. If you find changes in actual operations, it may be due to our iterative updates. You can also fork [this repo](https://github.com/Comfy-Org/docs) and help us improve this documentation.

## [​](http://docs.comfy.org#links-connect-nodes) Links connect nodes

In the terminology of ComfyUI, the lines or curves between nodes are called ***links***. They’re also known as ***connections*** or wires. Links can be displayed in several ways, such as curves, right angles, straight lines, or completely hidden.

You can modify the link style in **Setup Menu** —&gt; **Display (Lite Graph)** —&gt; **Graph** —&gt; **Link Render Mode**.

You can also temporarily hide links in the **Canvas Menu**.

Link display is crucial. Depending on the situation, it may be necessary to see all links. Especially when learning, sharing, or even just understanding workflows, the visibility of links enables users to follow the flow of data through the graph. For packaged workflows that aren’t intended to be altered, it might make sense to hide the links to reduce clutter.

### [​](http://docs.comfy.org#reroute-node) Reroute node

If legibility of the graph structure is important, then link wires can be manually routed in the 2D space of the graph with a tiny node called **Reroute**. Its purpose is to position the beginning and/or end points of link wires to ensure visibility. We can design a workflow so that link wires don’t pass behind nodes, don’t cross other link wires, and so on.

We are also continuously improving the native reroute functionality in litegraph. We recommend using this feature in the future to reorganize connections.

## [​](http://docs.comfy.org#color-coding) Color-coding

The data type of node properties is indicated by color coding of input/output ports and link connection wires. We can always tell which inputs and outputs can be connected to one another by their color. Ports can only be connected to other ports of the same color to ensure matching data types.

Common data types:

Data typeColordiffusion modellavenderCLIP modelyellowVAE modelroseconditioningorangelatent imagepinkpixel imagebluemaskgreennumber (integer or float)light greenmeshbright green

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/core-concepts/links.mdx)

[Previous](http://docs.comfy.org/essentials/core-concepts/properties)

[Mask EditorLearn how to use the Mask Editor in ComfyUI, including settings and usage instructions  
\
Next](http://docs.comfy.org/interface/maskeditor)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Links connect nodes](http://docs.comfy.org#links-connect-nodes)
- [Reroute node](http://docs.comfy.org#reroute-node)
- [Color-coding](http://docs.comfy.org#color-coding)

<!-- END Built_In_Node/essentials/core-concepts/links.md -->


<!-- BEGIN Built_In_Node/essentials/core-concepts/models.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Models

# Models

## [​](http://docs.comfy.org#models-are-essential) Models are essential

Models are essential building blocks for media generation workflows. They can be combined and mixed to achieve different creative effects.

The word ***model*** has many different meanings. Here, it means a data file carrying information that is required for a node graph to do its work. Specifically, it’s a data structure that *models* some function. As a verb, to model something means to represent it or provide an example.

The primary example of a model data file in ComfyUI is an AI ***diffusion model***. This is a large set of data that represents the complex relationships among text strings and images, making it possible to translate words into pictures or vice versa. Other examples of common models used for image generation are language models such as CLIP, and upscaling models such as RealESRGAN.

## [​](http://docs.comfy.org#model-files) Model files

Model files are absolutely required for generative media production. Nothing can happen in a workflow if the model files are not found. Models are not included in the ComfyUI installation, but ComfyUI can often automatically download and install missing model files. Many models can be downloaded and installed from the **ComfyUI Manager** window. Models can also be found at websites such as [huggingface.co](https://huggingface.co), [civitai.green](https://civitai.green), and [github.com](https://github.com).

### [​](http://docs.comfy.org#using-models-in-comfyui) Using Models in ComfyUI

1. Download and place them in the ComfyUI program directory
   
   1. Within the **models** folder, you’ll find subfolders for various types of models, such as **checkpoints**
   2. The **ComfyUI Manager** helps to automate the process of searching, downloading, and installing
   3. Restart ComfyUI if it’s running
2. In your workflow, create the node appropriate to the model type, e.g. **Load Checkpoint**, **Load LoRA**, **Load VAE**
3. In the loader node, choose the model you wish to use
4. Connect the loader node to other nodes in your workflow

### [​](http://docs.comfy.org#file-size) File size

Models can be extremely large files relative to image files. A typical uncompressed image may require a few megabytes of disk storage. Generative AI models can be tens of thousands of times larger, up to tens of gigabytes per model. They take up a great deal of disk space and take a long time to transfer over a network.

## [​](http://docs.comfy.org#model-training-and-refinement) Model training and refinement

A generative AI model is created by training a machine learning program on a very large set of data, such as pairs of images and text descriptions. An AI model doesn’t store the training data explicitly, but rather it stores the correlations that are implicit within the data.

Organizations and companies such as Stability AI and Black Forest Labs release “base” models that carry large amounts of generic information. These are general purpose generative AI models. Commonly, the base models need to be ***refined*** in order to get high quality generative outputs. A dedicated community of people work to refine the base models. The new, refined models produce better output, provide new or different functionality, and/or use fewer resources. Refined models can usually be run on systems with less computing power and/or memory.

## [​](http://docs.comfy.org#auxiliary-models) Auxiliary models

Model functionality can be extended with auxiliary models. For example, art directing a text-to-image workflow to achieve a specific result may be difficult or impossible using a diffusion model alone. Additional models can refine a diffusion model within the workflow graph to produce desired results. Examples include **LoRA** (Low Rank Adaptation), a small model that is trained on a specific subject; **ControlNet**, a model that helps control composition using a guide image; and **Inpainting**, a model that allows certain diffusion models to generate new content within an existing image.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/core-concepts/models.mdx)

[Previous](http://docs.comfy.org/interface/maskeditor)

[DependenciesUnderstand dependencies in ComfyUI  
\
Next](http://docs.comfy.org/essentials/core-concepts/dependencies)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Models are essential](http://docs.comfy.org#models-are-essential)
- [Model files](http://docs.comfy.org#model-files)
- [Using Models in ComfyUI](http://docs.comfy.org#using-models-in-comfyui)
- [File size](http://docs.comfy.org#file-size)
- [Model training and refinement](http://docs.comfy.org#model-training-and-refinement)
- [Auxiliary models](http://docs.comfy.org#auxiliary-models)

<!-- END Built_In_Node/essentials/core-concepts/models.md -->


<!-- BEGIN Built_In_Node/essentials/core-concepts/nodes.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Nodes

# Nodes

Understand the concept of a node in ComfyUI.

In ComfyUI, nodes are the fundamental building blocks for executing tasks. Each node is an independently built module, whether it’s a **Comfy Core** node or a **Custom Node**, with its own unique functionality. Nodes connect to each other through links, allowing us to build complex functionality like assembling LEGO blocks. The combinations of different nodes create the unlimited possibilities of ComfyUI.

For example, in the K-Sampler node, you can see it has multiple inputs and outputs, and also includes multiple parameter settings. These parameters determine the logic of node execution. Behind each node is well-written Python logic, allowing you to achieve corresponding functionality without having to write code yourself.

As ComfyUI is still in rapid iteration and development, we are continuously improving it every day. Therefore, some operations mentioned in this article may change or be omitted. Please refer to the actual interface. If you find changes in actual operations, it may be due to our iterative updates. You can also fork [this repo](https://github.com/Comfy-Org/docs) and help us improve this documentation.

## [​](http://docs.comfy.org#nodes-perform-operations) Nodes perform operations

In computer science, a ***node*** is a container for information, usually including programmed instructions to perform some task. Nodes almost never exist in isolation, they’re almost always connected to other nodes in a networked graph. In ComfyUI, nodes take the visual form of boxes that are connected to each other.

ComfyUI nodes are usually ***function operators***. This means that they operate on some data to perform a function. A function is a process that accepts input data, performs some operation on it, and produces output data. In other words, nodes do some work, contributing to the completion of a task such as generating an image. So ComfyUI nodes almost always have at least one input or output, and usually have multiple inputs and outputs.

## [​](http://docs.comfy.org#different-node-states) Different Node States

In ComfyUI, nodes have multiple states. Here are some common node states:

1. **Normal State**: The default state
2. **Running State**: The running state, typically displayed when a node is executing after you start running the workflow
3. **Error State**: Node error, typically displayed after running the workflow if there’s a problem with the node’s input, indicated by red marking of the erroneous input node. You need to fix the problematic input to ensure the workflow runs correctly
4. **Missing State**: This state usually appears after importing workflows, with two possibilities:
   
   - Comfy Core native node missing: This usually happens because ComfyUI has been updated, but you’re using an older version of ComfyUI. You need to update ComfyUI to resolve this issue
   - Custom node missing: The workflow uses custom nodes developed by third-party authors, but your local ComfyUI version doesn’t have these custom nodes installed. You can use [ComfyUI-Manager](https://github.com/Comfy-Org/ComfyUI-Manager) to find and install the missing custom nodes

## [​](http://docs.comfy.org#connections-between-nodes) Connections Between Nodes

In ComfyUI, nodes are connected through [links](http://docs.comfy.org/essentials/core-concepts/links), allowing data of the same type to flow between different processing units to achieve the final result.

Each node receives some input, processes it through its module, and converts it to corresponding output. Connections between different nodes must conform to the data type requirements. In ComfyUI, we use different colors to distinguish node data types. Below are some basic data types:

Data typeColordiffusion modellavenderCLIP modelyellowVAE modelroseconditioningorangelatent imagepinkpixel imagebluemaskgreennumber (integer or float)light greenmeshbright green

As ComfyUI evolves, we may expand to more data types to meet the needs of more scenarios.

### [​](http://docs.comfy.org#connecting-and-disconnecting-nodes) Connecting and Disconnecting Nodes

**Connecting**: Drag from the output point of one node to the input of the same color on another node to connect them **Disconnecting**: Click on the input endpoint and drag the mouse left button to disconnect, or cancel the connection through the midpoint menu of the link

## [​](http://docs.comfy.org#node-appearance) Node Appearance

We provide various style settings for you to customize the appearance of nodes:

- Modify styles
- Double-click the node title to modify the node name
- Switch node inputs between input sockets and widgets through the context menu
- Resize the node using the bottom right corner

### [​](http://docs.comfy.org#node-badges) Node Badges

We provide multiple node badge display features, such as:

- Node ID
- Node source

Currently, **Comfy Core nodes** use a fox icon for display, while custom nodes use their names. This way you can quickly understand which node package a node comes from.

You can set the corresponding display in the menu:

## [​](http://docs.comfy.org#node-context-menus) Node Context Menus

Node context menus are mainly divided into two types:

- Context menu for the node itself
- Context menu for inputs/outputs

### [​](http://docs.comfy.org#node-context-menu) Node Context Menu

By right-clicking on a node, you can expand the corresponding node context menu:

In the node’s right-click context menu, you can:

- Adjust the node’s color style
- Modify the title
- Clone, copy, or delete the node
- Set the node’s mode

In this menu, besides appearance-related settings, the following menu operations are important:

- **Mode**: Set the node’s mode: Always, Never, Bypass
- **Toggle between Widget and Input mode for node inputs**: Switch between widget and input mode for node inputs

#### [​](http://docs.comfy.org#mode) Mode

For modes, you may notice that we currently provide: Always, Never, On Event, On Trigger - four modes, but actually only **Always** and **Never** are effective. **On Event** and **On Trigger** are currently ineffective as we haven’t fully implemented this feature. Additionally, you can understand **Bypass** as a mode. Below is an explanation of the available modes:

- **Always**: The default node mode. The node will execute whenever it runs for the first time or when any of its inputs change since the last execution
- **Never**: The node will never execute under any circumstances, as if it’s been deleted. Subsequent nodes cannot read or receive any data from it
- **Bypass**: The node will never execute under any circumstances, but subsequent nodes can still try to obtain data that hasn’t been processed by this node

Below is a comparison of the `Never` and `Bypass` modes:

In this comparison example, you can see that both workflows apply two LoRA models simultaneously, with the difference being that one `Load LoRA` node is set to `Never` mode while the other is set to `Bypass` mode.

- The node set to `Never` mode causes subsequent nodes to show errors because they don’t receive any input data
- The node set to `Bypass` mode still allows subsequent nodes to receive unprocessed data, so they load the output data from the first `Load LoRA` node, allowing the subsequent workflow to continue running normally

#### [​](http://docs.comfy.org#switching-between-widget-and-input-mode-for-node-inputs) Switching Between Widget and Input Mode for Node Inputs

In some cases, we need to use output results from other nodes as input. In this case, we can switch between widget and input mode for node inputs.

Here’s a very simple example:

By switching the K-Sampler’s Seed from widget to input mode, multiple nodes can share the same seed, achieving variable uniformity across multiple samplers. Comparing the first node with the subsequent two nodes, you can see that the seed in the latter two nodes is in input mode. You can also convert it back to widget mode:

After frontend version v1.16.0, we improved this feature. Now you only need to directly connect the input line to the corresponding widget to complete this process

> Say goodbye to annoying widget &lt;&gt; socket conversion starting from frontend version v1.16.0! Now each widget just always have an associated input socket by default [#ComfyUI](https://twitter.com/hashtag/ComfyUI?src=hash&ref_src=twsrc%5Etfw) [pic.twitter.com/sP9HHKyGYW](https://t.co/sP9HHKyGYW)
> 
> — Chenlei Hu (@HclHno3) [April 7, 2025](https://twitter.com/HclHno3/status/1909059259536375961?ref_src=twsrc%5Etfw)

### [​](http://docs.comfy.org#input%2Foutput-context-menu) Input/Output Context Menu

This context menu is mainly related to the data type of the corresponding input/output:

When dragging the input/output of a node, if a connection appears but you haven’t connected to another node’s input or output, releasing the mouse will pop up a context menu for the input/output, used to quickly add related types of nodes. You can adjust the number of node suggestions in the settings:

## [​](http://docs.comfy.org#node-selection-toolbox) Node Selection Toolbox

The **Node Selection Toolbox** is a floating tool that provides quick operations for nodes. When you select a node, it hovers above the selected node. Through this toolbox, you can:

- Change the node’s color
- Quickly set the node to Bypass mode (not execute during runtime)
- Lock the node
- Delete the node

Of course, these functions can also be found in the right-click menu of the corresponding node. The node selection toolbox just provides a shortcut operation. If you want to disable this feature, you can turn it off in the settings.

## [​](http://docs.comfy.org#node-groups) Node Groups

In ComfyUI, you can select multiple parts of a workflow simultaneously, then use the right-click menu to merge them into a node group, making that part a reusable module that can be repeatedly called in your ComfyUI.

## [​](http://docs.comfy.org#custom-nodes) Custom Nodes

ComfyUI includes many powerful nodes in the base installation package. These are known as **Comfy Core** nodes. Additionally, the ComfyUI community has created an amazing array of [***custom nodes***](https://registry.comfy.org) to perform a wide variety of functions.

## [​](http://docs.comfy.org#comfyui-manager) ComfyUI Manager

The **ComfyUI Manager** window makes it easy to perform custom node management tasks such as search, install, update, disable, and uninstall. The Manager is included in the ComfyUI desktop application, but not in the ComfyUI server application.

### [​](http://docs.comfy.org#installing-the-comfyui-manager) Installing the ComfyUI Manager

If you’re running the ComfyUI server application, you need to install the Manager. If ComfyUI is running, shut it down before proceeding.

The first step is to install Git, a command-line application for software version control. Git will download the ComfyUI Manager from [github.com](https://github.com). Download Git from [git-scm.com](https://git-scm.com/) and install it.

Once Git is installed, navigate to the ComfyUI server program directory, to the folder labeled **custom\_nodes**. Open up a command window or terminal. Make sure that the command line displays the current directory path as **custom\_nodes**. Enter the following command. This will download the Manager. Technically, this is known as *cloning a Git repository*.

```bash
git clone https://github.com/ltdrdata/ComfyUI-Manager.git
```

For details or special cases, see [ComfyUI Manager Install](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#installation).

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/core-concepts/nodes.mdx)

[Previous](http://docs.comfy.org/essentials/core-concepts/workflow)

[Properties  
\
Next](http://docs.comfy.org/essentials/core-concepts/properties)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Nodes perform operations](http://docs.comfy.org#nodes-perform-operations)
- [Different Node States](http://docs.comfy.org#different-node-states)
- [Connections Between Nodes](http://docs.comfy.org#connections-between-nodes)
- [Connecting and Disconnecting Nodes](http://docs.comfy.org#connecting-and-disconnecting-nodes)
- [Node Appearance](http://docs.comfy.org#node-appearance)
- [Node Badges](http://docs.comfy.org#node-badges)
- [Node Context Menus](http://docs.comfy.org#node-context-menus)
- [Node Context Menu](http://docs.comfy.org#node-context-menu)
- [Mode](http://docs.comfy.org#mode)
- [Switching Between Widget and Input Mode for Node Inputs](http://docs.comfy.org#switching-between-widget-and-input-mode-for-node-inputs)
- [Input/Output Context Menu](http://docs.comfy.org#input%2Foutput-context-menu)
- [Node Selection Toolbox](http://docs.comfy.org#node-selection-toolbox)
- [Node Groups](http://docs.comfy.org#node-groups)
- [Custom Nodes](http://docs.comfy.org#custom-nodes)
- [ComfyUI Manager](http://docs.comfy.org#comfyui-manager)
- [Installing the ComfyUI Manager](http://docs.comfy.org#installing-the-comfyui-manager)

<!-- END Built_In_Node/essentials/core-concepts/nodes.md -->


<!-- BEGIN Built_In_Node/essentials/core-concepts/properties.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Properties

# Properties

## [​](http://docs.comfy.org#nodes-are-containers-for-properties) Nodes are containers for properties

Nodes usually have ***properties***. Also known as ***parameters*** or ***attributes***, node properties are variables that can be changed. Some properties can be adjusted manually by the user, using a data entry field called a ***widget***. Other properties can be driven automatically by other nodes connected to the property ***input slot*** or port. Usually, a property can be converted from widget to input and vice versa, allowing users to control property values manually or automatically.

Properties can take many forms and hold many different types of information. For example, a **Load Checkpoint** node has a single property:  the file path to the generative model checkpoint file. A **KSampler** node has multiple properties such as the number of sampling **steps**, **CFG** scale, **sampler\_name**, etc.

## [​](http://docs.comfy.org#data-types) Data types

Information can come in many different forms, called ***data types***. For example, alphanumeric text is known as a ***string***, a whole number is an ***integer***, and a number with a decimal point is known as a ***floating point*** number or ***float***. New data types are always being added to ComfyUI.

ComfyUI is written in the Python scripting language, which is very forgiving about data types. By contrast, the ComfyUI environment is very ***strongly typed***. This means that different data types can’t be mixed up. For example, we can’t connect an image output to an integer input. This is a huge benefit to users, guiding them to proper workflow construction and preventing program errors.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/core-concepts/properties.mdx)

[Previous](http://docs.comfy.org/essentials/core-concepts/nodes)

[LinksUnderstand connection links in ComfyUI  
\
Next](http://docs.comfy.org/essentials/core-concepts/links)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Nodes are containers for properties](http://docs.comfy.org#nodes-are-containers-for-properties)
- [Data types](http://docs.comfy.org#data-types)

<!-- END Built_In_Node/essentials/core-concepts/properties.md -->


<!-- BEGIN Built_In_Node/essentials/core-concepts/workflow.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Workflow

# Workflow

## [​](http://docs.comfy.org#a-graph-of-nodes) A graph of nodes

ComfyUI is an environment for building and running generative content ***workflows***. In this context, a workflow is defined as a collection of program objects called ***nodes*** that are connected to each other, forming a network. This network is also known as a ***graph***.

A ComfyUI workflow can generate any type of media: image, video, audio, AI model, AI agent, and so on.

## [​](http://docs.comfy.org#sample-workflows) Sample workflows

To get started, try out some of the [official workflows](https://comfyanonymous.github.io/ComfyUI_examples). These use only the Core nodes included in the ComfyUI installation. A thriving community of developers has created a rich [ecosystem](https://registry.comfy.org) of custom nodes to extend the functionality of ComfyUI.

### [​](http://docs.comfy.org#simple-example) Simple Example

## [​](http://docs.comfy.org#visual-programming) Visual programming

A node-based computer program like ComfyUI provides a level of power and flexibility that can’t be achieved with traditional menu- and button-driven applications. The ComfyUI node graph is not limited by the tools provided in a traditional computer application. It’s a high-level ***visual programming environment*** allowing users to design complex systems without needing to write program code or understand advanced mathematics.

Many other computer applications use this same node graph paradigm. Examples include the compositing application called Nuke, the 3D programs Maya and Blender, the Unreal real-time graphics engine, and the interactive media authoring program called Max.

### [​](http://docs.comfy.org#more-complex-example) More Complex Example

## [​](http://docs.comfy.org#procedural-framework) Procedural framework

Another term used to describe a node-based application is ***procedural framework***. Procedural means generative: some procedure or algorithm is employed to generate content such as a 3D model or a musical composition.

ComfyUI is all of these things: a node graph, a visual programming environment, and a procedural framework. What makes ComfyUI different (and amazing!) is that its radically open structure allows us to generate any type of media asset such as picture, movie, sound, 3D model, AI model, etc.

In the context of ComfyUI, the term ***workflow*** is a synonym for the node network or graph. It corresponds to the ***scene graph*** in a 3D or multimedia program: the network of all of the nodes within a particular disk file. 3D programs call this a ***scene file***. Video editing, compositing, and multimedia programs usually call it a ***project file***.

## [​](http://docs.comfy.org#saving-workflows) Saving workflows

The ComfyUI workflow is automatically saved in the metadata of any generated image, allowing users to open and use the graph that generated the image. A workflow can also be stored in a human-readable text file that follows the JSON data format. This is necessary for media formats that don’t support metadata. ComfyUI workflows stored as JSON files are very small, allowing convenient versioning, archiving, and sharing of graphs, independently of any generated media.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/core-concepts/workflow.mdx)

[Previous](http://docs.comfy.org/interface/credits)

[NodesUnderstand the concept of a node in ComfyUI.  
\
Next](http://docs.comfy.org/essentials/core-concepts/nodes)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [A graph of nodes](http://docs.comfy.org#a-graph-of-nodes)
- [Sample workflows](http://docs.comfy.org#sample-workflows)
- [Simple Example](http://docs.comfy.org#simple-example)
- [Visual programming](http://docs.comfy.org#visual-programming)
- [More Complex Example](http://docs.comfy.org#more-complex-example)
- [Procedural framework](http://docs.comfy.org#procedural-framework)
- [Saving workflows](http://docs.comfy.org#saving-workflows)

<!-- END Built_In_Node/essentials/core-concepts/workflow.md -->


<!-- BEGIN Built_In_Node/get_started/first_generation.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Getting Started with AI Image Generation

# Getting Started with AI Image Generation

This tutorial will guide you through your first image generation with ComfyUI, covering basic interface operations like workflow loading, model installation, and image generation

This guide aims to help you understand ComfyUI’s basic operations and complete your first image generation. We’ll cover:

1. Loading example workflows
   
   - Loading from ComfyUI’s workflow templates
   - Loading from images with workflow metadata
2. Model installation guidance
   
   - Automatic model installation
   - Manual model installation
   - Using ComfyUI Manager for model installation
3. Completing your first text-to-image generation

## [​](http://docs.comfy.org#about-text-to-image) About Text-to-Image

Text-to-Image is a fundamental AI drawing feature that generates images from text descriptions. It’s one of the most commonly used functions in AI art generation. You can think of the process as telling your requirements (positive and negative prompts) to an artist (the drawing model), who will then create what you want. Detailed explanations about text-to-image will be covered in the [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image) chapter.

## [​](http://docs.comfy.org#comfyui-text-to-image-workflow-tutorial) ComfyUI Text-to-Image Workflow Tutorial

### [​](http://docs.comfy.org#1-launch-comfyui) 1. Launch ComfyUI

Make sure you’ve followed the installation guide to start ComfyUI and can successfully enter the ComfyUI interface.

If you have not installed ComfyUI, please choose a suitable version to install based on your device.

ComfyUI Desktop (Recommended)

ComfyUI Desktop currently supports standalone installation for **Windows and MacOS (ARM)**, currently in Beta

- Code is open source on [Github](https://github.com/Comfy-Org/desktop)

You can choose the appropriate installation for your system and hardware below

- Windows
- MacOS(Apple Silicon)
- Linux

[**ComfyUI Desktop (Windows) Installation Guide**  
\
Suitable for **Windows** version with **Nvidia** GPU](http://docs.comfy.org/installation/desktop/windows)

[**ComfyUI Desktop (Windows) Installation Guide**  
\
Suitable for **Windows** version with **Nvidia** GPU](http://docs.comfy.org/installation/desktop/windows)

[**ComfyUI Desktop (MacOS) Installation Guide**  
\
Suitable for MacOS with **Apple Silicon**](http://docs.comfy.org/installation/desktop/macos)

ComfyUI Desktop **currently has no Linux prebuilds**, please visit the [Manual Installation](http://docs.comfy.org/installation/manual_install) section to install ComfyUI

ComfyUI Portable (Windows)

[**ComfyUI Portable (Windows) Installation Guide**  
\
Supports **Windows** ComfyUI version running on **Nvidia GPUs** or **CPU-only**, always use the latest commits and completely portable.](http://docs.comfy.org/installation/comfyui_portable_windows)

Manual Installation

[**ComfyUI Manual Installation Guide**  
\
Supports all system types and GPU types (Nvidia, AMD, Intel, Apple Silicon, Ascend NPU, Cambricon MLU)](http://docs.comfy.org/installation/manual_install)

### [​](http://docs.comfy.org#2-load-default-text-to-image-workflow) 2. Load Default Text-to-Image Workflow

ComfyUI usually loads the default text-to-image workflow automatically when launched. However, you can try different methods to load workflows to familiarize yourself with ComfyUI’s basic operations:

- Load from Workflow Template
- Load from Images with Metadata
- Load from workflow.json

Follow the numbered steps in the image:

1. Click the **Fit View** button in the bottom right to ensure any loaded workflow isn’t hidden
2. Click the **folder icon (workflows)** in the sidebar
3. Click the **Browse example workflows** button at the top of the Workflows panel

Continue with:

4. Select the first default workflow **Image Generation** to load it

Alternatively, you can select **Browse workflow templates** from the workflow menu

Follow the numbered steps in the image:

1. Click the **Fit View** button in the bottom right to ensure any loaded workflow isn’t hidden
2. Click the **folder icon (workflows)** in the sidebar
3. Click the **Browse example workflows** button at the top of the Workflows panel

Continue with:

4. Select the first default workflow **Image Generation** to load it

Alternatively, you can select **Browse workflow templates** from the workflow menu

All images generated by ComfyUI contain metadata including workflow information. You can load workflows by:

- Dragging and dropping a ComfyUI-generated image into the interface
- Using menu **Workflows** -&gt; **Open** to open an image

Try loading the workflow using this example image:

ComfyUI workflows can be stored in JSON format. You can export workflows using menu **Workflows** -&gt; **Export**.

Try downloading and loading this example workflow:

[Download text-to-image.json](https://github.com/Comfy-Org/docs/blob/main/public/text-to-image.json)

After downloading, use menu **Workflows** -&gt; **Open** to load the JSON file.

### [​](http://docs.comfy.org#3-model-installation) 3. Model Installation

Most ComfyUI installations don’t include base models by default. After loading the workflow, if you don’t have the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) model installed, you’ll see this prompt:

All models are stored in `<your ComfyUI installation>/ComfyUI/models/` with subfolders like `checkpoints`, `embeddings`, `vae`, `lora`, `upscale_model`, etc. ComfyUI detects models in these folders and paths configured in `extra_models_config.yaml` at startup.

You can install models through:

- Automatic Download
- ComfyUI Manager
- Manual Installation

After you click the **Download** button, ComfyUI will execute the download, and different behaviors will be performed depending on the version you are using.

- ComfyUI Desktop
- ComfyUI Portable

The desktop version will automatically complete the model download and save it to the `<your ComfyUI installation location>/ComfyUI/models/checkpoints` directory. You can wait for the installation to complete or view the installation progress in the model panel on the sidebar.

If everything goes smoothly, the model should be able to download locally. If the download fails for a long time, please try other installation methods.

The desktop version will automatically complete the model download and save it to the `<your ComfyUI installation location>/ComfyUI/models/checkpoints` directory. You can wait for the installation to complete or view the installation progress in the model panel on the sidebar.

If everything goes smoothly, the model should be able to download locally. If the download fails for a long time, please try other installation methods.

The browser will execute file downloads. Please save the file to the `<your ComfyUI installation location>/ComfyUI_windows_portable/ComfyUI/models/checkpoints` directory after the download is complete.

After you click the **Download** button, ComfyUI will execute the download, and different behaviors will be performed depending on the version you are using.

- ComfyUI Desktop
- ComfyUI Portable

The desktop version will automatically complete the model download and save it to the `<your ComfyUI installation location>/ComfyUI/models/checkpoints` directory. You can wait for the installation to complete or view the installation progress in the model panel on the sidebar.

If everything goes smoothly, the model should be able to download locally. If the download fails for a long time, please try other installation methods.

The desktop version will automatically complete the model download and save it to the `<your ComfyUI installation location>/ComfyUI/models/checkpoints` directory. You can wait for the installation to complete or view the installation progress in the model panel on the sidebar.

If everything goes smoothly, the model should be able to download locally. If the download fails for a long time, please try other installation methods.

The browser will execute file downloads. Please save the file to the `<your ComfyUI installation location>/ComfyUI_windows_portable/ComfyUI/models/checkpoints` directory after the download is complete.

ComfyUI Manager is a tool for managing custom nodes, models, and plugins.

1

Open ComfyUI Manager

Click the `Manager` button to open ComfyUI Manager

2

Open Model Manager

Click `Model Manager`

3

Search and Install Model

1. Search for `v1-5-pruned-emaonly.ckpt`
2. Click `install` on the desired model

Visit [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) and follow this guide:

Save the downloaded file to:

- ComfyUI Desktop
- ComfyUI Portable

Save to `<your ComfyUI installation>/ComfyUI/models/checkpoints`

Save to `<your ComfyUI installation>/ComfyUI/models/checkpoints`

Save to `ComfyUI_windows_portable/ComfyUI/models/checkpoints`

Refresh or restart ComfyUI after saving.

### [​](http://docs.comfy.org#4-load-model-and-generate-your-first-image) 4. Load Model and Generate Your First Image

After installing the model:

1. In the **Load Checkpoint** node, ensure **v1-5-pruned-emaonly-fp16.safetensors** is selected
2. Click `Queue` or press `Ctrl + Enter` to generate

The result will appear in the **Save Image** node. Right-click to save locally.

For detailed text-to-image instructions, see our comprehensive guide:

[**ComfyUI Text-to-Image Workflow Guide**  
\
Click here for detailed text-to-image workflow instructions](http://docs.comfy.org/tutorials/basic/text-to-image)

## [​](http://docs.comfy.org#troubleshooting) Troubleshooting

### [​](http://docs.comfy.org#model-loading-issues) Model Loading Issues

If the `Load Checkpoint` node shows no models or displays “null”, verify your model installation location and try refreshing or restarting ComfyUI.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/get_started/first_generation.mdx)

[Previous](http://docs.comfy.org/installation/manual_install)

[Interface OverviewIn this article, we will briefly introduce the basic user interface of ComfyUI, familiarizing you with the various parts of the ComfyUI interface.  
\
Next](http://docs.comfy.org/interface/overview)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [About Text-to-Image](http://docs.comfy.org#about-text-to-image)
- [ComfyUI Text-to-Image Workflow Tutorial](http://docs.comfy.org#comfyui-text-to-image-workflow-tutorial)
- [1. Launch ComfyUI](http://docs.comfy.org#1-launch-comfyui)
- [2. Load Default Text-to-Image Workflow](http://docs.comfy.org#2-load-default-text-to-image-workflow)
- [3. Model Installation](http://docs.comfy.org#3-model-installation)
- [4. Load Model and Generate Your First Image](http://docs.comfy.org#4-load-model-and-generate-your-first-image)
- [Troubleshooting](http://docs.comfy.org#troubleshooting)
- [Model Loading Issues](http://docs.comfy.org#model-loading-issues)

<!-- END Built_In_Node/get_started/first_generation.md -->


<!-- BEGIN Built_In_Node/get_started/introduction.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Introduction

# Introduction

Official documentation for ComfyUI. Contribute [here](https://github.com/Comfy-Org/docs).

## [​](http://docs.comfy.org#comfyui) [ComfyUI](https://github.com/comfyanonymous/ComfyUI)

The most powerful and modular stable diffusion GUI and backend. Written by [comfyanonymous](https://github.com/comfyanonymous) and other [contributors](https://github.com/comfyanonymous/ComfyUI/graphs/contributors).

- **ComfyUI** is a node-based interface and inference engine for generative AI
- Users can combine various AI models and operations through nodes to achieve highly customizable and controllable content generation
- ComfyUI is completely open source and can run on your local device

## [​](http://docs.comfy.org#getting-started-with-comfyui) Getting Started with ComfyUI

### [​](http://docs.comfy.org#comfyui-installation) ComfyUI Installation

ComfyUI currently offers multiple installation methods, supporting Windows, MacOS, and Linux systems:

ComfyUI Desktop (Recommended)

ComfyUI Desktop currently supports standalone installation for **Windows and MacOS (ARM)**, currently in Beta

- Code is open source on [Github](https://github.com/Comfy-Org/desktop)

You can choose the appropriate installation for your system and hardware below

- Windows
- MacOS(Apple Silicon)
- Linux

[**ComfyUI Desktop (Windows) Installation Guide**  
\
Suitable for **Windows** version with **Nvidia** GPU](http://docs.comfy.org/installation/desktop/windows)

[**ComfyUI Desktop (Windows) Installation Guide**  
\
Suitable for **Windows** version with **Nvidia** GPU](http://docs.comfy.org/installation/desktop/windows)

[**ComfyUI Desktop (MacOS) Installation Guide**  
\
Suitable for MacOS with **Apple Silicon**](http://docs.comfy.org/installation/desktop/macos)

ComfyUI Desktop **currently has no Linux prebuilds**, please visit the [Manual Installation](http://docs.comfy.org/installation/manual_install) section to install ComfyUI

ComfyUI Portable (Windows)

[**ComfyUI Portable (Windows) Installation Guide**  
\
Supports **Windows** ComfyUI version running on **Nvidia GPUs** or **CPU-only**, always use the latest commits and completely portable.](http://docs.comfy.org/installation/comfyui_portable_windows)

Manual Installation

[**ComfyUI Manual Installation Guide**  
\
Supports all system types and GPU types (Nvidia, AMD, Intel, Apple Silicon, Ascend NPU, Cambricon MLU)](http://docs.comfy.org/installation/manual_install)

## [​](http://docs.comfy.org#contributing-to-comfyui-ecosystem) Contributing to ComfyUI Ecosystem

If you’re planning to develop ComfyUI custom nodes (plugins), please read the following section.

[**Custom Node Development Guide**  
\
Learn how to build a custom node (plugin) for ComfyUI](http://docs.comfy.org/custom-nodes/overview)

## [​](http://docs.comfy.org#contributing-to-documentation) Contributing to Documentation

Fork the documentation [repo](https://github.com/comfyanonymous/ComfyUI) on Github and submit a PR to us

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/get_started/introduction.mdx)

[System RequirementsThis guide introduces some system requirements for ComfyUI, including hardware and software requirements  
\
Next](http://docs.comfy.org/installation/system_requirements)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [ComfyUI](http://docs.comfy.org#comfyui)
- [Getting Started with ComfyUI](http://docs.comfy.org#getting-started-with-comfyui)
- [ComfyUI Installation](http://docs.comfy.org#comfyui-installation)
- [Contributing to ComfyUI Ecosystem](http://docs.comfy.org#contributing-to-comfyui-ecosystem)
- [Contributing to Documentation](http://docs.comfy.org#contributing-to-documentation)

<!-- END Built_In_Node/get_started/introduction.md -->


<!-- BEGIN Built_In_Node/installation/comfyui_portable_windows.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
  
  - [System Requirements](http://docs.comfy.org/installation/system_requirements)
  - Desktop(recommended)
  - [ComfyUI(portable) Windows](http://docs.comfy.org/installation/comfyui_portable_windows)
  - [Manual Installation](http://docs.comfy.org/installation/manual_install)
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI(portable) Windows

# ComfyUI(portable) Windows

This tutorial will guide you on how to download and start using ComfyUI Portable and run the corresponding programs

For Nvidia 50 series (Blackwell) GPUs, please refer to the [System Requirements](http://docs.comfy.org/installation/nvidia-50-series) section to ensure your system meets the requirements for ComfyUI.

**ComfyUI Portable** is a standalone packaged complete ComfyUI Windows version that has integrated an independent **Python (python\_embeded)** required for ComfyUI to run. You only need to extract it to use it. Currently, the portable version supports running through **Nvidia GPU** or **CPU**.

This guide section will walk you through installing ComfyUI Portable.

## [​](http://docs.comfy.org#download-comfyui-portable) Download ComfyUI Portable

You can get the latest ComfyUI Portable download link by clicking the link below

[Download ComfyUI Portable](https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z)

After downloading, you can use decompression software like [7-ZIP](https://7-zip.org/) to extract the compressed package

The file structure and description after extracting the portable version are as follows:

```plaintext
ComfyUI_windows_portable
├── 📂ComfyUI                   // ComfyUI main program
├── 📂python_embeded            // Independent Python environment
├── 📂update                    // Batch scripts for upgrading portable version
├── README_VERY_IMPORTANT.txt   // ComfyUI Portable usage instructions in English
├── run_cpu.bat                 // Double click to start ComfyUI (CPU only)
└── run_nvidia_gpu.bat          // Double click to start ComfyUI (Nvidia GPU)
```

## [​](http://docs.comfy.org#how-to-launch-comfyui) How to Launch ComfyUI

Double click either `run_nvidia_gpu.bat` or `run_cpu.bat` depending on your computer’s configuration to launch ComfyUI. You will see the command running as shown in the image below

When you see something similar to the image

```plaintext
To see the GUI go to: http://127.0.0.1:8188
```

At this point, your ComfyUI service has started. Normally, ComfyUI will automatically open your default browser and navigate to `http://127.0.0.1:8188`. If it doesn’t open automatically, please manually open your browser and visit this address.

During use, please do not close the corresponding command line window, otherwise ComfyUI will stop running

## [​](http://docs.comfy.org#first-image-generation) First Image Generation

After successful installation, you can refer to the section below to start your ComfyUI journey~

[**First Image Generation**  
\
This tutorial will guide you through your first model installation and text-to-image generation](http://docs.comfy.org/get_started/first_generation)

## [​](http://docs.comfy.org#additional-comfyui-portable-instructions) Additional ComfyUI Portable Instructions

### [​](http://docs.comfy.org#1-upgrading-comfyui-portable) 1. Upgrading ComfyUI Portable

You can use the batch commands in the update folder to upgrade your ComfyUI Portable version

```plaintext
ComfyUI_windows_portable
└─ 📂update
   ├── update.py
   ├── update_comfyui.bat                          // Update ComfyUI to the latest commit version
   ├── update_comfyui_and_python_dependencies.bat  // Only use when you have issues with your runtime environment
   └── update_comfyui_stable.bat                   // Update ComfyUI to the latest stable version
```

### [​](http://docs.comfy.org#2-comfyui-model-sharing-and-custom-model-directory-configuration) 2. ComfyUI Model Sharing and Custom Model Directory Configuration

If you are also using [A1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui) or want to customize your model storage location, you can modify the following file to complete the configuration

```plaintext
ComfyUI_windows_portable
└─ 📂ComfyUI
  └── extra_model_paths.yaml.example  // This file is the configuration template
```

Please copy and rename the `extra_model_paths.yaml.example` to `extra_model_paths.yaml`.

Below is the original configuration file content, which you can modify according to your needs

```yaml
#Rename this to extra_model_paths.yaml and ComfyUI will load it


#config for a1111 ui
#all you have to do is change the base_path to where yours is installed
a111:
    base_path: path/to/stable-diffusion-webui/

    checkpoints: models/Stable-diffusion
    configs: models/Stable-diffusion
    vae: models/VAE
    loras: |
         models/Lora
         models/LyCORIS
    upscale_models: |
                  models/ESRGAN
                  models/RealESRGAN
                  models/SwinIR
    embeddings: embeddings
    hypernetworks: models/hypernetworks
    controlnet: models/ControlNet

#config for comfyui
#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.

#comfyui:
#     base_path: path/to/comfyui/
#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads
#     #is_default: true
#     checkpoints: models/checkpoints/
#     clip: models/clip/
#     clip_vision: models/clip_vision/
#     configs: models/configs/
#     controlnet: models/controlnet/
#     diffusion_models: |
#                  models/diffusion_models
#                  models/unet
#     embeddings: models/embeddings/
#     loras: models/loras/
#     upscale_models: models/upscale_models/
#     vae: models/vae/

#other_ui:
#    base_path: path/to/ui
#    checkpoints: models/checkpoints
#    gligen: models/gligen
#    custom_nodes: path/custom_nodes

```

For example, if your WebUI is located at `D:\stable-diffusion-webui\`, you can modify the corresponding configuration to

```yaml
a111:
    base_path: D:\stable-diffusion-webui\
    checkpoints: models/Stable-diffusion
    configs: models/Stable-diffusion
    vae: models/VAE
    loras: |
         models/Lora
         models/LyCORIS
    upscale_models: |
                  models/ESRGAN
                  models/RealESRGAN
                  models/SwinIR
    embeddings: embeddings
    hypernetworks: models/hypernetworks
    controlnet: models/ControlNet
```

This way, models under paths like `D:\stable-diffusion-webui\models\Stable-diffusion\` can be detected and used by ComfyUI. Similarly, you can add other custom model location configurations

### [​](http://docs.comfy.org#3-setting-up-lan-access-for-comfyui-portable) 3. Setting Up LAN Access for ComfyUI Portable

If your ComfyUI is running on a local network and you want other devices to access ComfyUI, you can modify the `run_nvidia_gpu.bat` or `run_cpu.bat` file using Notepad to complete the configuration. This is mainly done by adding `--listen` to specify the listening address. Below is an example of the `run_nvidia_gpu.bat` file command with the `--listen` parameter added

```plaintext
.\python_embeded\python.exe -s ComfyUI\main.py --listen --windows-standalone-build
pause
```

After enabling ComfyUI, you will notice the final running address will become

```plaintext
Starting server

To see the GUI go to: http://0.0.0.0:8188
To see the GUI go to: http://[::]:8188
```

You can press `WIN + R` and type `cmd` to open the command prompt, then enter `ipconfig` to view your local IP address. Other devices can then access ComfyUI by entering `http://your-local-IP:8188` in their browser.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/installation/comfyui_portable_windows.mdx)

[Previous](http://docs.comfy.org/installation/desktop/linux)

[Manual Installation  
\
Next](http://docs.comfy.org/installation/manual_install)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Download ComfyUI Portable](http://docs.comfy.org#download-comfyui-portable)
- [How to Launch ComfyUI](http://docs.comfy.org#how-to-launch-comfyui)
- [First Image Generation](http://docs.comfy.org#first-image-generation)
- [Additional ComfyUI Portable Instructions](http://docs.comfy.org#additional-comfyui-portable-instructions)
- [1. Upgrading ComfyUI Portable](http://docs.comfy.org#1-upgrading-comfyui-portable)
- [2. ComfyUI Model Sharing and Custom Model Directory Configuration](http://docs.comfy.org#2-comfyui-model-sharing-and-custom-model-directory-configuration)
- [3. Setting Up LAN Access for ComfyUI Portable](http://docs.comfy.org#3-setting-up-lan-access-for-comfyui-portable)

<!-- END Built_In_Node/installation/comfyui_portable_windows.md -->


<!-- BEGIN Built_In_Node/installation/desktop/linux.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
  
  - [System Requirements](http://docs.comfy.org/installation/system_requirements)
  - Desktop(recommended)
    
    - [Windows Desktop Version](http://docs.comfy.org/installation/desktop/windows)
    - [MacOS Desktop Version](http://docs.comfy.org/installation/desktop/macos)
    - [Linux Desktop Version](http://docs.comfy.org/installation/desktop/linux)
  - [ComfyUI(portable) Windows](http://docs.comfy.org/installation/comfyui_portable_windows)
  - [Manual Installation](http://docs.comfy.org/installation/manual_install)
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Linux Desktop Version

# Linux Desktop Version

This article introduces how to download, install and use ComfyUI Desktop for Linux

Linux pre-built packages are not yet available. Please try [manual installation.](http://docs.comfy.org/installation/manual_install)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/installation/desktop/linux.mdx)

[Previous](http://docs.comfy.org/installation/desktop/macos)

[ComfyUI(portable) WindowsThis tutorial will guide you on how to download and start using ComfyUI Portable and run the corresponding programs  
\
Next](http://docs.comfy.org/installation/comfyui_portable_windows)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

<!-- END Built_In_Node/installation/desktop/linux.md -->


<!-- BEGIN Built_In_Node/installation/desktop/macos.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
  
  - [System Requirements](http://docs.comfy.org/installation/system_requirements)
  - Desktop(recommended)
    
    - [Windows Desktop Version](http://docs.comfy.org/installation/desktop/windows)
    - [MacOS Desktop Version](http://docs.comfy.org/installation/desktop/macos)
    - [Linux Desktop Version](http://docs.comfy.org/installation/desktop/linux)
  - [ComfyUI(portable) Windows](http://docs.comfy.org/installation/comfyui_portable_windows)
  - [Manual Installation](http://docs.comfy.org/installation/manual_install)
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

MacOS Desktop Version

# MacOS Desktop Version

This article introduces how to download, install and use ComfyUI Desktop for MacOS

**ComfyUI Desktop** is a standalone installation version that can be installed like regular software. It supports quick installation and automatic configuration of the **Python environment and dependencies**, and supports one-click import of existing ComfyUI settings, models, workflows, and files.

ComfyUI Desktop is an open source project, please visit the full code [here](https://github.com/Comfy-Org/desktop).

ComfyUI Desktop (MacOS) only supports Apple Silicon

This tutorial will guide you through the software installation process and explain related configuration details.

As **ComfyUI Desktop** is still in **Beta** status, the actual installation process may change

## [​](http://docs.comfy.org#comfyui-desktop-macos-download) ComfyUI Desktop (MacOS) Download

Please click the button below to download the installation package for MacOS **ComfyUI Desktop**

[Download for MacOS](https://download.comfy.org/mac/dmg/arm64)

## [​](http://docs.comfy.org#comfyui-desktop-installation-steps) ComfyUI Desktop Installation Steps

Double-click the downloaded installation package file. As shown in the image, drag the **ComfyUI** application into the **Applications** folder following the arrow

If your folder shows as below with a prohibition sign on the icon after opening the installation package, it means your current system version is not compatible with **ComfyUI Desktop**

Then find the **ComfyUI icon** in **Launchpad** and click it to enter ComfyUI initialization settings

## [​](http://docs.comfy.org#comfyui-desktop-initialization-process) ComfyUI Desktop Initialization Process

1

Start Screen

- Normal Start
- Maintenance Page

Click **Get Started** to begin initialization

Click **Get Started** to begin initialization

There are many reasons you might have issues installing ComfyUI. Maybe a network connection failed when installing pytorch (15 GB). Or you don’t have git installed. The maintenance page automatically opens when it detects an issue and provides a way to resolve the issue.

You can use it to resolve most issues:

- Create a python virtual environment
- Reinstall all missing core dependencies to your Python virtual environment that’s managed by Desktop
- Install git, VC redis
- Choose a new install location

The default maintenance page displays the current error content

Clicking `All` allows you to view all the content that can be operated on currently

2

Select GPU

The three options are:

1. **MPS (Recommended):** Metal Performance Shaders (MPS) is an Apple framework that uses GPUs to accelerate computing and machine learning tasks on Apple devices, supporting frameworks like PyTorch.
2. **Manual Configuration:** You need to manually install and configure the python runtime environment. Don’t select this unless you know how to configure
3. **Enable CPU Mode:** For developers and special cases only. Don’t select this unless you’re sure you need it

Unless there are special circumstances, please select **MPS** as shown and click **Next** to proceed

3

Install location

In this step, you will select the installation location for the following related content of ComfyUI:

- **Python Environment**
- **Models Model Files**
- **Custom Nodes Custom Nodes**

Recommendations:

- Please create a separate empty folder as the installation directory for ComfyUI
- Please ensure that the disk has at least **5G** of disk space to ensure the normal installation of **ComfyUI Desktop**

Not all files are installed in this directory, some files will be located in the MacOS system directory, you can refer to the uninstallation section of this guide to complete the uninstallation of the ComfyUI desktop version

4

Migrate from Existing Installation (Optional)

In this step you can migrate your existing ComfyUI installation content to ComfyUI Desktop. Select your existing ComfyUI installation directory, and the installer will automatically recognize:

- **User Files**
- **Models:** Will not be copied, only linked with desktop version
- **Custom Nodes:** Nodes will be reinstalled

Don’t worry, this step won’t copy model files. You can check or uncheck options as needed. Click **Next** to continue

5

Desktop Settings

These are preference settings:

1. **Automatic Updates:** Whether to set automatic updates when ComfyUI updates are available
2. **Usage Metrics:** If enabled, we will collect **anonymous usage data** to help improve ComfyUI
3. **Mirror Settings:** Since the program needs internet access to download Python and complete environment installation, if you see a red ❌ during installation indicating this may cause installation failure, please follow the steps below

Expand the mirror settings to find the specific failing mirror. In this screenshot the error is **Python Install Mirror** failure.

For different mirror errors, you can refer to the following content to try to manually find different mirrors and replace them

The following cases mainly apply to users in China.

#### [​](http://docs.comfy.org#python-installation-mirror) Python Installation Mirror

If the default mirror is unavailable, please try using the mirror below.

```plaintext
https://python-standalone.org/mirror/astral-sh/python-build-standalone
```

If you need to find other alternative GitHub mirror addresses, please look for and construct a mirror address pointing to the releases of the `python-build-standalone` repository.

```plaintext
https://github.com/astral-sh/python-build-standalone/releases/download
```

Build a link in the following pattern

```plaintext
https://xxx/astral-sh/python-build-standalone/releases/download
```

#### [​](http://docs.comfy.org#pypi-mirror) PyPI Mirror

- Alibaba Cloud: [https://mirrors.aliyun.com/pypi/simple/](https://mirrors.aliyun.com/pypi/simple/)
- Tencent Cloud: [https://mirrors.cloud.tencent.com/pypi/simple/](https://mirrors.cloud.tencent.com/pypi/simple/)
- University of Science and Technology of China: [https://pypi.mirrors.ustc.edu.cn/simple/](https://pypi.mirrors.ustc.edu.cn/simple/)
- Shanghai Jiao Tong University: [https://pypi.sjtu.edu.cn/simple/](https://pypi.sjtu.edu.cn/simple/)

#### [​](http://docs.comfy.org#torch-mirror) Torch Mirror

- Aliyun: [https://mirrors.aliyun.com/pytorch-wheels/cu121/](https://mirrors.aliyun.com/pytorch-wheels/cu121/)

6

Complete the installation

If everything is correct, the installer will complete and automatically enter the ComfyUI Desktop interface, then the installation is successful

## [​](http://docs.comfy.org#first-image-generation) First Image Generation

After successful installation, you can refer to the section below to start your ComfyUI journey~

[**First Image Generation**  
\
This tutorial will guide you through your first model installation and text-to-image generation](http://docs.comfy.org/get_started/first_generation)

## [​](http://docs.comfy.org#how-to-update-comfyui-desktop) How to Update ComfyUI Desktop

Currently, ComfyUI Desktop updates use automatic detection updates, please ensure that automatic updates are enabled in the settings

You can also choose to manually check for available updates in the `Menu` —&gt; `Help` —&gt; `Check for Updates`

## [​](http://docs.comfy.org#how-to-uninstall-comfyui-desktop) How to Uninstall ComfyUI Desktop

For **ComfyUI Desktop**, you can directly delete **ComfyUI** from the **Applications** folder

If you want to completely remove all **ComfyUI Desktop** files, you can manually delete these folders:

- /Users/Library/Application Support/ComfyUI

The above operations will not delete your following folders. If you need to delete corresponding files, please delete manually:

- models files
- custom nodes
- input/output directories

## [​](http://docs.comfy.org#troubleshooting) Troubleshooting

### [​](http://docs.comfy.org#%E2%80%8Berror-identification%E2%80%8B) ​Error identification​

If installation fails, you should see the following screen

It is recommended to take these steps to find the error cause:

1. Click `Show Terminal` to view error output
2. Click `Open Logs` to view installation logs
3. Visit official forum to search for error reports
4. Click `Reinstall` to try reinstalling

Before submitting feedback, it’s recommended to provide the **error output** and **log files** to tools like **GPT**

As shown above, ask the GPT for the cause of the corresponding error, or remove ComfyUI completely and retry the installation.

### [​](http://docs.comfy.org#feedback-installation-failure) Feedback Installation Failure

If you encounter any errors during installation, please check if there are similar error reports or submit errors to us through:

- Github Issues: [https://github.com/Comfy-Org/desktop/issues](https://github.com/Comfy-Org/desktop/issues)
- Comfy Official Forum: [https://forum.comfy.org/](https://forum.comfy.org/)

When submitting error reports, please ensure you include the following logs and configuration files to help us locate and investigate the issue:

1. Log Files

FilenameDescriptionLocationmain.logContains logs related to desktop application and server startup from the Electron processcomfyui.logContains logs related to ComfyUI normal operation, such as core ComfyUI process terminal output

2. Configuration Files

FilenameDescriptionLocationextra\_models\_config.yamlContains additional paths where ComfyUI will search for models and custom nodesconfig.jsonContains application configuration. This file should not be edited directly

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/installation/desktop/macos.mdx)

[Previous](http://docs.comfy.org/installation/desktop/windows)

[Linux Desktop VersionThis article introduces how to download, install and use ComfyUI Desktop for Linux  
\
Next](http://docs.comfy.org/installation/desktop/linux)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [ComfyUI Desktop (MacOS) Download](http://docs.comfy.org#comfyui-desktop-macos-download)
- [ComfyUI Desktop Installation Steps](http://docs.comfy.org#comfyui-desktop-installation-steps)
- [ComfyUI Desktop Initialization Process](http://docs.comfy.org#comfyui-desktop-initialization-process)
- [First Image Generation](http://docs.comfy.org#first-image-generation)
- [How to Update ComfyUI Desktop](http://docs.comfy.org#how-to-update-comfyui-desktop)
- [How to Uninstall ComfyUI Desktop](http://docs.comfy.org#how-to-uninstall-comfyui-desktop)
- [Troubleshooting](http://docs.comfy.org#troubleshooting)
- [​Error identification​](http://docs.comfy.org#%E2%80%8Berror-identification%E2%80%8B)
- [Feedback Installation Failure](http://docs.comfy.org#feedback-installation-failure)

<!-- END Built_In_Node/installation/desktop/macos.md -->


<!-- BEGIN Built_In_Node/installation/desktop/windows.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
  
  - [System Requirements](http://docs.comfy.org/installation/system_requirements)
  - Desktop(recommended)
    
    - [Windows Desktop Version](http://docs.comfy.org/installation/desktop/windows)
    - [MacOS Desktop Version](http://docs.comfy.org/installation/desktop/macos)
    - [Linux Desktop Version](http://docs.comfy.org/installation/desktop/linux)
  - [ComfyUI(portable) Windows](http://docs.comfy.org/installation/comfyui_portable_windows)
  - [Manual Installation](http://docs.comfy.org/installation/manual_install)
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Windows Desktop Version

# Windows Desktop Version

This article introduces how to download, install and use ComfyUI Desktop for Windows

**ComfyUI Desktop** is a standalone installation version that can be installed like regular software. It supports quick installation and automatic configuration of the **Python environment and dependencies**, and supports one-click import of existing ComfyUI settings, models, workflows, and files. You can quickly migrate from an existing [ComfyUI Portable version](http://docs.comfy.org/installation/comfyui_portable_windows) to the Desktop version.

ComfyUI Desktop is an open source project, please visit the full code [here](https://github.com/Comfy-Org/desktop)

ComfyUI Desktop hardware requirements:

- NVIDIA GPU

This tutorial will guide you through the software installation process and explain related configuration details.

As **ComfyUI Desktop** is still in **Beta** status, the actual installation process may change

## [​](http://docs.comfy.org#comfyui-desktop-windows-download) ComfyUI Desktop (Windows) Download

Please click the button below to download the installation package for Windows **ComfyUI Desktop**

[Download for Windows (NVIDIA)](https://download.comfy.org/windows/nsis/x64)

## [​](http://docs.comfy.org#comfyui-desktop-installation-steps) ComfyUI Desktop Installation Steps

Double-click the downloaded installation package file, which will first perform an automatic installation and create a **ComfyUI Desktop** shortcut on the desktop

Double-click the corresponding shortcut to enter ComfyUI initialization settings

### [​](http://docs.comfy.org#comfyui-desktop-initialization-process) ComfyUI Desktop Initialization Process

1

Start Screen

- Normal Start
- Maintenance Page

Click **Get Started** to begin initialization

Click **Get Started** to begin initialization

There are many reasons you might have issues installing ComfyUI. Maybe a network connection failed when installing pytorch (15 GB). Or you don’t have git installed. The maintenance page automatically opens when it detects an issue and provides a way to resolve the issue.

You can use it to resolve most issues:

- Create a python virtual environment
- Reinstall all missing core dependencies to your Python virtual environment that’s managed by Desktop
- Install git, VC redis
- Choose a new install location

The default maintenance page displays the current error content

Clicking `All` allows you to view all the content that can be operated on currently

2

Select GPU

The three options are:

1. **Nvidia GPU (Recommended):** Direct support for pytorch and CUDA
2. **Manual Configuration:** You need to manually install and configure the python runtime environment. Don’t select this unless you know how to configure
3. **Enable CPU Mode:** For developers and special cases only. Don’t select this unless you’re sure you need it

Unless there are special circumstances, please select **NVIDIA** as shown and click **Next** to proceed

3

Install location

In this step, you will select the installation location for the following ComfyUI content:

- **Python Environment**
- **Models Model Files**
- **Custom Nodes Custom Nodes**

Recommendations:

- Please select a **solid-state drive** as the installation location, which will increase ComfyUI’s performance when accessing models.
- Please create a separate empty folder as the ComfyUI installation directory
- Please ensure that the corresponding disk has at least around **15G** of disk space to ensure the installation of ComfyUI Desktop

Not all files are installed in this directory, some files will still be installed on the C drive, and if you need to uninstall in the future, you can refer to the uninstallation section of this guide to complete the full uninstallation of ComfyUI Desktop

After completing this step, click **Next** to proceed to the next step

4

Migrate from Existing Installation (Optional)

In this step you can migrate your existing ComfyUI installation content to ComfyUI Desktop. As shown, I selected my original **D:\\ComfyUI\_windows\_portable\\ComfyUI** installation directory. The installer will automatically recognize:

- **User Files**
- **Models:** Will not be copied, only linked with desktop version
- **Custom Nodes:** Nodes will be reinstalled

Don’t worry, this step won’t copy model files. You can check or uncheck options as needed. Click **Next** to continue

5

Desktop Settings

These are preference settings:

1. **Automatic Updates:** Whether to set automatic updates when ComfyUI updates are available
2. **Usage Metrics:** If enabled, we will collect **anonymous usage data** to help improve ComfyUI
3. **Mirror Settings:** Since the program needs internet access to download Python and complete environment installation, if you see a red ❌ during installation indicating this may cause installation failure, please follow the steps below

Expand the mirror settings to find the specific failing mirror. In this screenshot the error is **Python Install Mirror** failure.

For different mirror errors, you can refer to the following content to try to manually find different mirrors and replace them

The following cases mainly apply to users in China.

#### [​](http://docs.comfy.org#python-installation-mirror) Python Installation Mirror

If the default mirror is unavailable, please try using the mirror below.

```plaintext
https://python-standalone.org/mirror/astral-sh/python-build-standalone
```

If you need to find other alternative GitHub mirror addresses, please look for and construct a mirror address pointing to the releases of the `python-build-standalone` repository.

```plaintext
https://github.com/astral-sh/python-build-standalone/releases/download
```

Build a link in the following pattern

```plaintext
https://xxx/astral-sh/python-build-standalone/releases/download
```

#### [​](http://docs.comfy.org#pypi-mirror) PyPI Mirror

- Alibaba Cloud: [https://mirrors.aliyun.com/pypi/simple/](https://mirrors.aliyun.com/pypi/simple/)
- Tencent Cloud: [https://mirrors.cloud.tencent.com/pypi/simple/](https://mirrors.cloud.tencent.com/pypi/simple/)
- University of Science and Technology of China: [https://pypi.mirrors.ustc.edu.cn/simple/](https://pypi.mirrors.ustc.edu.cn/simple/)
- Shanghai Jiao Tong University: [https://pypi.sjtu.edu.cn/simple/](https://pypi.sjtu.edu.cn/simple/)

#### [​](http://docs.comfy.org#torch-mirror) Torch Mirror

- Aliyun: [https://mirrors.aliyun.com/pytorch-wheels/cu121/](https://mirrors.aliyun.com/pytorch-wheels/cu121/)

6

Complete the installation

If everything is correct, the installer will complete and automatically enter the ComfyUI Desktop interface, then the installation is successful

## [​](http://docs.comfy.org#first-image-generation) First Image Generation

After successful installation, you can refer to the section below to start your ComfyUI journey~

[**First Image Generation**  
\
This tutorial will guide you through your first model installation and text-to-image generation](http://docs.comfy.org/get_started/first_generation)

## [​](http://docs.comfy.org#how-to-update-comfyui-desktop) How to Update ComfyUI Desktop

Currently, ComfyUI Desktop updates use automatic detection updates, please ensure that automatic updates are enabled in the settings

You can also choose to manually check for available updates in the `Menu` —&gt; `Help` —&gt; `Check for Updates`

## [​](http://docs.comfy.org#how-to-uninstall-comfyui-desktop) How to Uninstall ComfyUI Desktop

For **ComfyUI Desktop** you can use the system uninstall function in Windows Settings to complete software uninstallation

If you want to completely remove all **ComfyUI Desktop** files, you can manually delete these folders:

- C:\\Users&lt;your username&gt;\\AppData\\Local@comfyorgcomfyui-electron-updater
- C:\\Users&lt;your username&gt;\\AppData\\Local\\Programs@comfyorgcomfyui-electron
- C:\\Users&lt;your username&gt;\\AppData\\Roaming\\ComfyUI

The above operations will not delete your following folders. If you need to delete corresponding files, please delete manually:

- models files
- custom nodes
- input/output directories

## [​](http://docs.comfy.org#troubleshooting) Troubleshooting

### [​](http://docs.comfy.org#display-unsupported-devices) Display unsupported devices

Since ComfyUI Desktop (Windows) only supports **NVIDIA GPUs with CUDA**, you may see this screen if your device is not supported

- Please switch to a supported device
- Or consider using [ComfyUI Portable](http://docs.comfy.org/installation/comfyui_portable_windows) or through [manual installation](http://docs.comfy.org/installation/manual_install) to use ComfyUI

### [​](http://docs.comfy.org#%E2%80%8Berror-identification%E2%80%8B) ​Error identification​

If installation fails, you should see the following screen

It is recommended to take these steps to find the error cause:

1. Click `Show Terminal` to view error output
2. Click `Open Logs` to view installation logs
3. Visit official forum to search for error reports
4. Click `Reinstall` to try reinstalling

Before submitting feedback, it’s recommended to provide the **error output** and **log files** to tools like **GPT**

As shown above, ask the GPT for the cause of the corresponding error, or remove ComfyUI completely and retry the installation.

### [​](http://docs.comfy.org#feedback-installation-failure) Feedback Installation Failure

If you encounter any errors during installation, please check if there are similar error reports or submit errors to us through:

- Github Issues: [https://github.com/Comfy-Org/desktop/issues](https://github.com/Comfy-Org/desktop/issues)
- Comfy Official Forum: [https://forum.comfy.org/](https://forum.comfy.org/)

When submitting error reports, please ensure you include the following logs and configuration files to help us locate and investigate the issue:

1. Log Files

FilenameDescriptionLocationmain.logContains logs related to desktop application and server startup from the Electron processcomfyui.logContains logs related to ComfyUI normal operation, such as core ComfyUI process terminal output

2. Configuration Files

FilenameDescriptionLocationextra\_models\_config.yamlContains additional paths where ComfyUI will search for models and custom nodesconfig.jsonContains application configuration. This file should not be edited directly

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/installation/desktop/windows.mdx)

[Previous](http://docs.comfy.org/installation/system_requirements)

[MacOS Desktop VersionThis article introduces how to download, install and use ComfyUI Desktop for MacOS  
\
Next](http://docs.comfy.org/installation/desktop/macos)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [ComfyUI Desktop (Windows) Download](http://docs.comfy.org#comfyui-desktop-windows-download)
- [ComfyUI Desktop Installation Steps](http://docs.comfy.org#comfyui-desktop-installation-steps)
- [ComfyUI Desktop Initialization Process](http://docs.comfy.org#comfyui-desktop-initialization-process)
- [First Image Generation](http://docs.comfy.org#first-image-generation)
- [How to Update ComfyUI Desktop](http://docs.comfy.org#how-to-update-comfyui-desktop)
- [How to Uninstall ComfyUI Desktop](http://docs.comfy.org#how-to-uninstall-comfyui-desktop)
- [Troubleshooting](http://docs.comfy.org#troubleshooting)
- [Display unsupported devices](http://docs.comfy.org#display-unsupported-devices)
- [​Error identification​](http://docs.comfy.org#%E2%80%8Berror-identification%E2%80%8B)
- [Feedback Installation Failure](http://docs.comfy.org#feedback-installation-failure)

<!-- END Built_In_Node/installation/desktop/windows.md -->


<!-- BEGIN Built_In_Node/installation/manual_install.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
  
  - [System Requirements](http://docs.comfy.org/installation/system_requirements)
  - Desktop(recommended)
  - [ComfyUI(portable) Windows](http://docs.comfy.org/installation/comfyui_portable_windows)
  - [Manual Installation](http://docs.comfy.org/installation/manual_install)
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Manual Installation

# Manual Installation

For Nvidia 50 series (Blackwell) GPUs, please refer to the [System Requirements](http://docs.comfy.org/installation/nvidia-50-series) section to ensure your system meets the requirements for ComfyUI.

- Windows
- Linux
- MacOS

### [​](http://docs.comfy.org#clone-the-repository) Clone the repository

```bash
git clone git@github.com:comfyanonymous/ComfyUI.git
```

More [info](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) on git clone.

If you have not installed Microsoft Visual C++ Redistributable, please install it [here.](https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist?view=msvc-170)

### [​](http://docs.comfy.org#install-dependencies) Install Dependencies

1. [Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI.
   
   Create an environment with Conda.
   
   ```plaintext
   conda create -n comfyenv
   conda activate comfyenv
   ```
2. Install GPU Dependencies
   
   Nvidia
   
   ```plaintext
   conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
   ```
   
   Alternatively, you can install the nightly version of PyTorch.
   
   Install Nightly
   
   Install Nightly version (might be more risky)
   
   ```plaintext
   conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch-nightly -c nvidia
   ```
   
   AMD
   
   ```plaintext
   pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
   ```
   
   Alternatively, you can install the nightly version of PyTorch.
   
   Install Nightly
   
   Install Nightly version (might be more risky)
   
   ```plaintext
   pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0
   ```
   
   Mac ARM Silicon
   
   ```bash
   conda install pytorch-nightly::pytorch torchvision torchaudio -c pytorch-nightly
   ```
3. ```bash
   cd ComfyUI
   pip install -r requirements.txt
   ```
4. Start the application
   
   ```plaintext
   cd ComfyUI
   python main.py
   ```

### [​](http://docs.comfy.org#clone-the-repository) Clone the repository

```bash
git clone git@github.com:comfyanonymous/ComfyUI.git
```

More [info](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) on git clone.

If you have not installed Microsoft Visual C++ Redistributable, please install it [here.](https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist?view=msvc-170)

### [​](http://docs.comfy.org#install-dependencies) Install Dependencies

1. [Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI.
   
   Create an environment with Conda.
   
   ```plaintext
   conda create -n comfyenv
   conda activate comfyenv
   ```
2. Install GPU Dependencies
   
   Nvidia
   
   ```plaintext
   conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
   ```
   
   Alternatively, you can install the nightly version of PyTorch.
   
   Install Nightly
   
   Install Nightly version (might be more risky)
   
   ```plaintext
   conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch-nightly -c nvidia
   ```
   
   AMD
   
   ```plaintext
   pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
   ```
   
   Alternatively, you can install the nightly version of PyTorch.
   
   Install Nightly
   
   Install Nightly version (might be more risky)
   
   ```plaintext
   pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0
   ```
   
   Mac ARM Silicon
   
   ```bash
   conda install pytorch-nightly::pytorch torchvision torchaudio -c pytorch-nightly
   ```
3. ```bash
   cd ComfyUI
   pip install -r requirements.txt
   ```
4. Start the application
   
   ```plaintext
   cd ComfyUI
   python main.py
   ```

### [​](http://docs.comfy.org#clone-the-repository-2) Clone the repository

```bash
git clone git@github.com:comfyanonymous/ComfyUI.git
```

More [info](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) on git clone.

### [​](http://docs.comfy.org#install-dependencies-2) Install Dependencies

1. [Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI.
   
   Create an environment with Conda.
   
   ```plaintext
   conda create -n comfyenv
   conda activate comfyenv
   ```
2. Install GPU Dependencies
   
   Nvidia
   
   ```plaintext
   conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
   ```
   
   Alternatively, you can install the nightly version of PyTorch.
   
   Install Nightly
   
   Install Nightly version (might be more risky)
   
   ```plaintext
   conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch-nightly -c nvidia
   ```
   
   AMD
   
   ```plaintext
   pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
   ```
   
   Alternatively, you can install the nightly version of PyTorch.
   
   Install Nightly
   
   Install Nightly version (might be more risky)
   
   ```plaintext
   pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0
   ```
   
   Mac ARM Silicon
   
   ```bash
   conda install pytorch-nightly::pytorch torchvision torchaudio -c pytorch-nightly
   ```
3. ```bash
   cd ComfyUI
   pip install -r requirements.txt
   ```
4. Start the application
   
   ```plaintext
   cd ComfyUI
   python main.py
   ```

### [​](http://docs.comfy.org#clone-the-repository-3) Clone the repository

Open [Terminal application](https://support.apple.com/guide/terminal/open-or-quit-terminal-apd5265185d-f365-44cb-8b09-71a064a42125/mac).

```bash
git clone git@github.com:comfyanonymous/ComfyUI.git
```

More [info](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) on git clone.

### [​](http://docs.comfy.org#install-dependencies-3) Install Dependencies

1. [Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI.
   
   Create an environment with Conda.
   
   ```plaintext
   conda create -n comfyenv
   conda activate comfyenv
   ```
2. Install GPU Dependencies
   
   Nvidia
   
   ```plaintext
   conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
   ```
   
   Alternatively, you can install the nightly version of PyTorch.
   
   Install Nightly
   
   Install Nightly version (might be more risky)
   
   ```plaintext
   conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch-nightly -c nvidia
   ```
   
   AMD
   
   ```plaintext
   pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
   ```
   
   Alternatively, you can install the nightly version of PyTorch.
   
   Install Nightly
   
   Install Nightly version (might be more risky)
   
   ```plaintext
   pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0
   ```
   
   Mac ARM Silicon
   
   ```bash
   conda install pytorch-nightly::pytorch torchvision torchaudio -c pytorch-nightly
   ```
3. ```bash
   cd ComfyUI
   pip install -r requirements.txt
   ```
4. Start the application
   
   ```plaintext
   cd ComfyUI
   python main.py
   ```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/installation/manual_install.mdx)

[Previous](http://docs.comfy.org/installation/comfyui_portable_windows)

[First Image GenerationThis tutorial will guide you through your first image generation with ComfyUI, covering basic interface operations like workflow loading, model installation, and image generation  
\
Next](http://docs.comfy.org/get_started/first_generation)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

<!-- END Built_In_Node/installation/manual_install.md -->


<!-- BEGIN Built_In_Node/installation/system_requirements.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
  
  - [System Requirements](http://docs.comfy.org/installation/system_requirements)
  - Desktop(recommended)
  - [ComfyUI(portable) Windows](http://docs.comfy.org/installation/comfyui_portable_windows)
  - [Manual Installation](http://docs.comfy.org/installation/manual_install)
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

System Requirements

# System Requirements

This guide introduces some system requirements for ComfyUI, including hardware and software requirements

In this guide, we will introduce the system requirements for installing ComfyUI. Due to frequent updates of ComfyUI, this document may not be updated in a timely manner. Please refer to the relevant instructions in [ComfyUI](https://github.com/comfyanonymous/ComfyUI).

Regardless of which version of ComfyUI you use, it runs in a separate Python environment.

You can refer to the following sections to learn about the installation methods for different systems and versions of ComfyUI. In the installation of different versions, we have simply described the system requirements.

ComfyUI Desktop (Recommended)

ComfyUI Desktop currently supports standalone installation for **Windows and MacOS (ARM)**, currently in Beta

- Code is open source on [Github](https://github.com/Comfy-Org/desktop)

You can choose the appropriate installation for your system and hardware below

- Windows
- MacOS(Apple Silicon)
- Linux

[**ComfyUI Desktop (Windows) Installation Guide**  
\
Suitable for **Windows** version with **Nvidia** GPU](http://docs.comfy.org/installation/desktop/windows)

[**ComfyUI Desktop (Windows) Installation Guide**  
\
Suitable for **Windows** version with **Nvidia** GPU](http://docs.comfy.org/installation/desktop/windows)

[**ComfyUI Desktop (MacOS) Installation Guide**  
\
Suitable for MacOS with **Apple Silicon**](http://docs.comfy.org/installation/desktop/macos)

ComfyUI Desktop **currently has no Linux prebuilds**, please visit the [Manual Installation](http://docs.comfy.org/installation/manual_install) section to install ComfyUI

ComfyUI Portable (Windows)

[**ComfyUI Portable (Windows) Installation Guide**  
\
Supports **Windows** ComfyUI version running on **Nvidia GPUs** or **CPU-only**, always use the latest commits and completely portable.](http://docs.comfy.org/installation/comfyui_portable_windows)

Manual Installation

[**ComfyUI Manual Installation Guide**  
\
Supports all system types and GPU types (Nvidia, AMD, Intel, Apple Silicon, Ascend NPU, Cambricon MLU)](http://docs.comfy.org/installation/manual_install)

## [​](http://docs.comfy.org#nvidia-50-series-gpu-requirements) Nvidia 50 Series GPU Requirements

To make your Nvidia 50 series GPU (Blackwell architecture) work properly with ComfyUI, you need a PyTorch version that supports CUDA 12.8 or newer. Currently (March 2025), the stable version of PyTorch does not yet support the Blackwell architecture, so you need to use a nightly build version.

Related discussions on this issue are concentrated [here](https://github.com/comfyanonymous/ComfyUI/discussions/6643).

### [​](http://docs.comfy.org#for-windows-users) For Windows Users

**Recommended Option:** Download the standalone ComfyUI Portable version with nightly pytorch 2.7 cu128:

- [Click here to download](https://github.com/comfyanonymous/ComfyUI/releases/download/latest/ComfyUI_windows_portable_nvidia_or_cpu_nightly_pytorch.7z)

**Other Options:** Older torch 2.6 Windows package:

- [Click here to download the standalone ComfyUI package with a cuda 12.8 torch build](https://github.com/comfyanonymous/ComfyUI/releases/download/latest/ComfyUI_cu128_50XX.7z)

### [​](http://docs.comfy.org#manual-installation) Manual Installation

Windows and Linux users can install the PyTorch nightly version using the following command:

```bash
pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128
```

### [​](http://docs.comfy.org#docker-container-alternative) Docker Container Alternative

You can try the PyTorch container provided by Nvidia, which might offer better performance.

Container address: [https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch)

Usage method:

```bash
docker run -p 8188:8188 --gpus all -it --rm nvcr.io/nvidia/pytorch:25.01-py3
```

Inside the Docker container, execute the following commands:

```bash
git clone https://github.com/comfyanonymous/ComfyUI
cd ComfyUI
grep -v 'torchaudio\|torchvision' requirements.txt > temp_requirements.txt
pip install -r temp_requirements.txt
python main.py --listen
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/installation/system_requirements.mdx)

[Previous](http://docs.comfy.org/get_started/introduction)

[Windows Desktop VersionThis article introduces how to download, install and use ComfyUI Desktop for Windows  
\
Next](http://docs.comfy.org/installation/desktop/windows)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Nvidia 50 Series GPU Requirements](http://docs.comfy.org#nvidia-50-series-gpu-requirements)
- [For Windows Users](http://docs.comfy.org#for-windows-users)
- [Manual Installation](http://docs.comfy.org#manual-installation)
- [Docker Container Alternative](http://docs.comfy.org#docker-container-alternative)

<!-- END Built_In_Node/installation/system_requirements.md -->


<!-- BEGIN Built_In_Node/interface/credits.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Credits Management

# Credits Management

In this article, we will introduce ComfyUI’s credit management features, including how to obtain, use, and view credits.

The credit system was added to support the `API Nodes`, as calling closed-source AI models requires token consumption, so proper credit management is necessary. By default, the credits interface is not displayed. Please first log in to your ComfyUI account in `Settings` -&gt; `User`, and then you can view your associated account’s credit information in `Settings` -&gt; `Credits`.

ComfyUI will always remain fully open-source and free for local users.

## [​](http://docs.comfy.org#how-to-purchase-credits%3F) How to Purchase Credits?

Below is a demonstration video for purchasing credits:

Detailed steps are as follows:

1

Log in to your ComfyUI account

Log in to your ComfyUI account in `Settings` -&gt; `User`

2

Go to \`Settings\` -&gt; \`Credits\` to purchase credits

After logging in, you should see the `Credits` option added to the menu

Go to `Settings` -&gt; `Credits` to purchase credits

3

Set the amount of credits to purchase

In the popup, set the purchase amount and click the `Buy` button

4

Make payment through Stripe

On the payment page, please follow these steps:

1. Select the currency for payment
2. Confirm that the email is the same as your ComfyUI registration email
3. Choose your payment method

<!--THE END-->

- Credit Card
- WeChat (only supported when paying in USD)

<!--THE END-->

4. Click the `Pay` button or the `Generate QR Code` button to complete the payment process

5

Complete payment and check your credit balance

After completing the payment, please return to `Menu` -&gt; `Credits` to check if your balance has been updated. Try refreshing the interface or restarting if necessary

## [​](http://docs.comfy.org#frequently-asked-questions) Frequently Asked Questions

Can credits go negative?

No, when your credit balance is negative, you will not be able to run API Nodes

Can I get a refund for unused credits?

Currently, we do not support refunds

How do I check my current balance and usage?

Click on `Settings` -&gt; `Credits` to see your current balance and access the `Credit History` entry

Can I share my credits with other users?

You can log into the same account on multiple devices, but we do not support sharing credits with other users

How do I know how many credits I've consumed each time?

Due to different image sizes and generation quantities, the `Tokens` and `Credits` consumed each time vary. In `Settings` -&gt; `Credits`, you can see the credits consumed each time and the corresponding credit history

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/interface/credits.mdx)

[Previous](http://docs.comfy.org/interface/user)

[Workflow  
\
Next](http://docs.comfy.org/essentials/core-concepts/workflow)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [How to Purchase Credits?](http://docs.comfy.org#how-to-purchase-credits%3F)
- [Frequently Asked Questions](http://docs.comfy.org#frequently-asked-questions)

<!-- END Built_In_Node/interface/credits.md -->


<!-- BEGIN Built_In_Node/interface/maskeditor.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Mask Editor - Create and Edit Masks in ComfyUI

# Mask Editor - Create and Edit Masks in ComfyUI

Learn how to use the Mask Editor in ComfyUI, including settings and usage instructions

The Mask Editor is a very useful feature in ComfyUI that allows users to create and edit masks within images without needing to use other applications.

The Mask Editor is currently triggered through the `Load Image` node. After uploading an image, you can right-click on the node and select `Open in MaskEditor` from the menu to open the Mask Editor.

You can then click with your mouse on the image to create and edit masks.

## [​](http://docs.comfy.org#demo-video) Demo Video

Your browser does not support the video tag.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/interface/maskeditor.mdx)

[Previous](http://docs.comfy.org/essentials/core-concepts/links)

[Models  
\
Next](http://docs.comfy.org/essentials/core-concepts/models)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Demo Video](http://docs.comfy.org#demo-video)

<!-- END Built_In_Node/interface/maskeditor.md -->


<!-- BEGIN Built_In_Node/interface/overview.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Interface Overview

# ComfyUI Interface Overview

In this article, we will briefly introduce the basic user interface of ComfyUI, familiarizing you with the various parts of the ComfyUI interface.

The visual interface is currently the way most users utilize ComfyUI to call the [ComfyUI Server](http://docs.comfy.org/essentials/comfyui-server/comms_overview) to generate corresponding media resources. It provides a visual interface for users to operate and organize workflows, debug workflows, and create amazing works.

Typically, when you start the ComfyUI server, you will see an interface like this:

If you are an earlier user, you may have seen the previous menu interface like this:

Currently, the [ComfyUI frontend](https://github.com/Comfy-Org/ComfyUI_frontend) is a separate project, released and maintained as an independent pip package. If you want to contribute, you can fork this [repository](https://github.com/Comfy-Org/ComfyUI_frontend) and submit a pull request.

## [​](http://docs.comfy.org#localization-support) Localization Support

Currently, ComfyUI supports: English, Chinese, Russian, French, Japanese, and Korean. If you need to switch the interface language to your preferred language, you can click the **Settings gear icon** and then select your desired language under `Comfy` —&gt; `Locale`.

## [​](http://docs.comfy.org#new-menu-interface) New Menu Interface

### [​](http://docs.comfy.org#workspace-areas) Workspace Areas

Below are the main interface areas of ComfyUI and a brief introduction to each part.

Currently, apart from the main workflow interface, the ComfyUI interface is mainly divided into the following parts:

1. Menu Bar: Provides workflow, editing, help menus, workflow execution, ComfyUI Manager entry, etc.
2. Sidebar Panel Switch Buttons: Used to switch between workflow history queue, node library, model library, local user workflow browsing, etc.
3. Theme Switch Button: Quickly switch between ComfyUI’s default dark theme and light theme
4. Settings: Click to open the settings button
5. Canvas Menu: Provides zoom in, zoom out, and auto-fit operations for the ComfyUI canvas

### [​](http://docs.comfy.org#menu-bar-functions) Menu Bar Functions

The image above shows the corresponding functions of the top menu bar, including common features, which we will explain in detail in the specific function usage section.

### [​](http://docs.comfy.org#sidebar-panel-buttons) Sidebar Panel Buttons

In the current ComfyUI, we provide four side panels with the following functions:

1. Workflow History Queue (Queue): All queue information for ComfyUI executing media content generation
2. Node Library: All nodes in ComfyUI, including `Comfy Core` and your installed custom nodes, can be found here
3. Model Library: Models in your local `ComfyUI/models` directory can be found here
4. Local User Workflows (Workflows): Your locally saved workflows can be found here

## [​](http://docs.comfy.org#old-menu-version) Old Menu Version

Currently, ComfyUI enables the new interface by default. If you prefer to use the old interface, you can click the **Settings gear icon** and then set `Use new menu` to `disabled` under `Comfy` —&gt; `Menu` to switch to the old menu version.

The old menu interface only supports English.

The function annotations for the old menu interface are explained below:

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/interface/overview.mdx)

[Previous](http://docs.comfy.org/get_started/first_generation)

[Account ManagementIn this document, we will introduce the account management features of ComfyUI, including account login, registration, and logout operations.  
\
Next](http://docs.comfy.org/interface/user)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Localization Support](http://docs.comfy.org#localization-support)
- [New Menu Interface](http://docs.comfy.org#new-menu-interface)
- [Workspace Areas](http://docs.comfy.org#workspace-areas)
- [Menu Bar Functions](http://docs.comfy.org#menu-bar-functions)
- [Sidebar Panel Buttons](http://docs.comfy.org#sidebar-panel-buttons)
- [Old Menu Version](http://docs.comfy.org#old-menu-version)

<!-- END Built_In_Node/interface/overview.md -->


<!-- BEGIN Built_In_Node/interface/shortcuts.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Keyboard Shortcuts and Custom Settings

# ComfyUI Keyboard Shortcuts and Custom Settings

Keyboard and mouse shortcuts for ComfyUI and related settings

Currently, ComfyUI supports custom keyboard shortcuts. You can set the shortcuts by clicking on `Settings (gear icon)` —&gt; `Keybinding`.

In the corresponding menu, you can see all the current shortcut settings for ComfyUI. Click the `edit icon` before the corresponding command to customize the shortcut.

Below is the current list of shortcuts for ComfyUI, which you can customize as needed.

- Windows/Linux
- MacOS

ShortcutCommandCtrl + EnterQueue up current graph for generationCtrl + Shift + EnterQueue up current graph as first for generationCtrl + Z / Ctrl + YUndo/RedoCtrl + SSave workflowCtrl + OLoad workflowCtrl + ASelect all nodesAlt + CCollapse/uncollapse selected nodesCtrl + MMute/unmute selected nodesCtrl + BBypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)Delete  
BackspaceDelete selected nodesCtrl + Delete  
Ctrl + BackspaceDelete the current graphSpaceMove the canvas around when held and moving the cursorCtrl + Click  
Shift + ClickAdd clicked node to selectionCtrl + C/Ctrl + VCopy and paste selected nodes (without maintaining connections to outputs of unselected nodes)Ctrl + C/Ctrl + Shift + VCopy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes)Shift + DragMove multiple selected nodes at the same timeCtrl + DLoad default graphQToggle visibility of the queueHToggle visibility of historyRRefresh graphDouble-Click LMBQuick search for nodes to add

ShortcutCommandCtrl + EnterQueue up current graph for generationCtrl + Shift + EnterQueue up current graph as first for generationCtrl + Z / Ctrl + YUndo/RedoCtrl + SSave workflowCtrl + OLoad workflowCtrl + ASelect all nodesAlt + CCollapse/uncollapse selected nodesCtrl + MMute/unmute selected nodesCtrl + BBypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)Delete  
BackspaceDelete selected nodesCtrl + Delete  
Ctrl + BackspaceDelete the current graphSpaceMove the canvas around when held and moving the cursorCtrl + Click  
Shift + ClickAdd clicked node to selectionCtrl + C/Ctrl + VCopy and paste selected nodes (without maintaining connections to outputs of unselected nodes)Ctrl + C/Ctrl + Shift + VCopy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes)Shift + DragMove multiple selected nodes at the same timeCtrl + DLoad default graphQToggle visibility of the queueHToggle visibility of historyRRefresh graphDouble-Click LMBQuick search for nodes to add

KeybindExplanationCmd ⌘ + EnterQueue up current graph for generationCmd ⌘ + Shift + EnterQueue up current graph as first for generationCmd ⌘ + Z/Cmd ⌘ + YUndo/RedoCmd ⌘ + SSave workflowCmd ⌘ + OLoad workflowCmd ⌘ + ASelect all nodesOpt ⌥ + CCollapse/uncollapse selected nodesCmd ⌘ + MMute/unmute selected nodesCmd ⌘ + BBypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)Delete  
BackspaceDelete selected nodesCmd ⌘ + Delete  
Cmd ⌘ + BackspaceDelete the current graphSpaceMove the canvas around when held and moving the cursorCmd ⌘ + Click  
Shift + ClickAdd clicked node to selectionCmd ⌘ + C / Cmd ⌘ + VCopy and paste selected nodes (without maintaining connections to outputs of unselected nodes)Cmd ⌘ + C / Cmd ⌘ + Shift + VCopy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes)Shift + DragMove multiple selected nodes at the same timeCmd ⌘ + DLoad default graphQToggle visibility of the queueHToggle visibility of historyRRefresh graphDouble-Click LMBQuick search for nodes to add

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/interface/shortcuts.mdx)

[Previous](http://docs.comfy.org/essentials/core-concepts/dependencies)

[Text to ImageThis guide will help you understand the concept of text-to-image in AI art generation and complete a text-to-image workflow in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/basic/text-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

<!-- END Built_In_Node/interface/shortcuts.md -->


<!-- BEGIN Built_In_Node/interface/user.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Account Management

# Account Management

In this document, we will introduce the account management features of ComfyUI, including account login, registration, and logout operations.

The account system was added to support `API Nodes`, which enable calls to closed-source model APIs, greatly expanding the possibilities of ComfyUI. Since these API calls consume tokens, we have added a corresponding user system.

Currently, we support the following login methods:

- Email login
- Google login
- Github login
- API Key login (for non-whitelisted site authorization)

We will provide relevant login requirements and explanations in this document.

## [​](http://docs.comfy.org#comfyui-version-requirements) ComfyUI Version Requirements

You may need to use at least [ComfyUI v0.3.0](https://github.com/comfyanonymous/ComfyUI/releases/tag/v0.3.30) to use the account system. Ensure that the corresponding frontend version is at least `1.17.11`. Sometimes the frontend may fail to install and revert to an older version, so please check if the frontend version is greater than `1.17.11` in `Settings` -&gt; `About`.

In some regions, network restrictions may prevent normal access to the login API, causing timeouts or failures. Before logging in, please **ensure that your network environment does not restrict access to the corresponding API**, and make sure you can access sites like Google or Github.

As we are still rapidly iterating and updating, related features may change. If there are no special circumstances, please try to update to the latest version to access the relevant features.

## [​](http://docs.comfy.org#network-requirements) Network Requirements

To login to ComfyUI account, you must be in a secure network environment:

- Only allow access from `127.0.0.1` or `localhost`.
- Do not support using the `--listen` parameter to access the API node through a local network.
- If you are using a non-SSL certificate or a site that does not start with `https`, you may not be able to successfully log in.
- You may not be able to log in on a site that is not in our whitelist (but you can log in using an API Key now).
- Ensure you can connect to our service normally (some regions may require a proxy).

## [​](http://docs.comfy.org#how-to-log-in) How to Log In

Log in via `Settings` -&gt; `User`:

## [​](http://docs.comfy.org#login-methods) Login Methods

If this is your first login, please create an account first.

## [​](http://docs.comfy.org#logging-in-with-an-api-key) Logging in with an API Key

Since not all ComfyUI deployments are on our domain authorization whitelist, we have provided API Key login in a recent update (2025-05-10) for logging in through non-whitelisted sites. Below are the steps for logging in with an API Key:

- Have an API Key
- No API Key, Apply for an API Key First

1

Select Comfy API Key Login on the Login Screen

Select `Comfy API Key` login in the login popup

2

Enter Your API Key

1. Enter your API Key and save it
2. If you don’t have an API Key, click the `Get one here` link to go to [https://platform.comfy.org/login](https://platform.comfy.org/login) and log in to obtain it.

3

Login Successful

After a successful login, you can see the corresponding API Key login information in the settings menu

1

Select Comfy API Key Login on the Login Screen

Select `Comfy API Key` login in the login popup

2

Enter Your API Key

1. Enter your API Key and save it
2. If you don’t have an API Key, click the `Get one here` link to go to [https://platform.comfy.org/login](https://platform.comfy.org/login) and log in to obtain it.

3

Login Successful

After a successful login, you can see the corresponding API Key login information in the settings menu

Please refer to the following steps to apply for and obtain an API Key:

1

Visit https://platform.comfy.org/login and Log In

Please visit [https://platform.comfy.org/login](https://platform.comfy.org/login) and log in with the corresponding account

2

Click \`+ New\` in API Keys to Create an API Key

Click `+ New` in API Keys to create an API Key

3

Enter API Key Name

1. (Required) Enter the API Key name,
2. Click `Generate` to create

4

Save the Obtained API Key

Since the API Key is only visible upon first creation, please save it immediately after creation. It cannot be viewed later, so please keep it safe.

5

API Key Management

For unused API Keys or those at risk of being leaked, you can click `Delete` to remove them to prevent unnecessary losses.

6

(Optional) Log Out

If you have obtained an API Key and are logged in on a public device, please log out promptly.

## [​](http://docs.comfy.org#post-login-status) Post-Login Status

After logging in, a login button is displayed in the top menu bar of the ComfyUI interface. You can open the corresponding login interface through this button and log out of the corresponding account in the settings menu.

## [​](http://docs.comfy.org#frequently-asked-questions) Frequently Asked Questions

Are there any login device restrictions?

We do not restrict login devices. You can log in to your account on any device, but please note that your account information may be accessed by other devices, so do not log in to your account on public devices.

Why can't I log in with a LAN IP?

We do not currently support logging in via LAN IP because the security of the LAN environment is uncontrollable. Therefore, the current version does not fully support LAN IP login. We may consider handling LAN situations in the future.

Why can't I log in on some websites?

Our login service has a whitelist, so you may not be able to log in to ComfyUI deployed on some servers.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/interface/user.mdx)

[Previous](http://docs.comfy.org/interface/overview)

[Credits ManagementIn this article, we will introduce ComfyUI's credit management features, including how to obtain, use, and view credits.  
\
Next](http://docs.comfy.org/interface/credits)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [ComfyUI Version Requirements](http://docs.comfy.org#comfyui-version-requirements)
- [Network Requirements](http://docs.comfy.org#network-requirements)
- [How to Log In](http://docs.comfy.org#how-to-log-in)
- [Login Methods](http://docs.comfy.org#login-methods)
- [Logging in with an API Key](http://docs.comfy.org#logging-in-with-an-api-key)
- [Post-Login Status](http://docs.comfy.org#post-login-status)
- [Frequently Asked Questions](http://docs.comfy.org#frequently-asked-questions)

<!-- END Built_In_Node/interface/user.md -->


<!-- BEGIN Built_In_Node/tutorials/3d/hunyuan3D-2.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
  
  - [Hunyuan3D-2](http://docs.comfy.org/tutorials/3d/hunyuan3D-2)
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Hunyuan3D-2 Examples

# ComfyUI Hunyuan3D-2 Examples

This guide will demonstrate how to use Hunyuan3D-2 in ComfyUI to generate 3D assets.

# [​](http://docs.comfy.org#hunyuan3d-2-0-introduction) Hunyuan3D 2.0 Introduction

[Hunyuan3D 2.0](https://github.com/Tencent/Hunyuan3D-2) is an open-source 3D asset generation model released by Tencent, capable of generating high-fidelity 3D models with high-resolution texture maps through text or images.

Hunyuan3D 2.0 adopts a two-stage generation approach, first generating a geometry model without textures, then synthesizing high-resolution texture maps. This effectively separates the complexity of shape and texture generation. Below are the two core components of Hunyuan3D 2.0:

1. **Geometry Generation Model (Hunyuan3D-DiT)**: Based on a flow diffusion Transformer architecture, it generates untextured geometric models that precisely match input conditions.
2. **Texture Generation Model (Hunyuan3D-Paint)**: Combines geometric conditions and multi-view diffusion techniques to add high-resolution textures to models, supporting PBR materials.

**Key Advantages**

- **High-Precision Generation**: Sharp geometric structures, rich texture colors, support for PBR material generation, achieving near-realistic lighting effects.
- **Diverse Usage Methods**: Provides code calls, Blender plugins, Gradio applications, and online experience through the official website, suitable for different user needs.
- **Lightweight and Compatibility**: The Hunyuan3D-2mini model requires only 5GB VRAM, the standard version needs 6GB VRAM for shape generation, and the complete process (shape + texture) requires only 12GB VRAM.

Recently (March 18, 2025), Hunyuan3D 2.0 also introduced a multi-view shape generation model (Hunyuan3D-2mv), which supports generating more detailed geometric structures from inputs at different angles.

This example includes three workflows:

- Using Hunyuan3D-2mv with multiple view inputs to generate 3D models
- Using Hunyuan3D-2mv-turbo with multiple view inputs to generate 3D models
- Using Hunyuan3D-2 with a single view input to generate 3D models

ComfyUI now natively supports Hunyuan3D-2mv, but does not yet support texture and material generation. Please make sure you have updated to the latest version of [ComfyUI](https://github.com/comfyanonymous/ComfyUI) before starting.

The workflow example PNG images in this tutorial contain workflow JSON in their metadata:

- You can drag them directly into ComfyUI
- Or use the menu `Workflows` -&gt; `Open (ctrl+o)`

This will load the corresponding workflow and prompt you to download the required models. The generated `.glb` format models will be output to the `ComfyUI/output/mesh` folder.

## [​](http://docs.comfy.org#comfyui-hunyuan3d-2mv-workflow-example) ComfyUI Hunyuan3D-2mv Workflow Example

In the Hunyuan3D-2mv workflow, we’ll use multi-view images to generate a 3D model. Note that multiple view images are not mandatory in this workflow - you can use only the `front` view image to generate a 3D model.

### [​](http://docs.comfy.org#1-workflow) 1. Workflow

Please download the images below and drag into ComfyUI to load the workflow.

Download the images below we will use them as input images.

In this example, the input images have already been preprocessed to remove excess background. In actual use, you can use custom nodes like [ComfyUI\_essentials](https://github.com/cubiq/ComfyUI_essentials) to automatically remove excess background.

### [​](http://docs.comfy.org#2-manual-model-installation) 2. Manual Model Installation

Download the model below and save it to the corresponding ComfyUI folder

- hunyuan3d-dit-v2-mv: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2-mv.safetensors`

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── hunyuan3d-dit-v2-mv.safetensors  // renamed file
```

### [​](http://docs.comfy.org#3-steps-to-run-the-workflow) 3. Steps to Run the Workflow

1. Ensure that the Image Only Checkpoint Loader(img2vid model) has loaded our downloaded and renamed `hunyuan3d-dit-v2-mv.safetensors` model
2. Load the corresponding view images in each of the `Load Image` nodes
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

If you need to add more views, make sure to load other view images in the `Hunyuan3Dv2ConditioningMultiView` node, and ensure that you load the corresponding view images in the `Load Image` nodes.

## [​](http://docs.comfy.org#hunyuan3d-2mv-turbo-workflow) Hunyuan3D-2mv-turbo Workflow

In the Hunyuan3D-2mv-turbo workflow, we’ll use the Hunyuan3D-2mv-turbo model to generate 3D models. This model is a step distillation version of Hunyuan3D-2mv, allowing for faster 3D model generation. In this version of the workflow, we set `cfg` to 1.0 and add a `flux guidance` node to control the `distilled cfg` generation.

### [​](http://docs.comfy.org#1-workflow-2) 1. Workflow

Please download the images below and drag into ComfyUI to load the workflow.

Download the images below we will use them as input images.

### [​](http://docs.comfy.org#2-manual-model-installation-2) 2. Manual Model Installation

Download the model below and save it to the corresponding ComfyUI folder

- hunyuan3d-dit-v2-mv-turbo: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv-turbo/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2-mv-turbo.safetensors`

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── hunyuan3d-dit-v2-mv-turbo.safetensors  // renamed file
```

### [​](http://docs.comfy.org#3-steps-to-run-the-workflow-2) 3. Steps to Run the Workflow

1. Ensure that the `Image Only Checkpoint Loader(img2vid model)` node has loaded our renamed `hunyuan3d-dit-v2-mv-turbo.safetensors` model
2. Load the corresponding view images in each of the `Load Image` nodes
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## [​](http://docs.comfy.org#hunyuan3d-2-single-view-workflow) Hunyuan3D-2 Single View Workflow

In the Hunyuan3D-2 workflow, we’ll use the Hunyuan3D-2 model to generate 3D models. This model is not a multi-view model. In this workflow, we use the `Hunyuan3Dv2Conditioning` node instead of the `Hunyuan3Dv2ConditioningMultiView` node.

### [​](http://docs.comfy.org#1-workflow-3) 1. Workflow

Please download the image below and drag it into ComfyUI to load the workflow.

Download the image below we will use it as input image.

### [​](http://docs.comfy.org#2-manual-model-installation-3) 2. Manual Model Installation

Download the model below and save it to the corresponding ComfyUI folder

- hunyuan3d-dit-v2-0: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2/resolve/main/hunyuan3d-dit-v2-0/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2.safetensors`

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── hunyuan3d-dit-v2.safetensors  // renamed file
```

### [​](http://docs.comfy.org#3-steps-to-run-the-workflow-3) 3. Steps to Run the Workflow

1. Ensure that the `Image Only Checkpoint Loader(img2vid model)` node has loaded our renamed `hunyuan3d-dit-v2.safetensors` model
2. Load the image in the `Load Image` node
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## [​](http://docs.comfy.org#community-resources) Community Resources

Below are ComfyUI community resources related to Hunyuan3D-2

- [ComfyUI-Hunyuan3DWrapper](https://github.com/kijai/ComfyUI-Hunyuan3DWrapper)
- [Kijai/Hunyuan3D-2\_safetensors](https://huggingface.co/Kijai/Hunyuan3D-2_safetensors/tree/main)
- [ComfyUI-3D-Pack](https://github.com/MrForExample/ComfyUI-3D-Pack)

## [​](http://docs.comfy.org#hunyuan3d-2-0-open-source-model-series) Hunyuan3D 2.0 Open-Source Model Series

Currently, Hunyuan3D 2.0 has open-sourced multiple models covering the complete 3D generation process. You can visit [Hunyuan3D-2](https://github.com/Tencent/Hunyuan3D-2) for more information.

**Hunyuan3D-2mini Series**

ModelDescriptionDateParametersHuggingfaceHunyuan3D-DiT-v2-miniMini Image to Shape Model2025-03-180.6B[Visit](https://huggingface.co/tencent/Hunyuan3D-2mini/tree/main/hunyuan3d-dit-v2-mini)

**Hunyuan3D-2mv Series**

ModelDescriptionDateParametersHuggingfaceHunyuan3D-DiT-v2-mv-FastGuidance Distillation Version, can halve DIT inference time2025-03-181.1B[Visit](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv-fast)Hunyuan3D-DiT-v2-mvMulti-view Image to Shape Model, suitable for 3D creation requiring multiple angles to understand the scene2025-03-181.1B[Visit](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv)

**Hunyuan3D-2 Series**

ModelDescriptionDateParametersHuggingfaceHunyuan3D-DiT-v2-0-FastGuidance Distillation Model2025-02-031.1B[Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0-fast)Hunyuan3D-DiT-v2-0Image to Shape Model2025-01-211.1B[Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0)Hunyuan3D-Paint-v2-0Texture Generation Model2025-01-211.3B[Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-paint-v2-0)Hunyuan3D-Delight-v2-0Image Delight Model2025-01-211.3B[Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-delight-v2-0)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/3d/hunyuan3D-2.mdx)

[Previous](http://docs.comfy.org/tutorials/image/hidream/hidream-e1)

[LTX-Video  
\
Next](http://docs.comfy.org/tutorials/video/ltxv)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Hunyuan3D 2.0 Introduction](http://docs.comfy.org#hunyuan3d-2-0-introduction)
- [ComfyUI Hunyuan3D-2mv Workflow Example](http://docs.comfy.org#comfyui-hunyuan3d-2mv-workflow-example)
- [1. Workflow](http://docs.comfy.org#1-workflow)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow)
- [Hunyuan3D-2mv-turbo Workflow](http://docs.comfy.org#hunyuan3d-2mv-turbo-workflow)
- [1. Workflow](http://docs.comfy.org#1-workflow-2)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation-2)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow-2)
- [Hunyuan3D-2 Single View Workflow](http://docs.comfy.org#hunyuan3d-2-single-view-workflow)
- [1. Workflow](http://docs.comfy.org#1-workflow-3)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation-3)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow-3)
- [Community Resources](http://docs.comfy.org#community-resources)
- [Hunyuan3D 2.0 Open-Source Model Series](http://docs.comfy.org#hunyuan3d-2-0-open-source-model-series)

<!-- END Built_In_Node/tutorials/3d/hunyuan3D-2.md -->


<!-- BEGIN Built_In_Node/tutorials/api-nodes/black-forest-labs/flux-1-1-pro-ultra-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
    
    - [Flux 1.1 Pro Ultra Image](http://docs.comfy.org/tutorials/api-nodes/black-forest-labs/flux-1-1-pro-ultra-image)
  - Stability AI
  - Ideogram
  - Luma
  - OpenAI
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Flux 1.1 Pro Ultra Image API Node ComfyUI Official Workflow Examples

# Flux 1.1 Pro Ultra Image API Node ComfyUI Official Workflow Examples

This guide covers how to use the Flux 1.1 Pro Ultra Image API node in ComfyUI

FLUX 1.1 Pro Ultra is a high-performance AI image generation tool by BlackForestLabs, featuring ultra-high resolution and efficient generation capabilities. It supports up to 4MP resolution (4x the standard version) while keeping single image generation time under 10 seconds - 2.5x faster than similar high-resolution models.

The tool offers two core modes:

- **Ultra Mode**: Designed for high-resolution needs, perfect for advertising and e-commerce where detail magnification is important. It accurately reflects prompts while maintaining generation speed.
- **Raw Mode**: Focuses on natural realism, optimizing skin tones, lighting, and landscape details. Reduces the “AI look” and is ideal for photography and realistic style creation.

We now support the Flux 1.1 Pro Ultra Image node in ComfyUI. This guide will cover:

- Flux 1.1 Pro Text-to-Image
- Flux 1.1 Pro Image-to-Image (Remix)

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#flux-1-1-pro-ultra-image-node-documentation) Flux 1.1 Pro Ultra Image Node Documentation

Check the following documentation for detailed node parameter settings:

- [Flux 1.1 Pro Ultra Image](http://docs.comfy.org/built-in-nodes/api-node/image/bfl/flux-pro-ultra-image)

## [​](http://docs.comfy.org#flux-1-1-%5Bpro%5D-text-to-image-tutorial) Flux 1.1 \[pro] Text-to-Image Tutorial

### [​](http://docs.comfy.org#1-download-workflow-file) 1. Download Workflow File

Download and drag the following file into ComfyUI to load the workflow:

### [​](http://docs.comfy.org#2-complete-the-workflow-steps) 2. Complete the Workflow Steps

Follow the numbered steps to complete the basic workflow:

1. (Optional) Modify the prompt in the `Flux 1.1 [pro] Ultra Image` node
2. (Optional) Set `raw` parameter to `false` for more realistic output
3. Click `Run` or use shortcut `Ctrl(cmd) + Enter` to generate the image
4. After the API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory

## [​](http://docs.comfy.org#flux-1-1%5Bpro%5D-image-to-image-tutorial) Flux 1.1\[pro] Image-to-Image Tutorial

When adding an `image_prompt` to the node input, the output will blend features from the input image (Remix). The `image_prompt_strength` value affects the blend ratio: higher values make the output more similar to the input image.

### [​](http://docs.comfy.org#1-download-workflow-file-2) 1. Download Workflow File

Download and drag the following file into ComfyUI, or right-click the purple node in the Text-to-Image workflow and set `mode` to `always` to enable `image_prompt` input:

We’ll use this image as input:

### [​](http://docs.comfy.org#2-complete-the-workflow-steps-2) 2. Complete the Workflow Steps

Follow these numbered steps:

1. Click **Upload** on the `Load Image` node to upload your input image
2. (Optional) Adjust `image_prompt_strength` in `Flux 1.1 [pro] Ultra Image` to change the blend ratio
3. Click `Run` or use shortcut `Ctrl(cmd) + Enter` to generate the image
4. After the API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory

Here’s a comparison of outputs with different `image_prompt_strength` values:

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/black-forest-labs/flux-1-1-pro-ultra-image.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/pricing)

[Stable Image UltraThis article will introduce how to use the Stability AI Stable Image Ultra API node's text-to-image and image-to-image capabilities in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-image-ultra)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Flux 1.1 Pro Ultra Image Node Documentation](http://docs.comfy.org#flux-1-1-pro-ultra-image-node-documentation)
- [Flux 1.1 \[pro\] Text-to-Image Tutorial](http://docs.comfy.org#flux-1-1-%5Bpro%5D-text-to-image-tutorial)
- [1. Download Workflow File](http://docs.comfy.org#1-download-workflow-file)
- [2. Complete the Workflow Steps](http://docs.comfy.org#2-complete-the-workflow-steps)
- [Flux 1.1\[pro\] Image-to-Image Tutorial](http://docs.comfy.org#flux-1-1%5Bpro%5D-image-to-image-tutorial)
- [1. Download Workflow File](http://docs.comfy.org#1-download-workflow-file-2)
- [2. Complete the Workflow Steps](http://docs.comfy.org#2-complete-the-workflow-steps-2)

<!-- END Built_In_Node/tutorials/api-nodes/black-forest-labs/flux-1-1-pro-ultra-image.md -->


<!-- BEGIN Built_In_Node/tutorials/api-nodes/faq.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
  - Luma
  - OpenAI
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

FAQs about API Nodes

# FAQs about API Nodes

Some FAQs you may encounter when using API Nodes.

This article addresses common questions regarding the use of API nodes.

Why can't I find the API nodes?

Please update your ComfyUI to the latest version (the latest commit or the latest [desktop version](https://www.comfy.org/download)). We may add more API support in the future, and the corresponding nodes will be updated, so please keep your ComfyUI up to date.

Please note that you need to distinguish between the nightly version and the release version. In some cases, the latest `release` version may not be updated in time compared to the `nightly` version. Since we are still iterating quickly, please ensure you are using the latest version when you cannot find the corresponding node.

Why can't I use / log in to the API Nodes?

API access requires that your current request is based on a secure network environment. The current requirements for API access are as follows:

- The local network only allows access from `127.0.0.1` or `localhost`, which may mean that you cannot use the API Nodes in a ComfyUI service started with the `--listen` parameter in a LAN environment.
- Able to access our API service normally (a proxy service may be required in some regions).
- Your account does not have enough [credits](http://docs.comfy.org/interface/credits).

Why can't I use API node even after logging in, or why does it keep asking me to log in while using?

- Currently, only `127.0.0.1` or `localhost` access is supported.
- Ensure your account has enough credits.

Can API Nodes be used for free?

API Nodes require credits for API calls to closed-source models, so they do not support free usage.

How to purchase credits?

Please refer to the following documentation:

1. [Comfy Account](http://docs.comfy.org/interface/user): Find the `User` section in the settings menu to log in.
2. [Credits](http://docs.comfy.org/interface/credits): After logging in, the settings interface will show the credits menu. You can purchase credits in `Settings` → `Credits`. We use a prepaid system, so there will be no unexpected charges.
3. Complete the payment through Stripe.
4. Check if the credits have been updated. If not, try restarting or refreshing the page.

Are unused credits refundable?

Currently, we do not support refunds for credits. If you believe there is an error resulting in unused balance due to technical issues, please [contact support](mailto:support@comfy.org).

Can credits go negative?

Credits cannot go negative, so please ensure you have enough credits before making the corresponding API calls.

Where can I check usage and expenses?

Please visit the [Credits](http://docs.comfy.org/interface/credits) menu after logging in to check the corresponding credits.

Is it possible to use my own API Key?

Currently, the API Nodes are still in the testing phase and do not support this feature yet, but we have considered adding it.

Do credits expire?

No, your credits do not expire.

Can credits be transferred or shared?

No, your credits cannot be transferred to other users and are limited to the currently logged-in account, but we do not restrict the number of devices that can log in.

Can I use the same account on different devices?

We do not limit the number of devices that can log in; you can use your account anywhere you want.

How can I request for my account or information to be deleted??

Email a request to [support@comfy.org](mailto:support@comfy.org) and we will delete your information

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/faq.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/overview)

[PricingThis article lists the pricing of the current API Nodes.  
\
Next](http://docs.comfy.org/tutorials/api-nodes/pricing)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

<!-- END Built_In_Node/tutorials/api-nodes/faq.md -->


<!-- BEGIN Built_In_Node/tutorials/api-nodes/ideogram/ideogram-v3.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
    
    - [Ideogram 3.0](http://docs.comfy.org/tutorials/api-nodes/ideogram/ideogram-v3)
  - Luma
  - OpenAI
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Ideogram 3.0 API Node Official Examples

# ComfyUI Ideogram 3.0 API Node Official Examples

This guide covers how to use the Ideogram 3.0 API node in ComfyUI

Ideogram 3.0 is a powerful text-to-image model by Ideogram, known for its photorealistic quality, accurate text rendering, and consistent style control.

The [Ideogram V3](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3) node currently supports two modes:

- Text-to-Image mode
- Image Editing mode (when both image and mask inputs are provided)

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#ideogram-3-0-node-documentation) Ideogram 3.0 Node Documentation

Check the following documentation for detailed node parameter settings:

- [Ideogram V3](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3)

## [​](http://docs.comfy.org#ideogram-3-0-api-node-text-to-image-mode) Ideogram 3.0 API Node Text-to-Image Mode

When using [Ideogram V3](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3) without image and mask inputs, the node operates in Text-to-Image mode.

### [​](http://docs.comfy.org#1-download-workflow-file) 1. Download Workflow File

Download and drag the following file into ComfyUI to load the workflow:

### [​](http://docs.comfy.org#2-complete-the-workflow-steps) 2. Complete the Workflow Steps

Follow the numbered steps to complete the basic workflow:

1. Enter your image description in the `prompt` field of the `Ideogram V3` node
2. Click `Run` or use shortcut `Ctrl(cmd) + Enter` to generate the image
3. After the API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory

## [​](http://docs.comfy.org#ideogram-3-0-api-node-image-editing-mode) Ideogram 3.0 API Node Image Editing Mode

\[To be updated]

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/ideogram/ideogram-v3.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image)

[Luma Text to ImageThis guide explains how to use the Luma Text to Image API node in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Ideogram 3.0 Node Documentation](http://docs.comfy.org#ideogram-3-0-node-documentation)
- [Ideogram 3.0 API Node Text-to-Image Mode](http://docs.comfy.org#ideogram-3-0-api-node-text-to-image-mode)
- [1. Download Workflow File](http://docs.comfy.org#1-download-workflow-file)
- [2. Complete the Workflow Steps](http://docs.comfy.org#2-complete-the-workflow-steps)
- [Ideogram 3.0 API Node Image Editing Mode](http://docs.comfy.org#ideogram-3-0-api-node-image-editing-mode)

<!-- END Built_In_Node/tutorials/api-nodes/ideogram/ideogram-v3.md -->


<!-- BEGIN Built_In_Node/tutorials/api-nodes/luma/luma-image-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
  - Luma
    
    - [Luma Text to Image](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image)
    - [Luma Image to Image](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-image)
    - [Luma Text to Video](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video)
    - [Luma Image to Video](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video)
  - OpenAI
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Image to Image API Node ComfyUI Official Example

# Luma Image to Image API Node ComfyUI Official Example

This guide covers how to use the Luma Image to Image API node in ComfyUI

The [Luma Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image) node allows you to modify existing images based on text prompts using Luma AI technology, while preserving certain features and structures from the original image.

In this guide, we’ll show you how to set up an image-to-image workflow using this node.

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#luma-image-to-image-node-documentation) Luma Image to Image Node Documentation

Check the following documentation for detailed node parameter settings:

[**Luma Image to Image Node Documentation**  
\
Luma Image to Image API Node Documentation](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image)

## [​](http://docs.comfy.org#luma-image-to-image-api-node-workflow) Luma Image to Image API Node Workflow

This feature works well for changing objects and shapes. However, it may not be ideal for color changes. We recommend using lower weight values, around 0.0 to 0.1.

### [​](http://docs.comfy.org#1-download-workflow-file) 1. Download Workflow File

Download and drag the following image into ComfyUI to load the workflow (workflow information is included in the image metadata):

Download this image to use as input:

### [​](http://docs.comfy.org#2-complete-the-workflow-steps) 2. Complete the Workflow Steps

Follow these numbered steps:

1. Click **Upload** on the `Load Image` node to upload your input image
2. (Optional) Modify the workflow prompts
3. (Optional) Adjust `image_weight` to change input image influence (lower values stay closer to original)
4. Click `Run` or use shortcut `Ctrl(cmd) + Enter` to generate the image
5. After API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory

### [​](http://docs.comfy.org#3-results-with-different-image-weight-values) 3. Results with Different `image_weight` Values

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/luma/luma-image-to-image.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image)

[Luma Text to VideoLearn how to use the Luma Text to Video API node in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Luma Image to Image Node Documentation](http://docs.comfy.org#luma-image-to-image-node-documentation)
- [Luma Image to Image API Node Workflow](http://docs.comfy.org#luma-image-to-image-api-node-workflow)
- [1. Download Workflow File](http://docs.comfy.org#1-download-workflow-file)
- [2. Complete the Workflow Steps](http://docs.comfy.org#2-complete-the-workflow-steps)
- [3. Results with Different image\_weight Values](http://docs.comfy.org#3-results-with-different-image-weight-values)

<!-- END Built_In_Node/tutorials/api-nodes/luma/luma-image-to-image.md -->


<!-- BEGIN Built_In_Node/tutorials/api-nodes/luma/luma-image-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
  - Luma
    
    - [Luma Text to Image](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image)
    - [Luma Image to Image](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-image)
    - [Luma Text to Video](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video)
    - [Luma Image to Video](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video)
  - OpenAI
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Image to Video API Node ComfyUI Official Example

# Luma Image to Video API Node ComfyUI Official Example

Learn how to use the Luma Image to Video API node in ComfyUI

The [Luma Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video) node allows you to convert static images into smooth, dynamic videos using Luma AI’s advanced technology, bringing life and motion to your images.

In this guide, we’ll show you how to set up a workflow for image-to-video conversion using this node.

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#luma-image-to-video-node-documentation) Luma Image to Video Node Documentation

Check out the following documentation to learn more about the node’s parameters:

[**Luma Image to Video Node Docs**  
\
Luma Image to Video API node documentation](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video)

[**Luma Concepts Node Docs**  
\
Luma Concepts API node documentation](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)

## [​](http://docs.comfy.org#image-to-video-workflow-with-luma-api-node) Image to Video Workflow with Luma API Node

The Luma Image to Video node requires at least one image input (`first_image` or `last_image`) along with text prompts to determine the video’s motion effects. In this guide, we’ve created an example using `first_image` and `luma_concepts` to showcase Luma AI’s video generation capabilities.

### [​](http://docs.comfy.org#1-download-the-workflow) 1. Download the Workflow

The workflow information is included in the metadata of the video below. Download and drag it into ComfyUI to load the workflow.

Download the following image to use as input:

### [​](http://docs.comfy.org#2-follow-the-workflow-steps) 2. Follow the Workflow Steps

Follow these basic steps to run the workflow:

1. Upload your input image in the `first_image` node
2. (Optional) Write prompts in the Luma Image to Video node to describe how you want the image animated
3. (Optional) Modify the `Luma Concepts` node to control camera movement for professional cinematography
4. Click `Run` or use `Ctrl(cmd) + Enter` to generate the video
5. Once the API returns results, view the generated video in the `Save Video` node. The video will also be saved to the `ComfyUI/output/` directory

### [​](http://docs.comfy.org#3-additional-notes) 3. Additional Notes

- **Image Input Requirements**: At least one of `first_image` or `last_image` is required, with a maximum of 1 image per input
- **Luma Concepts**: Controls camera movement for professional video effects
- **Seed Parameter**: Only determines if the node should rerun, doesn’t affect generation results
- **Enable Input Nodes**: Right-click on purple “Bypass” mode nodes and set “mode” to “always” to enable inputs
- **Model Selection**: Different video generation models have unique characteristics, adjustable via the model parameter
- **Resolution and Duration**: Adjust output video resolution and length using resolution and duration parameters

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/luma/luma-image-to-video.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video)

[GPT-Image-1Learn how to use the OpenAI GPT-Image-1 API node to generate images in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/openai/gpt-image-1)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Luma Image to Video Node Documentation](http://docs.comfy.org#luma-image-to-video-node-documentation)
- [Image to Video Workflow with Luma API Node](http://docs.comfy.org#image-to-video-workflow-with-luma-api-node)
- [1. Download the Workflow](http://docs.comfy.org#1-download-the-workflow)
- [2. Follow the Workflow Steps](http://docs.comfy.org#2-follow-the-workflow-steps)
- [3. Additional Notes](http://docs.comfy.org#3-additional-notes)

<!-- END Built_In_Node/tutorials/api-nodes/luma/luma-image-to-video.md -->


<!-- BEGIN Built_In_Node/tutorials/api-nodes/luma/luma-text-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
  - Luma
    
    - [Luma Text to Image](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image)
    - [Luma Image to Image](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-image)
    - [Luma Text to Video](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video)
    - [Luma Image to Video](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video)
  - OpenAI
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Text to Image API Node ComfyUI Official Example

# Luma Text to Image API Node ComfyUI Official Example

This guide explains how to use the Luma Text to Image API node in ComfyUI

The [Luma Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image) node allows you to generate high-quality images from text prompts using Luma AI’s advanced technology, capable of creating photorealistic content and artistic style images.

In this guide, we’ll show you how to set up workflows using this node for text-to-image generation.

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#luma-text-to-image-node-documentation) Luma Text to Image Node Documentation

You can refer to the following documentation for detailed parameter settings:

[**Luma Text to Image Node Documentation**  
\
Luma Text to Image API Node Documentation](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image)

[**Luma Reference Node Documentation**  
\
Luma Reference API Node Documentation](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-reference)

## [​](http://docs.comfy.org#luma-text-to-image-api-node-workflow) Luma Text to Image API Node Workflow

When the `Luma Text to Image` node is used without any image inputs, it functions as a text-to-image workflow. In this guide, we’ve created examples using `style_image` and `image_luma_ref` to showcase Luma AI’s excellent image processing capabilities.

### [​](http://docs.comfy.org#1-download-workflow-files) 1. Download Workflow Files

The workflow information is included in the metadata of the image below. Download and drag it into ComfyUI to load the workflow.

Please download these images for input:

### [​](http://docs.comfy.org#2-follow-steps-to-run-the-workflow) 2. Follow Steps to Run the Workflow

Follow the numbered steps in the image to complete the basic workflow:

1. Upload the reference image in the `Load image` node
2. Upload the style reference image in the `Load image (renamed to styleref)` node
3. (Optional) Modify the prompts in the `Luma Text to Image` node
4. (Optional) Adjust the `style_image_weight` to control the style reference image’s influence
5. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to generate the image
6. After the API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory

### [​](http://docs.comfy.org#3-additional-notes) 3. Additional Notes

- The [node](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image) allows up to 4 reference images and character references simultaneously.
- To enable multiple image inputs, right-click on the purple “Bypassed” nodes and set their `mode` to `always`

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/luma/luma-text-to-image.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/ideogram/ideogram-v3)

[Luma Image to ImageThis guide covers how to use the Luma Image to Image API node in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Luma Text to Image Node Documentation](http://docs.comfy.org#luma-text-to-image-node-documentation)
- [Luma Text to Image API Node Workflow](http://docs.comfy.org#luma-text-to-image-api-node-workflow)
- [1. Download Workflow Files](http://docs.comfy.org#1-download-workflow-files)
- [2. Follow Steps to Run the Workflow](http://docs.comfy.org#2-follow-steps-to-run-the-workflow)
- [3. Additional Notes](http://docs.comfy.org#3-additional-notes)

<!-- END Built_In_Node/tutorials/api-nodes/luma/luma-text-to-image.md -->


<!-- BEGIN Built_In_Node/tutorials/api-nodes/luma/luma-text-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
  - Luma
    
    - [Luma Text to Image](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image)
    - [Luma Image to Image](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-image)
    - [Luma Text to Video](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video)
    - [Luma Image to Video](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video)
  - OpenAI
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Text to Video API Node ComfyUI Official Guide

# Luma Text to Video API Node ComfyUI Official Guide

Learn how to use the Luma Text to Video API node in ComfyUI

The [Luma Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video) node allows you to create high-quality, smooth videos from text descriptions using Luma AI’s innovative video generation technology.

In this guide, we’ll show you how to set up a text-to-video workflow using this node.

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#luma-text-to-video-node-documentation) Luma Text to Video Node Documentation

Check out the following documentation to learn more about the node parameters:

[**Luma Text to Video Node Docs**  
\
Documentation for the Luma Text to Video API node](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video)

[**Luma Concepts Node Docs**  
\
Documentation for the Luma Concepts API node](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)

## [​](http://docs.comfy.org#text-to-video-workflow-with-luma-api-node) Text to Video Workflow with Luma API Node

The Luma Text to Video node requires text prompts to describe the video content. In this guide, we’ve created examples using `prompt` and `luma_concepts` to showcase Luma AI’s excellent video generation capabilities.

### [​](http://docs.comfy.org#1-download-the-workflow) 1. Download the Workflow

The workflow information is included in the metadata of the video below. Download and drag it into ComfyUI to load the workflow.

### [​](http://docs.comfy.org#2-follow-the-steps) 2. Follow the Steps

Follow these basic steps to run the workflow:

1. Write your prompt in the `Luma Text to Video` node to describe the video content you want
2. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to generate the video
3. After the API returns results, you can view the generated video in the `Save Video` node. The video will also be saved to the `ComfyUI/output/` directory

> (Optional) Modify the `Luma Concepts` node to control camera movements and add professional cinematography

### [​](http://docs.comfy.org#3-additional-notes) 3. Additional Notes

- **Writing Prompts**: Describe scenes, subjects, actions, and mood in detail for best results
- **Luma Concepts**: Mainly used for camera control to create professional video shots
- **Seed Parameter**: Only determines if the node should rerun, doesn’t affect generation results
- **Model Selection**: Different video models have different features, adjustable via the model parameter
- **Resolution and Duration**: Adjust output video resolution and length using these parameters
- **Ray 1.6 Model Note**: Duration and resolution parameters don’t work when using the Ray 1.6 model

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/luma/luma-text-to-video.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-image)

[Luma Image to VideoLearn how to use the Luma Image to Video API node in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Luma Text to Video Node Documentation](http://docs.comfy.org#luma-text-to-video-node-documentation)
- [Text to Video Workflow with Luma API Node](http://docs.comfy.org#text-to-video-workflow-with-luma-api-node)
- [1. Download the Workflow](http://docs.comfy.org#1-download-the-workflow)
- [2. Follow the Steps](http://docs.comfy.org#2-follow-the-steps)
- [3. Additional Notes](http://docs.comfy.org#3-additional-notes)

<!-- END Built_In_Node/tutorials/api-nodes/luma/luma-text-to-video.md -->


<!-- BEGIN Built_In_Node/tutorials/api-nodes/openai/dall-e-2.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
  - Luma
  - OpenAI
    
    - [GPT-Image-1](http://docs.comfy.org/tutorials/api-nodes/openai/gpt-image-1)
    - [DALL·E 2](http://docs.comfy.org/tutorials/api-nodes/openai/dall-e-2)
    - [DALL·E 3](http://docs.comfy.org/tutorials/api-nodes/openai/dall-e-3)
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

OpenAI DALL·E 2 Node

# OpenAI DALL·E 2 Node

Learn how to use the OpenAI DALL·E 2 API node to generate images in ComfyUI

OpenAI DALL·E 2 is part of the ComfyUI API Nodes series, allowing users to generate images through OpenAI’s **DALL·E 2** model.

This node supports:

- Text-to-image generation
- Image editing functionality (inpainting through masks)

## [​](http://docs.comfy.org#node-overview) Node Overview

The **OpenAI DALL·E 2** node generates images synchronously through OpenAI’s image generation API. It receives text prompts and returns images that match the description.

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#parameter-description) Parameter Description

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterDescription`prompt`Text prompt describing the image content you want to generate

### [​](http://docs.comfy.org#widget-parameters) Widget Parameters

ParameterDescriptionOptions/RangeDefault Value`seed`Seed value for image generation (currently not implemented in the backend)0 to 2^31-10`size`Output image dimensions”256x256”, “512x512”, “1024x1024""1024x1024”`n`Number of images to generate1 to 81

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterDescriptionOptions/RangeDefault Value`image`Optional reference image for image editingAny image inputNone`mask`Optional mask for local inpaintingMask inputNone

## [​](http://docs.comfy.org#usage-method) Usage Method

## [​](http://docs.comfy.org#workflow-examples) Workflow Examples

This API node currently supports two workflows:

- Text to Image
- Inpainting

Image to Image workflow is not supported

### [​](http://docs.comfy.org#text-to-image-example) Text to Image Example

The image below contains a simple text-to-image workflow. Please download the corresponding image and drag it into ComfyUI to load the workflow.

The corresponding example is very simple

You only need to load the `OpenAI DALL·E 2` node, input the description of the image you want to generate in the `prompt` node, connect a `Save Image` node, and then run the workflow.

### [​](http://docs.comfy.org#inpainting-workflow) Inpainting Workflow

DALL·E 2 supports image editing functionality, allowing you to use a mask to specify the area to be replaced. Below is a simple inpainting workflow example:

#### [​](http://docs.comfy.org#1-workflow-file-download) 1. Workflow File Download

Download the image below and drag it into ComfyUI to load the corresponding workflow.

We will use the image below as input:

#### [​](http://docs.comfy.org#2-workflow-file-usage-instructions) 2. Workflow File Usage Instructions

Since this workflow is relatively simple, if you want to manually implement the corresponding workflow yourself, you can follow the steps below:

1. Use the `Load Image` node to load the image
2. Right-click on the load image node and select `MaskEditor`
3. In the mask editor, use the brush to draw the area you want to redraw
4. Connect the loaded image to the `image` input of the **OpenAI DALL·E 2** node
5. Connect the mask to the `mask` input of the **OpenAI DALL·E 2** node
6. Edit the prompt in the `prompt` node
7. Run the workflow

**Notes**

- If you want to use the image editing functionality, you must provide both an image and a mask (both are required)
- The mask and image must be the same size
- When inputting large images, the node will automatically resize the image to an appropriate size
- The URLs returned by the API are valid for a short period, please save the results promptly
- Each generation consumes credits, charged according to image size and quantity

## [​](http://docs.comfy.org#faqs) FAQs

Why can't I find the API nodes?

Please update your ComfyUI to the latest version (the latest commit or the latest [desktop version](https://www.comfy.org/download)). We may add more API support in the future, and the corresponding nodes will be updated, so please keep your ComfyUI up to date.

Please note that you need to distinguish between the nightly version and the release version. In some cases, the latest `release` version may not be updated in time compared to the `nightly` version. Since we are still iterating quickly, please ensure you are using the latest version when you cannot find the corresponding node.

Why can't I use / log in to the API Nodes?

API access requires that your current request is based on a secure network environment. The current requirements for API access are as follows:

- The local network only allows access from `127.0.0.1` or `localhost`, which may mean that you cannot use the API Nodes in a ComfyUI service started with the `--listen` parameter in a LAN environment.
- Able to access our API service normally (a proxy service may be required in some regions).
- Your account does not have enough [credits](http://docs.comfy.org/interface/credits).

Why can't I use API node even after logging in, or why does it keep asking me to log in while using?

- Currently, only `127.0.0.1` or `localhost` access is supported.
- Ensure your account has enough credits.

Can API Nodes be used for free?

API Nodes require credits for API calls to closed-source models, so they do not support free usage.

How to purchase credits?

Please refer to the following documentation:

1. [Comfy Account](http://docs.comfy.org/interface/user): Find the `User` section in the settings menu to log in.
2. [Credits](http://docs.comfy.org/interface/credits): After logging in, the settings interface will show the credits menu. You can purchase credits in `Settings` → `Credits`. We use a prepaid system, so there will be no unexpected charges.
3. Complete the payment through Stripe.
4. Check if the credits have been updated. If not, try restarting or refreshing the page.

Are unused credits refundable?

Currently, we do not support refunds for credits. If you believe there is an error resulting in unused balance due to technical issues, please [contact support](mailto:support@comfy.org).

Can credits go negative?

Credits cannot go negative, so please ensure you have enough credits before making the corresponding API calls.

Where can I check usage and expenses?

Please visit the [Credits](http://docs.comfy.org/interface/credits) menu after logging in to check the corresponding credits.

Is it possible to use my own API Key?

Currently, the API Nodes are still in the testing phase and do not support this feature yet, but we have considered adding it.

Do credits expire?

No, your credits do not expire.

Can credits be transferred or shared?

No, your credits cannot be transferred to other users and are limited to the currently logged-in account, but we do not restrict the number of devices that can log in.

Can I use the same account on different devices?

We do not limit the number of devices that can log in; you can use your account anywhere you want.

How can I request for my account or information to be deleted??

Email a request to [support@comfy.org](mailto:support@comfy.org) and we will delete your information

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/openai/dall-e-2.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/openai/gpt-image-1)

[DALL·E 3Learn how to use the OpenAI DALL·E 3 API node to generate images in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/openai/dall-e-3)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Overview](http://docs.comfy.org#node-overview)
- [Parameter Description](http://docs.comfy.org#parameter-description)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Widget Parameters](http://docs.comfy.org#widget-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Usage Method](http://docs.comfy.org#usage-method)
- [Workflow Examples](http://docs.comfy.org#workflow-examples)
- [Text to Image Example](http://docs.comfy.org#text-to-image-example)
- [Inpainting Workflow](http://docs.comfy.org#inpainting-workflow)
- [1. Workflow File Download](http://docs.comfy.org#1-workflow-file-download)
- [2. Workflow File Usage Instructions](http://docs.comfy.org#2-workflow-file-usage-instructions)
- [FAQs](http://docs.comfy.org#faqs)

<!-- END Built_In_Node/tutorials/api-nodes/openai/dall-e-2.md -->


<!-- BEGIN Built_In_Node/tutorials/api-nodes/openai/dall-e-3.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
  - Luma
  - OpenAI
    
    - [GPT-Image-1](http://docs.comfy.org/tutorials/api-nodes/openai/gpt-image-1)
    - [DALL·E 2](http://docs.comfy.org/tutorials/api-nodes/openai/dall-e-2)
    - [DALL·E 3](http://docs.comfy.org/tutorials/api-nodes/openai/dall-e-3)
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

OpenAI DALL·E 3 Node

# OpenAI DALL·E 3 Node

Learn how to use the OpenAI DALL·E 3 API node to generate images in ComfyUI

OpenAI DALL·E 3 is part of the ComfyUI API Nodes series, allowing users to generate images through OpenAI’s **DALL·E 3** model. This node supports text-to-image generation functionality.

## [​](http://docs.comfy.org#node-overview) Node Overview

DALL·E 3 is OpenAI’s latest image generation model, capable of creating detailed and high-quality images based on text prompts. Through this node in ComfyUI, you can directly access DALL·E 3’s generation capabilities without leaving the ComfyUI interface.

The **OpenAI DALL·E 3** node generates images synchronously through OpenAI’s image generation API. It receives text prompts and returns images that match the description.

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#parameter-details) Parameter Details

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDescriptionpromptTextText prompt for generating images. Supports multi-line input, can describe in detail the image content you want to generate.

### [​](http://docs.comfy.org#widget-parameters) Widget Parameters

ParameterTypeOptionsDefault ValueDescriptionseedInteger0-21474836470Random seed used to control the generation resultqualityOptionstandard, hdstandardImage quality setting. The “hd” option generates higher quality images but may require more computational resourcesstyleOptionnatural, vividnaturalImage style. “Vivid” tends to generate hyperrealistic and dramatic images, while “natural” produces more natural, less exaggerated imagessizeOption1024x1024, 1024x1792, 1792x10241024x1024Size of the generated image. You can choose square or rectangular images in different orientations

## [​](http://docs.comfy.org#usage-examples) Usage Examples

You can download the image below and drag it into ComfyUI to load the corresponding workflow

Since the corresponding workflow is very simple, you can also directly add the **OpenAI DALL·E 3** node in ComfyUI, input the description of the image you want to generate, and then run the workflow

1. Add the **OpenAI DALL·E 3** node in ComfyUI
2. Enter the description of the image you want to generate in the prompt text box
3. Adjust optional parameters as needed (quality, style, size, etc.)
4. Run the workflow to generate the image

## [​](http://docs.comfy.org#faqs) FAQs

Why can't I find the API nodes?

Please update your ComfyUI to the latest version (the latest commit or the latest [desktop version](https://www.comfy.org/download)). We may add more API support in the future, and the corresponding nodes will be updated, so please keep your ComfyUI up to date.

Please note that you need to distinguish between the nightly version and the release version. In some cases, the latest `release` version may not be updated in time compared to the `nightly` version. Since we are still iterating quickly, please ensure you are using the latest version when you cannot find the corresponding node.

Why can't I use / log in to the API Nodes?

API access requires that your current request is based on a secure network environment. The current requirements for API access are as follows:

- The local network only allows access from `127.0.0.1` or `localhost`, which may mean that you cannot use the API Nodes in a ComfyUI service started with the `--listen` parameter in a LAN environment.
- Able to access our API service normally (a proxy service may be required in some regions).
- Your account does not have enough [credits](http://docs.comfy.org/interface/credits).

Why can't I use API node even after logging in, or why does it keep asking me to log in while using?

- Currently, only `127.0.0.1` or `localhost` access is supported.
- Ensure your account has enough credits.

Can API Nodes be used for free?

API Nodes require credits for API calls to closed-source models, so they do not support free usage.

How to purchase credits?

Please refer to the following documentation:

1. [Comfy Account](http://docs.comfy.org/interface/user): Find the `User` section in the settings menu to log in.
2. [Credits](http://docs.comfy.org/interface/credits): After logging in, the settings interface will show the credits menu. You can purchase credits in `Settings` → `Credits`. We use a prepaid system, so there will be no unexpected charges.
3. Complete the payment through Stripe.
4. Check if the credits have been updated. If not, try restarting or refreshing the page.

Are unused credits refundable?

Currently, we do not support refunds for credits. If you believe there is an error resulting in unused balance due to technical issues, please [contact support](mailto:support@comfy.org).

Can credits go negative?

Credits cannot go negative, so please ensure you have enough credits before making the corresponding API calls.

Where can I check usage and expenses?

Please visit the [Credits](http://docs.comfy.org/interface/credits) menu after logging in to check the corresponding credits.

Is it possible to use my own API Key?

Currently, the API Nodes are still in the testing phase and do not support this feature yet, but we have considered adding it.

Do credits expire?

No, your credits do not expire.

Can credits be transferred or shared?

No, your credits cannot be transferred to other users and are limited to the currently logged-in account, but we do not restrict the number of devices that can log in.

Can I use the same account on different devices?

We do not limit the number of devices that can log in; you can use your account anywhere you want.

How can I request for my account or information to be deleted??

Email a request to [support@comfy.org](mailto:support@comfy.org) and we will delete your information

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/openai/dall-e-3.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/openai/dall-e-2)

[Recraft Text to ImageLearn how to use the Recraft Text to Image API node in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Overview](http://docs.comfy.org#node-overview)
- [Parameter Details](http://docs.comfy.org#parameter-details)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Widget Parameters](http://docs.comfy.org#widget-parameters)
- [Usage Examples](http://docs.comfy.org#usage-examples)
- [FAQs](http://docs.comfy.org#faqs)

<!-- END Built_In_Node/tutorials/api-nodes/openai/dall-e-3.md -->


<!-- BEGIN Built_In_Node/tutorials/api-nodes/openai/gpt-image-1.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
  - Luma
  - OpenAI
    
    - [GPT-Image-1](http://docs.comfy.org/tutorials/api-nodes/openai/gpt-image-1)
    - [DALL·E 2](http://docs.comfy.org/tutorials/api-nodes/openai/dall-e-2)
    - [DALL·E 3](http://docs.comfy.org/tutorials/api-nodes/openai/dall-e-3)
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

OpenAI GPT-Image-1 Node

# OpenAI GPT-Image-1 Node

Learn how to use the OpenAI GPT-Image-1 API node to generate images in ComfyUI

OpenAI GPT-Image-1 is part of the ComfyUI API nodes series that allows users to generate images through OpenAI’s **GPT-Image-1** model. This is the same model used for image generation in ChatGPT 4o.

This node supports:

- Text-to-image generation
- Image editing functionality (inpainting through masks)

## [​](http://docs.comfy.org#node-overview) Node Overview

The **OpenAI GPT-Image-1** node synchronously generates images through OpenAI’s image generation API. It receives text prompts and returns images matching the description. GPT-Image-1 is OpenAI’s most advanced image generation model currently available, capable of creating highly detailed and realistic images.

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#parameter-description) Parameter Description

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDescription`prompt`TextText prompt describing the image content you want to generate

### [​](http://docs.comfy.org#widget-parameters) Widget Parameters

ParameterTypeOptionsDefaultDescription`seed`Integer0-21474836470Random seed used to control generation results`quality`Optionlow, medium, highlowImage quality setting, affects cost and generation time`background`Optionopaque, transparentopaqueWhether the returned image has a background`size`Optionauto, 1024x1024, 1024x1536, 1536x1024autoSize of the generated image`n`Integer1-81Number of images to generate

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeOptionsDefaultDescription`image`ImageAny image inputNoneOptional reference image for image editing`mask`MaskMask inputNoneOptional mask for inpainting (white areas will be replaced)

## [​](http://docs.comfy.org#usage-examples) Usage Examples

### [​](http://docs.comfy.org#text-to-image-example) Text-to-Image Example

The image below contains a simple text-to-image workflow. Please download the image and drag it into ComfyUI to load the corresponding workflow.

The corresponding workflow is very simple:

You only need to load the `OpenAI GPT-Image-1` node, input the description of the image you want to generate in the `prompt` node, connect a `Save Image` node, and then run the workflow.

### [​](http://docs.comfy.org#image-to-image-example) Image-to-Image Example

The image below contains a simple image-to-image workflow. Please download the image and drag it into ComfyUI to load the corresponding workflow.

We will use the image below as input:

In this workflow, we use the `OpenAI GPT-Image-1` node to generate images and the `Load Image` node to load the input image, then connect it to the `image` input of the `OpenAI GPT-Image-1` node.

### [​](http://docs.comfy.org#multiple-image-input-example) Multiple Image Input Example

Please download the image below and drag it into ComfyUI to load the corresponding workflow.

Use the hat image below as an additional input image.

The corresponding workflow is shown in the image below:

The `Batch Images` node is used to load multiple images into the `OpenAI GPT-Image-1` node.

### [​](http://docs.comfy.org#inpainting-workflow) Inpainting Workflow

GPT-Image-1 also supports image editing functionality, allowing you to specify areas to replace using a mask. Below is a simple inpainting workflow example:

Download the image below and drag it into ComfyUI to load the corresponding workflow. We will continue to use the input image from the image-to-image workflow section.

The corresponding workflow is shown in the image

Compared to the image-to-image workflow, we use the MaskEditor in the `Load Image` node through the right-click menu to draw a mask, then connect it to the `mask` input of the `OpenAI GPT-Image-1` node to complete the workflow.

**Notes**

- The mask and image must be the same size
- When inputting large images, the node will automatically resize the image to an appropriate size

## [​](http://docs.comfy.org#faqs) FAQs

Why can't I find the API nodes?

Please update your ComfyUI to the latest version (the latest commit or the latest [desktop version](https://www.comfy.org/download)). We may add more API support in the future, and the corresponding nodes will be updated, so please keep your ComfyUI up to date.

Please note that you need to distinguish between the nightly version and the release version. In some cases, the latest `release` version may not be updated in time compared to the `nightly` version. Since we are still iterating quickly, please ensure you are using the latest version when you cannot find the corresponding node.

Why can't I use / log in to the API Nodes?

API access requires that your current request is based on a secure network environment. The current requirements for API access are as follows:

- The local network only allows access from `127.0.0.1` or `localhost`, which may mean that you cannot use the API Nodes in a ComfyUI service started with the `--listen` parameter in a LAN environment.
- Able to access our API service normally (a proxy service may be required in some regions).
- Your account does not have enough [credits](http://docs.comfy.org/interface/credits).

Why can't I use API node even after logging in, or why does it keep asking me to log in while using?

- Currently, only `127.0.0.1` or `localhost` access is supported.
- Ensure your account has enough credits.

Can API Nodes be used for free?

API Nodes require credits for API calls to closed-source models, so they do not support free usage.

How to purchase credits?

Please refer to the following documentation:

1. [Comfy Account](http://docs.comfy.org/interface/user): Find the `User` section in the settings menu to log in.
2. [Credits](http://docs.comfy.org/interface/credits): After logging in, the settings interface will show the credits menu. You can purchase credits in `Settings` → `Credits`. We use a prepaid system, so there will be no unexpected charges.
3. Complete the payment through Stripe.
4. Check if the credits have been updated. If not, try restarting or refreshing the page.

Are unused credits refundable?

Currently, we do not support refunds for credits. If you believe there is an error resulting in unused balance due to technical issues, please [contact support](mailto:support@comfy.org).

Can credits go negative?

Credits cannot go negative, so please ensure you have enough credits before making the corresponding API calls.

Where can I check usage and expenses?

Please visit the [Credits](http://docs.comfy.org/interface/credits) menu after logging in to check the corresponding credits.

Is it possible to use my own API Key?

Currently, the API Nodes are still in the testing phase and do not support this feature yet, but we have considered adding it.

Do credits expire?

No, your credits do not expire.

Can credits be transferred or shared?

No, your credits cannot be transferred to other users and are limited to the currently logged-in account, but we do not restrict the number of devices that can log in.

Can I use the same account on different devices?

We do not limit the number of devices that can log in; you can use your account anywhere you want.

How can I request for my account or information to be deleted??

Email a request to [support@comfy.org](mailto:support@comfy.org) and we will delete your information

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/openai/gpt-image-1.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video)

[DALL·E 2Learn how to use the OpenAI DALL·E 2 API node to generate images in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/openai/dall-e-2)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Overview](http://docs.comfy.org#node-overview)
- [Parameter Description](http://docs.comfy.org#parameter-description)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Widget Parameters](http://docs.comfy.org#widget-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Usage Examples](http://docs.comfy.org#usage-examples)
- [Text-to-Image Example](http://docs.comfy.org#text-to-image-example)
- [Image-to-Image Example](http://docs.comfy.org#image-to-image-example)
- [Multiple Image Input Example](http://docs.comfy.org#multiple-image-input-example)
- [Inpainting Workflow](http://docs.comfy.org#inpainting-workflow)
- [FAQs](http://docs.comfy.org#faqs)

<!-- END Built_In_Node/tutorials/api-nodes/openai/gpt-image-1.md -->


<!-- BEGIN Built_In_Node/tutorials/api-nodes/overview.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
  - Luma
  - OpenAI
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

API Nodes

# API Nodes

In this article, we will introduce ComfyUI’s API Nodes and related information.

API Nodes are ComfyUI’s new way of calling closed-source models through API requests, providing ComfyUI users with access to external state-of-the-art AI models without complex API key setup.

## [​](http://docs.comfy.org#what-are-api-nodes%3F) What are API Nodes?

API Nodes are a set of special nodes that connect to external API services, allowing you to use closed-source or third-party hosted AI models directly in your ComfyUI workflows. These nodes are designed to seamlessly integrate the capabilities of external models while maintaining the open-source nature of ComfyUI’s core.

Currently supported models include:

- Black Forest Labs Flux 1.1\[pro] Ultra, Flux .1\[pro]
- Kling 2.0, 1.6, 1.5 &amp; Various Effects
- Luma Photon, Ray2, Ray1.6
- MiniMax Text-to-Video, Image-to-Video
- PixVerse V4 &amp; Effects
- Recraft V3, V2 &amp; Various Tools
- Stability AI Stable Image Ultra, Stable Diffusion 3.5 Large
- Google Veo2
- Ideogram V3, V2, V1
- OpenAI GPT4o image
- Pika 2.2

## [​](http://docs.comfy.org#prerequisites-for-using-api-nodes) Prerequisites for Using API Nodes

To use API Nodes, the following requirements must be met:

### [​](http://docs.comfy.org#1-comfyui-version-requirements) 1. ComfyUI Version Requirements

Please update your ComfyUI to the latest version, as we may add more API support in the future, and corresponding nodes will be updated, so please keep your ComfyUI up to date.

Please note the distinction between nightly and release versions. We recommend using the `nightly` version (which is the latest code commit), as the release version may not be updated in a timely manner. This refers to the development version and the stable version, and since we are still rapidly iterating, this document may not be updated promptly, so please pay attention to the version differences.

### [​](http://docs.comfy.org#2-network-environment-requirements) 2. Network Environment Requirements

API access requires that your current requests are based on a secure network environment. The current requirements for API access are as follows:

- Local networks only allow access from `127.0.0.1`. We do not support accessing via LAN IPs without `https` as it is insecure. This may mean that you cannot use API Nodes in a ComfyUI service launched with the `--listen` parameter in a LAN environment.
- You must be able to access our API services normally (in some regions, you may need to use a proxy service).

Accessing in an insecure context poses significant risks, which may result in the following consequences:

1. Authentication may be stolen, leading to the leakage of your account information.
2. Your account may be maliciously used, resulting in financial losses.

Even if we open this restriction in the future, we strongly advise against accessing API services through insecure network requests due to the high risks involved.

### [​](http://docs.comfy.org#3-account-and-credits-requirements) 3. Account and Credits Requirements

You need to be logged into your ComfyUI with a [Comfy account](http://docs.comfy.org/zh-CN/interface/user) and have a credit balance of [credits](http://docs.comfy.org/zh-CN/interface/credits) greater than 0.

Please refer to the corresponding documentation for account and credits to ensure this requirement:

- [Comfy account](http://docs.comfy.org/zh-CN/interface/user): Find the `User` section in the settings menu to log in.
- [Credits](http://docs.comfy.org/zh-CN/interface/credits): After logging in, the settings interface will show a credits menu where you can purchase credits. We use a prepaid system, so there will be no unexpected charges.

### [​](http://docs.comfy.org#4-using-the-corresponding-nodes) 4. Using the Corresponding Nodes

**Add to Workflow**: Add the API node to your workflow just like you would with other nodes. **Run**: Set the parameters and then run the workflow.

## [​](http://docs.comfy.org#log-in-with-api-key-on-non-whitelisted-websites) Log in with API Key on non-whitelisted websites

Currently, we have set up a whitelist to restrict the websites where you can log in to your ComfyUI account. If you need to log in to your ComfyUI account on some non-whitelisted websites, please refer to the account management section to learn how to log in using an API Key. In this case, the corresponding website does not need to be on our whitelist.

[**Account Management**  
\
Learn how to log in with ComfyUI API Key](http://docs.comfy.org/interface/user#logging-in-with-an-api-key)

## [​](http://docs.comfy.org#advantages-of-api-nodes) Advantages of API Nodes

API Nodes provide several important advantages for ComfyUI users:

- **Access to closed-source models**: Use state-of-the-art AI models without having to deploy them yourself
- **Seamless integration**: API nodes are fully compatible with other ComfyUI nodes and can be combined to create complex workflows
- **Simplified experience**: No need to manage API keys or handle complex API requests
- **Controlled costs**: The prepaid system ensures you have complete control over your spending with no unexpected charges

## [​](http://docs.comfy.org#pricing) Pricing

[**API Node Pricing**  
\
Please refer to the pricing page for the corresponding API pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)

## [​](http://docs.comfy.org#about-open-source-and-opt-in) About Open Source and Opt-in

It’s important to note that **API Nodes are completely optional**. ComfyUI will always remain fully open-source and free for local users. API nodes are designed as an “opt-in” feature, providing convenience for those who want access to external SOTA (state-of-the-art) models.

## [​](http://docs.comfy.org#use-cases) Use Cases

A powerful application of API Nodes is combining the output of external models with local nodes. For example:

- Using GPT-Image-1 to generate a base image, then transforming it into video with a local `wan` node
- Combining externally generated images with local upscaling or style transfer nodes
- Creating hybrid workflows that leverage the advantages of both closed-source and open-source models

This flexibility makes ComfyUI a truly universal generative AI interface, integrating various AI capabilities into a unified workflow, opening up more possibilities

## [​](http://docs.comfy.org#faqs) FAQs

Why can't I find the API nodes?

Please update your ComfyUI to the latest version (the latest commit or the latest [desktop version](https://www.comfy.org/download)). We may add more API support in the future, and the corresponding nodes will be updated, so please keep your ComfyUI up to date.

Please note that you need to distinguish between the nightly version and the release version. In some cases, the latest `release` version may not be updated in time compared to the `nightly` version. Since we are still iterating quickly, please ensure you are using the latest version when you cannot find the corresponding node.

Why can't I use / log in to the API Nodes?

API access requires that your current request is based on a secure network environment. The current requirements for API access are as follows:

- The local network only allows access from `127.0.0.1` or `localhost`, which may mean that you cannot use the API Nodes in a ComfyUI service started with the `--listen` parameter in a LAN environment.
- Able to access our API service normally (a proxy service may be required in some regions).
- Your account does not have enough [credits](http://docs.comfy.org/interface/credits).

Why can't I use API node even after logging in, or why does it keep asking me to log in while using?

- Currently, only `127.0.0.1` or `localhost` access is supported.
- Ensure your account has enough credits.

Can API Nodes be used for free?

API Nodes require credits for API calls to closed-source models, so they do not support free usage.

How to purchase credits?

Please refer to the following documentation:

1. [Comfy Account](http://docs.comfy.org/interface/user): Find the `User` section in the settings menu to log in.
2. [Credits](http://docs.comfy.org/interface/credits): After logging in, the settings interface will show the credits menu. You can purchase credits in `Settings` → `Credits`. We use a prepaid system, so there will be no unexpected charges.
3. Complete the payment through Stripe.
4. Check if the credits have been updated. If not, try restarting or refreshing the page.

Are unused credits refundable?

Currently, we do not support refunds for credits. If you believe there is an error resulting in unused balance due to technical issues, please [contact support](mailto:support@comfy.org).

Can credits go negative?

Credits cannot go negative, so please ensure you have enough credits before making the corresponding API calls.

Where can I check usage and expenses?

Please visit the [Credits](http://docs.comfy.org/interface/credits) menu after logging in to check the corresponding credits.

Is it possible to use my own API Key?

Currently, the API Nodes are still in the testing phase and do not support this feature yet, but we have considered adding it.

Do credits expire?

No, your credits do not expire.

Can credits be transferred or shared?

No, your credits cannot be transferred to other users and are limited to the currently logged-in account, but we do not restrict the number of devices that can log in.

Can I use the same account on different devices?

We do not limit the number of devices that can log in; you can use your account anywhere you want.

How can I request for my account or information to be deleted??

Email a request to [support@comfy.org](mailto:support@comfy.org) and we will delete your information

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/overview.mdx)

[Previous](http://docs.comfy.org/tutorials/audio/ace-step/ace-step-v1)

[FAQsSome FAQs you may encounter when using API Nodes.  
\
Next](http://docs.comfy.org/tutorials/api-nodes/faq)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [What are API Nodes?](http://docs.comfy.org#what-are-api-nodes%3F)
- [Prerequisites for Using API Nodes](http://docs.comfy.org#prerequisites-for-using-api-nodes)
- [1. ComfyUI Version Requirements](http://docs.comfy.org#1-comfyui-version-requirements)
- [2. Network Environment Requirements](http://docs.comfy.org#2-network-environment-requirements)
- [3. Account and Credits Requirements](http://docs.comfy.org#3-account-and-credits-requirements)
- [4. Using the Corresponding Nodes](http://docs.comfy.org#4-using-the-corresponding-nodes)
- [Log in with API Key on non-whitelisted websites](http://docs.comfy.org#log-in-with-api-key-on-non-whitelisted-websites)
- [Advantages of API Nodes](http://docs.comfy.org#advantages-of-api-nodes)
- [Pricing](http://docs.comfy.org#pricing)
- [About Open Source and Opt-in](http://docs.comfy.org#about-open-source-and-opt-in)
- [Use Cases](http://docs.comfy.org#use-cases)
- [FAQs](http://docs.comfy.org#faqs)

<!-- END Built_In_Node/tutorials/api-nodes/overview.md -->


<!-- BEGIN Built_In_Node/tutorials/api-nodes/pricing.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
  - Luma
  - OpenAI
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Pricing

# Pricing

This article lists the pricing of the current API Nodes.

The following table lists the pricing of the current API Nodes.

APISpecsPriceOpenAIdall-e-2, 1024×1024$0.02OpenAIdall-e-2, 256×256$0.016OpenAIdall-e-2, 512×512$0.018OpenAIdall-e-3, 1024×1024, hd$0.08OpenAIdall-e-3, 1024×1792, hd$0.12OpenAIdall-e-3, 1024×1024, standard$0.04OpenAIdall-e-3, 1024×1792, standard$0.08OpenAIgpt-image-1, input image tokens$10 / 1M tokensOpenAIgpt-image-1, input text tokens$5 / 1M tokensOpenAIgpt-image-1, output tokens$40 / 1M tokensBFLflux-dev$0.025BFLflux-pro-1.1$0.04BFLflux-pro-1.1-ultra$0.06BFLflux-pro-1.1-pro$0.05BFLflux tools (edit, fill, expand, canny)$0.05Veoveo-2.0-generate-001$0.5 / secondMiniMaxI2V-01-Director$0.43MiniMaxI2V-01-live$0.43MiniMaxI2V-01$0.43MiniMaxS2V-01 (not enabled yet)$0.65MiniMaxT2V-01-Director$0.43MiniMaxT2V-01$0.43IdeogramV2\_edit$0.08IdeogramV2\_TURBO\_edit$0.05IdeogramV1\_generate$0.06IdeogramV1\_TURBO\_generate$0.02IdeogramV2A\_generate$0.04IdeogramV2A\_TURBO\_generate$0.025IdeogramV2\_generate$0.08IdeogramV2\_TURBO\_generate$0.05IdeogramV2\_reframe$0.08IdeogramV2\_TURBO\_reframe$0.05IdeogramV1\_remix$0.02IdeogramV2A\_remix$0.04IdeogramV2A\_TURBO\_remix$0.025IdeogramV2\_remix$0.08IdeogramV2\_TURBO\_remix$0.05IdeogramV3\_edit\_BALANCED$0.06IdeogramV3\_edit\_QUALITY$0.09IdeogramV3\_edit\_TURBO$0.03IdeogramV3\_generate\_BALANCED$0.06IdeogramV3\_generate\_QUALITY$0.09IdeogramV3\_generate\_TURBO$0.03IdeogramV3\_reframe\_QUALITY$0.09IdeogramV3\_reframe\_BALANCED$0.06IdeogramV3\_reframe\_TURBO$0.03IdeogramV3\_remix\_BALANCED$0.06IdeogramV3\_remix\_QUALITY$0.09IdeogramV3\_remix\_TURBO$0.03IdeogramV3\_replace-background\_QUALITY$0.09IdeogramV3\_replace-background\_TURBO$0.03IdeogramV3\_replace-background\_BALANCED$0.06Runwaygen3a\_turbo$0.05 / sRunwaygen4\_turbo$0.05 / sRecraftrecraftv2\_digital\_illustration$0.022Recraftrecraftv2\_icon$0.044Recraftrecraftv2\_logo\_raster$0.022Recraftrecraftv2\_realistic\_image$0.022Recraftrecraftv2\_vector\_illustration$0.044Recraftrecraftv3\_digital\_illustration$0.04Recraftrecraftv3\_logo\_raster  $0.04Recraftrecraftv3\_realistic\_image$0.04Recraftrecraftv3\_vector\_illustration$0.08Recraftv1/images/removeBackground$0.01Recraftv1/images/imageToImage0.04−0.04 - 0.04−0.08Recraftv1/images/inpaint0.04−0.04 - 0.04−0.08Recraftv1/images/replaceBackground0.04−0.04 - 0.04−0.08Recraftv1/images/vectorize$0.01Recraftv1/images/creativeUpscale$0.25Recraftv1/images/crispUpscale$0.004Lumaray-1-6$0.0032 / 1M pixelsLumaray-2$0.0064 / 1M pixelsLumarayflash-2$0.0022 / 1M pixelsLumaText to Image$0.0019 / 1M pixelsLumaImage to Image$0.0073 / 1M pixelsKlingpro\_kling-v5$0.49Klingpro\_kling-v1-6$0.49Klingpro\_kling-v1$0.49Klingpro\_kling-v2-master$1.4Klingstd\_kling-v1-5$0.28Klingstd\_kling-v1-6$0.28Klingstd\_kling-v1$0.14Klingstd\_kling-v2-master$1.4Klingtext to image (kling-v1)$0.0035Klingimage to image (kling-v1)$0.0035Klingtext to image (kling-v1-5)$0.014Klingtext to image (kling-v2)$0.014Klingimage to image (kling-v1-5)$0.028KlingVirtual Try On (kolors v1, v1-5)0.07Klingvideo extension$0.28Klingvideo effects (hug, kiss, heart\_gesture)Priced the same as t2v based on mode, model, and duration.Klingvideo effects (fuzzyfuzzy/squish/expansion)$0.28Klingvideo effects (dizzydizzy/bloombloom)$0.49Klinglip sync (5s)$0.07Klinglip sync (10s)$0.14PixVerse8s, v4\_360p\_normal i2v/i2v/template$0.9PixVerse8s, v4\_540p\_normal$0.9PixVerse8s, v4\_720p\_normal$1.2PixVerse5s, v4\_360p\_fast$0.9PixVerse5s, v4\_540p\_fast$0.9PixVerse5s, v4\_720p\_fast$1.2PixVerse5s, v4\_1080p\_fast$1.2PixVerse5s, v4\_360p\_normal$0.45PixVerse5s, v4\_540p\_normal$0.45PixVerse5s, v4\_720p\_normal$0.6Stability AIv2beta/stable-image/generate/ultra$0.08Stability AIv2beta/stable-image/generate/core0.03Stability AIv2beta/stable-image/generate/sd3.50.035−0.035 - 0.035−0.065Stability AIv2beta/stable-image/upscale0.01−0.01 - 0.01−0.25Pika5s\_generate/v2.2/i2v\_1080p$0.45Pika5s\_generate/v2.2/i2v\_720p$0.2Pika5s\_generate/v2.2/pikaframes\_1080p$0.3Pika5s\_generate/v2.2/pikaframes\_720p$0.2Pika5s\_generate/v2.2/pikascenes\_1080p$0.5Pika5s\_generate/v2.2/pikascenes\_720p$0.3Pika5s\_generate/v2.2/t2v\_1080p$0.45Pika5s\_generate/v2.2/t2v\_720p$0.2Pika10s\_generate/v2.2/i2v\_1080p$1Pika10s\_generate/v2.2/i2v\_720p$0.6Pika10s\_generate/v2.2/pikaframes\_1080p$1Pika10s\_generate/v2.2/pikaframes\_720p$0.25Pika10s\_generate/v2.2/pikascenes\_1080p$1.5Pika10s\_generate/v2.2/pikascenes\_720p$0.4Pika10s\_generate/v2.2/t2v\_1080p$1Pika10s\_generate/v2.2/t2v\_720p$0.6Pika5s\_pikaffects\_720p$0.45Pika5s\_pikadditions\_720p$0.30Pika5s\_pikaswaps\_720p$0.30

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/pricing.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/faq)

[Flux 1.1 Pro Ultra ImageThis guide covers how to use the Flux 1.1 Pro Ultra Image API node in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/black-forest-labs/flux-1-1-pro-ultra-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

<!-- END Built_In_Node/tutorials/api-nodes/pricing.md -->


<!-- BEGIN Built_In_Node/tutorials/api-nodes/recraft/recraft-text-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
  - Luma
  - OpenAI
  - Recraft
    
    - [Recraft Text to Image](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Text to Image API Node ComfyUI Official Example

# Recraft Text to Image API Node ComfyUI Official Example

Learn how to use the Recraft Text to Image API node in ComfyUI

The [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image) node allows you to create high-quality images in various styles using Recraft AI’s image generation technology based on text descriptions.

In this guide, we’ll show you how to set up a text-to-image workflow using this node.

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#recraft-text-to-image-api-node-workflow) Recraft Text to Image API Node Workflow

### [​](http://docs.comfy.org#1-download-the-workflow-file) 1. Download the Workflow File

The workflow information is included in the metadata of the image below. Download and drag it into ComfyUI to load the workflow.

### [​](http://docs.comfy.org#2-follow-the-steps-to-run-the-workflow) 2. Follow the Steps to Run the Workflow

Follow these numbered steps to run the basic workflow:

1. (Optional) Change the `Recraft Color RGB` in the `Color` node to your desired color
2. (Optional) Modify the `Recraft Style` node to control the visual style, such as digital art, realistic photo, or logo design. This group includes other style nodes you can enable as needed
3. (Optional) Edit the `prompt` parameter in the `Recraft Text to Image` node. You can also change the `size` parameter
4. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to generate the image
5. After the API returns the result, you can view the generated image in the `Save Image` node. The image will also be saved to the `ComfyUI/output/` directory

> (Optional) We’ve included a **Convert to SVG** group in the workflow. Since the `Recraft Vectorize Image` node in this group consumes additional credits, enable it only when you need to convert the generated image to SVG format

### [​](http://docs.comfy.org#3-additional-notes) 3. Additional Notes

- **Recraft Style**: Offers various preset styles like realistic photos, digital art, and logo designs
- **Seed Parameter**: Only used to determine if the node should run again, the actual generation result is not affected by the seed value

## [​](http://docs.comfy.org#related-node-documentation) Related Node Documentation

Check the following documentation for detailed parameter settings of the nodes

[**Recraft Text to Image Node Documentation**  
\
Documentation for the Recraft Text to Image API node](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)

[**Recraft Style Node Documentation**  
\
Documentation for the Recraft Style - Realistic Image API node](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)

[**Recraft Controls Node Documentation**  
\
Documentation for the Recraft Controls API node](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/recraft/recraft-text-to-image.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/openai/dall-e-3)

[Contributing  
\
Next](http://docs.comfy.org/community/contributing)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Recraft Text to Image API Node Workflow](http://docs.comfy.org#recraft-text-to-image-api-node-workflow)
- [1. Download the Workflow File](http://docs.comfy.org#1-download-the-workflow-file)
- [2. Follow the Steps to Run the Workflow](http://docs.comfy.org#2-follow-the-steps-to-run-the-workflow)
- [3. Additional Notes](http://docs.comfy.org#3-additional-notes)
- [Related Node Documentation](http://docs.comfy.org#related-node-documentation)

<!-- END Built_In_Node/tutorials/api-nodes/recraft/recraft-text-to-image.md -->


<!-- BEGIN Built_In_Node/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
    
    - [Stable Image Ultra](http://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-image-ultra)
    - [Stable Diffusion 3.5 Image](http://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image)
  - Ideogram
  - Luma
  - OpenAI
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Stability AI Stable Diffusion 3.5 API Node ComfyUI Official Example

# Stability AI Stable Diffusion 3.5 API Node ComfyUI Official Example

This article will introduce how to use Stability AI Stable Diffusion 3.5 API node’s text-to-image and image-to-image capabilities in ComfyUI

The [Stability AI Stable Diffusion 3.5 Image](http://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image) node allows you to use Stability AI’s Stable Diffusion 3.5 model to create high-quality, detail-rich image content through text prompts or reference images.

In this guide, we will show you how to set up workflows for both text-to-image and image-to-image generation using this node.

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#stability-ai-stable-diffusion-3-5-text-to-image-workflow) Stability AI Stable Diffusion 3.5 Text-to-Image Workflow

### [​](http://docs.comfy.org#1-workflow-file-download) 1. Workflow File Download

The image below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow.

### [​](http://docs.comfy.org#2-complete-the-workflow-step-by-step) 2. Complete the Workflow Step by Step

You can follow the numbered steps in the image to complete the basic text-to-image workflow:

1. (Optional) Modify the `prompt` parameter in the `Stability AI Stable Diffusion 3.5 Image` node to input your desired image description. More detailed prompts often result in better image quality.
2. (Optional) Select the `model` parameter to choose which SD 3.5 model version to use.
3. (Optional) Select the `style_preset` parameter to control the visual style of the image. Different presets produce images with different stylistic characteristics, such as “cinematic” or “anime”. Select “None” to not apply any specific style.
4. (Optional) Edit the `String(Multiline)` to modify negative prompts, specifying elements you don’t want to appear in the generated image.
5. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation.
6. After the API returns results, you can view the generated image in the `Save Image` node. The image will also be saved to the `ComfyUI/output/` directory.

### [​](http://docs.comfy.org#3-additional-notes) 3. Additional Notes

- **Prompt**: The prompt is one of the most important parameters in the generation process. Detailed, clear descriptions lead to better results. Can include elements like scene, subject, colors, lighting, and style.
- **CFG Scale**: Controls how closely the generator follows the prompt. Higher values make the image more closely match the prompt description, but too high may result in oversaturated or unnatural results.
- **Style Preset**: Offers various preset styles for quickly defining the overall style of the image.
- **Negative Prompt**: Used to specify elements you don’t want to appear in the generated image.
- **Seed Parameter**: Can be used to reproduce or fine-tune generation results, helpful for iteration during creation.
- Currently the `Load Image` node is in “Bypass” mode. To enable it, refer to the step guide and right-click the node to set “Mode” to “Always” to enable input, switching to image-to-image mode.
- `image_denoise` has no effect when there is no input image.

## [​](http://docs.comfy.org#stability-ai-stable-diffusion-3-5-image-to-image-workflow) Stability AI Stable Diffusion 3.5 Image-to-Image Workflow

### [​](http://docs.comfy.org#1-workflow-file-download-2) 1. Workflow File Download

The image below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow.

Download the image below to use as input !\[Stability AI Stable Diffusion 3.5 Image-to-Image Workflow Input Image](

### [​](http://docs.comfy.org#2-complete-the-workflow-step-by-step-2) 2. Complete the Workflow Step by Step

You can follow the numbered steps in the image to complete the image-to-image workflow:

1. Load a reference image through the `Load Image` node, which will serve as the basis for generation.
2. (Optional) Modify the `prompt` parameter in the `Stability AI Stable Diffusion 3.5 Image` node to describe elements you want to change or enhance in the reference image.
3. (Optional) Select the `style_preset` parameter to control the visual style of the image. Different presets produce images with different stylistic characteristics.
4. (Optional|Important) Adjust the `image_denoise` parameter (range 0.0-1.0) to control how much the original image is modified:
   
   - Values closer to 0.0 make the generated image more similar to the input reference image (at 0.0, it’s basically identical to the original)
   - Values closer to 1.0 make the generated image more like pure text-to-image generation (at 1.0, it’s as if no reference image was provided)
5. (Optional) Edit the `String(Multiline)` to modify negative prompts, specifying elements you don’t want to appear in the generated image.
6. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation.
7. After the API returns results, you can view the generated image in the `Save Image` node. The image will also be saved to the `ComfyUI/output/` directory.

### [​](http://docs.comfy.org#3-additional-notes-2) 3. Additional Notes

The image below shows a comparison of results with and without input image using the same parameter settings:

**Image Denoise**: This parameter determines how much of the original image’s features are preserved during generation. It’s the most crucial adjustment parameter in image-to-image mode. The image below shows the effects of different denoising strengths:

- **Reference Image Selection**: Choosing images with clear subjects and good composition usually yields better results.
- **Prompt Tips**: In image-to-image mode, prompts should focus more on elements you want to change or enhance, rather than describing everything already present in the image.
- **Mode Switching**: When an input image is provided, the node automatically switches from text-to-image mode to image-to-image mode, and aspect ratio parameters are ignored.

## [​](http://docs.comfy.org#related-node-details) Related Node Details

You can refer to the documentation below to understand detailed parameter settings for the corresponding node

[**Stability Stable Diffusion 3.5 Image Node Documentation**  
\
Stability Stable Diffusion 3.5 Image API Node Documentation](http://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-image-ultra)

[Ideogram 3.0This guide covers how to use the Ideogram 3.0 API node in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/ideogram/ideogram-v3)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Stability AI Stable Diffusion 3.5 Text-to-Image Workflow](http://docs.comfy.org#stability-ai-stable-diffusion-3-5-text-to-image-workflow)
- [1. Workflow File Download](http://docs.comfy.org#1-workflow-file-download)
- [2. Complete the Workflow Step by Step](http://docs.comfy.org#2-complete-the-workflow-step-by-step)
- [3. Additional Notes](http://docs.comfy.org#3-additional-notes)
- [Stability AI Stable Diffusion 3.5 Image-to-Image Workflow](http://docs.comfy.org#stability-ai-stable-diffusion-3-5-image-to-image-workflow)
- [1. Workflow File Download](http://docs.comfy.org#1-workflow-file-download-2)
- [2. Complete the Workflow Step by Step](http://docs.comfy.org#2-complete-the-workflow-step-by-step-2)
- [3. Additional Notes](http://docs.comfy.org#3-additional-notes-2)
- [Related Node Details](http://docs.comfy.org#related-node-details)

<!-- END Built_In_Node/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image.md -->


<!-- BEGIN Built_In_Node/tutorials/api-nodes/stability-ai/stable-image-ultra.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
    
    - [Stable Image Ultra](http://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-image-ultra)
    - [Stable Diffusion 3.5 Image](http://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image)
  - Ideogram
  - Luma
  - OpenAI
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Stability AI Stable Image Ultra API Node ComfyUI Official Example

# Stability AI Stable Image Ultra API Node ComfyUI Official Example

This article will introduce how to use the Stability AI Stable Image Ultra API node’s text-to-image and image-to-image capabilities in ComfyUI

The [Stability Stable Image Ultra](http://docs.comfy.org/built-in-nodes/api-node/image/stability/stability-stable-image-ultra) node allows you to use Stability AI’s Stable Image Ultra model to create high-quality, detailed image content through text prompts or reference images.

In this guide, we will show you how to set up workflows for both text-to-image and image-to-image generation using this node.

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#stability-ai-stable-image-ultra-text-to-image-workflow) Stability AI Stable Image Ultra Text-to-Image Workflow

### [​](http://docs.comfy.org#1-workflow-file-download) 1. Workflow File Download

The workflow information is included in the metadata of the image below. Please download and drag it into ComfyUI to load the corresponding workflow.

### [​](http://docs.comfy.org#2-complete-the-workflow-execution-step-by-step) 2. Complete the Workflow Execution Step by Step

You can follow the numbered steps in the image to complete the basic text-to-image workflow:

1. (Optional) Modify the `prompt` parameter in the `Stability AI Stable Image Ultra` node to input your desired image description. More detailed prompts often lead to better image quality. You can use the `(word:weight)` format to control specific word weights, for example: `The sky was crisp (blue:0.3) and (green:0.8)` indicates the sky is blue and green, but green is more prominent.
2. (Optional) Select the `style_preset` parameter to control the visual style of the image. Different preset styles will produce images with different stylistic characteristics, such as “cinematic”, “anime”, etc. Selecting “None” will not apply any specific style.
3. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation.
4. After the API returns the result, you can view the generated image in the `Save Image` node, and the image will also be saved to the `ComfyUI/output/` directory.

### [​](http://docs.comfy.org#3-additional-notes) 3. Additional Notes

- **Prompt**: The prompt is one of the most important parameters in the generation process. Detailed, clear descriptions will lead to better results. It can include elements like scene, subject, colors, lighting, and style.
- **Style Preset**: Provides multiple preset styles such as cinematic, anime, digital art, etc., which can quickly define the overall style of the image.
- **Negative Prompt**: Used to specify elements you don’t want to appear in the generated image, helping avoid common issues like extra limbs or distorted faces.
- **Seed Parameter**: Can be used to reproduce or fine-tune generation results, helpful for iteration during the creative process.
- Currently, the `Load Image` node is in “Bypass” mode. To enable it, refer to the step guide and right-click on the corresponding node to set “Mode” to “Always” to enable input, which will switch to image-to-image mode.

## [​](http://docs.comfy.org#stability-ai-stable-image-ultra-image-to-image-workflow) Stability AI Stable Image Ultra Image-to-Image Workflow

### [​](http://docs.comfy.org#1-workflow-file-download-2) 1. Workflow File Download

The workflow information is included in the metadata of the image below. Please download and drag it into ComfyUI to load the corresponding workflow.

Download the image below which we will use as input

### [​](http://docs.comfy.org#2-complete-the-workflow-execution-step-by-step-2) 2. Complete the Workflow Execution Step by Step

You can follow the numbered steps in the image to complete the image-to-image workflow:

1. Load a reference image through the `Load Image` node, which will serve as the basis for generation.
2. (Optional) Modify the `prompt` parameter in the `Stability Stable Image Ultra` node to describe elements you want to change or enhance in the reference image.
3. (Optional) Adjust the `image_denoise` parameter (range 0.0-1.0) to control the degree of modification to the original image:
   
   - Values closer to 0.0 will make the generated image more similar to the input reference image
   - Values closer to 1.0 will make the generated image more like pure text-to-image generation
4. (Optional) You can also set `style_preset` and other parameters to further control the generation effect.
5. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation.
6. After the API returns the result, you can view the generated image in the `Save Image` node, and the image will also be saved to the `ComfyUI/output/` directory.

### [​](http://docs.comfy.org#3-additional-notes-2) 3. Additional Notes

**Image Denoise**: This parameter determines how much of the original image’s features are preserved during generation, and is the most crucial adjustment parameter in image-to-image mode. The image below shows the effects of different denoising strengths

- **Reference Image Selection**: Choosing images with clear subjects and good composition usually leads to better results.
- **Prompt Tips**: In image-to-image mode, prompts should focus more on what you want to change or enhance, rather than describing all elements already present in the image.

## [​](http://docs.comfy.org#related-node-documentation) Related Node Documentation

You can refer to the documentation below for detailed parameter settings and more information about the corresponding nodes

[**Stability Stable Image Ultra Node Documentation**  
\
Stability Stable Image Ultra API Node Documentation](http://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/stability-ai/stable-image-ultra.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/black-forest-labs/flux-1-1-pro-ultra-image)

[Stable Diffusion 3.5 ImageThis article will introduce how to use Stability AI Stable Diffusion 3.5 API node's text-to-image and image-to-image capabilities in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Stability AI Stable Image Ultra Text-to-Image Workflow](http://docs.comfy.org#stability-ai-stable-image-ultra-text-to-image-workflow)
- [1. Workflow File Download](http://docs.comfy.org#1-workflow-file-download)
- [2. Complete the Workflow Execution Step by Step](http://docs.comfy.org#2-complete-the-workflow-execution-step-by-step)
- [3. Additional Notes](http://docs.comfy.org#3-additional-notes)
- [Stability AI Stable Image Ultra Image-to-Image Workflow](http://docs.comfy.org#stability-ai-stable-image-ultra-image-to-image-workflow)
- [1. Workflow File Download](http://docs.comfy.org#1-workflow-file-download-2)
- [2. Complete the Workflow Execution Step by Step](http://docs.comfy.org#2-complete-the-workflow-execution-step-by-step-2)
- [3. Additional Notes](http://docs.comfy.org#3-additional-notes-2)
- [Related Node Documentation](http://docs.comfy.org#related-node-documentation)

<!-- END Built_In_Node/tutorials/api-nodes/stability-ai/stable-image-ultra.md -->


<!-- BEGIN Built_In_Node/tutorials/audio/ace-step/ace-step-v1.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
  
  - [ACE-Step Music Generation](http://docs.comfy.org/tutorials/audio/ace-step/ace-step-v1)
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI ACE-Step Native Example

# ComfyUI ACE-Step Native Example

This guide will help you create dynamic music using the ACE-Step model in ComfyUI

ACE-Step is an open-source foundational music generation model jointly developed by Chinese team StepFun and ACE Studio, aimed at providing music creators with efficient, flexible and high-quality music generation and editing tools.

The model is released under the [Apache-2.0](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-license) license and is free for commercial use.

As a powerful music generation foundation, ACE-Step provides rich extensibility. Through fine-tuning techniques like LoRA and ControlNet, developers can customize the model according to their actual needs. Whether it’s audio editing, vocal synthesis, accompaniment production, voice cloning or style transfer applications, ACE-Step provides stable and reliable technical support. This flexible architecture greatly simplifies the development process of music AI applications, allowing more creators to quickly apply AI technology to music creation.

Currently, ACE-Step has released related training code, including LoRA model training, and the corresponding ControlNet training code will be released in the future. You can visit their [Github](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-roadmap) to learn more details.

## [​](http://docs.comfy.org#ace-step-comfyui-text-to-audio-generation-workflow-example) ACE-Step ComfyUI Text-to-Audio Generation Workflow Example

### [​](http://docs.comfy.org#1-download-workflow-and-related-models) 1. Download Workflow and Related Models

Click the button below to download the corresponding workflow file. Drag it into ComfyUI to load the workflow information. The workflow includes model download information.

Click the button below to download the corresponding workflow file. Drag it into ComfyUI to load the workflow information. The workflow includes model download information.

[Download Json Format Workflow File](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/audio/ace-step/ace_step_1_t2m.json)

You can also manually download [ace\_step\_v1\_3.5b.safetensors](https://huggingface.co/Comfy-Org/ACE-Step_ComfyUI_repackaged/blob/main/all_in_one/ace_step_v1_3.5b.safetensors) and save it to the `ComfyUI/models/checkpoints` folder

### [​](http://docs.comfy.org#2-complete-the-workflow-step-by-step) 2. Complete the Workflow Step by Step

1. Ensure the `Load Checkpoints` node has loaded the `ace_step_v1_3.5b.safetensors` model
2. Input corresponding music styles etc. in the `tags` field of `TextEncodeAceStepAudio`
3. Input corresponding lyrics in the `lyrics` field of `TextEncodeAceStepAudio`
4. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the generation
5. After the generation is complete, you can view the generated audio in the `Save Audio` node. You can click to play and preview. The audio will also be saved to `ComfyUI/output/audio` (subdirectory determined by the `Save Audio` node).

## [​](http://docs.comfy.org#ace-step-comfyui-audio-to-audio-workflow) ACE-Step ComfyUI Audio-to-Audio Workflow

Similar to image-to-image workflows, you can input a piece of music and use the workflow below to resample and generate music. You can also adjust the difference from the original audio by controlling the `denoise` parameter in the `Ksampler`.

### [​](http://docs.comfy.org#1-download-workflow-file) 1. Download Workflow File

Click the button below to download the corresponding workflow file. Drag it into ComfyUI to load the workflow information.

[Download Json Format Workflow File](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/audio/ace-stepace_step_1_m2m_editing.json)

### [​](http://docs.comfy.org#2-complete-the-workflow-step-by-step-2) 2. Complete the Workflow Step by Step

1. Ensure the `Load Checkpoints` node has loaded the `ace_step_v1_3.5b.safetensors` model
2. Upload the music you want to edit in the `LoadAudio` node (you can use the results generated from the text-to-audio workflow in this article)
3. Input corresponding music styles etc. in the `tags` field of `TextEncodeAceStepAudio`
4. Input corresponding lyrics in the `lyrics` field of `TextEncodeAceStepAudio`, you can refer to the prompt guide part (still updating) or the lyrics examples on the ACE-Step project page
5. Modify the `denoise` parameter in the `Ksampler` node to adjust the amount of noise added during sampling, which controls the similarity to the original audio (smaller values result in greater similarity to the original audio; if set to `1.00`, it can be considered as if there is no audio input)
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the audio generation
7. After the generation is complete, you can view the generated audio in the `Save Audio` node. You can click to play and preview. The audio will also be saved to `ComfyUI/output/audio` (subdirectory determined by the `Save Audio` node).

You can also implement the lyrics modification and editing functionality from the ACE-Step project page, modifying the original lyrics to change the audio effect.

### [​](http://docs.comfy.org#3-lyrics-modification-and-editing-example) 3. Lyrics Modification and Editing Example

\[To be updated]

## [​](http://docs.comfy.org#ace-step-prompt-guide) ACE-Step Prompt Guide

ACE currently uses two types of prompts: `tags` and `lyrics`.

- `tags`: Mainly used to describe music styles, scenes, etc. Similar to prompts we use for other generations, they primarily describe the overall style and requirements of the audio, separated by English commas
- `lyrics`: Mainly used to describe lyrics, supporting lyric structure tags such as \[verse], \[chorus], and \[bridge] to distinguish different parts of the lyrics. You can also input instrument names for purely instrumental music

You can find rich examples of `tags` and `lyrics` on the [ACE-Step model homepage](https://ace-step.github.io/). You can refer to these examples to try corresponding prompts. This document’s prompt guide is organized based on the project to help you quickly try combinations to achieve your desired effect.

### [​](http://docs.comfy.org#tags-prompt) Tags (prompt)

#### [​](http://docs.comfy.org#mainstream-music-styles) Mainstream Music Styles

Use short tag combinations to generate specific music styles

- electronic
- rock
- pop
- funk
- soul
- cyberpunk
- Acid jazz
- electro
- em (electronic music)
- soft electric drums
- melodic

#### [​](http://docs.comfy.org#scene-types) Scene Types

Combine specific usage scenarios and atmospheres to generate music that matches the corresponding mood

- background music for parties
- radio broadcasts
- workout playlists

#### [​](http://docs.comfy.org#instrumental-elements) Instrumental Elements

- saxophone
- jazz
- piano, violin

#### [​](http://docs.comfy.org#vocal-types) Vocal Types

- female voice
- male voice
- clean vocals

#### [​](http://docs.comfy.org#professional-terms) Professional Terms

Use some professional terms commonly used in music to precisely control music effects

- 110 bpm (beats per minute is 110)
- fast tempo
- slow tempo
- loops
- fills
- acoustic guitar
- electric bass

### [​](http://docs.comfy.org#lyrics) Lyrics

#### [​](http://docs.comfy.org#lyric-structure-tags) Lyric Structure Tags

- \[outro]
- \[verse]
- \[chorus]
- \[bridge]

#### [​](http://docs.comfy.org#multilingual-support) Multilingual Support

- ACE-Step V1 supports multiple languages. When used, ACE-Step converts different languages into English letters and then generates music.
- In ComfyUI, we haven’t fully implemented the conversion of all languages to English letters. Currently, only [Japanese hiragana and katakana characters](https://github.com/comfyanonymous/ComfyUI/commit/5d3cc85e13833aeb6ef9242cdae243083e30c6fc) are implemented. So if you need to use multiple languages for music generation, you need to first convert the corresponding language to English letters, and then input the language code abbreviation at the beginning of the `lyrics`, such as Chinese `[zh]`, Korean `[ko]`, etc.

For example:

```plaintext
[zh]ni hao
[ko]an nyeong
```

Currently, ACE-Step supports 19 languages, but the following ten languages have better support:

- English
- Chinese: \[zh]
- Russian: \[ru]
- Spanish: \[es]
- Japanese: \[ja]
- German: \[de]
- French: \[fr]
- Portuguese: \[pt]
- Italian: \[it]
- Korean: \[ko]

The language tags above have not been fully tested at the time of writing this documentation. If any language tag is incorrect, please [submit an issue to our documentation repository](https://github.com/Comfy-Org/docs/issues) and we will make timely corrections.

## [​](http://docs.comfy.org#ace-step-related-resources) ACE-Step Related Resources

- [Project Page](https://ace-step.github.io/)
- [Hugging Face](https://huggingface.co/ACE-Step/ACE-Step-v1-3.5B)
- [GitHub](https://github.com/ace-step/ACE-Step)
- [Training Scripts](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-train)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/audio/ace-step/ace-step-v1.mdx)

[Previous](http://docs.comfy.org/tutorials/video/wan/wan-flf)

[OverviewIn this article, we will introduce ComfyUI's API Nodes and related information.  
\
Next](http://docs.comfy.org/tutorials/api-nodes/overview)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [ACE-Step ComfyUI Text-to-Audio Generation Workflow Example](http://docs.comfy.org#ace-step-comfyui-text-to-audio-generation-workflow-example)
- [1. Download Workflow and Related Models](http://docs.comfy.org#1-download-workflow-and-related-models)
- [2. Complete the Workflow Step by Step](http://docs.comfy.org#2-complete-the-workflow-step-by-step)
- [ACE-Step ComfyUI Audio-to-Audio Workflow](http://docs.comfy.org#ace-step-comfyui-audio-to-audio-workflow)
- [1. Download Workflow File](http://docs.comfy.org#1-download-workflow-file)
- [2. Complete the Workflow Step by Step](http://docs.comfy.org#2-complete-the-workflow-step-by-step-2)
- [3. Lyrics Modification and Editing Example](http://docs.comfy.org#3-lyrics-modification-and-editing-example)
- [ACE-Step Prompt Guide](http://docs.comfy.org#ace-step-prompt-guide)
- [Tags (prompt)](http://docs.comfy.org#tags-prompt)
- [Mainstream Music Styles](http://docs.comfy.org#mainstream-music-styles)
- [Scene Types](http://docs.comfy.org#scene-types)
- [Instrumental Elements](http://docs.comfy.org#instrumental-elements)
- [Vocal Types](http://docs.comfy.org#vocal-types)
- [Professional Terms](http://docs.comfy.org#professional-terms)
- [Lyrics](http://docs.comfy.org#lyrics)
- [Lyric Structure Tags](http://docs.comfy.org#lyric-structure-tags)
- [Multilingual Support](http://docs.comfy.org#multilingual-support)
- [ACE-Step Related Resources](http://docs.comfy.org#ace-step-related-resources)

<!-- END Built_In_Node/tutorials/audio/ace-step/ace-step-v1.md -->


<!-- BEGIN Built_In_Node/tutorials/basic/image-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
  
  - [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image)
  - [Image to Image](http://docs.comfy.org/tutorials/basic/image-to-image)
  - [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint)
  - [Outpaint](http://docs.comfy.org/tutorials/basic/outpaint)
  - [Upscale](http://docs.comfy.org/tutorials/basic/upscale)
  - [LoRA](http://docs.comfy.org/tutorials/basic/lora)
  - [Multiple LoRAs](http://docs.comfy.org/tutorials/basic/multiple-loras)
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Image to Image Workflow

# ComfyUI Image to Image Workflow

This guide will help you understand and complete an image to image workflow

## [​](http://docs.comfy.org#what-is-image-to-image) What is Image to Image

Image to Image is a workflow in ComfyUI that allows users to input an image and generate a new image based on it.

Image to Image can be used in scenarios such as:

- Converting original image styles, like transforming realistic photos into artistic styles
- Converting line art into realistic images
- Image restoration
- Colorizing old photos
- … and other scenarios

To explain it with an analogy: It’s like asking an artist to create a specific piece based on your reference image.

If you carefully compare this tutorial with the [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image) tutorial, you’ll notice that the Image to Image process is very similar to Text to Image, just with an additional input reference image as a condition. In Text to Image, we let the artist (image model) create freely based on our prompts, while in Image to Image, we let the artist create based on both our reference image and prompts.

## [​](http://docs.comfy.org#comfyui-image-to-image-workflow-example-guide) ComfyUI Image to Image Workflow Example Guide

### [​](http://docs.comfy.org#model-installation) Model Installation

Download the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) file and put it in your `ComfyUI/models/checkpoints` folder.

### [​](http://docs.comfy.org#image-to-image-workflow-and-input-image) Image to Image Workflow and Input Image

Download the image below and **drag it into ComfyUI** to load the workflow:

Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`.

Download the image below and we will use it as the input image:

### [​](http://docs.comfy.org#complete-the-workflow-step-by-step) Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

1. Ensure `Load Checkpoint` loads **v1-5-pruned-emaonly-fp16.safetensors**
2. Upload the input image to the `Load Image` node
3. Click `Queue` or press `Ctrl/Cmd + Enter` to generate

## [​](http://docs.comfy.org#key-points-of-image-to-image-workflow) Key Points of Image to Image Workflow

The key to the Image to Image workflow lies in the `denoise` parameter in the `KSampler` node, which should be **less than 1**

If you’ve adjusted the `denoise` parameter and generated images, you’ll notice:

- The smaller the `denoise` value, the smaller the difference between the generated image and the reference image
- The larger the `denoise` value, the larger the difference between the generated image and the reference image

This is because `denoise` determines the strength of noise added to the latent space image after converting the reference image. If `denoise` is 1, the latent space image will become completely random noise, making it the same as the latent space generated by the `empty latent image` node, losing all characteristics of the reference image.

For the corresponding principles, please refer to the principle explanation in the [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image) tutorial.

## [​](http://docs.comfy.org#try-it-yourself) Try It Yourself

1. Try modifying the `denoise` parameter in the **KSampler** node, gradually changing it from 1 to 0, and observe the changes in the generated images
2. Replace with your own prompts and reference images to generate your own image effects

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/basic/image-to-image.mdx)

[Previous](http://docs.comfy.org/tutorials/basic/text-to-image)

[InpaintThis guide will introduce you to the inpainting workflow in ComfyUI, walk you through an inpainting example, and cover topics like using the mask editor  
\
Next](http://docs.comfy.org/tutorials/basic/inpaint)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [What is Image to Image](http://docs.comfy.org#what-is-image-to-image)
- [ComfyUI Image to Image Workflow Example Guide](http://docs.comfy.org#comfyui-image-to-image-workflow-example-guide)
- [Model Installation](http://docs.comfy.org#model-installation)
- [Image to Image Workflow and Input Image](http://docs.comfy.org#image-to-image-workflow-and-input-image)
- [Complete the Workflow Step by Step](http://docs.comfy.org#complete-the-workflow-step-by-step)
- [Key Points of Image to Image Workflow](http://docs.comfy.org#key-points-of-image-to-image-workflow)
- [Try It Yourself](http://docs.comfy.org#try-it-yourself)

<!-- END Built_In_Node/tutorials/basic/image-to-image.md -->


<!-- BEGIN Built_In_Node/tutorials/basic/inpaint.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
  
  - [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image)
  - [Image to Image](http://docs.comfy.org/tutorials/basic/image-to-image)
  - [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint)
  - [Outpaint](http://docs.comfy.org/tutorials/basic/outpaint)
  - [Upscale](http://docs.comfy.org/tutorials/basic/upscale)
  - [LoRA](http://docs.comfy.org/tutorials/basic/lora)
  - [Multiple LoRAs](http://docs.comfy.org/tutorials/basic/multiple-loras)
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Inpainting Workflow

# ComfyUI Inpainting Workflow

This guide will introduce you to the inpainting workflow in ComfyUI, walk you through an inpainting example, and cover topics like using the mask editor

This article will introduce the concept of inpainting in AI image generation and guide you through creating an inpainting workflow in ComfyUI. We’ll cover:

- Using inpainting workflows to modify images
- Using the ComfyUI mask editor to draw masks
- `VAE Encoder (for Inpainting)` node

## [​](http://docs.comfy.org#about-inpainting) About Inpainting

In AI image generation, we often encounter situations where we’re satisfied with the overall image but there are elements we don’t want or that contain errors. Simply regenerating might produce a completely different image, so using inpainting to fix specific parts becomes very useful.

It’s like having an **artist (AI model)** paint a picture, but we’re still not satisfied with the specific details. We need to tell the artist **which areas to adjust (mask)**, and then let them **repaint (inpaint)** according to our requirements.

Common inpainting scenarios include:

- **Defect Repair:** Removing unwanted objects, fixing incorrect AI-generated body parts, etc.
- **Detail Optimization:** Precisely adjusting local elements (like modifying clothing textures, adjusting facial expressions)
- And other scenarios

## [​](http://docs.comfy.org#comfyui-inpainting-workflow-example) ComfyUI Inpainting Workflow Example

### [​](http://docs.comfy.org#model-and-resource-preparation) Model and Resource Preparation

#### [​](http://docs.comfy.org#1-model-installation) 1. Model Installation

Download the [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors) file and put it in your `ComfyUI/models/checkpoints` folder:

#### [​](http://docs.comfy.org#2-inpainting-asset) 2. Inpainting Asset

Please download the following image which we’ll use as input:

This image already contains an alpha channel (transparency mask), so you don’t need to manually draw a mask. This tutorial will also cover how to use the mask editor to draw masks.

#### [​](http://docs.comfy.org#3-inpainting-workflow) 3. Inpainting Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:

Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`.

### [​](http://docs.comfy.org#comfyui-inpainting-workflow-example-explanation) ComfyUI Inpainting Workflow Example Explanation

Follow the steps in the diagram below to ensure the workflow runs correctly.

1. Ensure `Load Checkpoint` loads `512-inpainting-ema.safetensors`
2. Upload the input image to the `Load Image` node
3. Click `Queue` or use `Ctrl + Enter` to generate

For comparison, here’s the result using the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) model:

You will find that the results generated by the [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors) model have better inpainting effects and more natural transitions. This is because this model is specifically designed for inpainting, which helps us better control the generation area, resulting in improved inpainting effects.

Do you remember the analogy we’ve been using? Different models are like artists with varying abilities, but each artist has their own limits. Choosing the right model can help you achieve better generation results.

You can try these approaches to achieve better results:

1. Modify positive and negative prompts with more specific descriptions
2. Try multiple runs using different seeds in the `KSampler` for different generation results
3. After learning about the mask editor in this tutorial, you can re-inpaint the generated results to achieve satisfactory outcomes.

Next, we’ll learn about using the **Mask Editor**. While our input image already includes an `alpha` transparency channel (the area we want to edit), so manual mask drawing isn’t necessary, you’ll often use the Mask Editor to create masks in practical applications.

### [​](http://docs.comfy.org#using-the-mask-editor) Using the Mask Editor

First right-click the `Save Image` node and select `Copy(Clipspace)`:

Then right-click the **Load Image** node and select `Paste(Clipspace)`:

Right-click the **Load Image** node again and select `Open in MaskEditor`:

1. Adjust brush parameters on the right panel
2. Use eraser to correct mistakes
3. Click `Save` when finished

The drawn content will be used as a Mask input to the VAE Encoder (for Inpainting) node for encoding

Then try adjusting your prompts and generating again until you achieve satisfactory results.

## [​](http://docs.comfy.org#vae-encoder-for-inpainting-node) VAE Encoder (for Inpainting) Node

Comparing this workflow with [Text-to-Image](http://docs.comfy.org/tutorials/basic/text-to-image) and [Image-to-Image](http://docs.comfy.org/tutorials/basic/image-to-image), you’ll notice the main differences are in the VAE section’s conditional inputs. In this workflow, we use the **VAE Encoder (for Inpainting)** node, specifically designed for inpainting to help us better control the generation area and achieve better results.

**Input Types**

Parameter NameFunction`pixels`Input image to be encoded into latent space.`vae`VAE model used to encode the image from pixel space to latent space.`mask`Image mask specifying which areas need modification.`grow_mask_by`Pixel value to expand the original mask outward, ensuring a transition area around the mask to avoid hard edges between inpainted and original areas.

**Output Types**

Parameter NameFunction`latent`Image encoded into latent space by the VAE.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/basic/inpaint.mdx)

[Previous](http://docs.comfy.org/tutorials/basic/image-to-image)

[OutpaintThis guide will introduce you to the outpainting workflow in ComfyUI and walk you through an outpainting example  
\
Next](http://docs.comfy.org/tutorials/basic/outpaint)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [About Inpainting](http://docs.comfy.org#about-inpainting)
- [ComfyUI Inpainting Workflow Example](http://docs.comfy.org#comfyui-inpainting-workflow-example)
- [Model and Resource Preparation](http://docs.comfy.org#model-and-resource-preparation)
- [1. Model Installation](http://docs.comfy.org#1-model-installation)
- [2. Inpainting Asset](http://docs.comfy.org#2-inpainting-asset)
- [3. Inpainting Workflow](http://docs.comfy.org#3-inpainting-workflow)
- [ComfyUI Inpainting Workflow Example Explanation](http://docs.comfy.org#comfyui-inpainting-workflow-example-explanation)
- [Using the Mask Editor](http://docs.comfy.org#using-the-mask-editor)
- [VAE Encoder (for Inpainting) Node](http://docs.comfy.org#vae-encoder-for-inpainting-node)

<!-- END Built_In_Node/tutorials/basic/inpaint.md -->


<!-- BEGIN Built_In_Node/tutorials/basic/lora.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
  
  - [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image)
  - [Image to Image](http://docs.comfy.org/tutorials/basic/image-to-image)
  - [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint)
  - [Outpaint](http://docs.comfy.org/tutorials/basic/outpaint)
  - [Upscale](http://docs.comfy.org/tutorials/basic/upscale)
  - [LoRA](http://docs.comfy.org/tutorials/basic/lora)
  - [Multiple LoRAs](http://docs.comfy.org/tutorials/basic/multiple-loras)
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI LoRA Example

# ComfyUI LoRA Example

This guide will help you understand and use a single LoRA model

**LoRA (Low-Rank Adaptation)** is an efficient technique for fine-tuning large generative models like Stable Diffusion. It introduces trainable low-rank matrices to the pre-trained model, adjusting only a portion of parameters rather than retraining the entire model, thus achieving optimization for specific tasks at a lower computational cost. Compared to base models like SD1.5, LoRA models are smaller and easier to train.

The image above compares generation with the same parameters using [dreamshaper\_8](https://civitai.com/models/4384?modelVersionId=128713) directly versus using the [blindbox\_V1Mix](https://civitai.com/models/25995/blindbox) LoRA model. As you can see, by using a LoRA model, we can generate images in different styles without adjusting the base model.

We will demonstrate how to use a LoRA model. All LoRA variants: Lycoris, loha, lokr, locon, etc… are used in the same way.

In this example, we will learn how to load and use a LoRA model in [ComfyUI](https://github.com/comfyanonymous/ComfyUI), covering the following topics:

1. Installing a LoRA model
2. Generating images using a LoRA model
3. A simple introduction to the `Load LoRA` node

## [​](http://docs.comfy.org#required-model-installation) Required Model Installation

Download the [dreamshaper\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model&format=SafeTensor&size=pruned&fp=fp16) file and put it in your `ComfyUI/models/checkpoints` folder.

Download the [blindbox\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model&format=SafeTensor&size=full&fp=fp16) file and put it in your `ComfyUI/models/loras` folder.

## [​](http://docs.comfy.org#lora-workflow-file) LoRA Workflow File

Download the image below and **drag it into ComfyUI** to load the workflow.

Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`.

## [​](http://docs.comfy.org#complete-the-workflow-step-by-step) Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

1. Ensure `Load Checkpoint` loads `dreamshaper_8.safetensors`
2. Ensure `Load LoRA` loads `blindbox_V1Mix.safetensors`
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to generate the image

## [​](http://docs.comfy.org#load-lora-node-introduction) Load LoRA Node Introduction

Models in the `ComfyUI\models\loras` folder will be detected by ComfyUI and can be loaded using this node.

### [​](http://docs.comfy.org#input-types) Input Types

Parameter NameFunction`model`Connect to the base model`clip`Connect to the CLIP model`lora_name`Select the LoRA model to load and use`strength_model`Affects how strongly the LoRA influences the model weights; higher values make the LoRA style stronger`strength_clip`Affects how strongly the LoRA influences the CLIP text embeddings

### [​](http://docs.comfy.org#output-types) Output Types

Parameter NameFunction`model`Outputs the model with LoRA adjustments applied`clip`Outputs the CLIP model with LoRA adjustments applied

This node supports chain connections, allowing multiple `Load LoRA` nodes to be linked in series to apply multiple LoRA models. For more details, please refer to [ComfyUI Multiple LoRAs Example](http://docs.comfy.org/tutorials/basic/multiple-loras)

## [​](http://docs.comfy.org#try-it-yourself) Try It Yourself

1. Try modifying the prompt or adjusting different parameters of the `Load LoRA` node, such as `strength_model`, to observe changes in the generated images and become familiar with the `Load LoRA` node.
2. Visit [CivitAI](https://civitai.com/models) to download other kinds of LoRA models and try using them.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/basic/lora.mdx)

[Previous](http://docs.comfy.org/tutorials/basic/upscale)

[Multiple LoRAsThis guide demonstrates how to apply multiple LoRA models simultaneously in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/basic/multiple-loras)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Required Model Installation](http://docs.comfy.org#required-model-installation)
- [LoRA Workflow File](http://docs.comfy.org#lora-workflow-file)
- [Complete the Workflow Step by Step](http://docs.comfy.org#complete-the-workflow-step-by-step)
- [Load LoRA Node Introduction](http://docs.comfy.org#load-lora-node-introduction)
- [Input Types](http://docs.comfy.org#input-types)
- [Output Types](http://docs.comfy.org#output-types)
- [Try It Yourself](http://docs.comfy.org#try-it-yourself)

<!-- END Built_In_Node/tutorials/basic/lora.md -->


<!-- BEGIN Built_In_Node/tutorials/basic/multiple-loras.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
  
  - [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image)
  - [Image to Image](http://docs.comfy.org/tutorials/basic/image-to-image)
  - [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint)
  - [Outpaint](http://docs.comfy.org/tutorials/basic/outpaint)
  - [Upscale](http://docs.comfy.org/tutorials/basic/upscale)
  - [LoRA](http://docs.comfy.org/tutorials/basic/lora)
  - [Multiple LoRAs](http://docs.comfy.org/tutorials/basic/multiple-loras)
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Multiple LoRAs Example

# ComfyUI Multiple LoRAs Example

This guide demonstrates how to apply multiple LoRA models simultaneously in ComfyUI

In our [ComfyUI LoRA Example](http://docs.comfy.org/tutorials/basic/lora), we introduced how to load and use a single LoRA model, mentioning the node’s chain connection capability.

This tutorial demonstrates chaining multiple `Load LoRA` nodes to apply two LoRA models simultaneously: [blindbox\_V1Mix](https://civitai.com/models/25995?modelVersionId=32988) and [MoXinV1](https://civitai.com/models/12597?modelVersionId=14856).

The comparison below shows individual effects of these LoRAs using identical parameters:

By chaining multiple LoRA models, we achieve a blended style in the final output:

## [​](http://docs.comfy.org#model-installation) Model Installation

Download the [dreamshaper\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model&format=SafeTensor&size=pruned&fp=fp16) file and put it in your `ComfyUI/models/checkpoints` folder.

Download the [blindbox\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model&format=SafeTensor&size=full&fp=fp16) file and put it in your `ComfyUI/models/loras` folder.

Download the [MoXinV1.safetensors](https://civitai.com/api/download/models/14856?type=Model&format=SafeTensor&size=full&fp=fp16) file and put it in your `ComfyUI/models/loras` folder.

## [​](http://docs.comfy.org#multi-lora-workflow) Multi-LoRA Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:

Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`.

## [​](http://docs.comfy.org#complete-the-workflow-step-by-step) Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

1. Ensure `Load Checkpoint` loads **dreamshaper\_8.safetensors**
2. Ensure first `Load LoRA` loads **blindbox\_V1Mix.safetensors**
3. Ensure second `Load LoRA` loads **MoXinV1.safetensors**
4. Click `Queue` or press `Ctrl/Cmd + Enter` to generate

## [​](http://docs.comfy.org#try-it-yourself) Try It Yourself

1. Adjust `strength_model` values in both `Load LoRA` nodes to control each LoRA’s influence
2. Explore [CivitAI](https://civitai.com/models) for additional LoRAs and create custom combinations

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/basic/multiple-loras.mdx)

[Previous](http://docs.comfy.org/tutorials/basic/lora)

[ControlNetThis guide will introduce you to the basic concepts of ControlNet and demonstrate how to generate corresponding images in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/controlnet/controlnet)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Model Installation](http://docs.comfy.org#model-installation)
- [Multi-LoRA Workflow](http://docs.comfy.org#multi-lora-workflow)
- [Complete the Workflow Step by Step](http://docs.comfy.org#complete-the-workflow-step-by-step)
- [Try It Yourself](http://docs.comfy.org#try-it-yourself)

<!-- END Built_In_Node/tutorials/basic/multiple-loras.md -->


<!-- BEGIN Built_In_Node/tutorials/basic/outpaint.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
  
  - [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image)
  - [Image to Image](http://docs.comfy.org/tutorials/basic/image-to-image)
  - [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint)
  - [Outpaint](http://docs.comfy.org/tutorials/basic/outpaint)
  - [Upscale](http://docs.comfy.org/tutorials/basic/upscale)
  - [LoRA](http://docs.comfy.org/tutorials/basic/lora)
  - [Multiple LoRAs](http://docs.comfy.org/tutorials/basic/multiple-loras)
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Outpainting Workflow Example

# ComfyUI Outpainting Workflow Example

This guide will introduce you to the outpainting workflow in ComfyUI and walk you through an outpainting example

This guide will introduce you to the concept of outpainting in AI image generation and how to create an outpainting workflow in ComfyUI. We will cover:

- Using outpainting workflow to extend an image
- Understanding and using outpainting-related nodes in ComfyUI
- Mastering the basic outpainting process

## [​](http://docs.comfy.org#about-outpainting) About Outpainting

In AI image generation, we often encounter situations where an existing image has good composition but the canvas area is too small, requiring us to extend the canvas to get a larger scene. This is where outpainting comes in.

Basically, it requires similar content to [Inpainting](http://docs.comfy.org/tutorials/basic/inpaint), but we use different nodes to **build the mask**.

Outpainting applications include:

- **Scene Extension:** Expand the scene range of the original image to show a more complete environment
- **Composition Adjustment:** Optimize the overall composition by extending the canvas
- **Content Addition:** Add more related scene elements to the original image

## [​](http://docs.comfy.org#comfyui-outpainting-workflow-example-explanation) ComfyUI Outpainting Workflow Example Explanation

### [​](http://docs.comfy.org#preparation) Preparation

#### [​](http://docs.comfy.org#1-model-installation) 1. Model Installation

Download the following model file and save it to `ComfyUI/models/checkpoints` directory:

- [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors)

#### [​](http://docs.comfy.org#2-input-image) 2. Input Image

Prepare an image you want to extend. In this example, we will use the following image:

#### [​](http://docs.comfy.org#3-outpainting-workflow) 3. Outpainting Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:

Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`.

### [​](http://docs.comfy.org#outpainting-workflow-usage-explanation) Outpainting Workflow Usage Explanation

The key steps of the outpainting workflow are as follows:

1. Load the locally installed model file in the `Load Checkpoint` node
2. Click the `Upload` button in the `Load Image` node to upload your image
3. Click the `Queue` button or use the shortcut `Ctrl + Enter` to execute the image generation

In this workflow, we mainly use the `Pad Image for outpainting` node to control the direction and range of image extension. This is actually an [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint.mdx) workflow, but we use different nodes to build the mask.

### [​](http://docs.comfy.org#pad-image-for-outpainting-node) Pad Image for outpainting Node

This node accepts an input image and outputs an extended image with a corresponding mask, where the mask is built based on the node parameters.

#### [​](http://docs.comfy.org#input-parameters) Input Parameters

Parameter NameFunction`image`Input image`left`Left padding amount`top`Top padding amount`right`Right padding amount`bottom`Bottom padding amount`feathering`Controls the smoothness of the transition between the original image and the added padding, higher values create smoother transitions

#### [​](http://docs.comfy.org#output-parameters) Output Parameters

Parameter NameFunction`image`Output `image` represents the padded image`mask`Output `mask` indicates the original image area and the added padding area

#### [​](http://docs.comfy.org#node-output-content) Node Output Content

After processing by the `Pad Image for outpainting` node, the output image and mask preview are as follows:

You can see the corresponding output results:

- The `Image` output is the extended image
- The `Mask` output is the mask marking the extension areas

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/basic/outpaint.mdx)

[Previous](http://docs.comfy.org/tutorials/basic/inpaint)

[UpscaleThis guide explains the concept of image upscaling in AI drawing and demonstrates how to implement an image upscaling workflow in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/basic/upscale)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [About Outpainting](http://docs.comfy.org#about-outpainting)
- [ComfyUI Outpainting Workflow Example Explanation](http://docs.comfy.org#comfyui-outpainting-workflow-example-explanation)
- [Preparation](http://docs.comfy.org#preparation)
- [1. Model Installation](http://docs.comfy.org#1-model-installation)
- [2. Input Image](http://docs.comfy.org#2-input-image)
- [3. Outpainting Workflow](http://docs.comfy.org#3-outpainting-workflow)
- [Outpainting Workflow Usage Explanation](http://docs.comfy.org#outpainting-workflow-usage-explanation)
- [Pad Image for outpainting Node](http://docs.comfy.org#pad-image-for-outpainting-node)
- [Input Parameters](http://docs.comfy.org#input-parameters)
- [Output Parameters](http://docs.comfy.org#output-parameters)
- [Node Output Content](http://docs.comfy.org#node-output-content)

<!-- END Built_In_Node/tutorials/basic/outpaint.md -->


<!-- BEGIN Built_In_Node/tutorials/basic/text-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
  
  - [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image)
  - [Image to Image](http://docs.comfy.org/tutorials/basic/image-to-image)
  - [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint)
  - [Outpaint](http://docs.comfy.org/tutorials/basic/outpaint)
  - [Upscale](http://docs.comfy.org/tutorials/basic/upscale)
  - [LoRA](http://docs.comfy.org/tutorials/basic/lora)
  - [Multiple LoRAs](http://docs.comfy.org/tutorials/basic/multiple-loras)
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Text to Image Workflow

# ComfyUI Text to Image Workflow

This guide will help you understand the concept of text-to-image in AI art generation and complete a text-to-image workflow in ComfyUI

This guide aims to introduce you to ComfyUI’s text-to-image workflow and help you understand the functionality and usage of various ComfyUI nodes.

In this document, we will:

- Complete a text-to-image workflow
- Gain a basic understanding of diffusion model principles
- Learn about the functions and roles of workflow nodes
- Get an initial understanding of the SD1.5 model

We’ll start by running a text-to-image workflow, followed by explanations of related concepts. Please choose the relevant sections based on your needs.

## [​](http://docs.comfy.org#about-text-to-image) About Text to Image

**Text to Image** is a fundamental process in AI art generation that creates images from text descriptions, with **diffusion models** at its core.

The text-to-image process requires the following elements:

- **Artist:** The image generation model
- **Canvas:** The latent space
- **Image Requirements (Prompts):** Including positive prompts (elements you want in the image) and negative prompts (elements you don’t want)

This text-to-image generation process can be simply understood as telling your requirements (positive and negative prompts) to an **artist (the image model)**, who then creates what you want based on these requirements.

## [​](http://docs.comfy.org#comfyui-text-to-image-workflow-example-guide) ComfyUI Text to Image Workflow Example Guide

### [​](http://docs.comfy.org#1-preparation) 1. Preparation

Ensure you have at least one SD1.5 model file in your `ComfyUI/models/checkpoints` folder, such as [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors)

If you haven’t installed it yet, please refer to the model installation section in [Getting Started with ComfyUI AI Art Generation](http://docs.comfy.org/get_started/first_generation).

### [​](http://docs.comfy.org#2-loading-the-text-to-image-workflow) 2. Loading the Text to Image Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:

Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`.

### [​](http://docs.comfy.org#3-loading-the-model-and-generating-your-first-image) 3. Loading the Model and Generating Your First Image

After installing the image model, follow the steps in the image below to load the model and generate your first image

Follow these steps according to the image numbers:

1. In the **Load Checkpoint** node, use the arrows or click the text area to ensure **v1-5-pruned-emaonly-fp16.safetensors** is selected, and the left/right arrows don’t show **null** text
2. Click the `Queue` button or use the shortcut `Ctrl + Enter` to execute image generation

After the process completes, you should see the resulting image in the **Save Image** node interface, which you can right-click to save locally

If you’re not satisfied with the result, try running the generation multiple times. Each time you run it, **KSampler** will use a different random seed based on the `seed` parameter, so each generation will produce different results

### [​](http://docs.comfy.org#4-start-experimenting) 4. Start Experimenting

Try modifying the text in the **CLIP Text Encoder**

The `Positive` connection to the KSampler node represents positive prompts, while the `Negative` connection represents negative prompts

Here are some basic prompting principles for the SD1.5 model:

- Use English whenever possible
- Separate prompts with English commas `,`
- Use phrases rather than long sentences
- Use specific descriptions
- Use expressions like `(golden hour:1.2)` to increase the weight of specific keywords, making them more likely to appear in the image. `1.2` is the weight, `golden hour` is the keyword
- Use keywords like `masterpiece, best quality, 4k` to improve generation quality

Here are several prompt examples you can try, or use your own prompts for generation:

**1. Anime Style**

Positive prompts:

```plaintext
anime style, 1girl with long pink hair, cherry blossom background, studio ghibli aesthetic, soft lighting, intricate details

masterpiece, best quality, 4k
```

Negative prompts:

```plaintext
low quality, blurry, deformed hands, extra fingers
```

**2. Realistic Style**

Positive prompts:

```plaintext
(ultra realistic portrait:1.3), (elegant woman in crimson silk dress:1.2), 
full body, soft cinematic lighting, (golden hour:1.2), 
(fujifilm XT4:1.1), shallow depth of field, 
(skin texture details:1.3), (film grain:1.1), 
gentle wind flow, warm color grading, (perfect facial symmetry:1.3)
```

Negative prompts:

```plaintext
(deformed, cartoon, anime, doll, plastic skin, overexposed, blurry, extra fingers)
```

**3. Specific Artist Style**

Positive prompts:

```plaintext
fantasy elf, detailed character, glowing magic, vibrant colors, long flowing hair, elegant armor, ethereal beauty, mystical forest, magical aura, high detail, soft lighting, fantasy portrait, Artgerm style
```

Negative prompts:

```plaintext
blurry, low detail, cartoonish, unrealistic anatomy, out of focus, cluttered, flat lighting
```

## [​](http://docs.comfy.org#text-to-image-working-principles) Text to Image Working Principles

The entire text-to-image process can be understood as a **reverse diffusion process**. The [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) we downloaded is a pre-trained model that can **generate target images from pure Gaussian noise**. We only need to input our prompts, and it can generate target images through denoising random noise.

We need to understand two concepts:

1. **Latent Space:** Latent Space is an abstract data representation method in diffusion models. Converting images from pixel space to latent space reduces storage space and makes it easier to train diffusion models and reduce denoising complexity. It’s like architects using blueprints (latent space) for design rather than designing directly on the building (pixel space), maintaining structural features while significantly reducing modification costs
2. **Pixel Space:** Pixel Space is the storage space for images, which is the final image we see, used to store pixel values.

If you want to learn more about diffusion models, you can read these papers:

- [Denoising Diffusion Probabilistic Models (DDPM)](https://arxiv.org/pdf/2006.11239)
- [Denoising Diffusion Implicit Models (DDIM)](https://arxiv.org/pdf/2010.02502)
- [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752)

## [​](http://docs.comfy.org#comfyui-text-to-image-workflow-node-explanation) ComfyUI Text to Image Workflow Node Explanation

### [​](http://docs.comfy.org#a-load-checkpoint-node) A. Load Checkpoint Node

This node is typically used to load the image generation model. A `checkpoint` usually contains three components: `MODEL (UNet)`, `CLIP`, and `VAE`

- `MODEL (UNet)`: The UNet model responsible for noise prediction and image generation during the diffusion process
- `CLIP`: The text encoder that converts our text prompts into vectors that the model can understand, as the model cannot directly understand text prompts
- `VAE`: The Variational AutoEncoder that converts images between pixel space and latent space, as diffusion models work in latent space while our images are in pixel space

### [​](http://docs.comfy.org#b-empty-latent-image-node) B. Empty Latent Image Node

Defines a latent space that outputs to the KSampler node. The Empty Latent Image node constructs a **pure noise latent space**

You can think of its function as defining the canvas size, which determines the dimensions of our final generated image

### [​](http://docs.comfy.org#c-clip-text-encoder-node) C. CLIP Text Encoder Node

Used to encode prompts, which are your requirements for the image

- The `Positive` condition input connected to the KSampler node represents positive prompts (elements you want in the image)
- The `Negative` condition input connected to the KSampler node represents negative prompts (elements you don’t want in the image)

The prompts are encoded into semantic vectors by the `CLIP` component from the `Load Checkpoint` node and output as conditions to the KSampler node

### [​](http://docs.comfy.org#d-ksampler-node) D. KSampler Node

The **KSampler** is the core of the entire workflow, where the entire noise denoising process occurs, ultimately outputting a latent space image

Here’s an explanation of the KSampler node parameters:

Parameter NameDescriptionFunction**model**Diffusion model used for denoisingDetermines the style and quality of generated images**positive**Positive prompt condition encodingGuides generation to include specified elements**negative**Negative prompt condition encodingSuppresses unwanted content**latent\_image**Latent space image to be denoisedServes as the input carrier for noise initialization**seed**Random seed for noise generationControls generation randomness**control\_after\_generate**Seed control mode after generationDetermines seed variation pattern in batch generation**steps**Number of denoising iterationsMore steps mean finer details but longer processing time**cfg**Classifier-free guidance scaleControls prompt constraint strength (too high leads to overfitting)**sampler\_name**Sampling algorithm nameDetermines the mathematical method for denoising path**scheduler**Scheduler typeControls noise decay rate and step size allocation**denoise**Denoising strength coefficientControls noise strength added to latent space, 0.0 preserves original input features, 1.0 is complete noise

In the KSampler node, the latent space uses `seed` as an initialization parameter to construct random noise, and semantic vectors `Positive` and `Negative` are input as conditions to the diffusion model

Then, based on the number of denoising steps specified by the `steps` parameter, denoising is performed. Each denoising step uses the denoising strength coefficient specified by the `denoise` parameter to denoise the latent space and generate a new latent space image

### [​](http://docs.comfy.org#e-vae-decode-node) E. VAE Decode Node

Converts the latent space image output from the **KSampler** into a pixel space image

### [​](http://docs.comfy.org#f-save-image-node) F. Save Image Node

Previews and saves the decoded image from latent space to the local `ComfyUI/output` folder

## [​](http://docs.comfy.org#introduction-to-sd1-5-model) Introduction to SD1.5 Model

**SD1.5 (Stable Diffusion 1.5)** is an AI image generation model developed by [Stability AI](https://stability.ai/). It’s the foundational version of the Stable Diffusion series, trained on **512×512** resolution images, making it particularly good at generating images at this resolution. With a size of about 4GB, it runs smoothly on **consumer-grade GPUs (e.g., 6GB VRAM)**. Currently, SD1.5 has a rich ecosystem, supporting various plugins (like ControlNet, LoRA) and optimization tools. As a milestone model in AI art generation, SD1.5 remains the best entry-level choice thanks to its open-source nature, lightweight architecture, and rich ecosystem. Although newer versions like SDXL/SD3 have been released, its value for consumer-grade hardware remains unmatched.

### [​](http://docs.comfy.org#basic-information) Basic Information

- **Release Date**: October 2022
- **Core Architecture**: Based on Latent Diffusion Model (LDM)
- **Training Data**: LAION-Aesthetics v2.5 dataset (approximately 590M training steps)
- **Open Source Features**: Fully open-source model/code/training data

### [​](http://docs.comfy.org#advantages-and-limitations) Advantages and Limitations

Model Advantages:

- Lightweight: Small size, only about 4GB, runs smoothly on consumer GPUs
- Low Entry Barrier: Supports a wide range of plugins and optimization tools
- Mature Ecosystem: Extensive plugin and tool support
- Fast Generation: Smooth operation on consumer GPUs

Model Limitations:

- Detail Handling: Hands/complex lighting prone to distortion
- Resolution Limits: Quality degrades for direct 1024x1024 generation
- Prompt Dependency: Requires precise English descriptions for control

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/basic/text-to-image.mdx)

[Previous](http://docs.comfy.org/interface/shortcuts)

[Image to ImageThis guide will help you understand and complete an image to image workflow  
\
Next](http://docs.comfy.org/tutorials/basic/image-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [About Text to Image](http://docs.comfy.org#about-text-to-image)
- [ComfyUI Text to Image Workflow Example Guide](http://docs.comfy.org#comfyui-text-to-image-workflow-example-guide)
- [1. Preparation](http://docs.comfy.org#1-preparation)
- [2. Loading the Text to Image Workflow](http://docs.comfy.org#2-loading-the-text-to-image-workflow)
- [3. Loading the Model and Generating Your First Image](http://docs.comfy.org#3-loading-the-model-and-generating-your-first-image)
- [4. Start Experimenting](http://docs.comfy.org#4-start-experimenting)
- [Text to Image Working Principles](http://docs.comfy.org#text-to-image-working-principles)
- [ComfyUI Text to Image Workflow Node Explanation](http://docs.comfy.org#comfyui-text-to-image-workflow-node-explanation)
- [A. Load Checkpoint Node](http://docs.comfy.org#a-load-checkpoint-node)
- [B. Empty Latent Image Node](http://docs.comfy.org#b-empty-latent-image-node)
- [C. CLIP Text Encoder Node](http://docs.comfy.org#c-clip-text-encoder-node)
- [D. KSampler Node](http://docs.comfy.org#d-ksampler-node)
- [E. VAE Decode Node](http://docs.comfy.org#e-vae-decode-node)
- [F. Save Image Node](http://docs.comfy.org#f-save-image-node)
- [Introduction to SD1.5 Model](http://docs.comfy.org#introduction-to-sd1-5-model)
- [Basic Information](http://docs.comfy.org#basic-information)
- [Advantages and Limitations](http://docs.comfy.org#advantages-and-limitations)

<!-- END Built_In_Node/tutorials/basic/text-to-image.md -->


<!-- BEGIN Built_In_Node/tutorials/basic/upscale.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
  
  - [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image)
  - [Image to Image](http://docs.comfy.org/tutorials/basic/image-to-image)
  - [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint)
  - [Outpaint](http://docs.comfy.org/tutorials/basic/outpaint)
  - [Upscale](http://docs.comfy.org/tutorials/basic/upscale)
  - [LoRA](http://docs.comfy.org/tutorials/basic/lora)
  - [Multiple LoRAs](http://docs.comfy.org/tutorials/basic/multiple-loras)
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Image Upscale Workflow

# ComfyUI Image Upscale Workflow

This guide explains the concept of image upscaling in AI drawing and demonstrates how to implement an image upscaling workflow in ComfyUI

## [​](http://docs.comfy.org#what-is-image-upscaling%3F) What is Image Upscaling?

Image Upscaling is the process of converting low-resolution images to high-resolution using algorithms. Unlike traditional interpolation methods, AI upscaling models (like ESRGAN) can intelligently reconstruct details while maintaining image quality.

For instance, the default SD1.5 model often struggles with large-size image generation. To achieve high-resolution results,we typically generate smaller images first and then use upscaling techniques.

This article covers one of many upscaling methods in ComfyUI. In this tutorial, we’ll guide you through:

1. Downloading and installing upscaling models
2. Performing basic image upscaling
3. Combining text-to-image workflows with upscaling

## [​](http://docs.comfy.org#upscaling-workflow) Upscaling Workflow

### [​](http://docs.comfy.org#model-installation) Model Installation

Required ESRGAN models download:

1

Visit OpenModelDB

Visit [OpenModelDB](https://openmodeldb.info/) to search and download upscaling models (e.g., RealESRGAN)

As shown:

1. Filter models by image type using the category selector
2. The model’s magnification factor is indicated in the top-right corner (e.g., 2x in the screenshot)

We’ll use the [4x-ESRGAN](https://openmodeldb.info/models/4x-ESRGAN) model for this tutorial. Click the `Download` button on the model detail page.

2

Save Model Files in Directory

Save the model file (.pth) in `ComfyUI/models/upscale_models` directory

### [​](http://docs.comfy.org#workflow-and-assets) Workflow and Assets

Download and drag the following image into ComfyUI to load the basic upscaling workflow:

Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`.

Use this image in smaller size as input:

### [​](http://docs.comfy.org#complete-the-workflow-step-by-step) Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

1. Ensure `Load Upscale Model` loads `4x-ESRGAN.pth`
2. Upload the input image to the `Load Image` node
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to generate the image

The core components are the `Load Upscale Model` and `Upscale Image (Using Model)` nodes, which receive an image input and upscale it using the selected model.

## [​](http://docs.comfy.org#text-to-image-combined-workflow) Text-to-Image Combined Workflow

After mastering basic upscaling, we can combine it with the [text-to-image](http://docs.comfy.org/tutorials/basic/text-to-image) workflow. For text-to-image basics, refer to the [text-to-image tutorial](http://docs.comfy.org/tutorials/basic/text-to-image).

Download and drag this image into ComfyUI to load the combined workflow:

This workflow connects the text-to-image output image directly to the upscaling nodes for final processing.

## [​](http://docs.comfy.org#additional-tips) Additional Tips

Model characteristics:

- **RealESRGAN**: General-purpose upscaling
- **BSRGAN**: Excels with text and sharp edges
- **SwinIR**: Preserves natural textures, ideal for landscapes

<!--THE END-->

1. **Chained Upscaling**: Combine multiple upscale nodes (e.g., 2x → 4x) for ultra-high magnification
2. **Hybrid Workflow**: Connect upscale nodes after generation for “generate+enhance” pipelines
3. **Comparative Testing**: Different models perform better on specific image types - test multiple options

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/basic/upscale.mdx)

[Previous](http://docs.comfy.org/tutorials/basic/outpaint)

[LoRAThis guide will help you understand and use a single LoRA model  
\
Next](http://docs.comfy.org/tutorials/basic/lora)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [What is Image Upscaling?](http://docs.comfy.org#what-is-image-upscaling%3F)
- [Upscaling Workflow](http://docs.comfy.org#upscaling-workflow)
- [Model Installation](http://docs.comfy.org#model-installation)
- [Workflow and Assets](http://docs.comfy.org#workflow-and-assets)
- [Complete the Workflow Step by Step](http://docs.comfy.org#complete-the-workflow-step-by-step)
- [Text-to-Image Combined Workflow](http://docs.comfy.org#text-to-image-combined-workflow)
- [Additional Tips](http://docs.comfy.org#additional-tips)

<!-- END Built_In_Node/tutorials/basic/upscale.md -->


<!-- BEGIN Built_In_Node/tutorials/controlnet/controlnet.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
  
  - [ControlNet](http://docs.comfy.org/tutorials/controlnet/controlnet)
  - [Pose ControlNet](http://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass)
  - [Depth ControlNet](http://docs.comfy.org/tutorials/controlnet/depth-controlnet)
  - [Depth T2I Adapter](http://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter)
  - [Mixing ControlNet](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets)
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI ControlNet Usage Example

# ComfyUI ControlNet Usage Example

This guide will introduce you to the basic concepts of ControlNet and demonstrate how to generate corresponding images in ComfyUI

Achieving precise control over image creation in AI image generation cannot be done with just one click. It typically requires numerous generation attempts to produce a satisfactory image. However, the emergence of **ControlNet** has effectively addressed this challenge.

ControlNet is a conditional control generation model based on diffusion models (such as Stable Diffusion), first proposed by [Lvmin Zhang](https://lllyasviel.github.io/) and Maneesh Agrawala et al. in 2023 in the paper [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543).

ControlNet models significantly enhance the controllability of image generation and the ability to reproduce details by introducing multimodal input conditions, such as edge detection maps, depth maps, and pose keypoints.

These conditioning constraints make image generation more controllable, allowing multiple ControlNet models to be used simultaneously during the drawing process for better results.

Before ControlNet, we could only rely on the model to generate images repeatedly until we were satisfied with the results, which involved a lot of randomness.

With the advent of ControlNet, we can control image generation by introducing additional conditions. For example, we can use a simple sketch to guide the image generation process, producing images that closely align with our sketch.

In this example, we will guide you through installing and using ControlNet models in [ComfyUI](https://github.com/comfyanonymous/ComfyUI), and complete a sketch-controlled image generation example.

The workflows for other types of ControlNet V1.1 models are similar to this example. You only need to select the appropriate model and upload the corresponding reference image based on your needs.

## [​](http://docs.comfy.org#controlnet-image-preprocessing-information) ControlNet Image Preprocessing Information

Different types of ControlNet models typically require different types of reference images:

> Image source: [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

Since the current **Comfy Core** nodes do not include all types of **preprocessors**, in the actual examples in this documentation, we will provide pre-processed images. However, in practical use, you may need to use custom nodes to preprocess images to meet the requirements of different ControlNet models. Below are some relevant custom nodes:

- [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
- [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

## [​](http://docs.comfy.org#comfyui-controlnet-workflow-example-explanation) ComfyUI ControlNet Workflow Example Explanation

### [​](http://docs.comfy.org#1-controlnet-workflow-assets) 1. ControlNet Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`. This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.

Please download the image below, which we will use as input:

### [​](http://docs.comfy.org#2-manual-model-installation) 2. Manual Model Installation

If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:

- [dreamCreationVirtual3DECommerce\_v10.safetensors](https://civitai.com/api/download/models/731340?type=Model&format=SafeTensor&size=full&fp=fp16)
- [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)
- [control\_v11p\_sd15\_scribble\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── dreamCreationVirtual3DECommerce_v10.safetensors
│   ├── vae/
│   │   └── vae-ft-mse-840000-ema-pruned.safetensors
│   └── controlnet/
│       └── control_v11p_sd15_scribble_fp16.safetensors
```

In this example, you could also use the VAE model embedded in dreamCreationVirtual3DECommerce\_v10.safetensors, but we’re following the model author’s recommendation to use a separate VAE model.

### [​](http://docs.comfy.org#3-step-by-step-workflow-execution) 3. Step-by-Step Workflow Execution

1. Ensure that `Load Checkpoint` can load **dreamCreationVirtual3DECommerce\_v10.safetensors**
2. Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**
3. Click `Upload` in the `Load Image` node to upload the input image provided earlier
4. Ensure that `Load ControlNet` can load **control\_v11p\_sd15\_scribble\_fp16.safetensors**
5. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## [​](http://docs.comfy.org#related-node-explanations) Related Node Explanations

### [​](http://docs.comfy.org#load-controlnet-node-explanation) Load ControlNet Node Explanation

Models located in `ComfyUI\models\controlnet` will be detected by ComfyUI and can be loaded through this node.

### [​](http://docs.comfy.org#apply-controlnet-node-explanation) Apply ControlNet Node Explanation

This node accepts the ControlNet model loaded by `load controlnet` and generates corresponding control conditions based on the input image.

**Input Types**

Parameter NameFunction`positive`Positive conditioning`negative`Negative conditioning`control_net`The ControlNet model to be applied`image`Preprocessed image used as reference for ControlNet application`vae`VAE model input`strength`Strength of ControlNet application; higher values increase ControlNet’s influence on the generated image`start_percent`Determines when to start applying ControlNet as a percentage; e.g., 0.2 means ControlNet guidance begins when 20% of diffusion is complete`end_percent`Determines when to stop applying ControlNet as a percentage; e.g., 0.8 means ControlNet guidance stops when 80% of diffusion is complete

**Output Types**

Parameter NameFunction`positive`Positive conditioning data processed by ControlNet`negative`Negative conditioning data processed by ControlNet

You can use chain connections to apply multiple ControlNet models, as shown in the image below. You can also refer to the [Mixing ControlNet Models](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets.mdx) guide to learn more about combining multiple ControlNet models.

You might see the `Apply ControlNet(Old)` node in some early workflows, which is an early version of the ControlNet node. It is currently deprecated and not visible by default in search and node lists. To enable it, go to **Settings** —&gt; **comfy** —&gt; **Node** and enable the `Show deprecated nodes in search` option. However, it’s recommended to use the new node.

## [​](http://docs.comfy.org#start-your-exploration) Start Your Exploration

1. Try creating similar sketches, or even draw your own, and use ControlNet models to generate images to experience the benefits of ControlNet.
2. Adjust the `Control Strength` parameter in the Apply ControlNet node to control the influence of the ControlNet model on the generated image.
3. Visit the [ControlNet-v1-1\_fp16\_safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/tree/main) repository to download other types of ControlNet models and try using them to generate images.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/controlnet/controlnet.mdx)

[Previous](http://docs.comfy.org/tutorials/basic/multiple-loras)

[Pose ControlNetThis guide will introduce you to the basic concepts of Pose ControlNet, and demonstrate how to generate large-sized images in ComfyUI using a two-pass generation approach  
\
Next](http://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [ControlNet Image Preprocessing Information](http://docs.comfy.org#controlnet-image-preprocessing-information)
- [ComfyUI ControlNet Workflow Example Explanation](http://docs.comfy.org#comfyui-controlnet-workflow-example-explanation)
- [1. ControlNet Workflow Assets](http://docs.comfy.org#1-controlnet-workflow-assets)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation)
- [3. Step-by-Step Workflow Execution](http://docs.comfy.org#3-step-by-step-workflow-execution)
- [Related Node Explanations](http://docs.comfy.org#related-node-explanations)
- [Load ControlNet Node Explanation](http://docs.comfy.org#load-controlnet-node-explanation)
- [Apply ControlNet Node Explanation](http://docs.comfy.org#apply-controlnet-node-explanation)
- [Start Your Exploration](http://docs.comfy.org#start-your-exploration)

<!-- END Built_In_Node/tutorials/controlnet/controlnet.md -->


<!-- BEGIN Built_In_Node/tutorials/controlnet/depth-controlnet.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
  
  - [ControlNet](http://docs.comfy.org/tutorials/controlnet/controlnet)
  - [Pose ControlNet](http://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass)
  - [Depth ControlNet](http://docs.comfy.org/tutorials/controlnet/depth-controlnet)
  - [Depth T2I Adapter](http://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter)
  - [Mixing ControlNet](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets)
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Depth ControlNet Usage Example

# ComfyUI Depth ControlNet Usage Example

This guide will introduce you to the basic concepts of Depth ControlNet and demonstrate how to generate corresponding images in ComfyUI

## [​](http://docs.comfy.org#introduction-to-depth-maps-and-depth-controlnet) Introduction to Depth Maps and Depth ControlNet

A depth map is a special type of image that uses grayscale values to represent the distance between objects in a scene and the observer or camera. In a depth map, the grayscale value is inversely proportional to distance: brighter areas (closer to white) indicate objects that are closer, while darker areas (closer to black) indicate objects that are farther away.

Depth ControlNet is a ControlNet model specifically trained to understand and utilize depth map information. It helps AI correctly interpret spatial relationships, ensuring that generated images conform to the spatial structure specified by the depth map, thereby enabling precise control over three-dimensional spatial layouts.

### [​](http://docs.comfy.org#application-scenarios-for-depth-maps-with-controlnet) Application Scenarios for Depth Maps with ControlNet

Depth maps have numerous applications in various scenarios:

1. **Portrait Scenes**: Control the spatial relationship between subjects and backgrounds, avoiding distortion in critical areas such as faces
2. **Landscape Scenes**: Control the hierarchical relationships between foreground, middle ground, and background
3. **Architectural Scenes**: Control the spatial structure and perspective relationships of buildings
4. **Product Showcase**: Control the separation and spatial positioning of products against their backgrounds

In this example, we will use a depth map to generate an architectural visualization scene.

## [​](http://docs.comfy.org#comfyui-controlnet-workflow-example-explanation) ComfyUI ControlNet Workflow Example Explanation

### [​](http://docs.comfy.org#1-controlnet-workflow-assets) 1. ControlNet Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`. This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.

Please download the image below, which we will use as input:

### [​](http://docs.comfy.org#2-model-installation) 2. Model Installation

If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:

- [architecturerealmix\_v11.safetensors](https://civitai.com/api/download/models/431755?type=Model&format=SafeTensor&size=full&fp=fp16)
- [control\_v11f1p\_sd15\_depth\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors?download=true)

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── architecturerealmix_v11.safetensors
│   └── controlnet/
│       └── control_v11f1p_sd15_depth_fp16.safetensors
```

### [​](http://docs.comfy.org#3-step-by-step-workflow-execution) 3. Step-by-Step Workflow Execution

1. Ensure that `Load Checkpoint` can load **architecturerealmix\_v11.safetensors**
2. Ensure that `Load ControlNet` can load **control\_v11f1p\_sd15\_depth\_fp16.safetensors**
3. Click `Upload` in the `Load Image` node to upload the depth image provided earlier
4. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## [​](http://docs.comfy.org#combining-depth-control-with-other-techniques) Combining Depth Control with Other Techniques

Based on different creative needs, you can combine Depth ControlNet with other types of ControlNet to achieve better results:

1. **Depth + Lineart**: Maintain spatial relationships while reinforcing outlines, suitable for architecture, products, and character design
2. **Depth + Pose**: Control character posture while maintaining correct spatial relationships, suitable for character scenes

For more information on using multiple ControlNet models together, please refer to the [Mixing ControlNet](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets.mdx) example.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/controlnet/depth-controlnet.mdx)

[Previous](http://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass)

[Depth T2I AdapterThis guide will introduce you to the basic concepts of Depth T2I Adapter and demonstrate how to generate corresponding images in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Introduction to Depth Maps and Depth ControlNet](http://docs.comfy.org#introduction-to-depth-maps-and-depth-controlnet)
- [Application Scenarios for Depth Maps with ControlNet](http://docs.comfy.org#application-scenarios-for-depth-maps-with-controlnet)
- [ComfyUI ControlNet Workflow Example Explanation](http://docs.comfy.org#comfyui-controlnet-workflow-example-explanation)
- [1. ControlNet Workflow Assets](http://docs.comfy.org#1-controlnet-workflow-assets)
- [2. Model Installation](http://docs.comfy.org#2-model-installation)
- [3. Step-by-Step Workflow Execution](http://docs.comfy.org#3-step-by-step-workflow-execution)
- [Combining Depth Control with Other Techniques](http://docs.comfy.org#combining-depth-control-with-other-techniques)

<!-- END Built_In_Node/tutorials/controlnet/depth-controlnet.md -->


<!-- BEGIN Built_In_Node/tutorials/controlnet/depth-t2i-adapter.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
  
  - [ControlNet](http://docs.comfy.org/tutorials/controlnet/controlnet)
  - [Pose ControlNet](http://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass)
  - [Depth ControlNet](http://docs.comfy.org/tutorials/controlnet/depth-controlnet)
  - [Depth T2I Adapter](http://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter)
  - [Mixing ControlNet](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets)
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Depth T2I Adapter Usage Example

# ComfyUI Depth T2I Adapter Usage Example

This guide will introduce you to the basic concepts of Depth T2I Adapter and demonstrate how to generate corresponding images in ComfyUI

## [​](http://docs.comfy.org#introduction-to-t2i-adapter) Introduction to T2I Adapter

[T2I-Adapter](https://huggingface.co/TencentARC/T2I-Adapter) is a lightweight adapter developed by [Tencent ARC Lab](https://github.com/TencentARC) designed to enhance the structural, color, and style control capabilities of text-to-image generation models (such as Stable Diffusion). It works by aligning external conditions (such as edge detection maps, depth maps, sketches, or color reference images) with the model’s internal features, achieving high-precision control without modifying the original model structure. With only about 77M parameters (approximately 300MB in size), its inference speed is about 3 times faster than [ControlNet](https://github.com/lllyasviel/ControlNet-v1-1-nightly), and it supports multiple condition combinations (such as sketch + color grid). Application scenarios include line art to image conversion, color style transfer, multi-element scene generation, and more.

### [​](http://docs.comfy.org#comparison-between-t2i-adapter-and-controlnet) Comparison Between T2I Adapter and ControlNet

Although their functions are similar, there are notable differences in implementation and application:

1. **Lightweight Design**: T2I Adapter has fewer parameters and a smaller memory footprint
2. **Inference Speed**: T2I Adapter is typically about 3 times faster than ControlNet
3. **Control Precision**: ControlNet offers more precise control in certain scenarios, while T2I Adapter is more suitable for lightweight control
4. **Multi-condition Combination**: T2I Adapter shows more significant resource advantages when combining multiple conditions

### [​](http://docs.comfy.org#main-types-of-t2i-adapter) Main Types of T2I Adapter

T2I Adapter provides various types to control different aspects:

- **Depth**: Controls the spatial structure and depth relationships in images
- **Line Art (Canny/Sketch)**: Controls image edges and lines
- **Keypose**: Controls character poses and actions
- **Segmentation (Seg)**: Controls scene layout through semantic segmentation
- **Color**: Controls the overall color scheme of images

In ComfyUI, using T2I Adapter is similar to [ControlNet](http://docs.comfy.org/tutorials/controlnet/controlnet.mdx) in terms of interface and workflow. In this example, we will demonstrate how to use a depth T2I Adapter to control an interior scene.

## [​](http://docs.comfy.org#value-of-depth-t2i-adapter-applications) Value of Depth T2I Adapter Applications

Depth maps have several important applications in image generation:

1. **Spatial Layout Control**: Accurately describes three-dimensional spatial structures, suitable for interior design and architectural visualization
2. **Object Positioning**: Controls the relative position and size of objects in a scene, suitable for product showcases and scene construction
3. **Perspective Relationships**: Maintains reasonable perspective and proportions, suitable for landscape and urban scene generation
4. **Light and Shadow Layout**: Natural light and shadow distribution based on depth information, enhancing realism

We will use interior design as an example to demonstrate how to use the depth T2I Adapter, but these techniques are applicable to other scenarios as well.

## [​](http://docs.comfy.org#comfyui-depth-t2i-adapter-workflow-example-explanation) ComfyUI Depth T2I Adapter Workflow Example Explanation

### [​](http://docs.comfy.org#1-depth-t2i-adapter-workflow-assets) 1. Depth T2I Adapter Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`. This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.

Please download the image below, which we will use as input:

### [​](http://docs.comfy.org#2-model-installation) 2. Model Installation

If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:

- [interiordesignsuperm\_v2.safetensors](https://civitai.com/api/download/models/93152?type=Model&format=SafeTensor&size=full&fp=fp16)
- [t2iadapter\_depth\_sd15v2.pth](https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_depth_sd15v2.pth?download=true)

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── interiordesignsuperm_v2.safetensors
│   └── controlnet/
│       └── t2iadapter_depth_sd15v2.pth
```

### [​](http://docs.comfy.org#3-step-by-step-workflow-execution) 3. Step-by-Step Workflow Execution

1. Ensure that `Load Checkpoint` can load **interiordesignsuperm\_v2.safetensors**
2. Ensure that `Load ControlNet` can load **t2iadapter\_depth\_sd15v2.pth**
3. Click `Upload` in the `Load Image` node to upload the input image provided earlier
4. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## [​](http://docs.comfy.org#general-tips-for-using-t2i-adapter) General Tips for Using T2I Adapter

### [​](http://docs.comfy.org#input-image-quality-optimization) Input Image Quality Optimization

Regardless of the application scenario, high-quality input images are key to successfully using T2I Adapter:

1. **Moderate Contrast**: Control images (such as depth maps, line art) should have clear contrast, but not excessively extreme
2. **Clear Boundaries**: Ensure that major structures and element boundaries are clearly distinguishable in the control image
3. **Noise Control**: Try to avoid excessive noise in control images, especially for depth maps and line art
4. **Reasonable Layout**: Control images should have a reasonable spatial layout and element distribution

## [​](http://docs.comfy.org#characteristics-of-t2i-adapter-usage) Characteristics of T2I Adapter Usage

One major advantage of T2I Adapter is its ability to easily combine multiple conditions for complex control effects:

1. **Depth + Edge**: Control spatial layout while maintaining clear structural edges, suitable for architecture and interior design
2. **Line Art + Color**: Control shapes while specifying color schemes, suitable for character design and illustrations
3. **Pose + Segmentation**: Control character actions while defining scene areas, suitable for complex narrative scenes

Mixing different T2I Adapters, or combining them with other control methods (such as ControlNet, regional prompts, etc.), can further expand creative possibilities. To achieve mixing, simply chain multiple `Apply ControlNet` nodes together in the same way as described in [Mixing ControlNet](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets.mdx).

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/controlnet/depth-t2i-adapter.mdx)

[Previous](http://docs.comfy.org/tutorials/controlnet/depth-controlnet)

[Mixing ControlNetIn this example, we will demonstrate how to mix multiple ControlNets and learn to use multiple ControlNet models to control image generation  
\
Next](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Introduction to T2I Adapter](http://docs.comfy.org#introduction-to-t2i-adapter)
- [Comparison Between T2I Adapter and ControlNet](http://docs.comfy.org#comparison-between-t2i-adapter-and-controlnet)
- [Main Types of T2I Adapter](http://docs.comfy.org#main-types-of-t2i-adapter)
- [Value of Depth T2I Adapter Applications](http://docs.comfy.org#value-of-depth-t2i-adapter-applications)
- [ComfyUI Depth T2I Adapter Workflow Example Explanation](http://docs.comfy.org#comfyui-depth-t2i-adapter-workflow-example-explanation)
- [1. Depth T2I Adapter Workflow Assets](http://docs.comfy.org#1-depth-t2i-adapter-workflow-assets)
- [2. Model Installation](http://docs.comfy.org#2-model-installation)
- [3. Step-by-Step Workflow Execution](http://docs.comfy.org#3-step-by-step-workflow-execution)
- [General Tips for Using T2I Adapter](http://docs.comfy.org#general-tips-for-using-t2i-adapter)
- [Input Image Quality Optimization](http://docs.comfy.org#input-image-quality-optimization)
- [Characteristics of T2I Adapter Usage](http://docs.comfy.org#characteristics-of-t2i-adapter-usage)

<!-- END Built_In_Node/tutorials/controlnet/depth-t2i-adapter.md -->


<!-- BEGIN Built_In_Node/tutorials/controlnet/mixing-controlnets.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
  
  - [ControlNet](http://docs.comfy.org/tutorials/controlnet/controlnet)
  - [Pose ControlNet](http://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass)
  - [Depth ControlNet](http://docs.comfy.org/tutorials/controlnet/depth-controlnet)
  - [Depth T2I Adapter](http://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter)
  - [Mixing ControlNet](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets)
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Mixing ControlNet Examples

# ComfyUI Mixing ControlNet Examples

In this example, we will demonstrate how to mix multiple ControlNets and learn to use multiple ControlNet models to control image generation

In AI image generation, a single control condition often fails to meet the requirements of complex scenes. Mixing multiple ControlNets allows you to control different regions or aspects of an image simultaneously, achieving more precise control over image generation.

In certain scenarios, mixing ControlNets can leverage the characteristics of different control conditions to achieve more refined conditional control:

1. **Scene Complexity**: Complex scenes require multiple control conditions working together
2. **Fine-grained Control**: By adjusting the strength parameter of each ControlNet, you can precisely control the degree of influence for each part
3. **Complementary Effects**: Different types of ControlNets can complement each other, compensating for the limitations of single controls
4. **Creative Expression**: Combining different controls can produce unique creative effects

### [​](http://docs.comfy.org#how-to-mix-controlnets) How to Mix ControlNets

When mixing multiple ControlNets, each ControlNet influences the image generation process according to its applied area. ComfyUI enables multiple ControlNet conditions to be applied sequentially in a layered manner through chain connections in the `Apply ControlNet` node:

## [​](http://docs.comfy.org#comfyui-controlnet-regional-division-mixing-example) ComfyUI ControlNet Regional Division Mixing Example

In this example, we will use a combination of **Pose ControlNet** and **Scribble ControlNet** to generate a scene containing multiple elements: a character on the left controlled by Pose ControlNet and a cat on a scooter on the right controlled by Scribble ControlNet.

### [​](http://docs.comfy.org#1-controlnet-mixing-workflow-assets) 1. ControlNet Mixing Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

This workflow image contains Metadata, and can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`. The system will automatically detect and prompt to download the required models.

Input pose image (controls the character pose on the left):

Input scribble image (controls the cat and scooter on the right):

### [​](http://docs.comfy.org#2-manual-model-installation) 2. Manual Model Installation

If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:

- [awpainting\_v14.safetensors](https://civitai.com/api/download/models/624939?type=Model&format=SafeTensor&size=full&fp=fp16)
- [control\_v11p\_sd15\_scribble\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)
- [control\_v11p\_sd15\_openpose\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)
- [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── awpainting_v14.safetensors
│   ├── controlnet/
│   │   └── control_v11p_sd15_scribble_fp16.safetensors
│   │   └── control_v11p_sd15_openpose_fp16.safetensors
│   ├── vae/
│   │   └── vae-ft-mse-840000-ema-pruned.safetensors
```

### [​](http://docs.comfy.org#3-step-by-step-workflow-execution) 3. Step-by-Step Workflow Execution

Follow these steps according to the numbered markers in the image:

1. Ensure that `Load Checkpoint` can load **awpainting\_v14.safetensors**
2. Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**

First ControlNet group using the Openpose model: 3. Ensure that `Load ControlNet Model` loads **control\_v11p\_sd15\_openpose\_fp16.safetensors** 4. Click `Upload` in the `Load Image` node to upload the pose image provided earlier

Second ControlNet group using the Scribble model: 5. Ensure that `Load ControlNet Model` loads **control\_v11p\_sd15\_scribble\_fp16.safetensors** 6. Click `Upload` in the `Load Image` node to upload the scribble image provided earlier 7. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## [​](http://docs.comfy.org#workflow-explanation) Workflow Explanation

#### [​](http://docs.comfy.org#strength-balance) Strength Balance

When controlling different regions of an image, balancing the strength parameters is particularly important:

- If the ControlNet strength for one region is significantly higher than another, it may cause that region’s control effect to overpower and suppress the other region
- It’s recommended to set similar strength values for ControlNets controlling different regions, for example, both set to 1.0

#### [​](http://docs.comfy.org#prompt-techniques) Prompt Techniques

For regional division mixing, the prompt needs to include descriptions of both regions:

```plaintext
"A woman in red dress, a cat riding a scooter, detailed background, high quality"
```

Such a prompt covers both the character and the cat on the scooter, ensuring the model pays attention to both control regions.

## [​](http://docs.comfy.org#multi-dimensional-control-applications-for-a-single-subject) Multi-dimensional Control Applications for a Single Subject

In addition to the regional division mixing shown in this example, another common mixing approach is to apply multi-dimensional control to the same subject. For example:

- **Pose + Depth**: Control character posture and spatial sense
- **Pose + Canny**: Control character posture and edge details
- **Pose + Reference**: Control character posture while referencing a specific style

In this type of application, reference images for multiple ControlNets should be aligned to the same subject, and their strengths should be adjusted to ensure proper balance.

By combining different types of ControlNets and specifying their control regions, you can achieve precise control over elements in your image.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/controlnet/mixing-controlnets.mdx)

[Previous](http://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter)

[Flux.1 Text-to-ImageThis guide provides a brief introduction to the Flux.1 model and guides you through using the Flux.1 model for text-to-image generation with examples including the full version and the FP8 Checkpoint version.  
\
Next](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [How to Mix ControlNets](http://docs.comfy.org#how-to-mix-controlnets)
- [ComfyUI ControlNet Regional Division Mixing Example](http://docs.comfy.org#comfyui-controlnet-regional-division-mixing-example)
- [1. ControlNet Mixing Workflow Assets](http://docs.comfy.org#1-controlnet-mixing-workflow-assets)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation)
- [3. Step-by-Step Workflow Execution](http://docs.comfy.org#3-step-by-step-workflow-execution)
- [Workflow Explanation](http://docs.comfy.org#workflow-explanation)
- [Strength Balance](http://docs.comfy.org#strength-balance)
- [Prompt Techniques](http://docs.comfy.org#prompt-techniques)
- [Multi-dimensional Control Applications for a Single Subject](http://docs.comfy.org#multi-dimensional-control-applications-for-a-single-subject)

<!-- END Built_In_Node/tutorials/controlnet/mixing-controlnets.md -->


<!-- BEGIN Built_In_Node/tutorials/controlnet/pose-controlnet-2-pass.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
  
  - [ControlNet](http://docs.comfy.org/tutorials/controlnet/controlnet)
  - [Pose ControlNet](http://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass)
  - [Depth ControlNet](http://docs.comfy.org/tutorials/controlnet/depth-controlnet)
  - [Depth T2I Adapter](http://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter)
  - [Mixing ControlNet](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets)
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Pose ControlNet Usage Example

# ComfyUI Pose ControlNet Usage Example

This guide will introduce you to the basic concepts of Pose ControlNet, and demonstrate how to generate large-sized images in ComfyUI using a two-pass generation approach

## [​](http://docs.comfy.org#introduction-to-openpose) Introduction to OpenPose

[OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) is an open-source real-time multi-person pose estimation system developed by Carnegie Mellon University (CMU), representing a significant breakthrough in the field of computer vision. The system can simultaneously detect multiple people in an image, capturing:

- **Body skeleton**: 18 keypoints, including head, shoulders, elbows, wrists, hips, knees, and ankles
- **Facial expressions**: 70 facial keypoints for capturing micro-expressions and facial contours
- **Hand details**: 21 hand keypoints for precisely expressing finger positions and gestures
- **Foot posture**: 6 foot keypoints, recording standing postures and movement details

In AI image generation, skeleton structure maps generated by OpenPose serve as conditional inputs for ControlNet, enabling precise control over the posture, actions, and expressions of generated characters. This allows us to generate realistic human figures with expected poses and actions, greatly improving the controllability and practical value of AI-generated content. Particularly for early Stable Diffusion 1.5 series models, skeletal maps generated by OpenPose can effectively prevent issues with distorted character actions, limbs, and expressions.

## [​](http://docs.comfy.org#comfyui-2-pass-pose-controlnet-usage-example) ComfyUI 2-Pass Pose ControlNet Usage Example

### [​](http://docs.comfy.org#1-pose-controlnet-workflow-assets) 1. Pose ControlNet Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`. This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.

Please download the image below, which we will use as input:

### [​](http://docs.comfy.org#2-manual-model-installation) 2. Manual Model Installation

If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:

- [control\_v11p\_sd15\_openpose\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)
- [majicmixRealistic\_v7.safetensors](https://civitai.com/api/download/models/176425?type=Model&format=SafeTensor&size=pruned&fp=fp16)
- [japaneseStyleRealistic\_v20.safetensors](https://civitai.com/api/download/models/85426?type=Model&format=SafeTensor&size=pruned&fp=fp16)
- [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── majicmixRealistic_v7.safetensors
│   │   └── japaneseStyleRealistic_v20.safetensors
│   ├── vae/
│   │   └── vae-ft-mse-840000-ema-pruned.safetensors
│   └── controlnet/
│       └── control_v11p_sd15_openpose_fp16.safetensors
```

### [​](http://docs.comfy.org#3-step-by-step-workflow-execution) 3. Step-by-Step Workflow Execution

Follow these steps according to the numbered markers in the image:

1. Ensure that `Load Checkpoint` can load **majicmixRealistic\_v7.safetensors**
2. Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**
3. Ensure that `Load ControlNet Model` can load **control\_v11p\_sd15\_openpose\_fp16.safetensors**
4. Click the select button in the `Load Image` node to upload the pose input image provided earlier, or use your own OpenPose skeleton map
5. Ensure that `Load Checkpoint` can load **japaneseStyleRealistic\_v20.safetensors**
6. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## [​](http://docs.comfy.org#explanation-of-the-pose-controlnet-2-pass-workflow) Explanation of the Pose ControlNet 2-Pass Workflow

This workflow uses a two-pass image generation approach, dividing the image creation process into two phases:

### [​](http://docs.comfy.org#first-phase%3A-basic-pose-image-generation) First Phase: Basic Pose Image Generation

In the first phase, the **majicmixRealistic\_v7** model is combined with Pose ControlNet to generate an initial character pose image:

1. First, load the majicmixRealistic\_v7 model via the `Load Checkpoint` node
2. Load the pose control model through the `Load ControlNet Model` node
3. The input pose image is fed into the `Apply ControlNet` node and combined with positive and negative prompt conditions
4. The first `KSampler` node (typically using 20-30 steps) generates a basic character pose image
5. The pixel-space image for the first phase is obtained through `VAE Decode`

This phase primarily focuses on correct character posture, pose, and basic structure, ensuring that the generated character conforms to the input skeletal pose.

### [​](http://docs.comfy.org#second-phase%3A-style-optimization-and-detail-enhancement) Second Phase: Style Optimization and Detail Enhancement

In the second phase, the output image from the first phase is used as a reference, with the **japaneseStyleRealistic\_v20** model performing stylization and detail enhancement:

1. The image generated in the first phase creates a larger resolution latent space through the `Upscale latent` node
2. The second `Load Checkpoint` loads the japaneseStyleRealistic\_v20 model, which focuses on details and style
3. The second `KSampler` node uses a lower `denoise` strength (typically 0.4-0.6) for refinement, preserving the basic structure from the first phase
4. Finally, a higher quality, larger resolution image is output through the second `VAE Decode` and `Save Image` nodes

This phase primarily focuses on style consistency, detail richness, and enhancing overall image quality.

## [​](http://docs.comfy.org#advantages-of-2-pass-image-generation) Advantages of 2-Pass Image Generation

Compared to single-pass generation, the two-pass image generation method offers the following advantages:

1. **Higher Resolution**: Two-pass processing can generate high-resolution images beyond the capabilities of single-pass generation
2. **Style Blending**: Can combine advantages of different models, such as using a realistic model in the first phase and a stylized model in the second phase
3. **Better Details**: The second phase can focus on optimizing details without having to worry about overall structure
4. **Precise Control**: Once pose control is completed in the first phase, the second phase can focus on refining style and details
5. **Reduced GPU Load**: Generating in two passes allows for high-quality large images with limited GPU resources

To learn more about techniques for mixing multiple ControlNets, please refer to the [Mixing ControlNet Models](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets.mdx) tutorial.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/controlnet/pose-controlnet-2-pass.mdx)

[Previous](http://docs.comfy.org/tutorials/controlnet/controlnet)

[Depth ControlNetThis guide will introduce you to the basic concepts of Depth ControlNet and demonstrate how to generate corresponding images in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/controlnet/depth-controlnet)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Introduction to OpenPose](http://docs.comfy.org#introduction-to-openpose)
- [ComfyUI 2-Pass Pose ControlNet Usage Example](http://docs.comfy.org#comfyui-2-pass-pose-controlnet-usage-example)
- [1. Pose ControlNet Workflow Assets](http://docs.comfy.org#1-pose-controlnet-workflow-assets)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation)
- [3. Step-by-Step Workflow Execution](http://docs.comfy.org#3-step-by-step-workflow-execution)
- [Explanation of the Pose ControlNet 2-Pass Workflow](http://docs.comfy.org#explanation-of-the-pose-controlnet-2-pass-workflow)
- [First Phase: Basic Pose Image Generation](http://docs.comfy.org#first-phase%3A-basic-pose-image-generation)
- [Second Phase: Style Optimization and Detail Enhancement](http://docs.comfy.org#second-phase%3A-style-optimization-and-detail-enhancement)
- [Advantages of 2-Pass Image Generation](http://docs.comfy.org#advantages-of-2-pass-image-generation)

<!-- END Built_In_Node/tutorials/controlnet/pose-controlnet-2-pass.md -->


<!-- BEGIN Built_In_Node/tutorials/flux/flux-1-controlnet.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
  
  - [Flux.1 Text-to-Image](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image)
  - [Flux.1 fill dev](http://docs.comfy.org/tutorials/flux/flux-1-fill-dev)
  - [Flux.1 ControlNet](http://docs.comfy.org/tutorials/flux/flux-1-controlnet)
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Flux.1 ControlNet Examples

# ComfyUI Flux.1 ControlNet Examples

This guide will demonstrate workflow examples using Flux.1 ControlNet.

## [​](http://docs.comfy.org#flux-1-controlnet-model-introduction) FLUX.1 ControlNet Model Introduction

FLUX.1 Canny and Depth are two powerful models from the [FLUX.1 Tools](https://blackforestlabs.ai/flux-1-tools/) launched by [Black Forest Labs](https://blackforestlabs.ai/). This toolkit is designed to add control and guidance capabilities to FLUX.1, enabling users to modify and recreate real or generated images.

**FLUX.1-Depth-dev** and **FLUX.1-Canny-dev** are both 12B parameter Rectified Flow Transformer models that can generate images based on text descriptions while maintaining the structural features of the input image. The Depth version maintains the spatial structure of the source image through depth map extraction techniques, while the Canny version uses edge detection techniques to preserve the structural features of the source image, allowing users to choose the appropriate control method based on different needs.

Both models have the following features:

- Top-tier output quality and detail representation
- Excellent prompt following ability while maintaining consistency with the original image
- Trained using guided distillation techniques for improved efficiency
- Open weights for the research community
- API interfaces (pro version) and open-source weights (dev version)

Additionally, Black Forest Labs also provides **FLUX.1-Depth-dev-lora** and **FLUX.1-Canny-dev-lora** adapter versions extracted from the complete models. These can be applied to the FLUX.1 \[dev] base model to provide similar functionality with smaller file size, especially suitable for resource-constrained environments.

We will use the full version of **FLUX.1-Canny-dev** and **FLUX.1-Depth-dev-lora** to complete the workflow examples.

All workflow images’s Metadata contains the corresponding model download information. You can load the workflows by:

- Dragging them directly into ComfyUI
- Or using the menu `Workflows` -&gt; `Open（ctrl+o）`

If you’re not using the Desktop Version or some models can’t be downloaded automatically, please refer to the manual installation sections to save the model files to the corresponding folder.

For image preprocessors, you can use the following custom nodes to complete image preprocessing. In this example, we will provide processed images as input.

- [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
- [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

## [​](http://docs.comfy.org#flux-1-canny-dev-complete-version-workflow) FLUX.1-Canny-dev Complete Version Workflow

### [​](http://docs.comfy.org#1-workflow-and-asset) 1. Workflow and Asset

Please download the workflow image below and drag it into ComfyUI to load the workflow

Please download the image below, which we will use as the input image

### [​](http://docs.comfy.org#2-manual-models-installation) 2. Manual Models Installation

If you have previously used the [complete version of Flux related workflows](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image), then you only need to download the **flux1-canny-dev.safetensors** model file. Since you need to first agree to the terms of [black-forest-labs/FLUX.1-Canny-dev](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev), please visit the [black-forest-labs/FLUX.1-Canny-dev](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev) page and make sure you have agreed to the corresponding terms as shown in the image below.

Complete model list:

- [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
- [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
- [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
- [flux1-canny-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev/resolve/main/flux1-canny-dev.safetensors?download=true) (Please ensure you have agreed to the corresponding repo’s terms)

File storage location:

```plaintext
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── t5xxl_fp16.safetensors
│   ├── vae/
│   │   └── ae.safetensors
│   └── diffusion_models/
│       └── flux1-canny-dev.safetensors
```

### [​](http://docs.comfy.org#3-step-by-step-workflow-execution) 3. Step-by-Step Workflow Execution

1. Make sure `ae.safetensors` is loaded in the `Load VAE` node
2. Make sure `flux1-canny-dev.safetensors` is loaded in the `Load Diffusion Model` node
3. Make sure the following models are loaded in the `DualCLIPLoader` node:
   
   - clip\_name1: t5xxl\_fp16.safetensors
   - clip\_name2: clip\_l.safetensors
4. Upload the provided input image in the `Load Image` node
5. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

### [​](http://docs.comfy.org#4-start-your-experimentation) 4. Start Your Experimentation

Try using the [FLUX.1-Depth-dev](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev) model to complete the Depth version of the workflow

You can use the image below as input

Or use the following custom nodes to complete image preprocessing:

- [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
- [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

## [​](http://docs.comfy.org#flux-1-depth-dev-lora-workflow) FLUX.1-Depth-dev-lora Workflow

The LoRA version workflow builds on the complete version by adding the LoRA model. Compared to the [complete version of the Flux workflow](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image), it adds nodes for loading and using the corresponding LoRA model.

### [​](http://docs.comfy.org#1-workflow-and-asset-2) 1. Workflow and Asset

Please download the workflow image below and drag it into ComfyUI to load the workflow

Please download the image below, which we will use as the input image

### [​](http://docs.comfy.org#2-manual-model-download) 2. Manual Model Download

If you have previously used the [complete version of Flux related workflows](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image), then you only need to download the **flux1-depth-dev-lora.safetensors** model file.

Complete model list:

- [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
- [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
- [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
- [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors?download=true)
- [flux1-depth-dev-lora.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev-lora/resolve/main/flux1-depth-dev-lora.safetensors?download=true)

File storage location:

```plaintext
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── t5xxl_fp16.safetensors
│   ├── vae/
│   │   └── ae.safetensors
│   ├── diffusion_models/
│   │   └── flux1-dev.safetensors
│   └── loras/
│       └── flux1-depth-dev-lora.safetensors
```

### [​](http://docs.comfy.org#3-step-by-step-workflow-execution-2) 3. Step-by-Step Workflow Execution

1. Make sure `flux1-dev.safetensors` is loaded in the `Load Diffusion Model` node
2. Make sure `flux1-depth-dev-lora.safetensors` is loaded in the `LoraLoaderModelOnly` node
3. Make sure the following models are loaded in the `DualCLIPLoader` node:
   
   - clip\_name1: t5xxl\_fp16.safetensors
   - clip\_name2: clip\_l.safetensors
4. Upload the provided input image in the `Load Image` node
5. Make sure `ae.safetensors` is loaded in the `Load VAE` node
6. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

### [​](http://docs.comfy.org#4-start-your-experimentation-2) 4. Start Your Experimentation

Try using the [FLUX.1-Canny-dev-lora](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev-lora) model to complete the Canny version of the workflow

Use [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet) or [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux) to complete image preprocessing

## [​](http://docs.comfy.org#community-versions-of-flux-controlnets) Community Versions of Flux Controlnets

XLab and InstantX + Shakker Labs have released Controlnets for Flux.

**InstantX:**

- [FLUX.1-dev-Controlnet-Canny](https://huggingface.co/InstantX/FLUX.1-dev-Controlnet-Canny/blob/main/diffusion_pytorch_model.safetensors)
- [FLUX.1-dev-ControlNet-Depth](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Depth/blob/main/diffusion_pytorch_model.safetensors)
- [FLUX.1-dev-ControlNet-Union-Pro](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro/blob/main/diffusion_pytorch_model.safetensors)

**XLab**: [flux-controlnet-collections](https://huggingface.co/XLabs-AI/flux-controlnet-collections)

Place these files in the `ComfyUI/models/controlnet` directory.

You can visit [Flux Controlnet Example](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/flux_controlnet_example.png) to get the corresponding workflow image, and use the image from [here](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/girl_in_field.png) as the input image.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/flux/flux-1-controlnet.mdx)

[Previous](http://docs.comfy.org/tutorials/flux/flux-1-fill-dev)

[HiDream-I1This guide will walk you through completing a ComfyUI native HiDream-I1 text-to-image workflow example  
\
Next](http://docs.comfy.org/tutorials/image/hidream/hidream-i1)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [FLUX.1 ControlNet Model Introduction](http://docs.comfy.org#flux-1-controlnet-model-introduction)
- [FLUX.1-Canny-dev Complete Version Workflow](http://docs.comfy.org#flux-1-canny-dev-complete-version-workflow)
- [1. Workflow and Asset](http://docs.comfy.org#1-workflow-and-asset)
- [2. Manual Models Installation](http://docs.comfy.org#2-manual-models-installation)
- [3. Step-by-Step Workflow Execution](http://docs.comfy.org#3-step-by-step-workflow-execution)
- [4. Start Your Experimentation](http://docs.comfy.org#4-start-your-experimentation)
- [FLUX.1-Depth-dev-lora Workflow](http://docs.comfy.org#flux-1-depth-dev-lora-workflow)
- [1. Workflow and Asset](http://docs.comfy.org#1-workflow-and-asset-2)
- [2. Manual Model Download](http://docs.comfy.org#2-manual-model-download)
- [3. Step-by-Step Workflow Execution](http://docs.comfy.org#3-step-by-step-workflow-execution-2)
- [4. Start Your Experimentation](http://docs.comfy.org#4-start-your-experimentation-2)
- [Community Versions of Flux Controlnets](http://docs.comfy.org#community-versions-of-flux-controlnets)

<!-- END Built_In_Node/tutorials/flux/flux-1-controlnet.md -->


<!-- BEGIN Built_In_Node/tutorials/flux/flux-1-fill-dev.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
  
  - [Flux.1 Text-to-Image](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image)
  - [Flux.1 fill dev](http://docs.comfy.org/tutorials/flux/flux-1-fill-dev)
  - [Flux.1 ControlNet](http://docs.comfy.org/tutorials/flux/flux-1-controlnet)
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Flux.1 fill dev Example

# ComfyUI Flux.1 fill dev Example

This guide demonstrates how to use Flux.1 fill dev to create Inpainting and Outpainting workflows.

## [​](http://docs.comfy.org#introduction-to-flux-1-fill-dev-model) Introduction to Flux.1 fill dev Model

Flux.1 fill dev is one of the core tools in the [FLUX.1 Tools suite](https://blackforestlabs.ai/flux-1-tools/) launched by [Black Forest Labs](https://blackforestlabs.ai/), specifically designed for image inpainting and outpainting.

Key features of Flux.1 fill dev:

- Powerful image inpainting and outpainting capabilities, with results second only to the commercial version FLUX.1 Fill \[pro].
- Excellent prompt understanding and following ability, precisely capturing user intent while maintaining high consistency with the original image.
- Advanced guided distillation training technology, making the model more efficient while maintaining high-quality output.
- Friendly licensing terms, with generated outputs usable for personal, scientific, and commercial purposes, please refer to the [FLUX.1 \[dev\] non-commercial license](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md) for details.

Open Source Repository: [FLUX.1 \[dev\]](https://huggingface.co/black-forest-labs/FLUX.1-dev)

This guide will demonstrate inpainting and outpainting workflows based on the Flux.1 fill dev model. If you’re not familiar with inpainting and outpainting workflows, you can refer to [ComfyUI Layout Inpainting Example](http://docs.comfy.org/tutorials/basic/inpaint) and [ComfyUI Image Extension Example](http://docs.comfy.org/tutorials/basic/outpaint) for some related explanations.

## [​](http://docs.comfy.org#flux-1-fill-dev-and-related-models-installation) Flux.1 Fill dev and related models installation

Before we begin, let’s complete the installation of the Flux.1 Fill dev model files. The inpainting and outpainting workflows will use exactly the same model files. If you’ve previously used the full version of the [Flux.1 Text-to-Image workflow](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image), then you only need to download the **flux1-fill-dev.safetensors** model file in this section.

However, since downloading the corresponding model requires agreeing to the corresponding usage agreement, please visit the [black-forest-labs/FLUX.1-Fill-dev](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev) page and make sure you have agreed to the corresponding agreement as shown in the image below.

Complete model list:

- [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
- [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
- [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
- [flux1-fill-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev/resolve/main/flux1-fill-dev.safetensors?download=true)

File storage location:

```plaintext
ComfyUI/
├── models/
│   ├── text_encoders/
│   │    ├── clip_l.safetensors
│   │    └── t5xxl_fp16.safetensors
│   ├── vae/
│   │    └── ae.safetensors
│   └── diffusion_models/
│        └── flux1-fill-dev.safetensors
```

## [​](http://docs.comfy.org#flux-1-fill-dev-inpainting-workflow) Flux.1 Fill dev inpainting workflow

### [​](http://docs.comfy.org#1-inpainting-workflow-and-asset) 1. Inpainting workflow and asset

Please download the image below and drag it into ComfyUI to load the corresponding workflow

Please download the image below, we will use it as the input image

The corresponding image already contains an alpha channel, so you don’t need to draw a mask separately. If you want to draw your own mask, please [click here](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint_input_original.png) to get the image without a mask, and refer to the MaskEditor usage section in the [ComfyUI Layout Inpainting Example](http://docs.comfy.org/tutorials/basic/inpaint#using-the-mask-editor) to learn how to draw a mask in the `Load Image` node.

### [​](http://docs.comfy.org#2-steps-to-run-the-workflow) 2. Steps to run the workflow

1. Ensure the `Load Diffusion Model` node has `flux1-fill-dev.safetensors` loaded.
2. Ensure the `DualCLIPLoader` node has the following models loaded:
   
   - clip\_name1: `t5xxl_fp16.safetensors`
   - clip\_name2: `clip_l.safetensors`
3. Ensure the `Load VAE` node has `ae.safetensors` loaded.
4. Upload the input image provided in the document to the `Load Image` node; if you’re using the version without a mask, remember to complete the mask drawing using the mask editor
5. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## [​](http://docs.comfy.org#flux-1-fill-dev-outpainting-workflow) Flux.1 Fill dev Outpainting Workflow

### [​](http://docs.comfy.org#1-outpainting-workflow-and-asset) 1. Outpainting workflow and asset

Please download the image below and drag it into ComfyUI to load the corresponding workflow

Please download the image below, we will use it as the input image

### [​](http://docs.comfy.org#2-steps-to-run-the-workflow-2) 2. Steps to run the workflow

1. Ensure the `Load Diffusion Model` node has `flux1-fill-dev.safetensors` loaded.
2. Ensure the `DualCLIPLoader` node has the following models loaded:
   
   - clip\_name1: `t5xxl_fp16.safetensors`
   - clip\_name2: `clip_l.safetensors`
3. Ensure the `Load VAE` node has `ae.safetensors` loaded.
4. Upload the input image provided in the document to the `Load Image` node
5. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/flux/flux-1-fill-dev.mdx)

[Previous](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image)

[Flux.1 ControlNetThis guide will demonstrate workflow examples using Flux.1 ControlNet.  
\
Next](http://docs.comfy.org/tutorials/flux/flux-1-controlnet)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Introduction to Flux.1 fill dev Model](http://docs.comfy.org#introduction-to-flux-1-fill-dev-model)
- [Flux.1 Fill dev and related models installation](http://docs.comfy.org#flux-1-fill-dev-and-related-models-installation)
- [Flux.1 Fill dev inpainting workflow](http://docs.comfy.org#flux-1-fill-dev-inpainting-workflow)
- [1. Inpainting workflow and asset](http://docs.comfy.org#1-inpainting-workflow-and-asset)
- [2. Steps to run the workflow](http://docs.comfy.org#2-steps-to-run-the-workflow)
- [Flux.1 Fill dev Outpainting Workflow](http://docs.comfy.org#flux-1-fill-dev-outpainting-workflow)
- [1. Outpainting workflow and asset](http://docs.comfy.org#1-outpainting-workflow-and-asset)
- [2. Steps to run the workflow](http://docs.comfy.org#2-steps-to-run-the-workflow-2)

<!-- END Built_In_Node/tutorials/flux/flux-1-fill-dev.md -->


<!-- BEGIN Built_In_Node/tutorials/flux/flux-1-text-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
  
  - [Flux.1 Text-to-Image](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image)
  - [Flux.1 fill dev](http://docs.comfy.org/tutorials/flux/flux-1-fill-dev)
  - [Flux.1 ControlNet](http://docs.comfy.org/tutorials/flux/flux-1-controlnet)
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Flux.1 Text-to-Image Workflow Example

# ComfyUI Flux.1 Text-to-Image Workflow Example

This guide provides a brief introduction to the Flux.1 model and guides you through using the Flux.1 model for text-to-image generation with examples including the full version and the FP8 Checkpoint version.

Flux is one of the largest open-source text-to-image generation models, with 12B parameters and an original file size of approximately 23GB. It was developed by [Black Forest Labs](https://blackforestlabs.ai/), a team founded by former Stable Diffusion team members. Flux is known for its excellent image quality and flexibility, capable of generating high-quality, diverse images.

Currently, the Flux.1 model has several main versions:

- **Flux.1 Pro:** The best performing model, closed-source, only available through API calls.
- [**Flux.1 \[dev\]：**](https://huggingface.co/black-forest-labs/FLUX.1-dev) Open-source but limited to non-commercial use, distilled from the Pro version, with performance close to the Pro version.
- [**Flux.1 \[schnell\]：**](https://huggingface.co/black-forest-labs/FLUX.1-schnell) Uses the Apache2.0 license, requires only 4 steps to generate images, suitable for low-spec hardware.

**Flux.1 Model Features**

- **Hybrid Architecture:** Combines the advantages of Transformer networks and diffusion models, effectively integrating text and image information, improving the alignment accuracy between generated images and prompts, with excellent fidelity to complex prompts.
- **Parameter Scale:** Flux has 12B parameters, capturing more complex pattern relationships and generating more realistic, diverse images.
- **Supports Multiple Styles:** Supports diverse styles, with excellent performance for various types of images.

In this example, we’ll introduce text-to-image examples using both Flux.1 Dev and Flux.1 Schnell versions, including the full version model and the simplified FP8 Checkpoint version.

- **Flux Full Version:** Best performance, but requires larger VRAM resources and installation of multiple model files.
- **Flux FP8 Checkpoint:** Requires only one fp8 version of the model, but quality is slightly reduced compared to the full version.

All workflow images’s Metadata contains the corresponding model download information. You can load the workflows by:

- Dragging them directly into ComfyUI
- Or using the menu `Workflows` -&gt; `Open（ctrl+o）`

If you’re not using the Desktop Version or some models can’t be downloaded automatically, please refer to the manual installation sections to save the model files to the corresponding folder. Make sure your ComfyUI is updated to the latest version before starting.

## [​](http://docs.comfy.org#flux-1-full-version-text-to-image-example) Flux.1 Full Version Text-to-Image Example

If you can’t download models from [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), make sure you’ve logged into Huggingface and agreed to the corresponding repository’s license agreement.

### [​](http://docs.comfy.org#flux-1-dev) Flux.1 Dev

#### [​](http://docs.comfy.org#1-workflow-file) 1. Workflow File

Please download the image below and drag it into ComfyUI to load the workflow.

#### [​](http://docs.comfy.org#2-manual-model-installation) 2. Manual Model Installation

- The `flux1-dev.safetensors` file requires agreeing to the [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev) agreement before downloading via browser.
- If your VRAM is low, you can try using [t5xxl\_fp8\_e4m3fn.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors?download=true) to replace the `t5xxl_fp16.safetensors` file.

Please download the following model files:

- [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
- [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true) Recommended when your VRAM is greater than 32GB.
- [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
- [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors)

Storage location:

```plaintext
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── t5xxl_fp16.safetensors
│   ├── vae/
│   │   └── ae.safetensors
│   └── diffusion_models/
│       └── flux1-dev.safetensors
```

#### [​](http://docs.comfy.org#3-steps-to-run-the-workflow) 3. Steps to Run the Workflow

Please refer to the image below to ensure all model files are loaded correctly

1. Ensure the `DualCLIPLoader` node has the following models loaded:
   
   - clip\_name1: t5xxl\_fp16.safetensors
   - clip\_name2: clip\_l.safetensors
2. Ensure the `Load Diffusion Model` node has `flux1-dev.safetensors` loaded
3. Make sure the `Load VAE` node has `ae.safetensors` loaded
4. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

Thanks to Flux’s excellent prompt following capability, we don’t need any negative prompts

### [​](http://docs.comfy.org#flux-1-schnell) Flux.1 Schnell

#### [​](http://docs.comfy.org#1-workflow-file-2) 1. Workflow File

Please download the image below and drag it into ComfyUI to load the workflow.

#### [​](http://docs.comfy.org#2-manual-models-installation) 2. Manual Models Installation

In this workflow, only two model files are different from the Flux1 Dev version workflow. For t5xxl, you can still use the fp16 version for better results.

- **t5xxl\_fp16.safetensors** -&gt; **t5xxl\_fp8.safetensors**
- **flux1-dev.safetensors** -&gt; **flux1-schnell.safetensors**

Complete model file list:

- [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
- [t5xxl\_fp8\_e4m3fn.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors?download=true)
- [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
- [flux1-schnell.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/flux1-schnell.safetensors)

File storage location:

```plaintext
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── t5xxl_fp8_e4m3fn.safetensors
│   ├── vae/
│   │   └── ae.safetensors
│   └── diffusion_models/
│       └── flux1-schnell.safetensors
```

#### [​](http://docs.comfy.org#3-steps-to-run-the-workflow-2) 3. Steps to Run the Workflow

1. Ensure the `DualCLIPLoader` node has the following models loaded:
   
   - clip\_name1: t5xxl\_fp8\_e4m3fn.safetensors
   - clip\_name2: clip\_l.safetensors
2. Ensure the `Load Diffusion Model` node has `flux1-schnell.safetensors` loaded
3. Ensure the `Load VAE` node has `ae.safetensors` loaded
4. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## [​](http://docs.comfy.org#flux-1-fp8-checkpoint-version-text-to-image-example) Flux.1 FP8 Checkpoint Version Text-to-Image Example

The fp8 version is a quantized version of the original Flux.1 fp16 version. To some extent, the quality of this version will be lower than that of the fp16 version, but it also requires less VRAM, and you only need to install one model file to try running it.

### [​](http://docs.comfy.org#flux-1-dev-2) Flux.1 Dev

Please download the image below and drag it into ComfyUI to load the workflow.

Please download [flux1-dev-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-dev/resolve/main/flux1-dev-fp8.safetensors?download=true) and save it to the `ComfyUI/models/checkpoints/` directory.

Ensure that the corresponding `Load Checkpoint` node loads `flux1-dev-fp8.safetensors`, and you can try to run the workflow.

### [​](http://docs.comfy.org#flux-1-schnell-2) Flux.1 Schnell

Please download the image below and drag it into ComfyUI to load the workflow.

Please download [flux1-schnell-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-schnell/resolve/main/flux1-schnell-fp8.safetensors?download=true) and save it to the `ComfyUI/models/checkpoints/` directory.

Ensure that the corresponding `Load Checkpoint` node loads `flux1-schnell-fp8.safetensors`, and you can try to run the workflow.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/flux/flux-1-text-to-image.mdx)

[Previous](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets)

[Flux.1 fill devThis guide demonstrates how to use Flux.1 fill dev to create Inpainting and Outpainting workflows.  
\
Next](http://docs.comfy.org/tutorials/flux/flux-1-fill-dev)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Flux.1 Full Version Text-to-Image Example](http://docs.comfy.org#flux-1-full-version-text-to-image-example)
- [Flux.1 Dev](http://docs.comfy.org#flux-1-dev)
- [1. Workflow File](http://docs.comfy.org#1-workflow-file)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow)
- [Flux.1 Schnell](http://docs.comfy.org#flux-1-schnell)
- [1. Workflow File](http://docs.comfy.org#1-workflow-file-2)
- [2. Manual Models Installation](http://docs.comfy.org#2-manual-models-installation)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow-2)
- [Flux.1 FP8 Checkpoint Version Text-to-Image Example](http://docs.comfy.org#flux-1-fp8-checkpoint-version-text-to-image-example)
- [Flux.1 Dev](http://docs.comfy.org#flux-1-dev-2)
- [Flux.1 Schnell](http://docs.comfy.org#flux-1-schnell-2)

<!-- END Built_In_Node/tutorials/flux/flux-1-text-to-image.md -->


<!-- BEGIN Built_In_Node/tutorials/image/hidream/hidream-e1.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
  
  - HiDream
    
    - [HiDream-I1](http://docs.comfy.org/tutorials/image/hidream/hidream-i1)
    - [HiDream-e1](http://docs.comfy.org/tutorials/image/hidream/hidream-e1)
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Native HiDream-e1 Workflow Example

# ComfyUI Native HiDream-e1 Workflow Example

This guide will help you understand and complete the ComfyUI native HiDream-I1 text-to-image workflow example

HiDream-E1 is an interactive image editing large model officially open-sourced by HiDream-ai on April 28, 2025, built based on HiDream-I1.

It allows you to edit images using natural language. The model is released under the [MIT License](https://github.com/HiDream-ai/HiDream-E1?tab=MIT-1-ov-file), supporting use in personal projects, scientific research, and commercial applications. In combination with the previously released [hidream-i1](http://docs.comfy.org/zh-CN/tutorials/advanced/hidream), it enables **creative capabilities from image generation to editing**.

**ComfyUI now natively supports HiDream E1**. In this guide, we will help you complete the workflow example of using HiDream E1 in ComfyUI.

For reference, this workflow takes about 500s for the first run and 370s for the second run with 28 sampling steps on Google Colab L4 with 22.5GB VRAM.

### [​](http://docs.comfy.org#hidream-e1-information) HiDream-E1 Information

**HiDream-E1 Model Download** Currently, HiDream provides a full version. Here is the model information:

NameInference StepsResolutionHuggingFace RepositoryHiDream-E1-Full28768x768[🤗 HiDream-E1-Full](https://huggingface.co/HiDream-ai/HiDream-E1-Full)

-[Github](https://github.com/HiDream-ai/HiDream-E1)

## [​](http://docs.comfy.org#comfyui-native-hidream-e1-workflow-example) ComfyUI Native HiDream-e1 Workflow Example

Please upgrade your ComfyUI to the latest version (latest commit) before starting to ensure your ComfyUI has the relevant support.

### [​](http://docs.comfy.org#1-download-hidream-e1-workflow-and-related-files) 1. Download HiDream-e1 Workflow and Related Files

#### [​](http://docs.comfy.org#1-1-download-workflow-file) 1.1 Download Workflow File

Please download the image below and drag it into ComfyUI. The workflow already contains model download information, and after loading, it will prompt you to download the corresponding models.

#### [​](http://docs.comfy.org#1-2-download-input-image) 1.2 Download Input Image

Please download the image below, which we will use as input

### [​](http://docs.comfy.org#2-manual-model-installation-for-hidream-e1-related-models) 2. Manual Model Installation for HiDream-e1 Related Models

All models mentioned in this guide can be found [here](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/tree/main/split_files). Please download the corresponding files and save them to the appropriate folders.

The following model files are shared models that we will use. Please click the corresponding links to download and save according to the model file storage location. We will guide you to download the corresponding **diffusion models** in the workflow.

**text\_encoders**:

- [clip\_l\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors)
- [clip\_g\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_g_hidream.safetensors)
- [t5xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors) This model has been used in many workflows, you may have already downloaded this file.
- [llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors)

**VAE**

- [ae.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/vae/ae.safetensors) This is Flux’s VAE model. If you have used Flux workflows before, you may have already downloaded this file.

**diffusion models**

- [hidream\_e1\_full\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_e1_full_bf16.safetensors)

Model file storage location

```plaintext
📂 ComfyUI/
├── 📂 models/
│   ├── 📂 text_encoders/
│   │   ├─── clip_l_hidream.safetensors
│   │   ├─── clip_g_hidream.safetensors
│   │   ├─── t5xxl_fp8_e4m3fn_scaled.safetensors
│   │   └─── llama_3.1_8b_instruct_fp8_scaled.safetensors
│   └── 📂 vae/
│   │   └── ae.safetensors
│   └── 📂 diffusion_models/
│       └── hidream_e1_full_bf16.safetensors   
```

### [​](http://docs.comfy.org#3-complete-the-hidream-e1-workflow-step-by-step) 3. Complete the HiDream-e1 Workflow Step by Step

Follow these steps to complete the workflow:

1. Make sure the `Load Diffusion Model` node has loaded the `hidream_e1_full_bf16.safetensors` model
2. Ensure that the four corresponding text encoders are correctly loaded in the `QuadrupleCLIPLoader`
   
   - clip\_l\_hidream.safetensors
   - clip\_g\_hidream.safetensors
   - t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   - llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node is using the `ae.safetensors` file
4. Load the input image we downloaded earlier in the `Load Image` node
5. (Important) Enter **the prompt for how you want to modify the image** in the `Empty Text Encoder(Positive)` node
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to generate the image

### [​](http://docs.comfy.org#additional-notes-on-comfyui-hidream-e1-workflow) Additional Notes on ComfyUI HiDream-e1 Workflow

- You may need to modify the prompt multiple times or generate multiple times to get better results
- This model has difficulty maintaining consistency when changing image styles, so try to make your prompts as complete as possible
- As the model supports a resolution of 768\*768, in actual testing with other dimensions, the image performance is poor or even significantly different at other dimensions

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/image/hidream/hidream-e1.mdx)

[Previous](http://docs.comfy.org/tutorials/image/hidream/hidream-i1)

[Hunyuan3D-2This guide will demonstrate how to use Hunyuan3D-2 in ComfyUI to generate 3D assets.  
\
Next](http://docs.comfy.org/tutorials/3d/hunyuan3D-2)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [HiDream-E1 Information](http://docs.comfy.org#hidream-e1-information)
- [ComfyUI Native HiDream-e1 Workflow Example](http://docs.comfy.org#comfyui-native-hidream-e1-workflow-example)
- [1. Download HiDream-e1 Workflow and Related Files](http://docs.comfy.org#1-download-hidream-e1-workflow-and-related-files)
- [1.1 Download Workflow File](http://docs.comfy.org#1-1-download-workflow-file)
- [1.2 Download Input Image](http://docs.comfy.org#1-2-download-input-image)
- [2. Manual Model Installation for HiDream-e1 Related Models](http://docs.comfy.org#2-manual-model-installation-for-hidream-e1-related-models)
- [3. Complete the HiDream-e1 Workflow Step by Step](http://docs.comfy.org#3-complete-the-hidream-e1-workflow-step-by-step)
- [Additional Notes on ComfyUI HiDream-e1 Workflow](http://docs.comfy.org#additional-notes-on-comfyui-hidream-e1-workflow)

<!-- END Built_In_Node/tutorials/image/hidream/hidream-e1.md -->


<!-- BEGIN Built_In_Node/tutorials/image/hidream/hidream-i1.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
  
  - HiDream
    
    - [HiDream-I1](http://docs.comfy.org/tutorials/image/hidream/hidream-i1)
    - [HiDream-e1](http://docs.comfy.org/tutorials/image/hidream/hidream-e1)
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Native HiDream-I1 Text-to-Image Workflow Example

# ComfyUI Native HiDream-I1 Text-to-Image Workflow Example

This guide will walk you through completing a ComfyUI native HiDream-I1 text-to-image workflow example

HiDream-I1 is a text-to-image model officially open-sourced by HiDream-ai on April 7, 2025. The model has 17B parameters and is released under the [MIT license](https://github.com/HiDream-ai/HiDream-I1/blob/main/LICENSE), supporting personal projects, scientific research, and commercial use. It currently performs excellently in multiple benchmark tests.

## [​](http://docs.comfy.org#model-features) Model Features

**Hybrid Architecture Design** A combination of Diffusion Transformer (DiT) and Mixture of Experts (MoE) architecture:

- Based on Diffusion Transformer (DiT), with dual-stream MMDiT modules processing multimodal information and single-stream DiT modules optimizing global consistency.
- Dynamic routing mechanism flexibly allocates computing resources, enhancing complex scene processing capabilities and delivering excellent performance in color restoration, edge processing, and other details.

**Multimodal Text Encoder Integration** Integrates four text encoders:

- OpenCLIP ViT-bigG, OpenAI CLIP ViT-L (visual semantic alignment)
- T5-XXL (long text parsing)
- Llama-3.1-8B-Instruct (instruction understanding) This combination achieves SOTA performance in complex semantic parsing of colors, quantities, spatial relationships, etc., with Chinese prompt support significantly outperforming similar open-source models.

**Original Model Versions**

HiDream-ai provides three versions of the HiDream-I1 model to meet different needs. Below are the links to the original model repositories:

Model NameDescriptionInference StepsRepository LinkHiDream-I1-FullFull version50[🤗 HiDream-I1-Full](https://huggingface.co/HiDream-ai/HiDream-I1-Full)HiDream-I1-DevDistilled dev28[🤗 HiDream-I1-Dev](https://huggingface.co/HiDream-ai/HiDream-I1-Dev)HiDream-I1-FastDistilled fast16[🤗 HiDream-I1-Fast](https://huggingface.co/HiDream-ai/HiDream-I1-Fast)

## [​](http://docs.comfy.org#about-this-workflow-example) About This Workflow Example

In this example, we will use the repackaged version from ComfyOrg. You can find all the model files we’ll use in this example in the [HiDream-I1\_ComfyUI](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/) repository.

Before starting, please update your ComfyUI version to ensure it’s at least after this [commit](https://github.com/comfyanonymous/ComfyUI/commit/9ad792f92706e2179c58b2e5348164acafa69288) to make sure your ComfyUI has native support for HiDream

## [​](http://docs.comfy.org#hidream-i1-workflow) HiDream-I1 Workflow

The model requirements for different ComfyUI native HiDream-I1 workflows are basically the same, with only the [diffusion models](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/tree/main/split_files/diffusion_models) files being different.

If you don’t know which version to choose, please refer to the following suggestions:

- **HiDream-I1-Full** can generate the highest quality images
- **HiDream-I1-Dev** balances high-quality image generation with speed
- **HiDream-I1-Fast** can generate images in just 16 steps, suitable for scenarios requiring real-time iteration

For the **dev** and **fast** versions, negative prompts are not needed, so please set the `cfg` parameter to `1.0` during sampling. We have noted the corresponding parameter settings in the relevant workflows.

The full versions of all three versions require a lot of VRAM - you may need more than 27GB of VRAM to run them smoothly. In the corresponding workflow tutorials, we will use the **fp8** version as a demonstration example to ensure that most users can run it smoothly. However, we will still provide download links for different versions of the model in the corresponding examples, and you can choose the appropriate file based on your VRAM situation.

### [​](http://docs.comfy.org#model-installation) Model Installation

The following model files are common files that we will use. Please click on the corresponding links to download and save them according to the model file save location. We will guide you to download the corresponding **diffusion models** in the corresponding workflows.

**text\_encoders**：

- [clip\_l\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors)
- [clip\_g\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_g_hidream.safetensors)
- [t5xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors) This model has been used in many workflows, you may have already downloaded this file.
- [llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors)

**VAE**

- [ae.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/vae/ae.safetensors) This is Flux’s VAE model, if you have used Flux’s workflow before, you may have already downloaded this file.

**diffusion models** We will guide you to download the corresponding model files in the corresponding workflows.

Model file save location

```plaintext
📂 ComfyUI/
├── 📂 models/
│   ├── 📂 text_encoders/
│   │   ├─── clip_l_hidream.safetensors
│   │   ├─── clip_g_hidream.safetensors
│   │   ├─── t5xxl_fp8_e4m3fn_scaled.safetensors
│   │   └─── llama_3.1_8b_instruct_fp8_scaled.safetensors
│   └── 📂 vae/
│   │   └── ae.safetensors
│   └── 📂 diffusion_models/
│       └── ...               # We will guide you to install in the corresponding version workflow       
```

### [​](http://docs.comfy.org#hidream-i1-full-version-workflow) HiDream-I1 Full Version Workflow

#### [​](http://docs.comfy.org#1-model-file-download) 1. Model File Download

Please select the appropriate version based on your hardware. Click the link and download the corresponding model file to save it to the `ComfyUI/models/diffusion_models/` folder.

- FP8 version: [hidream\_i1\_full\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp8.safetensors?download=true) requires more than 16GB of VRAM
- Full version: [hidream\_i1\_full\_f16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp16.safetensors?download=true) requires more than 27GB of VRAM

#### [​](http://docs.comfy.org#2-workflow-file-download) 2. Workflow File Download

Please download the image below and drag it into ComfyUI to load the corresponding workflow

#### [​](http://docs.comfy.org#3-complete-the-workflow-step-by-step) 3. Complete the Workflow Step by Step

Complete the workflow execution step by step

1. Make sure the `Load Diffusion Model` node is using the `hidream_i1_full_fp8.safetensors` file
2. Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly
   
   - clip\_l\_hidream.safetensors
   - clip\_g\_hidream.safetensors
   - t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   - llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node is using the `ae.safetensors` file
4. For the **full** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `3.0`
5. For the `Ksampler` node, you need to make the following settings
   
   - Set `steps` to `50`
   - Set `cfg` to `5.0`
   - (Optional) Set `sampler` to `lcm`
   - (Optional) Set `scheduler` to `normal`
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

### [​](http://docs.comfy.org#hidream-i1-dev-version-workflow) HiDream-I1 Dev Version Workflow

#### [​](http://docs.comfy.org#1-model-file-download-2) 1. Model File Download

Please select the appropriate version based on your hardware, click the link and download the corresponding model file to save to the `ComfyUI/models/diffusion_models/` folder.

- FP8 version: [hidream\_i1\_dev\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_fp8.safetensors?download=true) requires more than 16GB of VRAM
- Full version: [hidream\_i1\_dev\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_bf16.safetensors?download=true) requires more than 27GB of VRAM

#### [​](http://docs.comfy.org#2-workflow-file-download-2) 2. Workflow File Download

Please download the image below and drag it into ComfyUI to load the corresponding workflow

#### [​](http://docs.comfy.org#3-complete-the-workflow-step-by-step-2) 3. Complete the Workflow Step by Step

Complete the workflow execution step by step

1. Make sure the `Load Diffusion Model` node is using the `hidream_i1_dev_fp8.safetensors` file
2. Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly
   
   - clip\_l\_hidream.safetensors
   - clip\_g\_hidream.safetensors
   - t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   - llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node is using the `ae.safetensors` file
4. For the **dev** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `6.0`
5. For the `Ksampler` node, you need to make the following settings
   
   - Set `steps` to `28`
   - (Important) Set `cfg` to `1.0`
   - (Optional) Set `sampler` to `lcm`
   - (Optional) Set `scheduler` to `normal`
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

### [​](http://docs.comfy.org#hidream-i1-fast-version-workflow) HiDream-I1 Fast Version Workflow

#### [​](http://docs.comfy.org#1-model-file-download-3) 1. Model File Download

Please select the appropriate version based on your hardware, click the link and download the corresponding model file to save to the `ComfyUI/models/diffusion_models/` folder.

- FP8 version: [hidream\_i1\_fast\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_fp8.safetensors?download=true) requires more than 16GB of VRAM
- Full version: [hidream\_i1\_fast\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_fp8.safetensors?download=true) requires more than 27GB of VRAM

#### [​](http://docs.comfy.org#2-workflow-file-download-3) 2. Workflow File Download

Please download the image below and drag it into ComfyUI to load the corresponding workflow

#### [​](http://docs.comfy.org#3-complete-the-workflow-step-by-step-3) 3. Complete the Workflow Step by Step

Complete the workflow execution step by step

1. Make sure the `Load Diffusion Model` node is using the `hidream_i1_fast_fp8.safetensors` file
2. Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly
   
   - clip\_l\_hidream.safetensors
   - clip\_g\_hidream.safetensors
   - t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   - llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node is using the `ae.safetensors` file
4. For the **fast** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `3.0`
5. For the `Ksampler` node, you need to make the following settings
   
   - Set `steps` to `16`
   - (Important) Set `cfg` to `1.0`
   - (Optional) Set `sampler` to `lcm`
   - (Optional) Set `scheduler` to `normal`
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## [​](http://docs.comfy.org#other-related-resources) Other Related Resources

### [​](http://docs.comfy.org#gguf-version-models) GGUF Version Models

- [HiDream-I1-Full-gguf](https://huggingface.co/city96/HiDream-I1-Full-gguf)
- [HiDream-I1-Dev-gguf](https://huggingface.co/city96/HiDream-I1-Dev-gguf)

You need to use the “Unet Loader (GGUF)” node in City96’s [ComfyUI-GGUF](https://github.com/city96/ComfyUI-GGUF) to replace the “Load Diffusion Model” node.

### [​](http://docs.comfy.org#nf4-version-models) NF4 Version Models

- [HiDream-I1-nf4](https://github.com/hykilpikonna/HiDream-I1-nf4)
- Use the [ComfyUI-HiDream-Sampler](https://github.com/SanDiegoDude/ComfyUI-HiDream-Sampler) node to use the NF4 version model.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/image/hidream/hidream-i1.mdx)

[Previous](http://docs.comfy.org/tutorials/flux/flux-1-controlnet)

[HiDream-e1This guide will help you understand and complete the ComfyUI native HiDream-I1 text-to-image workflow example  
\
Next](http://docs.comfy.org/tutorials/image/hidream/hidream-e1)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Model Features](http://docs.comfy.org#model-features)
- [About This Workflow Example](http://docs.comfy.org#about-this-workflow-example)
- [HiDream-I1 Workflow](http://docs.comfy.org#hidream-i1-workflow)
- [Model Installation](http://docs.comfy.org#model-installation)
- [HiDream-I1 Full Version Workflow](http://docs.comfy.org#hidream-i1-full-version-workflow)
- [1. Model File Download](http://docs.comfy.org#1-model-file-download)
- [2. Workflow File Download](http://docs.comfy.org#2-workflow-file-download)
- [3. Complete the Workflow Step by Step](http://docs.comfy.org#3-complete-the-workflow-step-by-step)
- [HiDream-I1 Dev Version Workflow](http://docs.comfy.org#hidream-i1-dev-version-workflow)
- [1. Model File Download](http://docs.comfy.org#1-model-file-download-2)
- [2. Workflow File Download](http://docs.comfy.org#2-workflow-file-download-2)
- [3. Complete the Workflow Step by Step](http://docs.comfy.org#3-complete-the-workflow-step-by-step-2)
- [HiDream-I1 Fast Version Workflow](http://docs.comfy.org#hidream-i1-fast-version-workflow)
- [1. Model File Download](http://docs.comfy.org#1-model-file-download-3)
- [2. Workflow File Download](http://docs.comfy.org#2-workflow-file-download-3)
- [3. Complete the Workflow Step by Step](http://docs.comfy.org#3-complete-the-workflow-step-by-step-3)
- [Other Related Resources](http://docs.comfy.org#other-related-resources)
- [GGUF Version Models](http://docs.comfy.org#gguf-version-models)
- [NF4 Version Models](http://docs.comfy.org#nf4-version-models)

<!-- END Built_In_Node/tutorials/image/hidream/hidream-i1.md -->


<!-- BEGIN Built_In_Node/tutorials/video/hunyuan-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
  
  - [LTX-Video](http://docs.comfy.org/tutorials/video/ltxv)
  - [Hunyuan Video](http://docs.comfy.org/tutorials/video/hunyuan-video)
  - Wan Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Hunyuan Video Examples

# ComfyUI Hunyuan Video Examples

This guide shows how to use Hunyuan Text-to-Video and Image-to-Video workflows in ComfyUI

Hunyuan Video series is developed and open-sourced by [Tencent](https://huggingface.co/tencent), featuring a hybrid architecture that supports both [Text-to-Video](https://github.com/Tencent/HunyuanVideo) and [Image-to-Video](https://github.com/Tencent/HunyuanVideo-I2V) generation with a parameter scale of 13B.

Technical features:

- **Core Architecture:** Uses a DiT (Diffusion Transformer) architecture similar to Sora, effectively fusing text, image, and motion information to improve consistency, quality, and alignment between generated video frames. A unified full-attention mechanism enables multi-view camera transitions while ensuring subject consistency.
- **3D VAE:** The custom 3D VAE compresses videos into a compact latent space, making image-to-video generation more efficient.
- **Superior Image-Video-Text Alignment:** Utilizing MLLM text encoders that excel in both image and video generation, better following text instructions, capturing details, and performing complex reasoning.

You can learn more through the official repositories: [Hunyuan Video](https://github.com/Tencent/HunyuanVideo) and [Hunyuan Video-I2V](https://github.com/Tencent/HunyuanVideo-I2V).

This guide will walk you through setting up both **Text-to-Video** and **Image-to-Video** workflows in ComfyUI.

The workflow images in this tutorial contain metadata with model download information.

Simply drag them into ComfyUI or use the menu `Workflows` -&gt; `Open (ctrl+o)` to load the corresponding workflow, which will prompt you to download the required models.

Alternatively, this guide provides direct model links if automatic downloads fail or you are not using the Desktop version. All models are available [here](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/tree/main/split_files) for download.

## [​](http://docs.comfy.org#shared-models-for-all-workflows) Shared Models for All Workflows

The following models are used in both Text-to-Video and Image-to-Video workflows. Please download and save them to the specified directories:

- [clip\_l.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/clip_l.safetensors?download=true)
- [llava\_llama3\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/llava_llama3_fp8_scaled.safetensors?download=true)
- [hunyuan\_video\_vae\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/vae/hunyuan_video_vae_bf16.safetensors?download=true)

Storage location:

```plaintext
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── llava_llama3_fp8_scaled.safetensors
│   ├── vae/
│   │   └── hunyuan_video_vae_bf16.safetensors
```

## [​](http://docs.comfy.org#hunyuan-text-to-video-workflow) Hunyuan Text-to-Video Workflow

Hunyuan Text-to-Video was open-sourced in December 2024, supporting 5-second short video generation through natural language descriptions in both Chinese and English.

### [​](http://docs.comfy.org#1-workflow) 1. Workflow

Download the image below and drag it into ComfyUI to load the workflow:

### [​](http://docs.comfy.org#2-manual-models-installation) 2. Manual Models Installation

Download [hunyuan\_video\_t2v\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_t2v_720p_bf16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models` folder.

Ensure you have all these model files in the correct locations:

```plaintext
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors                       // Shared model
│   │   └── llava_llama3_fp8_scaled.safetensors      // Shared model
│   ├── vae/
│   │   └── hunyuan_video_vae_bf16.safetensors       // Shared model
│   └── diffusion_models/
│       └── hunyuan_video_t2v_720p_bf16.safetensors  // T2V model
```

### [​](http://docs.comfy.org#3-steps-to-run-the-workflow) 3. Steps to Run the Workflow

1. Ensure the `DualCLIPLoader` node has loaded these models:
   
   - clip\_name1: clip\_l.safetensors
   - clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. Ensure the `Load Diffusion Model` node has loaded `hunyuan_video_t2v_720p_bf16.safetensors`
3. Ensure the `Load VAE` node has loaded `hunyuan_video_vae_bf16.safetensors`
4. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

When the `length` parameter in the `EmptyHunyuanLatentVideo` node is set to 1, the model can generate a static image.

## [​](http://docs.comfy.org#hunyuan-image-to-video-workflow) Hunyuan Image-to-Video Workflow

Hunyuan Image-to-Video model was open-sourced on March 6, 2025, based on the HunyuanVideo framework. It transforms static images into smooth, high-quality videos and also provides LoRA training code to customize special video effects like hair growth, object transformation, etc.

Currently, the Hunyuan Image-to-Video model has two versions:

- v1 “concat”: Better motion fluidity but less adherence to the image guidance
- v2 “replace”: Updated the day after v1, with better image guidance but seemingly less dynamic compared to v1

v1 “concat”

v2 “replace”

### [​](http://docs.comfy.org#shared-model-for-v1-and-v2-versions) Shared Model for v1 and v2 Versions

Download the following file and save it to the `ComfyUI/models/clip_vision` directory:

- [llava\_llama3\_vision.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/clip_vision/llava_llama3_vision.safetensors?download=true)

### [​](http://docs.comfy.org#v1-%E2%80%9Cconcat%E2%80%9D-image-to-video-workflow) V1 “concat” Image-to-Video Workflow

#### [​](http://docs.comfy.org#1-workflow-and-asset) 1. Workflow and Asset

Download the workflow image below and drag it into ComfyUI to load the workflow:

Download the image below, which we’ll use as the starting frame for the image-to-video generation:

#### [​](http://docs.comfy.org#2-related-models-manual-installation) 2. Related models manual installation

- [hunyuan\_video\_image\_to\_video\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_image_to_video_720p_bf16.safetensors?download=true)

Ensure you have all these model files in the correct locations:

```plaintext
ComfyUI/
├── models/
│   ├── clip_vision/
│   │   └── llava_llama3_vision.safetensors                     // I2V shared model
│   ├── text_encoders/
│   │   ├── clip_l.safetensors                                  // Shared model
│   │   └── llava_llama3_fp8_scaled.safetensors                 // Shared model
│   ├── vae/
│   │   └── hunyuan_video_vae_bf16.safetensors                  // Shared model
│   └── diffusion_models/
│       └── hunyuan_video_image_to_video_720p_bf16.safetensors  // I2V v1 "concat" version model
```

#### [​](http://docs.comfy.org#3-steps-to-run-the-workflow-2) 3. Steps to Run the Workflow

1. Ensure that `DualCLIPLoader` has loaded these models:
   
   - clip\_name1: clip\_l.safetensors
   - clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. Ensure that `Load CLIP Vision` has loaded `llava_llama3_vision.safetensors`
3. Ensure that `Load Image Model` has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`
4. Ensure that `Load VAE` has loaded `vae_name: hunyuan_video_vae_bf16.safetensors`
5. Ensure that `Load Diffusion Model` has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`
6. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

### [​](http://docs.comfy.org#v2-%E2%80%9Creplace%E2%80%9D-image-to-video-workflow) v2 “replace” Image-to-Video Workflow

The v2 workflow is essentially the same as the v1 workflow. You just need to download the **replace** model and use it in the `Load Diffusion Model` node.

#### [​](http://docs.comfy.org#1-workflow-and-asset-2) 1. Workflow and Asset

Download the workflow image below and drag it into ComfyUI to load the workflow:

Download the image below, which we’ll use as the starting frame for the image-to-video generation:

#### [​](http://docs.comfy.org#2-related-models-manual-installation-2) 2. Related models manual installation

- [hunyuan\_video\_v2\_replace\_image\_to\_video\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors?download=true)

Ensure you have all these model files in the correct locations:

```plaintext
ComfyUI/
├── models/
│   ├── clip_vision/
│   │   └── llava_llama3_vision.safetensors                                // I2V shared model
│   ├── text_encoders/
│   │   ├── clip_l.safetensors                                             // Shared model
│   │   └── llava_llama3_fp8_scaled.safetensors                            // Shared model
│   ├── vae/
│   │   └── hunyuan_video_vae_bf16.safetensors                             // Shared model
│   └── diffusion_models/
│       └── hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors  // V2 "replace" version model
```

#### [​](http://docs.comfy.org#3-steps-to-run-the-workflow-3) 3. Steps to Run the Workflow

1. Ensure the `DualCLIPLoader` node has loaded these models:
   
   - clip\_name1: clip\_l.safetensors
   - clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. Ensure the `Load CLIP Vision` node has loaded `llava_llama3_vision.safetensors`
3. Ensure the `Load Image Model` node has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`
4. Ensure the `Load VAE` node has loaded `hunyuan_video_vae_bf16.safetensors`
5. Ensure the `Load Diffusion Model` node has loaded `hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors`
6. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## [​](http://docs.comfy.org#try-it-yourself) Try it yourself

Here are some images and prompts we provide. Based on that content or make an adjustment to create your own video.

```plaintext
Futuristic robot dancing ballet, dynamic motion, fast motion, fast shot, moving scene
```

* * *

```plaintext
Samurai waving sword and hitting the camera. camera angle movement, zoom in, fast scene, super fast, dynamic
```

* * *

```plaintext
flying car fastly moving and flying through the city
```

* * *

```plaintext
cyberpunk car race in night city, dynamic, super fast, fast shot
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/video/hunyuan-video.mdx)

[Previous](http://docs.comfy.org/tutorials/video/ltxv)

[Wan VideoThis guide demonstrates how to generate videos with first and last frames using Wan2.1 Video in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/video/wan/wan-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Shared Models for All Workflows](http://docs.comfy.org#shared-models-for-all-workflows)
- [Hunyuan Text-to-Video Workflow](http://docs.comfy.org#hunyuan-text-to-video-workflow)
- [1. Workflow](http://docs.comfy.org#1-workflow)
- [2. Manual Models Installation](http://docs.comfy.org#2-manual-models-installation)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow)
- [Hunyuan Image-to-Video Workflow](http://docs.comfy.org#hunyuan-image-to-video-workflow)
- [Shared Model for v1 and v2 Versions](http://docs.comfy.org#shared-model-for-v1-and-v2-versions)
- [V1 “concat” Image-to-Video Workflow](http://docs.comfy.org#v1-%E2%80%9Cconcat%E2%80%9D-image-to-video-workflow)
- [1. Workflow and Asset](http://docs.comfy.org#1-workflow-and-asset)
- [2. Related models manual installation](http://docs.comfy.org#2-related-models-manual-installation)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow-2)
- [v2 “replace” Image-to-Video Workflow](http://docs.comfy.org#v2-%E2%80%9Creplace%E2%80%9D-image-to-video-workflow)
- [1. Workflow and Asset](http://docs.comfy.org#1-workflow-and-asset-2)
- [2. Related models manual installation](http://docs.comfy.org#2-related-models-manual-installation-2)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow-3)
- [Try it yourself](http://docs.comfy.org#try-it-yourself)

<!-- END Built_In_Node/tutorials/video/hunyuan-video.md -->


<!-- BEGIN Built_In_Node/tutorials/video/ltxv.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
  
  - [LTX-Video](http://docs.comfy.org/tutorials/video/ltxv)
  - [Hunyuan Video](http://docs.comfy.org/tutorials/video/hunyuan-video)
  - Wan Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

LTX-Video

# LTX-Video

[LTX-Video](https://huggingface.co/Lightricks/LTX-Video) is a very efficient video model by lightricks. The important thing with this model is to give it long descriptive prompts.

## [​](http://docs.comfy.org#multi-frame-control) Multi Frame Control

Allows you to control the video with a series of images. You can download the input images: [starting frame](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house1.png) and [ending frame](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house2.png).

Drag the video directly into ComfyUI to run the workflow.

## [​](http://docs.comfy.org#image-to-video) Image to Video

Allows you to control the video with a first [frame image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/i2v/girl1.png).

Drag the video directly into ComfyUI to run the workflow.

## [​](http://docs.comfy.org#text-to-video) Text to Video

Drag the video directly into ComfyUI to run the workflow.

## [​](http://docs.comfy.org#requirements) Requirements

Download the following models and place them in the locations specified below:

- [ltx-video-2b-v0.9.5.safetensors](https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltx-video-2b-v0.9.5.safetensors?download=true)
- [t5xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/mochi_preview_repackaged/resolve/main/split_files/text_encoders/t5xxl_fp16.safetensors?download=true)

```plaintext
├── checkpoints/
│   └── ltx-video-2b-v0.9.5.safetensors
└── text_encoders/
    └── t5xxl_fp16.safetensors
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/video/ltxv.mdx)

[Previous](http://docs.comfy.org/tutorials/3d/hunyuan3D-2)

[Hunyuan VideoThis guide shows how to use Hunyuan Text-to-Video and Image-to-Video workflows in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/video/hunyuan-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Multi Frame Control](http://docs.comfy.org#multi-frame-control)
- [Image to Video](http://docs.comfy.org#image-to-video)
- [Text to Video](http://docs.comfy.org#text-to-video)
- [Requirements](http://docs.comfy.org#requirements)

<!-- END Built_In_Node/tutorials/video/ltxv.md -->


<!-- BEGIN Built_In_Node/tutorials/video/wan/fun-control.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
  
  - [LTX-Video](http://docs.comfy.org/tutorials/video/ltxv)
  - [Hunyuan Video](http://docs.comfy.org/tutorials/video/hunyuan-video)
  - Wan Video
    
    - [Wan Video](http://docs.comfy.org/tutorials/video/wan/wan-video)
    - [Wan2.1 Fun Control](http://docs.comfy.org/tutorials/video/wan/fun-control)
    - [Wan2.1 Fun InP](http://docs.comfy.org/tutorials/video/wan/fun-inp)
    - [First-Last Frame](http://docs.comfy.org/tutorials/video/wan/wan-flf)
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Wan2.1 Fun Control Video Examples

# ComfyUI Wan2.1 Fun Control Video Examples

This guide demonstrates how to use Wan2.1 Fun Control in ComfyUI to generate videos with control videos

## [​](http://docs.comfy.org#about-wan2-1-fun-control) About Wan2.1-Fun-Control

**Wan2.1-Fun-Control** is an open-source video generation and control project developed by Alibaba team. It introduces innovative Control Codes mechanisms combined with deep learning and multimodal conditional inputs to generate high-quality videos that conform to preset control conditions. The project focuses on precisely guiding generated video content through multimodal control conditions.

Currently, the Fun Control model supports various control conditions, including **Canny (line art), Depth, OpenPose (human posture), MLSD (geometric edges), and trajectory control.** The model also supports multi-resolution video prediction with options for 512, 768, and 1024 resolutions at 16 frames per second, generating videos up to 81 frames (approximately 5 seconds) in length.

Model versions:

- **1.3B** Lightweight: Suitable for local deployment and quick inference with **lower VRAM requirements**
- **14B** High-performance: Model size reaches 32GB+, offering better results but **requiring higher VRAM**

Here are the relevant code repositories:

- [Wan2.1-Fun-1.3B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Control)
- [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control)
- Code repository: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)

ComfyUI now **natively supports** the Wan2.1 Fun Control model. Before starting this tutorial, please update your ComfyUI to ensure you’re using a version after [this commit](https://github.com/comfyanonymous/ComfyUI/commit/3661c833bcc41b788a7c9f0e7bc48524f8ee5f82).

In this guide, we’ll provide two workflows:

1. A workflow using only native Comfy Core nodes
2. A workflow using custom nodes

Due to current limitations in native nodes for video support, the native-only workflow ensures users can complete the process without installing custom nodes. However, we’ve found that providing a good user experience for video generation is challenging without custom nodes, so we’re providing both workflow versions in this guide.

## [​](http://docs.comfy.org#model-installation) Model Installation

You only need to install these models once. The workflow images also contain model download information, so you can choose your preferred download method.

The following models can be found at [Wan\_2.1\_ComfyUI\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) and [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334)

Click the corresponding links to download. If you’ve used Wan-related workflows before, you only need to download the **Diffusion models**.

**Diffusion models** - choose 1.3B or 14B. The 14B version has a larger file size (32GB) and higher VRAM requirements:

- [wan2.1\_fun\_control\_1.3B\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_control_1.3B_bf16.safetensors?download=true)
- [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control/blob/main/diffusion_pytorch_model.safetensors?download=true): Rename to `Wan2.1-Fun-14B-Control.safetensors` after downloading

**Text encoders** - choose one of the following models (fp16 precision has a larger size and higher performance requirements):

- [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
- [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

- [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

- [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

File storage location:

```plaintext
📂 ComfyUI/
├── 📂 models/
│   ├── 📂 diffusion_models/
│   │   └── wan2.1_fun_control_1.3B_bf16.safetensors
│   ├── 📂 text_encoders/
│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors
│   └── 📂 vae/
│   │   └── wan_2.1_vae.safetensors
│   └── 📂 clip_vision/
│       └──  clip_vision_h.safetensors                 
```

## [​](http://docs.comfy.org#comfyui-native-workflow) ComfyUI Native Workflow

In this workflow, we use videos converted to **WebP format** since the `Load Image` node doesn’t currently support mp4 format. We also use **Canny Edge** to preprocess the original video. Because many users encounter installation failures and environment issues when installing custom nodes, this version of the workflow uses only native nodes to ensure a smoother experience.

Thanks to our powerful ComfyUI authors who provide feature-rich nodes. If you want to directly check the related version, see [Workflow Using Custom Nodes](http://docs.comfy.org/_sites/docs.comfy.org/tutorials/video/wan/fun-control#workflow-using-custom-nodes).

### [​](http://docs.comfy.org#1-workflow-file-download) 1. Workflow File Download

#### [​](http://docs.comfy.org#1-1-workflow-file) 1.1 Workflow File

Download the image below and drag it into ComfyUI to load the workflow:

#### [​](http://docs.comfy.org#1-2-input-images-and-videos-download) 1.2 Input Images and Videos Download

Please download the following image and video for input:

### [​](http://docs.comfy.org#2-complete-the-workflow-step-by-step) 2. Complete the Workflow Step by Step

1. Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_control_1.3B_bf16.safetensors`
2. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
4. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
5. Upload the starting frame to the `Load Image` node (renamed to `Start_image`)
6. Upload the control video to the second `Load Image` node. Note: This node currently doesn’t support mp4, only WebP videos
7. (Optional) Modify the prompt (both English and Chinese are supported)
8. (Optional) Adjust the video size in `WanFunControlToVideo`, avoiding overly large dimensions
9. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

### [​](http://docs.comfy.org#3-usage-notes) 3. Usage Notes

- Since we need to input the same number of frames as the control video into the `WanFunControlToVideo` node, if the specified frame count exceeds the actual control video frames, the excess frames may display scenes not conforming to control conditions. We’ll address this issue in the [Workflow Using Custom Nodes](http://docs.comfy.org/_sites/docs.comfy.org/tutorials/video/wan/fun-control#workflow-using-custom-nodes)
- Avoid setting overly large dimensions, as this can make the sampling process very time-consuming. Try generating smaller images first, then upscale
- Use your imagination to build upon this workflow by adding text-to-image or other types of workflows to achieve direct text-to-video generation or style transfer
- Use tools like [ComfyUI-comfyui\_controlnet\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) for richer control options

## [​](http://docs.comfy.org#workflow-using-custom-nodes) Workflow Using Custom Nodes

We’ll need to install the following two custom nodes:

- [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)
- [ComfyUI-comfyui\_controlnet\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

You can use [ComfyUI Manager](https://github.com/Comfy-Org/ComfyUI-Manager) to install missing nodes or follow the installation instructions for each custom node package.

### [​](http://docs.comfy.org#1-workflow-file-download-2) 1. Workflow File Download

#### [​](http://docs.comfy.org#1-1-workflow-file-2) 1.1 Workflow File

Download the image below and drag it into ComfyUI to load the workflow:

Due to the large size of video files, you can also click [here](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_use_custom_nodes.json) to download the workflow file in JSON format.

#### [​](http://docs.comfy.org#1-2-input-images-and-videos-download-2) 1.2 Input Images and Videos Download

Please download the following image and video for input:

### [​](http://docs.comfy.org#2-complete-the-workflow-step-by-step-2) 2. Complete the Workflow Step by Step

> The model part is essentially the same. If you’ve already experienced the native-only workflow, you can directly upload the corresponding images and run it.

01. Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_control_1.3B_bf16.safetensors`
02. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
03. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
04. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
05. Upload the starting frame to the `Load Image` node
06. Upload an mp4 format video to the `Load Video(Upload)` custom node. Note that the workflow has adjusted the default `frame_load_cap`
07. For the current image, the `DWPose Estimator` only uses the `detect_face` option
08. (Optional) Modify the prompt (both English and Chinese are supported)
09. (Optional) Adjust the video size in `WanFunControlToVideo`, avoiding overly large dimensions
10. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

### [​](http://docs.comfy.org#3-workflow-notes) 3. Workflow Notes

Thanks to the ComfyUI community authors for their custom node packages:

- This example uses `Load Video(Upload)` to support mp4 videos
- The `video_info` obtained from `Load Video(Upload)` allows us to maintain the same `fps` for the output video
- You can replace `DWPose Estimator` with other preprocessors from the `ComfyUI-comfyui_controlnet_aux` node package
- Prompts support multiple languages

## [​](http://docs.comfy.org#usage-tips) Usage Tips

- A useful tip is that you can combine multiple image preprocessing techniques and then use the `Image Blend` node to achieve the goal of applying multiple control methods simultaneously.
- You can use the `Video Combine` node from `ComfyUI-VideoHelperSuite` to save videos in mp4 format
- We use `SaveAnimatedWEBP` because we currently don’t support embedding workflow into **mp4** and some other custom nodes may not support embedding workflow too. To preserve the workflow in the video, we choose `SaveAnimatedWEBP` node.
- In the `WanFunControlToVideo` node, `control_video` is not mandatory, so sometimes you can skip using a control video, first generate a very small video size like 320x320, and then use them as control video input to achieve consistent results.
- [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)
- [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/video/wan/fun-control.mdx)

[Previous](http://docs.comfy.org/tutorials/video/wan/wan-video)

[Wan2.1 Fun InPThis guide demonstrates how to use Wan2.1 Fun InP in ComfyUI to generate videos with first and last frame control  
\
Next](http://docs.comfy.org/tutorials/video/wan/fun-inp)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [About Wan2.1-Fun-Control](http://docs.comfy.org#about-wan2-1-fun-control)
- [Model Installation](http://docs.comfy.org#model-installation)
- [ComfyUI Native Workflow](http://docs.comfy.org#comfyui-native-workflow)
- [1. Workflow File Download](http://docs.comfy.org#1-workflow-file-download)
- [1.1 Workflow File](http://docs.comfy.org#1-1-workflow-file)
- [1.2 Input Images and Videos Download](http://docs.comfy.org#1-2-input-images-and-videos-download)
- [2. Complete the Workflow Step by Step](http://docs.comfy.org#2-complete-the-workflow-step-by-step)
- [3. Usage Notes](http://docs.comfy.org#3-usage-notes)
- [Workflow Using Custom Nodes](http://docs.comfy.org#workflow-using-custom-nodes)
- [1. Workflow File Download](http://docs.comfy.org#1-workflow-file-download-2)
- [1.1 Workflow File](http://docs.comfy.org#1-1-workflow-file-2)
- [1.2 Input Images and Videos Download](http://docs.comfy.org#1-2-input-images-and-videos-download-2)
- [2. Complete the Workflow Step by Step](http://docs.comfy.org#2-complete-the-workflow-step-by-step-2)
- [3. Workflow Notes](http://docs.comfy.org#3-workflow-notes)
- [Usage Tips](http://docs.comfy.org#usage-tips)

<!-- END Built_In_Node/tutorials/video/wan/fun-control.md -->


<!-- BEGIN Built_In_Node/tutorials/video/wan/fun-inp.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
  
  - [LTX-Video](http://docs.comfy.org/tutorials/video/ltxv)
  - [Hunyuan Video](http://docs.comfy.org/tutorials/video/hunyuan-video)
  - Wan Video
    
    - [Wan Video](http://docs.comfy.org/tutorials/video/wan/wan-video)
    - [Wan2.1 Fun Control](http://docs.comfy.org/tutorials/video/wan/fun-control)
    - [Wan2.1 Fun InP](http://docs.comfy.org/tutorials/video/wan/fun-inp)
    - [First-Last Frame](http://docs.comfy.org/tutorials/video/wan/wan-flf)
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Wan2.1 Fun InP Video Examples

# ComfyUI Wan2.1 Fun InP Video Examples

This guide demonstrates how to use Wan2.1 Fun InP in ComfyUI to generate videos with first and last frame control

## [​](http://docs.comfy.org#about-wan2-1-fun-inp) About Wan2.1-Fun-InP

**Wan-Fun InP** is an open-source video generation model released by Alibaba, part of the Wan2.1-Fun series, focusing on generating videos from images with first and last frame control.

**Key features**:

- **First and last frame control**: Supports inputting both first and last frame images to generate transitional video between them, enhancing video coherence and creative freedom. Compared to earlier community versions, Alibaba’s official model produces more stable and significantly higher quality results.
- **Multi-resolution support**: Supports generating videos at 512×512, 768×768, 1024×1024 and other resolutions to accommodate different scenario requirements.

**Model versions**:

- **1.3B** Lightweight: Suitable for local deployment and quick inference with **lower VRAM requirements**
- **14B** High-performance: Model size reaches 32GB+, offering better results but requiring **higher VRAM**

Below are the relevant model weights and code repositories:

- [Wan2.1-Fun-1.3B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Input)
- [Wan2.1-Fun-14B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Input)
- Code repository: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)

Currently, ComfyUI natively supports the Wan2.1 Fun InP model. Before starting this tutorial, please update your ComfyUI to ensure your version is after [this commit](https://github.com/comfyanonymous/ComfyUI/commit/0a1f8869c9998bbfcfeb2e97aa96a6d3e0a2b5df).

## [​](http://docs.comfy.org#wan2-1-fun-inp-workflow) Wan2.1 Fun InP Workflow

Download the image below and drag it into ComfyUI to load the workflow:

### [​](http://docs.comfy.org#1-workflow-file-download) 1. Workflow File Download

### [​](http://docs.comfy.org#2-manual-model-installation) 2. Manual Model Installation

If automatic model downloading is ineffective, please download the models manually and save them to the corresponding folders.

The following models can be found at [Wan\_2.1\_ComfyUI\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) and [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334)

**Diffusion models** - choose 1.3B or 14B. The 14B version has a larger file size (32GB) and higher VRAM requirements:

- [wan2.1\_fun\_inp\_1.3B\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_inp_1.3B_bf16.safetensors?download=true)
- [Wan2.1-Fun-14B-InP](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-InP/resolve/main/diffusion_pytorch_model.safetensors?download=true): Rename to `Wan2.1-Fun-14B-InP.safetensors` after downloading

**Text encoders** - choose one of the following models (fp16 precision has a larger size and higher performance requirements):

- [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
- [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

- [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

- [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

File storage location:

```plaintext
📂 ComfyUI/
├── 📂 models/
│   ├── 📂 diffusion_models/
│   │   └── wan2.1_fun_inp_1.3B_bf16.safetensors
│   ├── 📂 text_encoders/
│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors
│   └── 📂 vae/
│   │   └── wan_2.1_vae.safetensors
│   └── 📂 clip_vision/
│       └──  clip_vision_h.safetensors                 
```

### [​](http://docs.comfy.org#3-complete-the-workflow-step-by-step) 3. Complete the Workflow Step by Step

1. Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_inp_1.3B_bf16.safetensors`
2. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
4. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
5. Upload the starting frame to the `Load Image` node (renamed to `Start_image`)
6. Upload the ending frame to the second `Load Image` node
7. (Optional) Modify the prompt (both English and Chinese are supported)
8. (Optional) Adjust the video size in `WanFunInpaintToVideo`, avoiding overly large dimensions
9. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

### [​](http://docs.comfy.org#4-workflow-notes) 4. Workflow Notes

Please make sure to use the correct model, as `wan2.1_fun_inp_1.3B_bf16.safetensors` and `wan2.1_fun_control_1.3B_bf16.safetensors` are stored in the same folder and have very similar names. Ensure you’re using the right model.

- When using Wan Fun InP, you may need to frequently modify prompts to ensure the accuracy of the corresponding scene transitions.

## [​](http://docs.comfy.org#other-wan2-1-fun-inp-or-video-related-custom-node-packages) Other Wan2.1 Fun InP or video-related custom node packages

- [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)
- [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)
- [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/video/wan/fun-inp.mdx)

[Previous](http://docs.comfy.org/tutorials/video/wan/fun-control)

[First-Last FrameThis guide explains how to complete Wan2.1 FLF2V video generation examples in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/video/wan/wan-flf)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [About Wan2.1-Fun-InP](http://docs.comfy.org#about-wan2-1-fun-inp)
- [Wan2.1 Fun InP Workflow](http://docs.comfy.org#wan2-1-fun-inp-workflow)
- [1. Workflow File Download](http://docs.comfy.org#1-workflow-file-download)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation)
- [3. Complete the Workflow Step by Step](http://docs.comfy.org#3-complete-the-workflow-step-by-step)
- [4. Workflow Notes](http://docs.comfy.org#4-workflow-notes)
- [Other Wan2.1 Fun InP or video-related custom node packages](http://docs.comfy.org#other-wan2-1-fun-inp-or-video-related-custom-node-packages)

<!-- END Built_In_Node/tutorials/video/wan/fun-inp.md -->


<!-- BEGIN Built_In_Node/tutorials/video/wan/wan-flf.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
  
  - [LTX-Video](http://docs.comfy.org/tutorials/video/ltxv)
  - [Hunyuan Video](http://docs.comfy.org/tutorials/video/hunyuan-video)
  - Wan Video
    
    - [Wan Video](http://docs.comfy.org/tutorials/video/wan/wan-video)
    - [Wan2.1 Fun Control](http://docs.comfy.org/tutorials/video/wan/fun-control)
    - [Wan2.1 Fun InP](http://docs.comfy.org/tutorials/video/wan/fun-inp)
    - [First-Last Frame](http://docs.comfy.org/tutorials/video/wan/wan-flf)
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Wan2.1 FLF2V Native Example

# ComfyUI Wan2.1 FLF2V Native Example

This guide explains how to complete Wan2.1 FLF2V video generation examples in ComfyUI

Wan FLF2V (First-Last Frame Video Generation) is an open-source video generation model developed by the Alibaba Tongyi Wanxiang team. Its open-source license is [Apache 2.0](https://github.com/Wan-Video/Wan2.1?tab=Apache-2.0-1-ov-file). Users only need to provide two images as the starting and ending frames, and the model automatically generates intermediate transition frames, outputting a logically coherent and naturally flowing 720p high-definition video.

**Core Technical Highlights**

1. **Precise First-Last Frame Control**: The matching rate of first and last frames reaches 98%, defining video boundaries through starting and ending scenes, intelligently filling intermediate dynamic changes to achieve scene transitions and object morphing effects.
2. **Stable and Smooth Video Generation**: Using CLIP semantic features and cross-attention mechanisms, the video jitter rate is reduced by 37% compared to similar models, ensuring natural and smooth transitions.
3. **Multi-functional Creative Capabilities**: Supports dynamic embedding of Chinese and English subtitles, generation of anime/realistic/fantasy and other styles, adapting to different creative needs.
4. **720p HD Output**: Directly generates 1280×720 resolution videos without post-processing, suitable for social media and commercial applications.
5. **Open-source Ecosystem Support**: Model weights, code, and training framework are fully open-sourced, supporting deployment on mainstream AI platforms.

**Technical Principles and Architecture**

1. **DiT Architecture**: Based on diffusion models and Diffusion Transformer architecture, combined with Full Attention mechanism to optimize spatiotemporal dependency modeling, ensuring video coherence.
2. **3D Causal Variational Encoder**: Wan-VAE technology compresses HD frames to 1/128 size while retaining subtle dynamic details, significantly reducing memory requirements.
3. **Three-stage Training Strategy**: Starting from 480P resolution pre-training, gradually upgrading to 720P, balancing generation quality and computational efficiency through phased optimization.

**Related Links**

- **GitHub Repository**: [GitHub](https://github.com/Wan-Video/Wan2.1)
- **Hugging Face Model Page**: [Hugging Face](https://huggingface.co/Wan-AI/Wan2.1-FLF2V-14B-720P)
- **ModelScope Community**: [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-FLF2V-14B-720P)

## [​](http://docs.comfy.org#wan2-1-flf2v-720p-comfyui-native-workflow-example) Wan2.1 FLF2V 720P ComfyUI Native Workflow Example

### [​](http://docs.comfy.org#1-download-workflow-files-and-related-input-files) 1. Download Workflow Files and Related Input Files

Since this model is trained on high-resolution images, using smaller sizes may not yield good results. In the example, we use a size of 720 * 1280, which may cause users with lower VRAM hard to run smoothly and will take a long time to generate. If needed, please modify the video generation size at the beginning.

Please download the WebP file below, and drag it into ComfyUI to load the corresponding workflow. The workflow has embedded the corresponding model download file information.

Please download the two images below, which we will use as the starting and ending frames of the video

### [​](http://docs.comfy.org#2-manual-model-installation) 2. Manual Model Installation

If corresponding

All models involved in this guide can be found [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files).

**diffusion\_models** Choose one version based on your hardware conditions

- FP16:[wan2.1\_flf2v\_720p\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_flf2v_720p_14B_fp16.safetensors?download=true)
- FP8:[wan2.1\_flf2v\_720p\_14B\_fp8\_e4m3fn.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/blob/main/split_files/diffusion_models/wan2.1_flf2v_720p_14B_fp8_e4m3fn.safetensors)

If you have previously tried Wan Video related workflows, you may already have the following files.

Choose one version from **Text encoders** for download,

- [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
- [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

- [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

- [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

File Storage Location

```plaintext
ComfyUI/
├── models/
│   ├── diffusion_models/
│   │   └─── wan2.1_flf2v_720p_14B_fp16.safetensors           # or FP8 version
│   ├── text_encoders/
│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors           # or your chosen version
│   ├── vae/
│   │   └──  wan_2.1_vae.safetensors
│   └── clip_vision/
│       └──  clip_vision_h.safetensors   
```

### [​](http://docs.comfy.org#3-complete-workflow-execution-step-by-step) 3. Complete Workflow Execution Step by Step

1. Ensure the `Load Diffusion Model` node has loaded `wan2.1_flf2v_720p_14B_fp16.safetensors` or `wan2.1_flf2v_720p_14B_fp8_e4m3fn.safetensors`
2. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
4. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
5. Upload the starting frame to the `Start_image` node
6. Upload the ending frame to the `End_image` node
7. (Optional) Modify the positive and negative prompts, both Chinese and English are supported
8. (**Important**) In `WanFirstLastFrameToVideo`, modify the corresponding video size. We default to using a size of 720 * 1280 to achieve better results for the generated video, but this may cause issues running smoothly on lower memory. You can initially try adjusting it to a size such as 480 * 854 to ensure smooth operation, and then adjust it back to 720 * 1280 when you need to generate larger-sized videos to ensure quality results.
9. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/video/wan/wan-flf.mdx)

[Previous](http://docs.comfy.org/tutorials/video/wan/fun-inp)

[ACE-Step Music GenerationThis guide will help you create dynamic music using the ACE-Step model in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/audio/ace-step/ace-step-v1)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Wan2.1 FLF2V 720P ComfyUI Native Workflow Example](http://docs.comfy.org#wan2-1-flf2v-720p-comfyui-native-workflow-example)
- [1. Download Workflow Files and Related Input Files](http://docs.comfy.org#1-download-workflow-files-and-related-input-files)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation)
- [3. Complete Workflow Execution Step by Step](http://docs.comfy.org#3-complete-workflow-execution-step-by-step)

<!-- END Built_In_Node/tutorials/video/wan/wan-flf.md -->


<!-- BEGIN Built_In_Node/tutorials/video/wan/wan-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
  
  - [LTX-Video](http://docs.comfy.org/tutorials/video/ltxv)
  - [Hunyuan Video](http://docs.comfy.org/tutorials/video/hunyuan-video)
  - Wan Video
    
    - [Wan Video](http://docs.comfy.org/tutorials/video/wan/wan-video)
    - [Wan2.1 Fun Control](http://docs.comfy.org/tutorials/video/wan/fun-control)
    - [Wan2.1 Fun InP](http://docs.comfy.org/tutorials/video/wan/fun-inp)
    - [First-Last Frame](http://docs.comfy.org/tutorials/video/wan/wan-flf)
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Wan2.1 Video Examples

# ComfyUI Wan2.1 Video Examples

This guide demonstrates how to generate videos with first and last frames using Wan2.1 Video in ComfyUI

Wan2.1 Video series is a video generation model open-sourced by Alibaba in February 2025 under the [Apache 2.0 license](https://github.com/Wan-Video/Wan2.1?tab=Apache-2.0-1-ov-file). It offers two versions:

- 14B (14 billion parameters)
- 1.3B (1.3 billion parameters) Covering multiple tasks including text-to-video (T2V) and image-to-video (I2V). The model not only outperforms existing open-source models in performance but more importantly, its lightweight version requires only 8GB of VRAM to run, significantly lowering the barrier to entry.

<!--THE END-->

- [Wan2.1 Code Repository](https://github.com/Wan-Video/Wan2.1)
- [Wan2.1 Model Repository](https://huggingface.co/Wan-AI)

## [​](http://docs.comfy.org#wan2-1-comfyui-native-workflow-examples) Wan2.1 ComfyUI Native Workflow Examples

Please update ComfyUI to the latest version before starting the examples to make sure you have native Wan Video support.

## [​](http://docs.comfy.org#model-installation) Model Installation

All models mentioned in this guide can be found [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files). Below are the common models you’ll need for the examples in this guide, which you can download in advance:

Choose one version from **Text encoders** to download:

- [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
- [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

- [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

- [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

File storage locations:

```plaintext
ComfyUI/
├── models/
│   ├── diffusion_models/
│   ├── ...                  # Let's download the models in the corresponding workflow
│   ├── text_encoders/
│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors
│   └── vae/
│   │   └──  wan_2.1_vae.safetensors
│   └── clip_vision/
│       └──  clip_vision_h.safetensors   
```

For diffusion models, we’ll use the fp16 precision models in this guide because we’ve found that they perform better than the bf16 versions. If you need other precision versions, please visit [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models) to download them.

## [​](http://docs.comfy.org#wan2-1-text-to-video-workflow) Wan2.1 Text-to-Video Workflow

Before starting the workflow, please download [wan2.1\_t2v\_1.3B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_t2v_1.3B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.

> If you need other t2v precision versions, please visit [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models) to download them.

### [​](http://docs.comfy.org#1-workflow-file-download) 1. Workflow File Download

Download the file below and drag it into ComfyUI to load the corresponding workflow:

### [​](http://docs.comfy.org#2-complete-the-workflow-step-by-step) 2. Complete the Workflow Step by Step

1. Make sure the `Load Diffusion Model` node has loaded the `wan2.1_t2v_1.3B_fp16.safetensors` model
2. Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model
3. Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model
4. (Optional) You can modify the video dimensions in the `EmptyHunyuanLatentVideo` node if needed
5. (Optional) If you need to modify the prompts (positive and negative), make changes in the `CLIP Text Encoder` node at number `5`
6. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation

## [​](http://docs.comfy.org#wan2-1-image-to-video-workflow) Wan2.1 Image-to-Video Workflow

**Since Wan Video separates the 480P and 720P models**, we’ll need to provide examples for both resolutions in this guide. In addition to using different models, they also have slight parameter differences.

### [​](http://docs.comfy.org#480p-version) 480P Version

#### [​](http://docs.comfy.org#1-workflow-and-input-image) 1. Workflow and Input Image

Download the image below and drag it into ComfyUI to load the corresponding workflow:

We’ll use the following image as input:

#### [​](http://docs.comfy.org#2-model-download) 2. Model Download

Please download [wan2.1\_i2v\_480p\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_480p_14B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.

#### [​](http://docs.comfy.org#3-complete-the-workflow-step-by-step) 3. Complete the Workflow Step by Step

1. Make sure the `Load Diffusion Model` node has loaded the `wan2.1_i2v_480p_14B_fp16.safetensors` model
2. Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model
3. Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model
4. Make sure the `Load CLIP Vision` node has loaded the `clip_vision_h.safetensors` model
5. Upload the provided input image in the `Load Image` node
6. (Optional) Enter the video description content you want to generate in the `CLIP Text Encoder` node
7. (Optional) You can modify the video dimensions in the `WanImageToVideo` node if needed
8. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation

### [​](http://docs.comfy.org#720p-version) 720P Version

#### [​](http://docs.comfy.org#1-workflow-and-input-image-2) 1. Workflow and Input Image

Download the image below and drag it into ComfyUI to load the corresponding workflow:

We’ll use the following image as input:

#### [​](http://docs.comfy.org#2-model-download-2) 2. Model Download

Please download [wan2.1\_i2v\_720p\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_720p_14B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.

#### [​](http://docs.comfy.org#3-complete-the-workflow-step-by-step-2) 3. Complete the Workflow Step by Step

1. Make sure the `Load Diffusion Model` node has loaded the `wan2.1_i2v_720p_14B_fp16.safetensors` model
2. Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model
3. Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model
4. Make sure the `Load CLIP Vision` node has loaded the `clip_vision_h.safetensors` model
5. Upload the provided input image in the `Load Image` node
6. (Optional) Enter the video description content you want to generate in the `CLIP Text Encoder` node
7. (Optional) You can modify the video dimensions in the `WanImageToVideo` node if needed
8. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/video/wan/wan-video.mdx)

[Previous](http://docs.comfy.org/tutorials/video/hunyuan-video)

[Wan2.1 Fun ControlThis guide demonstrates how to use Wan2.1 Fun Control in ComfyUI to generate videos with control videos  
\
Next](http://docs.comfy.org/tutorials/video/wan/fun-control)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Wan2.1 ComfyUI Native Workflow Examples](http://docs.comfy.org#wan2-1-comfyui-native-workflow-examples)
- [Model Installation](http://docs.comfy.org#model-installation)
- [Wan2.1 Text-to-Video Workflow](http://docs.comfy.org#wan2-1-text-to-video-workflow)
- [1. Workflow File Download](http://docs.comfy.org#1-workflow-file-download)
- [2. Complete the Workflow Step by Step](http://docs.comfy.org#2-complete-the-workflow-step-by-step)
- [Wan2.1 Image-to-Video Workflow](http://docs.comfy.org#wan2-1-image-to-video-workflow)
- [480P Version](http://docs.comfy.org#480p-version)
- [1. Workflow and Input Image](http://docs.comfy.org#1-workflow-and-input-image)
- [2. Model Download](http://docs.comfy.org#2-model-download)
- [3. Complete the Workflow Step by Step](http://docs.comfy.org#3-complete-the-workflow-step-by-step)
- [720P Version](http://docs.comfy.org#720p-version)
- [1. Workflow and Input Image](http://docs.comfy.org#1-workflow-and-input-image-2)
- [2. Model Download](http://docs.comfy.org#2-model-download-2)
- [3. Complete the Workflow Step by Step](http://docs.comfy.org#3-complete-the-workflow-step-by-step-2)

<!-- END Built_In_Node/tutorials/video/wan/wan-video.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/bfl/flux-pro-ultra-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Introduction

# Introduction

Official documentation for ComfyUI. Contribute [here](https://github.com/Comfy-Org/docs).

## [​](http://docs.comfy.org#comfyui) [ComfyUI](https://github.com/comfyanonymous/ComfyUI)

The most powerful and modular stable diffusion GUI and backend. Written by [comfyanonymous](https://github.com/comfyanonymous) and other [contributors](https://github.com/comfyanonymous/ComfyUI/graphs/contributors).

- **ComfyUI** is a node-based interface and inference engine for generative AI
- Users can combine various AI models and operations through nodes to achieve highly customizable and controllable content generation
- ComfyUI is completely open source and can run on your local device

## [​](http://docs.comfy.org#getting-started-with-comfyui) Getting Started with ComfyUI

### [​](http://docs.comfy.org#comfyui-installation) ComfyUI Installation

ComfyUI currently offers multiple installation methods, supporting Windows, MacOS, and Linux systems:

ComfyUI Desktop (Recommended)

ComfyUI Desktop currently supports standalone installation for **Windows and MacOS (ARM)**, currently in Beta

- Code is open source on [Github](https://github.com/Comfy-Org/desktop)

You can choose the appropriate installation for your system and hardware below

- Windows
- MacOS(Apple Silicon)
- Linux

[**ComfyUI Desktop (Windows) Installation Guide**  
\
Suitable for **Windows** version with **Nvidia** GPU](http://docs.comfy.org/installation/desktop/windows)

[**ComfyUI Desktop (Windows) Installation Guide**  
\
Suitable for **Windows** version with **Nvidia** GPU](http://docs.comfy.org/installation/desktop/windows)

[**ComfyUI Desktop (MacOS) Installation Guide**  
\
Suitable for MacOS with **Apple Silicon**](http://docs.comfy.org/installation/desktop/macos)

ComfyUI Desktop **currently has no Linux prebuilds**, please visit the [Manual Installation](http://docs.comfy.org/installation/manual_install) section to install ComfyUI

ComfyUI Portable (Windows)

[**ComfyUI Portable (Windows) Installation Guide**  
\
Supports **Windows** ComfyUI version running on **Nvidia GPUs** or **CPU-only**, always use the latest commits and completely portable.](http://docs.comfy.org/installation/comfyui_portable_windows)

Manual Installation

[**ComfyUI Manual Installation Guide**  
\
Supports all system types and GPU types (Nvidia, AMD, Intel, Apple Silicon, Ascend NPU, Cambricon MLU)](http://docs.comfy.org/installation/manual_install)

## [​](http://docs.comfy.org#contributing-to-comfyui-ecosystem) Contributing to ComfyUI Ecosystem

If you’re planning to develop ComfyUI custom nodes (plugins), please read the following section.

[**Custom Node Development Guide**  
\
Learn how to build a custom node (plugin) for ComfyUI](http://docs.comfy.org/custom-nodes/overview)

## [​](http://docs.comfy.org#contributing-to-documentation) Contributing to Documentation

Fork the documentation [repo](https://github.com/comfyanonymous/ComfyUI) on Github and submit a PR to us

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/get_started/introduction.mdx)

[System RequirementsThis guide introduces some system requirements for ComfyUI, including hardware and software requirements  
\
Next](http://docs.comfy.org/installation/system_requirements)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [ComfyUI](http://docs.comfy.org#comfyui)
- [Getting Started with ComfyUI](http://docs.comfy.org#getting-started-with-comfyui)
- [ComfyUI Installation](http://docs.comfy.org#comfyui-installation)
- [Contributing to ComfyUI Ecosystem](http://docs.comfy.org#contributing-to-comfyui-ecosystem)
- [Contributing to Documentation](http://docs.comfy.org#contributing-to-documentation)

<!-- END Development/built-in-nodes/api-node/image/bfl/flux-pro-ultra-image.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/ideogram/ideogram-v1.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
  - Ideogram
    
    - [Ideogram V2](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v2)
    - [Ideogram V3](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3)
    - [Ideogram V1](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v1)
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Ideogram V1 - ComfyUI Native Node Documentation

# Ideogram V1 - ComfyUI Native Node Documentation

Node for creating precise text rendering images using Ideogram API

The Ideogram V1 node allows you to generate images with high-quality text rendering capabilities using Ideogram’s text-to-image API.

## [​](http://docs.comfy.org#parameter-description) Parameter Description

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing the content to generateturbobooleanFalseWhether to use turbo mode (faster but possibly lower quality)aspect\_ratioselect”1:1”Image aspect ratiomagic\_prompt\_optionselect”AUTO”Determines whether to use MagicPrompt in generation, options: AUTO, ON, OFFseedinteger0Random seed value (0-2147483647)negative\_promptstring""Specifies elements you don’t want in the imagenum\_imagesinteger1Number of images to generate (1-8)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image result

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated on 2025-05-03)]

```python
class IdeogramV1(ComfyNodeABC):
    """
    Generates images synchronously using the Ideogram V1 model.

    Images links are available for a limited period of time; if you would like to keep the image, you must download it.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation",
                    },
                ),
                "turbo": (
                    IO.BOOLEAN,
                    {
                        "default": False,
                        "tooltip": "Whether to use turbo mode (faster generation, potentially lower quality)",
                    }
                ),
            },
            "optional": {
                "aspect_ratio": (
                    IO.COMBO,
                    {
                        "options": list(V1_V2_RATIO_MAP.keys()),
                        "default": "1:1",
                        "tooltip": "The aspect ratio for image generation.",
                    },
                ),
                "magic_prompt_option": (
                    IO.COMBO,
                    {
                        "options": ["AUTO", "ON", "OFF"],
                        "default": "AUTO",
                        "tooltip": "Determine if MagicPrompt should be used in generation",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2147483647,
                        "step": 1,
                        "control_after_generate": True,
                        "display": "number",
                    },
                ),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Description of what to exclude from the image",
                    },
                ),
                "num_images": (
                    IO.INT,
                    {"default": 1, "min": 1, "max": 8, "step": 1, "display": "number"},
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = (IO.IMAGE,)
    FUNCTION = "api_call"
    CATEGORY = "api node/image/ideogram/v1"
    DESCRIPTION = cleandoc(__doc__ or "")
    API_NODE = True

    def api_call(
        self,
        prompt,
        turbo=False,
        aspect_ratio="1:1",
        magic_prompt_option="AUTO",
        seed=0,
        negative_prompt="",
        num_images=1,
        auth_token=None,
    ):
        # Determine the model based on turbo setting
        aspect_ratio = V1_V2_RATIO_MAP.get(aspect_ratio, None)
        model = "V_1_TURBO" if turbo else "V_1"

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/ideogram/generate",
                method=HttpMethod.POST,
                request_model=IdeogramGenerateRequest,
                response_model=IdeogramGenerateResponse,
            ),
            request=IdeogramGenerateRequest(
                image_request=ImageRequest(
                    prompt=prompt,
                    model=model,
                    num_images=num_images,
                    seed=seed,
                    aspect_ratio=aspect_ratio if aspect_ratio != "ASPECT_1_1" else None,
                    magic_prompt_option=(
                        magic_prompt_option if magic_prompt_option != "AUTO" else None
                    ),
                    negative_prompt=negative_prompt if negative_prompt else None,
                )
            ),
            auth_token=auth_token,
        )

        response = operation.execute()

        if not response.data or len(response.data) == 0:
            raise Exception("No images were generated in the response")

        image_urls = [image_data.url for image_data in response.data if image_data.url]

        if not image_urls:
            raise Exception("No image URLs were generated in the response")

        return (download_and_process_images(image_urls),)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/ideogram/ideogram-v1.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3)

[Stability Stable Image UltraA node that generates high-quality images using Stability AI's ultra stable diffusion model  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameter Description](http://docs.comfy.org#parameter-description)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/ideogram/ideogram-v1.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/ideogram/ideogram-v2.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
  - Ideogram
    
    - [Ideogram V2](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v2)
    - [Ideogram V3](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3)
    - [Ideogram V1](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v1)
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Ideogram V2 - ComfyUI Built-in Node Documentation

# Ideogram V2 - ComfyUI Built-in Node Documentation

Node for creating high-quality images and text rendering using Ideogram V2 API

The Ideogram V2 node allows you to generate more refined images using Ideogram’s second-generation AI model, with significant improvements in text rendering, image quality, and overall aesthetics.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing the content to generateturbobooleanFalseWhether to use turbo mode (faster generation, possibly lower quality)

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDefaultDescriptionaspect\_ratiodropdown”1:1”Image aspect ratio, effective when resolution is set to “Auto”resolutiondropdown”Auto”Output image resolution, if not set to “Auto”, it will override the aspect\_ratio settingmagic\_prompt\_optiondropdown”AUTO”Determines whether to use MagicPrompt feature during generation, options are \[“AUTO”, “ON”, “OFF”]seedinteger0Random seed value, range 0-2147483647style\_typedropdown”NONE”Generation style type (V2 only), options are \[“AUTO”, “GENERAL”, “REALISTIC”, “DESIGN”, “RENDER\_3D”, “ANIME”]negative\_promptstring""Specifies elements you don’t want to appear in the imagenum\_imagesinteger1Number of images to generate, range 1-8

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image(s)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated on 2025-05-03)]

```python

class IdeogramV2(ComfyNodeABC):
    """
    Generates images synchronously using the Ideogram V2 model.

    Images links are available for a limited period of time; if you would like to keep the image, you must download it.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation",
                    },
                ),
                "turbo": (
                    IO.BOOLEAN,
                    {
                        "default": False,
                        "tooltip": "Whether to use turbo mode (faster generation, potentially lower quality)",
                    }
                ),
            },
            "optional": {
                "aspect_ratio": (
                    IO.COMBO,
                    {
                        "options": list(V1_V2_RATIO_MAP.keys()),
                        "default": "1:1",
                        "tooltip": "The aspect ratio for image generation. Ignored if resolution is not set to AUTO.",
                    },
                ),
                "resolution": (
                    IO.COMBO,
                    {
                        "options": list(V1_V1_RES_MAP.keys()),
                        "default": "Auto",
                        "tooltip": "The resolution for image generation. If not set to AUTO, this overrides the aspect_ratio setting.",
                    },
                ),
                "magic_prompt_option": (
                    IO.COMBO,
                    {
                        "options": ["AUTO", "ON", "OFF"],
                        "default": "AUTO",
                        "tooltip": "Determine if MagicPrompt should be used in generation",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2147483647,
                        "step": 1,
                        "control_after_generate": True,
                        "display": "number",
                    },
                ),
                "style_type": (
                    IO.COMBO,
                    {
                        "options": ["AUTO", "GENERAL", "REALISTIC", "DESIGN", "RENDER_3D", "ANIME"],
                        "default": "NONE",
                        "tooltip": "Style type for generation (V2 only)",
                    },
                ),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Description of what to exclude from the image",
                    },
                ),
                "num_images": (
                    IO.INT,
                    {"default": 1, "min": 1, "max": 8, "step": 1, "display": "number"},
                ),
                #"color_palette": (
                #    IO.STRING,
                #    {
                #        "multiline": False,
                #        "default": "",
                #        "tooltip": "Color palette preset name or hex colors with weights",
                #    },
                #),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = (IO.IMAGE,)
    FUNCTION = "api_call"
    CATEGORY = "api node/image/ideogram/v2"
    DESCRIPTION = cleandoc(__doc__ or "")
    API_NODE = True

    def api_call(
        self,
        prompt,
        turbo=False,
        aspect_ratio="1:1",
        resolution="Auto",
        magic_prompt_option="AUTO",
        seed=0,
        style_type="NONE",
        negative_prompt="",
        num_images=1,
        color_palette="",
        auth_token=None,
    ):
        aspect_ratio = V1_V2_RATIO_MAP.get(aspect_ratio, None)
        resolution = V1_V1_RES_MAP.get(resolution, None)
        # Determine the model based on turbo setting
        model = "V_2_TURBO" if turbo else "V_2"

        # Handle resolution vs aspect_ratio logic
        # If resolution is not AUTO, it overrides aspect_ratio
        final_resolution = None
        final_aspect_ratio = None

        if resolution != "AUTO":
            final_resolution = resolution
        else:
            final_aspect_ratio = aspect_ratio if aspect_ratio != "ASPECT_1_1" else None

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/ideogram/generate",
                method=HttpMethod.POST,
                request_model=IdeogramGenerateRequest,
                response_model=IdeogramGenerateResponse,
            ),
            request=IdeogramGenerateRequest(
                image_request=ImageRequest(
                    prompt=prompt,
                    model=model,
                    num_images=num_images,
                    seed=seed,
                    aspect_ratio=final_aspect_ratio,
                    resolution=final_resolution,
                    magic_prompt_option=(
                        magic_prompt_option if magic_prompt_option != "AUTO" else None
                    ),
                    style_type=style_type if style_type != "NONE" else None,
                    negative_prompt=negative_prompt if negative_prompt else None,
                    color_palette=color_palette if color_palette else None,
                )
            ),
            auth_token=auth_token,
        )

        response = operation.execute()

        if not response.data or len(response.data) == 0:
            raise Exception("No images were generated in the response")

        image_urls = [image_data.url for image_data in response.data if image_data.url]

        if not image_urls:
            raise Exception("No image URLs were generated in the response")

        return (download_and_process_images(image_urls),)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/ideogram/ideogram-v2.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)

[Ideogram V3Node for creating top-quality images and text rendering using Ideogram's latest V3 API  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/ideogram/ideogram-v2.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/ideogram/ideogram-v3.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
  - Ideogram
    
    - [Ideogram V2](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v2)
    - [Ideogram V3](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3)
    - [Ideogram V1](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v1)
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Ideogram V3 - ComfyUI Native Node Documentation

# Ideogram V3 - ComfyUI Native Node Documentation

Node for creating top-quality images and text rendering using Ideogram’s latest V3 API

This node connects to the Ideogram V3 API to perform image generation tasks.

Currently, this node supports two image generation modes:

- **Text-to-Image Mode** - Generate new images from text prompts
- **Inpainting Mode** - Regenerate specific areas by providing an original image and mask

### [​](http://docs.comfy.org#text-to-image-mode) Text-to-Image Mode

This is the default mode, activated when no image or mask inputs are provided. Simply provide a prompt and the desired parameters:

1. Describe the image you want in the prompt field
2. Select an appropriate aspect ratio or resolution
3. Adjust other parameters like magic prompt, seed, and rendering quality
4. Run the node to generate the image

### [​](http://docs.comfy.org#inpainting-mode) Inpainting Mode

**Important Note**: This mode requires both image and mask inputs. If only one is provided, the node will throw an error.

1. Connect the original image to the `image` input port
2. Create a mask with the same dimensions as the original image, where white areas represent parts to be regenerated
3. Connect the mask to the `mask` input port
4. Describe what you want to generate in the masked area in the prompt
5. Run the node to perform local editing

## [​](http://docs.comfy.org#parameter-descriptions) Parameter Descriptions

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing the content to generateaspect\_ratiocombo”1:1”Image aspect ratio (text-to-image mode only)resolutioncombo”Auto”Image resolution, overrides aspect ratio when setmagic\_prompt\_optioncombo”AUTO”Magic prompt enhancement: AUTO, ON, or OFFseedint0Random seed value, 0 for random generationnum\_imagesint1Number of images to generate (1-8)rendering\_speedcombo”BALANCED”Rendering speed: BALANCED, TURBO, or QUALITY

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionimageimageInput image for inpainting mode (**must be provided with mask**)maskmaskMask for inpainting, white areas will be replaced (**must be provided with image**)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image

## [​](http://docs.comfy.org#how-it-works) How It Works

The Ideogram V3 node uses state-of-the-art AI models to process user input, capable of understanding complex design intentions and text layout requirements. It supports two main modes:

1. **Generation Mode**: Creates new images from text prompts
2. **Edit Mode**: Uses original image + mask combination, replacing only the areas specified by the mask

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class IdeogramV3(ComfyNodeABC):
    """
    Generates images synchronously using the Ideogram V3 model.

    Supports both regular image generation from text prompts and image editing with mask.
    Images links are available for a limited period of time; if you would like to keep the image, you must download it.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation or editing",
                    },
                ),
            },
            "optional": {
                "image": (
                    IO.IMAGE,
                    {
                        "default": None,
                        "tooltip": "Optional reference image for image editing.",
                    },
                ),
                "mask": (
                    IO.MASK,
                    {
                        "default": None,
                        "tooltip": "Optional mask for inpainting (white areas will be replaced)",
                    },
                ),
                "aspect_ratio": (
                    IO.COMBO,
                    {
                        "options": list(V3_RATIO_MAP.keys()),
                        "default": "1:1",
                        "tooltip": "The aspect ratio for image generation. Ignored if resolution is not set to Auto.",
                    },
                ),
                "resolution": (
                    IO.COMBO,
                    {
                        "options": V3_RESOLUTIONS,
                        "default": "Auto",
                        "tooltip": "The resolution for image generation. If not set to Auto, this overrides the aspect_ratio setting.",
                    },
                ),
                "magic_prompt_option": (
                    IO.COMBO,
                    {
                        "options": ["AUTO", "ON", "OFF"],
                        "default": "AUTO",
                        "tooltip": "Determine if MagicPrompt should be used in generation",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2147483647,
                        "step": 1,
                        "control_after_generate": True,
                        "display": "number",
                    },
                ),
                "num_images": (
                    IO.INT,
                    {"default": 1, "min": 1, "max": 8, "step": 1, "display": "number"},
                ),
                "rendering_speed": (
                    IO.COMBO,
                    {
                        "options": ["BALANCED", "TURBO", "QUALITY"],
                        "default": "BALANCED",
                        "tooltip": "Controls the trade-off between generation speed and quality",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = (IO.IMAGE,)
    FUNCTION = "api_call"
    CATEGORY = "api node/image/ideogram/v3"
    DESCRIPTION = cleandoc(__doc__ or "")
    API_NODE = True

    def api_call(
        self,
        prompt,
        image=None,
        mask=None,
        resolution="Auto",
        aspect_ratio="1:1",
        magic_prompt_option="AUTO",
        seed=0,
        num_images=1,
        rendering_speed="BALANCED",
        auth_token=None,
    ):
        # Check if both image and mask are provided for editing mode
        if image is not None and mask is not None:
            # Edit mode
            path = "/proxy/ideogram/ideogram-v3/edit"

            # Process image and mask
            input_tensor = image.squeeze().cpu()

            # Validate mask dimensions match image
            if mask.shape[1:] != image.shape[1:-1]:
                raise Exception("Mask and Image must be the same size")

            # Process image
            img_np = (input_tensor.numpy() * 255).astype(np.uint8)
            img = Image.fromarray(img_np)
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format="PNG")
            img_byte_arr.seek(0)
            img_binary = img_byte_arr
            img_binary.name = "image.png"

            # Process mask - white areas will be replaced
            mask_np = (mask.squeeze().cpu().numpy() * 255).astype(np.uint8)
            mask_img = Image.fromarray(mask_np)
            mask_byte_arr = io.BytesIO()
            mask_img.save(mask_byte_arr, format="PNG")
            mask_byte_arr.seek(0)
            mask_binary = mask_byte_arr
            mask_binary.name = "mask.png"

            # Create edit request
            edit_request = IdeogramV3EditRequest(
                prompt=prompt,
                rendering_speed=rendering_speed,
            )

            # Add optional parameters
            if magic_prompt_option != "AUTO":
                edit_request.magic_prompt = magic_prompt_option
            if seed != 0:
                edit_request.seed = seed
            if num_images > 1:
                edit_request.num_images = num_images

            # Execute the operation for edit mode
            operation = SynchronousOperation(
                endpoint=ApiEndpoint(
                    path=path,
                    method=HttpMethod.POST,
                    request_model=IdeogramV3EditRequest,
                    response_model=IdeogramGenerateResponse,
                ),
                request=edit_request,
                files={
                    "image": img_binary,
                    "mask": mask_binary,
                },
                content_type="multipart/form-data",
                auth_token=auth_token,
            )

        elif image is not None or mask is not None:
            # If only one of image or mask is provided, raise an error
            raise Exception("Ideogram V3 image editing requires both an image AND a mask")
        else:
            # Generation mode
            path = "/proxy/ideogram/ideogram-v3/generate"

            # Create generation request
            gen_request = IdeogramV3Request(
                prompt=prompt,
                rendering_speed=rendering_speed,
            )

            # Handle resolution vs aspect ratio
            if resolution != "Auto":
                gen_request.resolution = resolution
            elif aspect_ratio != "1:1":
                v3_aspect = V3_RATIO_MAP.get(aspect_ratio)
                if v3_aspect:
                    gen_request.aspect_ratio = v3_aspect

            # Add optional parameters
            if magic_prompt_option != "AUTO":
                gen_request.magic_prompt = magic_prompt_option
            if seed != 0:
                gen_request.seed = seed
            if num_images > 1:
                gen_request.num_images = num_images

            # Execute the operation for generation mode
            operation = SynchronousOperation(
                endpoint=ApiEndpoint(
                    path=path,
                    method=HttpMethod.POST,
                    request_model=IdeogramV3Request,
                    response_model=IdeogramGenerateResponse,
                ),
                request=gen_request,
                auth_token=auth_token,
            )

        # Execute the operation and process response
        response = operation.execute()

        if not response.data or len(response.data) == 0:
            raise Exception("No images were generated in the response")

        image_urls = [image_data.url for image_data in response.data if image_data.url]

        if not image_urls:
            raise Exception("No image URLs were generated in the response")

        return (download_and_process_images(image_urls),)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/ideogram/ideogram-v3.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v2)

[Ideogram V1Node for creating precise text rendering images using Ideogram API  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v1)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Text-to-Image Mode](http://docs.comfy.org#text-to-image-mode)
- [Inpainting Mode](http://docs.comfy.org#inpainting-mode)
- [Parameter Descriptions](http://docs.comfy.org#parameter-descriptions)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/ideogram/ideogram-v3.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/luma/luma-image-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
    
    - [Luma Reference](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-reference)
    - [Luma Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image)
    - [Luma Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image)
  - Recraft
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Image to Image - ComfyUI Built-in Node Documentation

# Luma Image to Image - ComfyUI Built-in Node Documentation

Node for modifying images using Luma AI

The Luma Image to Image node allows you to modify existing images using Luma AI technology based on text prompts, while preserving certain features and structure of the original image.

## [​](http://docs.comfy.org#node-function) Node Function

This node connects to Luma AI’s text-to-image API, enabling users to generate images through detailed text prompts. Luma AI is known for its excellent realism and detail, particularly excelling at generating photorealistic content and artistic style images.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing the content to generatemodelselect-Select which generation model to useaspect\_ratioselect16:9Set the aspect ratio of the output imageseedinteger0Seed value to determine if node should rerun, but actual results don’t depend on seedstyle\_image\_weightfloat1.0Weight of the style image, range 0.02-1.0, only effective when style\_image is provided

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

Without the following parameter inputs, the node functions in text-to-image mode

ParameterTypeDescriptionimage\_luma\_refLUMA\_REFLuma reference node connection, influences generation results through input images, can consider up to 4 imagesstyle\_imageimageStyle reference image, uses only 1 image, influences the style of generated images, adjusted through `style_image_weight`character\_imageimageAdds character features to the generated results, can be a batch of multiple images, up to 4 images

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageThe generated image

## [​](http://docs.comfy.org#usage-examples) Usage Examples

## [​](http://docs.comfy.org#how-it-works) How It Works

The Luma Image to Image node analyzes the input image and combines it with text prompts to guide the modification process. It uses Luma AI’s generation models to make creative changes to images based on prompts.

Node process:

1. First uploads the input image to ComfyAPI
2. Then sends the image URL with the prompt to Luma API
3. Waits for Luma AI to complete processing
4. Downloads and returns the generated image

The image\_weight parameter controls the degree of influence from the original image - values closer to 0 will preserve more of the original image features, while values closer to 1 allow for more substantial modifications.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated on 2025-05-05)]

```python

class LumaImageModifyNode(ComfyNodeABC):
    """
    Modifies images synchronously based on prompt and aspect ratio.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE,),
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation",
                    },
                ),
                "image_weight": (
                    IO.FLOAT,
                    {
                        "default": 1.0,
                        "min": 0.02,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Weight of the image; the closer to 0.0, the less the image will be modified.",
                    },
                ),
                "model": ([model.value for model in LumaImageModel],),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {},
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        model: str,
        image: torch.Tensor,
        image_weight: float,
        seed,
        auth_token=None,
        **kwargs,
    ):
        # first, upload image
        download_urls = upload_images_to_comfyapi(
            image, max_images=1, auth_token=auth_token
        )
        image_url = download_urls[0]
        # next, make Luma call with download url provided
        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/luma/generations/image",
                method=HttpMethod.POST,
                request_model=LumaImageGenerationRequest,
                response_model=LumaGeneration,
            ),
            request=LumaImageGenerationRequest(
                prompt=prompt,
                model=model,
                modify_image_ref=LumaModifyImageRef(
                    url=image_url, weight=round(image_weight, 2)
                ),
            ),
            auth_token=auth_token,
        )
        response_api: LumaGeneration = operation.execute()

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/luma/generations/{response_api.id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=LumaGeneration,
            ),
            completed_statuses=[LumaState.completed],
            failed_statuses=[LumaState.failed],
            status_extractor=lambda x: x.state,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        img_response = requests.get(response_poll.assets.image)
        img = process_image_response(img_response)
        return (img,)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/luma/luma-image-to-image.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image)

[Save SVGA utility node for saving SVG vector graphics to files  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Examples](http://docs.comfy.org#usage-examples)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/luma/luma-image-to-image.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/luma/luma-reference.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
    
    - [Luma Reference](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-reference)
    - [Luma Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image)
    - [Luma Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image)
  - Recraft
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Reference - ComfyUI Built-in Node Documentation

# Luma Reference - ComfyUI Built-in Node Documentation

Helper node providing reference images for Luma image generation

The Luma Reference node allows you to set reference images and weights to guide the creation process of Luma image generation nodes, making the generated images closer to specific features of the reference images.

## [​](http://docs.comfy.org#node-function) Node Function

This node works as a helper tool for Luma generation nodes, allowing users to provide reference images to influence generation results. It enables users to set the weight of reference images to control how much they affect the final result. Multiple Luma Reference nodes can be chained together, with a maximum of 4 working simultaneously according to API requirements.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageImage-Input image used as referenceweightFloat1.0Controls the strength of the reference image’s influence (0-1)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionluma\_refLUMA\_REFReference object containing image and weight

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Luma Text to Image Workflow Example**  
\
Luma Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image)

## [​](http://docs.comfy.org#how-it-works) How It Works

The Luma Reference node receives image input and allows setting a weight value. The node doesn’t directly generate or modify images but creates a reference object containing image data and weight information, which is then passed to Luma generation nodes.

During the generation process, Luma AI analyzes the features of the reference image and incorporates these features into the generation results based on the set weight. Higher weight values mean the generated image will be closer to the reference image’s features, while lower weight values indicate the reference image will only slightly influence the final result.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated on 2025-05-03)]

```python

class LumaReferenceNode(ComfyNodeABC):
    """
    Holds an image and weight for use with Luma Generate Image node.
    """

    RETURN_TYPES = (LumaIO.LUMA_REF,)
    RETURN_NAMES = ("luma_ref",)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "create_luma_reference"
    CATEGORY = "api node/image/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (
                    IO.IMAGE,
                    {
                        "tooltip": "Image to use as reference.",
                    },
                ),
                "weight": (
                    IO.FLOAT,
                    {
                        "default": 1.0,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Weight of image reference.",
                    },
                ),
            },
            "optional": {"luma_ref": (LumaIO.LUMA_REF,)},
        }

    def create_luma_reference(
        self, image: torch.Tensor, weight: float, luma_ref: LumaReferenceChain = None
    ):
        if luma_ref is not None:
            luma_ref = luma_ref.clone()
        else:
            luma_ref = LumaReferenceChain()
        luma_ref.add(LumaReference(image=image, weight=round(weight, 2)))
        return (luma_ref,)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/luma/luma-reference.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/bfl/flux-pro-ultra-image)

[Luma Text to ImageA node that converts text descriptions into high-quality images using Luma AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/luma/luma-reference.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/luma/luma-text-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
    
    - [Luma Reference](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-reference)
    - [Luma Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image)
    - [Luma Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image)
  - Recraft
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Text to Image - ComfyUI Native Node Documentation

# Luma Text to Image - ComfyUI Native Node Documentation

A node that converts text descriptions into high-quality images using Luma AI

The Luma Text to Image node allows you to create highly realistic and artistic images from text descriptions using Luma AI’s advanced image generation capabilities.

## [​](http://docs.comfy.org#node-function) Node Function

This node connects to Luma AI’s text-to-image API, enabling users to generate images through detailed text prompts. Luma AI is known for its excellent realism and detail representation, particularly excelling at producing photorealistic content and artistic style images.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptString""Text prompt describing the content to generatemodelSelect-Choose which generation model to useaspect\_ratioSelect16:9Set the output image’s aspect ratioseedInteger0Seed value to determine if the node should re-run, but actual results are independent of the seedstyle\_image\_weightFloat1.0Style image weight, range 0.0-1.0, only applies when style\_image is provided, higher means stronger style reference

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionimage\_luma\_refLUMA\_REFLuma reference node connection to influence generation with input images; up to 4 imagesstyle\_imageImageStyle reference image; only 1 image will be usedcharacter\_imageImageCharacter reference images; can be a batch of multiple, up to 4 images

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEImageGenerated image result

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Luma Text to Image Usage Example**  
\
Detailed guide for Luma Text to Image workflow](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image)

## [​](http://docs.comfy.org#how-it-works) How It Works

The Luma Text to Image node analyzes the text prompt provided by the user and creates corresponding images through Luma AI’s generation models. This process uses deep learning technology to understand text descriptions and convert them into visual representations. Users can fine-tune the generation process by adjusting various parameters, including resolution, guidance scale, and negative prompts.

Additionally, the node supports using reference images and concept guidance to further influence the generation results, allowing creators to more precisely achieve their creative vision.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated on 2025-05-03)]

```python

class LumaImageGenerationNode(ComfyNodeABC):
    """
    Generates images synchronously based on prompt and aspect ratio.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation",
                    },
                ),
                "model": ([model.value for model in LumaImageModel],),
                "aspect_ratio": (
                    [ratio.value for ratio in LumaAspectRatio],
                    {
                        "default": LumaAspectRatio.ratio_16_9,
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
                "style_image_weight": (
                    IO.FLOAT,
                    {
                        "default": 1.0,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Weight of style image. Ignored if no style_image provided.",
                    },
                ),
            },
            "optional": {
                "image_luma_ref": (
                    LumaIO.LUMA_REF,
                    {
                        "tooltip": "Luma Reference node connection to influence generation with input images; up to 4 images can be considered."
                    },
                ),
                "style_image": (
                    IO.IMAGE,
                    {"tooltip": "Style reference image; only 1 image will be used."},
                ),
                "character_image": (
                    IO.IMAGE,
                    {
                        "tooltip": "Character reference images; can be a batch of multiple, up to 4 images can be considered."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        model: str,
        aspect_ratio: str,
        seed,
        style_image_weight: float,
        image_luma_ref: LumaReferenceChain = None,
        style_image: torch.Tensor = None,
        character_image: torch.Tensor = None,
        auth_token=None,
        **kwargs,
    ):
        # handle image_luma_ref
        api_image_ref = None
        if image_luma_ref is not None:
            api_image_ref = self._convert_luma_refs(
                image_luma_ref, max_refs=4, auth_token=auth_token
            )
        # handle style_luma_ref
        api_style_ref = None
        if style_image is not None:
            api_style_ref = self._convert_style_image(
                style_image, weight=style_image_weight, auth_token=auth_token
            )
        # handle character_ref images
        character_ref = None
        if character_image is not None:
            download_urls = upload_images_to_comfyapi(
                character_image, max_images=4, auth_token=auth_token
            )
            character_ref = LumaCharacterRef(
                identity0=LumaImageIdentity(images=download_urls)
            )

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/luma/generations/image",
                method=HttpMethod.POST,
                request_model=LumaImageGenerationRequest,
                response_model=LumaGeneration,
            ),
            request=LumaImageGenerationRequest(
                prompt=prompt,
                model=model,
                aspect_ratio=aspect_ratio,
                image_ref=api_image_ref,
                style_ref=api_style_ref,
                character_ref=character_ref,
            ),
            auth_token=auth_token,
        )
        response_api: LumaGeneration = operation.execute()

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/luma/generations/{response_api.id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=LumaGeneration,
            ),
            completed_statuses=[LumaState.completed],
            failed_statuses=[LumaState.failed],
            status_extractor=lambda x: x.state,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        img_response = requests.get(response_poll.assets.image)
        img = process_image_response(img_response)
        return (img,)

    def _convert_luma_refs(
        self, luma_ref: LumaReferenceChain, max_refs: int, auth_token=None
    ):
        luma_urls = []
        ref_count = 0
        for ref in luma_ref.refs:
            download_urls = upload_images_to_comfyapi(
                ref.image, max_images=1, auth_token=auth_token
            )
            luma_urls.append(download_urls[0])
            ref_count += 1
            if ref_count >= max_refs:
                break
        return luma_ref.create_api_model(download_urls=luma_urls, max_refs=max_refs)

    def _convert_style_image(
        self, style_image: torch.Tensor, weight: float, auth_token=None
    ):
        chain = LumaReferenceChain(
            first_ref=LumaReference(image=style_image, weight=weight)
        )
        return self._convert_luma_refs(chain, max_refs=1, auth_token=auth_token)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/luma/luma-text-to-image.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-reference)

[Luma Image to ImageNode for modifying images using Luma AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/luma/luma-text-to-image.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/openai/openai-dalle2.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
  - Ideogram
  - Stability AI
  - OpenAI
    
    - [OpenAI GPT Image 1](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-gpt-image1)
    - [OpenAI DALL·E 2](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle2)
    - [OpenAI DALL·E 3](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle3)
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

OpenAI DALL·E 2 - ComfyUI Native Node Documentation

# OpenAI DALL·E 2 - ComfyUI Native Node Documentation

Node for generating images using OpenAI’s DALL·E 2 model

The OpenAI DALL·E 2 node allows you to use OpenAI’s DALL·E 2 API to generate creative images from text descriptions.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt for DALL·E to generate images, supports multi-line inputseedinteger0The result is not actually related to the seed, this parameter only determines whether to re-executesizeselect”1024x1024”Output image size, options: 256x256, 512x512, 1024x1024ninteger1Number of images to generate, range 1-8

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDefaultDescriptionimageimageNoneOptional reference image for image editingmaskmaskNoneOptional mask for inpainting (white areas will be replaced)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image(s)

## [​](http://docs.comfy.org#features) Features

- Basic function: Generate images from text prompts
- Image editing: When both image and mask parameters are provided, performs image editing (white masked areas will be replaced)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class OpenAIDalle2(ComfyNodeABC):
    """
    Generates images synchronously via OpenAI's DALL·E 2 endpoint.

    Uses the proxy at /proxy/openai/images/generations. Returned URLs are short‑lived,
    so download or cache results if you need to keep them.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Text prompt for DALL·E",
                    },
                ),
            },
            "optional": {
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2**31 - 1,
                        "step": 1,
                        "display": "number",
                        "control_after_generate": True,
                        "tooltip": "not implemented yet in backend",
                    },
                ),
                "size": (
                    IO.COMBO,
                    {
                        "options": ["256x256", "512x512", "1024x1024"],
                        "default": "1024x1024",
                        "tooltip": "Image size",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 8,
                        "step": 1,
                        "display": "number",
                        "tooltip": "How many images to generate",
                    },
                ),
                "image": (
                    IO.IMAGE,
                    {
                        "default": None,
                        "tooltip": "Optional reference image for image editing.",
                    },
                ),
                "mask": (
                    IO.MASK,
                    {
                        "default": None,
                        "tooltip": "Optional mask for inpainting (white areas will be replaced)",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = (IO.IMAGE,)
    FUNCTION = "api_call"
    CATEGORY = "api node/image/openai"
    DESCRIPTION = cleandoc(__doc__ or "")
    API_NODE = True

    def api_call(
        self,
        prompt,
        seed=0,
        image=None,
        mask=None,
        n=1,
        size="1024x1024",
        auth_token=None,
    ):
        model = "dall-e-2"
        path = "/proxy/openai/images/generations"
        content_type = "application/json"
        request_class = OpenAIImageGenerationRequest
        img_binary = None

        if image is not None and mask is not None:
            path = "/proxy/openai/images/edits"
            content_type = "multipart/form-data"
            request_class = OpenAIImageEditRequest

            input_tensor = image.squeeze().cpu()
            height, width, channels = input_tensor.shape
            rgba_tensor = torch.ones(height, width, 4, device="cpu")
            rgba_tensor[:, :, :channels] = input_tensor

            if mask.shape[1:] != image.shape[1:-1]:
                raise Exception("Mask and Image must be the same size")
            rgba_tensor[:, :, 3] = 1 - mask.squeeze().cpu()

            rgba_tensor = downscale_image_tensor(rgba_tensor.unsqueeze(0)).squeeze()

            image_np = (rgba_tensor.numpy() * 255).astype(np.uint8)
            img = Image.fromarray(image_np)
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format="PNG")
            img_byte_arr.seek(0)
            img_binary = img_byte_arr  # .getvalue()
            img_binary.name = "image.png"
        elif image is not None or mask is not None:
            raise Exception("Dall-E 2 image editing requires an image AND a mask")

        # Build the operation
        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=path,
                method=HttpMethod.POST,
                request_model=request_class,
                response_model=OpenAIImageGenerationResponse,
            ),
            request=request_class(
                model=model,
                prompt=prompt,
                n=n,
                size=size,
                seed=seed,
            ),
            files=(
                {
                    "image": img_binary,
                }
                if img_binary
                else None
            ),
            content_type=content_type,
            auth_token=auth_token,
        )

        response = operation.execute()

        img_tensor = validate_and_cast_response(response)
        return (img_tensor,)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/openai/openai-dalle2.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-gpt-image1)

[OpenAI DALL·E 3Node for generating high-quality images using OpenAI's DALL·E 3 model  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle3)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Features](http://docs.comfy.org#features)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/openai/openai-dalle2.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/openai/openai-dalle3.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
  - Ideogram
  - Stability AI
  - OpenAI
    
    - [OpenAI GPT Image 1](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-gpt-image1)
    - [OpenAI DALL·E 2](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle2)
    - [OpenAI DALL·E 3](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle3)
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

OpenAI DALL·E 3 - ComfyUI Native Node Documentation

# OpenAI DALL·E 3 - ComfyUI Native Node Documentation

Node for generating high-quality images using OpenAI’s DALL·E 3 model

This node connects to OpenAI’s DALL·E 3 API, allowing users to generate high-quality images through detailed text prompts. DALL·E 3 is OpenAI’s image generation model that offers significantly improved image quality, more accurate prompt understanding, and better detail rendering compared to previous versions.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#input-parameters) Input Parameters

ParameterTypeDefaultDescriptionpromptstring""Detailed text prompt describing what to generateseedinteger0Final result is not related to seed, this parameter only determines whether to re-executequalityselection”standard”Image quality, options: “standard” or “hd”styleselection”natural”Visual style, options: “natural” or “vivid”. “Vivid” makes the model tend to create more surreal and dramatic images, while “natural” generates more realistic, less surreal imagessizeselection”1024x1024”Output image size, options: “1024x1024”, “1024x1792”, or “1792x1024”

### [​](http://docs.comfy.org#output-parameters) Output Parameters

OutputTypeDescriptionIMAGEimageThe generated image result

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class OpenAIDalle3(ComfyNodeABC):
    """
    Generates images synchronously via OpenAI's DALL·E 3 endpoint.

    Uses the proxy at /proxy/openai/images/generations. Returned URLs are short‑lived,
    so download or cache results if you need to keep them.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Text prompt for DALL·E",
                    },
                ),
            },
            "optional": {
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2**31 - 1,
                        "step": 1,
                        "display": "number",
                        "control_after_generate": True,
                        "tooltip": "not implemented yet in backend",
                    },
                ),
                "quality": (
                    IO.COMBO,
                    {
                        "options": ["standard", "hd"],
                        "default": "standard",
                        "tooltip": "Image quality",
                    },
                ),
                "style": (
                    IO.COMBO,
                    {
                        "options": ["natural", "vivid"],
                        "default": "natural",
                        "tooltip": "Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images.",
                    },
                ),
                "size": (
                    IO.COMBO,
                    {
                        "options": ["1024x1024", "1024x1792", "1792x1024"],
                        "default": "1024x1024",
                        "tooltip": "Image size",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = (IO.IMAGE,)
    FUNCTION = "api_call"
    CATEGORY = "api node/image/openai"
    DESCRIPTION = cleandoc(__doc__ or "")
    API_NODE = True

    def api_call(
        self,
        prompt,
        seed=0,
        style="natural",
        quality="standard",
        size="1024x1024",
        auth_token=None,
    ):
        model = "dall-e-3"

        # build the operation
        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/openai/images/generations",
                method=HttpMethod.POST,
                request_model=OpenAIImageGenerationRequest,
                response_model=OpenAIImageGenerationResponse,
            ),
            request=OpenAIImageGenerationRequest(
                model=model,
                prompt=prompt,
                quality=quality,
                size=size,
                style=style,
                seed=seed,
            ),
            auth_token=auth_token,
        )

        response = operation.execute()

        img_tensor = validate_and_cast_response(response)
        return (img_tensor,)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/openai/openai-dalle3.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle2)

[MiniMax Image to VideoA node that converts static images to dynamic videos using MiniMax AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-image-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Input Parameters](http://docs.comfy.org#input-parameters)
- [Output Parameters](http://docs.comfy.org#output-parameters)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/openai/openai-dalle3.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/openai/openai-gpt-image1.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
  - Ideogram
  - Stability AI
  - OpenAI
    
    - [OpenAI GPT Image 1](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-gpt-image1)
    - [OpenAI DALL·E 2](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle2)
    - [OpenAI DALL·E 3](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle3)
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

OpenAI GPT Image 1 - ComfyUI Native Node Documentation

# OpenAI GPT Image 1 - ComfyUI Native Node Documentation

Node for generating images using OpenAI’s GPT-4 Vision model

This node connects to OpenAI’s GPT Image 1 API, allowing users to generate images through detailed text prompts. Unlike traditional DALL·E models, GPT Image 1 leverages GPT-4’s language understanding capabilities to handle more complex prompts and generate images that better match user intent.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing what to generatequalityselection”low”Image quality level, options: “low”, “medium”, “high”sizeselection”auto”Output image size, options: “auto”, “1024x1024”, “1024x1536”, “1536x1024”

### [​](http://docs.comfy.org#image-editing-parameters) Image Editing Parameters

ParameterTypeDescriptionimageimageInput image for editing, supports multiple imagesmaskmaskOptional mask to specify areas to modify (single image only)

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionbackgroundselectionBackground options: “opaque” or “transparent”seedintegerRandom seed (not yet implemented in backend)nintegerNumber of images to generate, range 1-8

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image result

## [​](http://docs.comfy.org#how-it-works) How It Works

The OpenAI GPT Image 1 node combines GPT-4’s language understanding with image generation. It analyzes the text prompt to understand its meaning and intent, then generates matching images.

In image editing mode, the node can modify existing images. Using a mask allows precise control over which areas to change. Note that mask input only works with single image input.

Users can control the output by adjusting parameters like quality level, size, background handling, and number of generations.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class OpenAIGPTImage1(ComfyNodeABC):
    """
    Generates images synchronously via OpenAI's GPT Image 1 endpoint.

    Uses the proxy at /proxy/openai/images/generations. Returned URLs are short‑lived,
    so download or cache results if you need to keep them.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Text prompt for GPT Image 1",
                    },
                ),
            },
            "optional": {
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2**31 - 1,
                        "step": 1,
                        "display": "number",
                        "control_after_generate": True,
                        "tooltip": "not implemented yet in backend",
                    },
                ),
                "quality": (
                    IO.COMBO,
                    {
                        "options": ["low", "medium", "high"],
                        "default": "low",
                        "tooltip": "Image quality, affects cost and generation time.",
                    },
                ),
                "background": (
                    IO.COMBO,
                    {
                        "options": ["opaque", "transparent"],
                        "default": "opaque",
                        "tooltip": "Return image with or without background",
                    },
                ),
                "size": (
                    IO.COMBO,
                    {
                        "options": ["auto", "1024x1024", "1024x1536", "1536x1024"],
                        "default": "auto",
                        "tooltip": "Image size",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 8,
                        "step": 1,
                        "display": "number",
                        "tooltip": "How many images to generate",
                    },
                ),
                "image": (
                    IO.IMAGE,
                    {
                        "default": None,
                        "tooltip": "Optional reference image for image editing.",
                    },
                ),
                "mask": (
                    IO.MASK,
                    {
                        "default": None,
                        "tooltip": "Optional mask for inpainting (white areas will be replaced)",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = (IO.IMAGE,)
    FUNCTION = "api_call"
    CATEGORY = "api node/image/openai"
    DESCRIPTION = cleandoc(__doc__ or "")
    API_NODE = True

    def api_call(
        self,
        prompt,
        seed=0,
        quality="low",
        background="opaque",
        image=None,
        mask=None,
        n=1,
        size="1024x1024",
        auth_token=None,
    ):
        model = "gpt-image-1"
        path = "/proxy/openai/images/generations"
        content_type="application/json"
        request_class = OpenAIImageGenerationRequest
        img_binaries = []
        mask_binary = None
        files = []

        if image is not None:
            path = "/proxy/openai/images/edits"
            request_class = OpenAIImageEditRequest
            content_type ="multipart/form-data"

            batch_size = image.shape[0]

            for i in range(batch_size):
                single_image = image[i : i + 1]
                scaled_image = downscale_image_tensor(single_image).squeeze()

                image_np = (scaled_image.numpy() * 255).astype(np.uint8)
                img = Image.fromarray(image_np)
                img_byte_arr = io.BytesIO()
                img.save(img_byte_arr, format="PNG")
                img_byte_arr.seek(0)
                img_binary = img_byte_arr
                img_binary.name = f"image_{i}.png"

                img_binaries.append(img_binary)
                if batch_size == 1:
                    files.append(("image", img_binary))
                else:
                    files.append(("image[]", img_binary))

        if mask is not None:
            if image.shape[0] != 1:
                raise Exception("Cannot use a mask with multiple image")
            if image is None:
                raise Exception("Cannot use a mask without an input image")
            if mask.shape[1:] != image.shape[1:-1]:
                raise Exception("Mask and Image must be the same size")
            batch, height, width = mask.shape
            rgba_mask = torch.zeros(height, width, 4, device="cpu")
            rgba_mask[:, :, 3] = 1 - mask.squeeze().cpu()

            scaled_mask = downscale_image_tensor(rgba_mask.unsqueeze(0)).squeeze()

            mask_np = (scaled_mask.numpy() * 255).astype(np.uint8)
            mask_img = Image.fromarray(mask_np)
            mask_img_byte_arr = io.BytesIO()
            mask_img.save(mask_img_byte_arr, format="PNG")
            mask_img_byte_arr.seek(0)
            mask_binary = mask_img_byte_arr
            mask_binary.name = "mask.png"
            files.append(("mask", mask_binary))

        # Build the operation
        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=path,
                method=HttpMethod.POST,
                request_model=request_class,
                response_model=OpenAIImageGenerationResponse,
            ),
            request=request_class(
                model=model,
                prompt=prompt,
                quality=quality,
                background=background,
                n=n,
                seed=seed,
                size=size,
            ),
            files=files if files else None,
            content_type=content_type,
            auth_token=auth_token,
        )

        response = operation.execute()

        img_tensor = validate_and_cast_response(response)
        return (img_tensor,)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/openai/openai-gpt-image1.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image)

[OpenAI DALL·E 2Node for generating images using OpenAI's DALL·E 2 model  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle2)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Image Editing Parameters](http://docs.comfy.org#image-editing-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/openai/openai-gpt-image1.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/recraft/recraft-color-rgb.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Color RGB - ComfyUI Native Node Documentation

# Recraft Color RGB - ComfyUI Native Node Documentation

Helper node for defining color controls in Recraft image generation

The Recraft Color RGB node lets you define precise RGB color values to control colors in Recraft image generation.

## [​](http://docs.comfy.org#node-function) Node Function

This node creates a color configuration object that connects to the Recraft Controls node to specify colors used in generated images.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionrinteger0Red channel (0-255)ginteger0Green channel (0-255)binteger0Blue channel (0-255)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionrecraft\_colorRecraft ColorColor config object to connect to Recraft Controls

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Recraft Text to Image Workflow Example**  
\
Recraft Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftColorRGBNode:
    """
    Create Recraft Color by choosing specific RGB values.
    """

    RETURN_TYPES = (RecraftIO.COLOR,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    RETURN_NAMES = ("recraft_color",)
    FUNCTION = "create_color"
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "r": (IO.INT, {
                    "default": 0,
                    "min": 0,
                    "max": 255,
                    "tooltip": "Red value of color."
                }),
                "g": (IO.INT, {
                    "default": 0,
                    "min": 0,
                    "max": 255,
                    "tooltip": "Green value of color."
                }),
                "b": (IO.INT, {
                    "default": 0,
                    "min": 0,
                    "max": 255,
                    "tooltip": "Blue value of color."
                }),
            },
            "optional": {
                "recraft_color": (RecraftIO.COLOR,),
            }
        }

    def create_color(self, r: int, g: int, b: int, recraft_color: RecraftColorChain=None):
        recraft_color = recraft_color.clone() if recraft_color else RecraftColorChain()
        recraft_color.add(RecraftColor(r, g, b))
        return (recraft_color, )

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-color-rgb.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)

[Recraft Text to ImageA Recraft API node that generates high-quality images from text descriptions  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/recraft/recraft-color-rgb.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/recraft/recraft-controls.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Controls - ComfyUI Native Node Documentation

# Recraft Controls - ComfyUI Native Node Documentation

Node providing advanced control parameters for Recraft image generation

The Recraft Controls node lets you define control parameters (like colors and background colors) to guide Recraft’s image generation process. This node combines multiple control inputs into a unified control object.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptioncolorsRecraft ColorColor controls for image generationbackground\_colorRecraft ColorBackground color control

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionrecraft\_controlsRecraft ControlsControl config object for Recraft generation nodes

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Recraft Text to Image Workflow Example**  
\
Recraft Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

## [​](http://docs.comfy.org#how-it-works) How It Works

Node process:

1. Collects input control parameters (colors and background\_color)
2. Combines these parameters into a structured control object
3. Outputs this control object for connecting to Recraft generation nodes

When connected to Recraft generation nodes, these control parameters influence the AI generation process. The AI considers multiple factors beyond just the text prompt’s semantic content. If color inputs are configured, the AI will try to use these colors appropriately in the generated image.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftControlsNode:
    """
    Create Recraft Controls for customizing Recraft generation.
    """

    RETURN_TYPES = (RecraftIO.CONTROLS,)
    RETURN_NAMES = ("recraft_controls",)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "create_controls"
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
            },
            "optional": {
                "colors": (RecraftIO.COLOR,),
                "background_color": (RecraftIO.COLOR,),
            }
        }

    def create_controls(self, colors: RecraftColorChain=None, background_color: RecraftColorChain=None):
        return (RecraftControls(colors=colors, background_color=background_color), )

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-controls.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)

[Recraft Replace BackgroundA Recraft API node that automatically detects foreground subjects and replaces backgrounds  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/recraft/recraft-controls.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/recraft/recraft-creative-upscale.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Creative Upscale - ComfyUI Native Node Documentation

# Recraft Creative Upscale - ComfyUI Native Node Documentation

A Recraft API node that uses AI to creatively enhance image details and resolution

The Recraft Creative Upscale node uses Recraft’s API to increase image resolution while creatively enhancing and enriching image details.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageimage-Input image to be creatively upscaled

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageHigh-resolution image after creative upscaling

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftCreativeUpscaleNode(RecraftCrispUpscaleNode):
    """
    Upscale image synchronously.
    Enhances a given raster image using ‘creative upscale’ tool, boosting resolution with a focus on refining small details and faces.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    RECRAFT_PATH = "/proxy/recraft/images/creativeUpscale"
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-creative-upscale.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)

[Recraft Image to ImageA Recraft API node that generates new images based on text prompts and reference images  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/recraft/recraft-creative-upscale.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Crisp Upscale - ComfyUI Native Node Documentation

# Recraft Crisp Upscale - ComfyUI Native Node Documentation

A Recraft API node that enhances image clarity and resolution using AI

The Recraft Crisp Upscale node uses Recraft’s API to improve image resolution and clarity.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageimage-Input image to be upscaled

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageUpscaled and enhanced output image

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftCrispUpscaleNode:
    """
    Upscale image synchronously.
    Enhances a given raster image using ‘crisp upscale’ tool, increasing image resolution, making the image sharper and cleaner.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    RECRAFT_PATH = "/proxy/recraft/images/crispUpscale"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
            },
            "optional": {
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        auth_token=None,
        **kwargs,
    ):
        images = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                path=self.RECRAFT_PATH,
                auth_token=auth_token,
            )
            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))
            pbar.update(1)

        images_tensor = torch.cat(images, dim=0)
        return (images_tensor,)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)

[Recraft Color RGBHelper node for defining color controls in Recraft image generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/recraft/recraft-image-inpainting.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Image Inpainting - ComfyUI Native Node Documentation

# Recraft Image Inpainting - ComfyUI Native Node Documentation

Selectively modify image regions using Recraft API

The Recraft Image Inpainting node lets you modify specific areas of an image while keeping the rest unchanged. By providing an image, mask and text prompt, you can generate new content to fill the selected areas.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageimage-Input image to modifymaskmask-Black and white mask defining areas to changepromptstring""Text describing what to generate in masked areaninteger1Number of results to generate (1-6)seedinteger0Random seed value

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionrecraft\_styleRecraft StyleStyle settings for generated contentnegative\_promptstringElements to avoid in generated contentrecraft\_controlsRecraft ControlsAdditional controls like colors

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageModified image result

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftImageInpaintingNode:
    """
    Modify image based on prompt and mask.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
                "mask": (IO.MASK, ),
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation.",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 6,
                        "tooltip": "The number of images to generate.",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "recraft_style": (RecraftIO.STYLEV3,),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        mask: torch.Tensor,
        prompt: str,
        n: int,
        seed,
        auth_token=None,
        recraft_style: RecraftStyle = None,
        negative_prompt: str = None,
        **kwargs,
    ):
        default_style = RecraftStyle(RecraftStyleV3.realistic_image)
        if recraft_style is None:
            recraft_style = default_style

        if not negative_prompt:
            negative_prompt = None

        request = RecraftImageGenerationRequest(
            prompt=prompt,
            negative_prompt=negative_prompt,
            model=RecraftModel.recraftv3,
            n=n,
            style=recraft_style.style,
            substyle=recraft_style.substyle,
            style_id=recraft_style.style_id,
            random_seed=seed,
        )

        # prepare mask tensor
        _, H, W, _ = image.shape
        mask = mask.unsqueeze(-1)
        mask = mask.movedim(-1,1)
        mask = common_upscale(mask, width=W, height=H, upscale_method="nearest-exact", crop="disabled")
        mask = mask.movedim(1,-1)
        mask = (mask > 0.5).float()

        images = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                mask=mask[i:i+1],
                path="/proxy/recraft/images/inpaint",
                request=request,
                auth_token=auth_token,
            )
            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))
            pbar.update(1)

        images_tensor = torch.cat(images, dim=0)
        return (images_tensor, )
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-image-inpainting.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)

[Recraft Vectorize ImageA Recraft API node that converts raster images to vector SVG format  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/recraft/recraft-image-inpainting.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/recraft/recraft-image-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Image to Image - ComfyUI Native Node Documentation

# Recraft Image to Image - ComfyUI Native Node Documentation

A Recraft API node that generates new images based on text prompts and reference images

The Recraft Image to Image node uses Recraft’s API to generate new images based on a reference image and text prompts.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageimage-Reference image inputpromptstring""Text description for the generated imageninteger1Number of images to generate (1-6)seedinteger0Random seed value

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionrecraft\_styleRecraft StyleStyle settings for generated imagesnegative\_promptstringElements to avoid in generated imagesrecraft\_controlsRecraft ControlsAdditional controls like colors

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image result

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class RecraftImageToImageNode:
    """
    Modify image based on prompt and strength.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation.",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 6,
                        "tooltip": "The number of images to generate.",
                    },
                ),
                "strength": (
                    IO.FLOAT,
                    {
                        "default": 0.5,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Defines the difference with the original image, should lie in [0, 1], where 0 means almost identical, and 1 means miserable similarity."
                    }
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "recraft_style": (RecraftIO.STYLEV3,),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
                "recraft_controls": (
                    RecraftIO.CONTROLS,
                    {
                        "tooltip": "Optional additional controls over the generation via the Recraft Controls node."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        prompt: str,
        n: int,
        strength: float,
        seed,
        auth_token=None,
        recraft_style: RecraftStyle = None,
        negative_prompt: str = None,
        recraft_controls: RecraftControls = None,
        **kwargs,
    ):
        default_style = RecraftStyle(RecraftStyleV3.realistic_image)
        if recraft_style is None:
            recraft_style = default_style

        controls_api = None
        if recraft_controls:
            controls_api = recraft_controls.create_api_model()

        if not negative_prompt:
            negative_prompt = None

        request = RecraftImageGenerationRequest(
            prompt=prompt,
            negative_prompt=negative_prompt,
            model=RecraftModel.recraftv3,
            n=n,
            strength=round(strength, 2),
            style=recraft_style.style,
            substyle=recraft_style.substyle,
            style_id=recraft_style.style_id,
            controls=controls_api,
            random_seed=seed,
        )

        images = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                path="/proxy/recraft/images/imageToImage",
                request=request,
                auth_token=auth_token,
            )
            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))
            pbar.update(1)

        images_tensor = torch.cat(images, dim=0)
        return (images_tensor, )
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-image-to-image.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)

[Recraft Crisp UpscaleA Recraft API node that enhances image clarity and resolution using AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/recraft/recraft-image-to-image.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/recraft/recraft-remove-background.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Remove Background - ComfyUI Native Node Documentation

# Recraft Remove Background - ComfyUI Native Node Documentation

A Recraft API node that automatically removes image backgrounds and creates transparent alpha channels

The Recraft Remove Background node uses Recraft’s API to intelligently detect and remove image backgrounds, creating images with transparent backgrounds and corresponding alpha masks.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageimage-Input image to remove background from

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageImage with background removed (with alpha channel)MASKmaskMask of the main subject (white areas are preserved)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftRemoveBackgroundNode:
    """
    Remove background from image, and return processed image and mask.
    """

    RETURN_TYPES = (IO.IMAGE, IO.MASK)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
            },
            "optional": {
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        auth_token=None,
        **kwargs,
    ):
        images = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                path="/proxy/recraft/images/removeBackground",
                auth_token=auth_token,
            )
            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))
            pbar.update(1)

        images_tensor = torch.cat(images, dim=0)
        # use alpha channel as masks, in B,H,W format
        masks_tensor = images_tensor[:,:,:,-1:].squeeze(-1)
        return (images_tensor, masks_tensor)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-remove-background.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)

[Recraft Style - Logo RasterHelper node for setting logo raster style in Recraft image generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/recraft/recraft-remove-background.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/recraft/recraft-replace-background.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Replace Background - ComfyUI Native Node Documentation

# Recraft Replace Background - ComfyUI Native Node Documentation

A Recraft API node that automatically detects foreground subjects and replaces backgrounds

The Recraft Replace Background node uses Recraft’s API to intelligently detect subjects in images and generate new backgrounds based on text prompts.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageimage-Input image with subject to preservepromptstring""Text prompt for background generationninteger1Number of images to generate (1-6)seedinteger0Random seed value for node re-runs

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionrecraft\_styleRecraft StyleStyle settings for background generationnegative\_promptstringText describing elements to avoid

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageFinal image with replaced background

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class RecraftReplaceBackgroundNode:
    """
    Replace background on image, based on provided prompt.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation.",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 6,
                        "tooltip": "The number of images to generate.",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "recraft_style": (RecraftIO.STYLEV3,),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        prompt: str,
        n: int,
        seed,
        auth_token=None,
        recraft_style: RecraftStyle = None,
        negative_prompt: str = None,
        **kwargs,
    ):
        default_style = RecraftStyle(RecraftStyleV3.realistic_image)
        if recraft_style is None:
            recraft_style = default_style

        if not negative_prompt:
            negative_prompt = None

        request = RecraftImageGenerationRequest(
            prompt=prompt,
            negative_prompt=negative_prompt,
            model=RecraftModel.recraftv3,
            n=n,
            style=recraft_style.style,
            substyle=recraft_style.substyle,
            style_id=recraft_style.style_id,
        )

        images = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                path="/proxy/recraft/images/replaceBackground",
                request=request,
                auth_token=auth_token,
            )
            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))
            pbar.update(1)

        images_tensor = torch.cat(images, dim=0)
        return (images_tensor, )

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-replace-background.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)

[Ideogram V2Node for creating high-quality images and text rendering using Ideogram V2 API  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v2)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/recraft/recraft-replace-background.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Style - Digital Illustration - ComfyUI Native Node Documentation

# Recraft Style - Digital Illustration - ComfyUI Native Node Documentation

A helper node for setting digital illustration style in Recraft image generation

This node creates a style configuration object that guides Recraft’s image generation process towards a digital illustration look.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionsubstyleselectNoneSpecific substyle of digital illustration

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionrecraft\_styleRecraft StyleStyle config object to connect to Recraft generation nodes

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Recraft Text to Image Workflow Example**  
\
Recraft Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftStyleV3DigitalIllustrationNode(RecraftStyleV3RealisticImageNode):
    """
    Select digital_illustration style and optional substyle.
    """

    RECRAFT_STYLE = RecraftStyleV3.digital_illustration

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)

[Recraft Remove BackgroundA Recraft API node that automatically removes image backgrounds and creates transparent alpha channels  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Style - Logo Raster - ComfyUI Built-in Node Documentation

# Recraft Style - Logo Raster - ComfyUI Built-in Node Documentation

Helper node for setting logo raster style in Recraft image generation

This node creates a style configuration object that guides Recraft’s image generation process toward professional logo design effects. By selecting different substyles, you can define the design style, complexity and use cases of the generated logo.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionsubstyleSelection-Specific substyle for logo raster (Required)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionrecraft\_styleRecraft StyleStyle configuration object, connects to Recraft generation node

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Recraft Text to Image Workflow Example**  
\
Recraft Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python
class RecraftStyleV3LogoRasterNode(RecraftStyleV3RealisticImageNode):
    """
    Select vector_illustration style and optional substyle.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "substyle": (get_v3_substyles(s.RECRAFT_STYLE, include_none=False),),
            }
        }

    RECRAFT_STYLE = RecraftStyleV3.logo_raster
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)

[Recraft ControlsNode providing advanced control parameters for Recraft image generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Style - Realistic Image - ComfyUI Native Node Documentation

# Recraft Style - Realistic Image - ComfyUI Native Node Documentation

A helper node for setting realistic photo style in Recraft image generation

The Recraft Style - Realistic Image node helps set up realistic photo styles for Recraft image generation, with various substyle options to control the visual characteristics of generated images.

## [​](http://docs.comfy.org#node-function) Node Function

This node creates a style configuration object that guides Recraft’s image generation process towards realistic photo effects.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionsubstyleselectNoneSpecific substyle of realistic photo (Required)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionrecraft\_styleRecraft StyleStyle config object to connect to Recraft generation nodes

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Recraft Text to Image Workflow Example**  
\
Recraft Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class RecraftStyleV3RealisticImageNode:
    """
    Select realistic_image style and optional substyle.
    """

    RETURN_TYPES = (RecraftIO.STYLEV3,)
    RETURN_NAMES = ("recraft_style",)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "create_style"
    CATEGORY = "api node/image/Recraft"

    RECRAFT_STYLE = RecraftStyleV3.realistic_image

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "substyle": (get_v3_substyles(s.RECRAFT_STYLE),),
            }
        }

    def create_style(self, substyle: str):
        if substyle == "None":
            substyle = None
        return (RecraftStyle(self.RECRAFT_STYLE, substyle),)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)

[Recraft Text to VectorA Recraft API node that generates scalable vector graphics from text descriptions  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/recraft/recraft-text-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Text to Image - ComfyUI Built-in Node Documentation

# Recraft Text to Image - ComfyUI Built-in Node Documentation

A Recraft API node that generates high-quality images from text descriptions

The Recraft Text to Image node lets you generate high-quality images from text prompts by directly connecting to Recraft AI’s image generation API to create images in various styles.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text description for the imagesizeselect1024x1024Output image sizenint1Number of images (1-6)seedint0Random seed value

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionrecraft\_styleRecraft StyleImage style setting, default is “realistic photo”negative\_promptstringElements to exclude from generationrecraft\_controlsRecraft ControlsAdditional control parameters (colors, etc.)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image(s)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftTextToImageNode:
    """
    Generates images synchronously based on prompt and resolution.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation.",
                    },
                ),
                "size": (
                    [res.value for res in RecraftImageSize],
                    {
                        "default": RecraftImageSize.res_1024x1024,
                        "tooltip": "The size of the generated image.",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 6,
                        "tooltip": "The number of images to generate.",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "recraft_style": (RecraftIO.STYLEV3,),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
                "recraft_controls": (
                    RecraftIO.CONTROLS,
                    {
                        "tooltip": "Optional additional controls over the generation via the Recraft Controls node."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        size: str,
        n: int,
        seed,
        recraft_style: RecraftStyle = None,
        negative_prompt: str = None,
        recraft_controls: RecraftControls = None,
        auth_token=None,
        **kwargs,
    ):
        default_style = RecraftStyle(RecraftStyleV3.realistic_image)
        if recraft_style is None:
            recraft_style = default_style

        controls_api = None
        if recraft_controls:
            controls_api = recraft_controls.create_api_model()

        if not negative_prompt:
            negative_prompt = None

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/recraft/image_generation",
                method=HttpMethod.POST,
                request_model=RecraftImageGenerationRequest,
                response_model=RecraftImageGenerationResponse,
            ),
            request=RecraftImageGenerationRequest(
                prompt=prompt,
                negative_prompt=negative_prompt,
                model=RecraftModel.recraftv3,
                size=size,
                n=n,
                style=recraft_style.style,
                substyle=recraft_style.substyle,
                style_id=recraft_style.style_id,
                controls=controls_api,
            ),
            auth_token=auth_token,
        )
        response: RecraftImageGenerationResponse = operation.execute()
        images = []
        for data in response.data:
            image = bytesio_to_image_tensor(
                download_url_to_bytesio(data.url, timeout=1024)
            )
            if len(image.shape) < 4:
                image = image.unsqueeze(0)
            images.append(image)
        output_image = torch.cat(images, dim=0)

        return (output_image,)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-text-to-image.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)

[Recraft Image InpaintingSelectively modify image regions using Recraft API  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/recraft/recraft-text-to-image.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/recraft/recraft-text-to-vector.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Text to Vector - ComfyUI Native Node Documentation

# Recraft Text to Vector - ComfyUI Native Node Documentation

A Recraft API node that generates scalable vector graphics from text descriptions

The Recraft Text to Vector node lets you create high-quality vector graphics (SVG format) from text descriptions using Recraft’s API. It’s perfect for making logos, icons, and infinitely scalable illustrations.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text description of the vector graphic to generatesubstyleselect-Vector style subtypesizeselect1024x1024Canvas size for the vector outputninteger1Number of results to generate (1-6)seedinteger0Random seed value

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionnegative\_promptstringElements to exclude from generationrecraft\_controlsRecraft ControlsAdditional control parameters (colors, etc.)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionSVGvectorGenerated SVG vector graphic, connect to SaveSVG node to save

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class RecraftTextToVectorNode:
    """
    Generates SVG synchronously based on prompt and resolution.
    """

    RETURN_TYPES = (RecraftIO.SVG,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation.",
                    },
                ),
                "substyle": (get_v3_substyles(RecraftStyleV3.vector_illustration),),
                "size": (
                    [res.value for res in RecraftImageSize],
                    {
                        "default": RecraftImageSize.res_1024x1024,
                        "tooltip": "The size of the generated image.",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 6,
                        "tooltip": "The number of images to generate.",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
                "recraft_controls": (
                    RecraftIO.CONTROLS,
                    {
                        "tooltip": "Optional additional controls over the generation via the Recraft Controls node."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        substyle: str,
        size: str,
        n: int,
        seed,
        negative_prompt: str = None,
        recraft_controls: RecraftControls = None,
        auth_token=None,
        **kwargs,
    ):
        # create RecraftStyle so strings will be formatted properly (i.e. "None" will become None)
        recraft_style = RecraftStyle(RecraftStyleV3.vector_illustration, substyle=substyle)

        controls_api = None
        if recraft_controls:
            controls_api = recraft_controls.create_api_model()

        if not negative_prompt:
            negative_prompt = None

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/recraft/image_generation",
                method=HttpMethod.POST,
                request_model=RecraftImageGenerationRequest,
                response_model=RecraftImageGenerationResponse,
            ),
            request=RecraftImageGenerationRequest(
                prompt=prompt,
                negative_prompt=negative_prompt,
                model=RecraftModel.recraftv3,
                size=size,
                n=n,
                style=recraft_style.style,
                substyle=recraft_style.substyle,
                controls=controls_api,
            ),
            auth_token=auth_token,
        )
        response: RecraftImageGenerationResponse = operation.execute()
        svg_data = []
        for data in response.data:
            svg_data.append(download_url_to_bytesio(data.url, timeout=1024))

        return (SVG(svg_data),)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-text-to-vector.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)

[Recraft Creative UpscaleA Recraft API node that uses AI to creatively enhance image details and resolution  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/recraft/recraft-text-to-vector.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/recraft/recraft-vectorize-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Vectorize Image - ComfyUI Built-in Node Documentation

# Recraft Vectorize Image - ComfyUI Built-in Node Documentation

A Recraft API node that converts raster images to vector SVG format

The Recraft Vectorize Image node uses Recraft’s API to convert raster images (like photos, PNGs or JPEGs) into vector SVG format.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageImage-Input image to be converted to vector

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionSVGVectorConverted SVG vector graphic, needs to be connected to SaveSVG node to save

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Recraft Text to Image Workflow Example**  
\
Recraft Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class RecraftVectorizeImageNode:
    """
    Generates SVG synchronously from an input image.
    """

    RETURN_TYPES = (RecraftIO.SVG,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
            },
            "optional": {
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        auth_token=None,
        **kwargs,
    ):
        svgs = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                path="/proxy/recraft/images/vectorize",
                auth_token=auth_token,
            )
            svgs.append(SVG(sub_bytes))
            pbar.update(1)

        return (SVG.combine_all(svgs), )

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-vectorize-image.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)

[Recraft Style - Digital IllustrationA helper node for setting digital illustration style in Recraft image generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/recraft/recraft-vectorize-image.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/recraft/save-svg.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Save SVG - ComfyUI Built-in Node Documentation

# Save SVG - ComfyUI Built-in Node Documentation

A utility node for saving SVG vector graphics to files

The Save SVG node allows you to save SVG data from Recraft vector generation nodes as usable files in the filesystem. This is an essential component for handling and exporting vector graphics.

## [​](http://docs.comfy.org#node-function) Node Function

This node receives SVG vector data and saves it as standard SVG files in the filesystem. It supports automatic file naming and save path specification, allowing vector graphics to be opened and edited in other software.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionsvgSVG-SVG vector data to savefilename\_prefixstring”recraft”Prefix for the filenameoutput\_dirstring-Output directory, defaults to ComfyUI output folder at `ComfyUI/output/svg/`indexinteger-1Save index, -1 means save all generated SVGs

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionSVGSVGPasses through the input SVG data

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Recraft Text to Image Workflow Example**  
\
Recraft Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python
class SaveSVGNode:
    """
    Save SVG files on disk.
    """

    def __init__(self):
        self.output_dir = folder_paths.get_output_directory()
        self.type = "output"
        self.prefix_append = ""

    RETURN_TYPES = ()
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "save_svg"
    CATEGORY = "api node/image/Recraft"
    OUTPUT_NODE = True

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "svg": (RecraftIO.SVG,),
                "filename_prefix": ("STRING", {"default": "svg/ComfyUI", "tooltip": "The prefix for the file to save. This may include formatting information such as %date:yyyy-MM-dd% or %Empty Latent Image.width% to include values from nodes."})
            },
            "hidden": {
                "prompt": "PROMPT",
                "extra_pnginfo": "EXTRA_PNGINFO"
            }
        }

    def save_svg(self, svg: SVG, filename_prefix="svg/ComfyUI", prompt=None, extra_pnginfo=None):
        filename_prefix += self.prefix_append
        full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(filename_prefix, self.output_dir)
        results = list()

        # Prepare metadata JSON
        metadata_dict = {}
        if prompt is not None:
            metadata_dict["prompt"] = prompt
        if extra_pnginfo is not None:
            metadata_dict.update(extra_pnginfo)

        # Convert metadata to JSON string
        metadata_json = json.dumps(metadata_dict, indent=2) if metadata_dict else None

        for batch_number, svg_bytes in enumerate(svg.data):
            filename_with_batch_num = filename.replace("%batch_num%", str(batch_number))
            file = f"{filename_with_batch_num}_{counter:05}_.svg"

            # Read SVG content
            svg_bytes.seek(0)
            svg_content = svg_bytes.read().decode('utf-8')

            # Inject metadata if available
            if metadata_json:
                # Create metadata element with CDATA section
                metadata_element = f"""  <metadata>
    <![CDATA[
{metadata_json}
    ]]>
  </metadata>
"""
                # Insert metadata after opening svg tag using regex
                import re
                svg_content = re.sub(r'(<svg[^>]*>)', r'\1\n' + metadata_element, svg_content)

            # Write the modified SVG to file
            with open(os.path.join(full_output_folder, file), 'wb') as svg_file:
                svg_file.write(svg_content.encode('utf-8'))

            results.append({
                "filename": file,
                "subfolder": subfolder,
                "type": self.type
            })
            counter += 1
        return { "ui": { "images": results } }

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/save-svg.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image)

[Recraft Style - Realistic ImageA helper node for setting realistic photo style in Recraft image generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/recraft/save-svg.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
  - Ideogram
  - Stability AI
    
    - [Stability Stable Image Ultra](http://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra)
    - [Stability AI SD 3.5 Image](http://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image)
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Stability AI Stable Diffusion 3.5 - ComfyUI Native Node Documentation

# Stability AI Stable Diffusion 3.5 - ComfyUI Native Node Documentation

A node that generates high-quality images using Stability AI Stable Diffusion 3.5 model

The Stability AI Stable Diffusion 3.5 Image node uses Stability AI’s Stable Diffusion 3.5 API to generate high-quality images. It supports both text-to-image and image-to-image generation, capable of creating detailed visual content from text prompts.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionpromptstring""What you want to see in the output image. Strong, descriptive prompts that clearly define elements, colors and themes will yield better resultsmodelselect-Choose which Stability SD 3.5 model to useaspect\_ratioselect”1:1”Width to height ratio of generated imagestyle\_presetselect”None”Optional preset style for the desired imagecfg\_scalefloat4.0How strictly the diffusion process adheres to the prompt text (higher values keep your image closer to your prompt). Range: 1.0 - 10.0, Step: 0.1seedinteger0Random seed for noise generation (0-4294967294)

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDefaultDescriptionimageimage-Input image. When provided, the node switches to image-to-image modenegative\_promptstring""Keywords of what you don’t want to see in the output image. This is an advanced featureimage\_denoisefloat0.5Denoising strength for input image. 0.0 yields image identical to input, 1.0 is as if no image was provided at all. Range: 0.0 - 1.0, Step: 0.01. Only effective when image is provided

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Stability AI Stable Diffusion 3.5 Image Workflow Example**  
\
Stability AI Stable Diffusion 3.5 Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image)

## [​](http://docs.comfy.org#notes) Notes

- When an input image is provided, the node switches from text-to-image mode to image-to-image mode
- In image-to-image mode, aspect ratio parameters are ignored
- Mode selection automatically switches based on whether an image is provided:
  
  - No image provided: text-to-image mode
  - Image provided: image-to-image mode
- If style\_preset is set to “None”, no preset style will be applied

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-07)]

```python
class StabilityStableImageSD_3_5Node:
    """
    Generates images synchronously based on prompt and resolution.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Stability AI"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "What you wish to see in the output image. A strong, descriptive prompt that clearly defines elements, colors, and subjects will lead to better results."
                    },
                ),
                "model": ([x.value for x in Stability_SD3_5_Model],),
                "aspect_ratio": ([x.value for x in StabilityAspectRatio],
                    {
                        "default": StabilityAspectRatio.ratio_1_1,
                        "tooltip": "Aspect ratio of generated image.",
                    },
                ),
                "style_preset": (get_stability_style_presets(),
                    {
                        "tooltip": "Optional desired style of generated image.",
                    },
                ),
                "cfg_scale": (
                    IO.FLOAT,
                    {
                        "default": 4.0,
                        "min": 1.0,
                        "max": 10.0,
                        "step": 0.1,
                        "tooltip": "How strictly the diffusion process adheres to the prompt text (higher values keep your image closer to your prompt)",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 4294967294,
                        "control_after_generate": True,
                        "tooltip": "The random seed used for creating the noise.",
                    },
                ),
            },
            "optional": {
                "image": (IO.IMAGE,),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "Keywords of what you do not wish to see in the output image. This is an advanced feature."
                    },
                ),
                "image_denoise": (
                    IO.FLOAT,
                    {
                        "default": 0.5,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Denoise of input image; 0.0 yields image identical to input, 1.0 is as if no image was provided at all.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(self, model: str, prompt: str, aspect_ratio: str, style_preset: str, seed: int, cfg_scale: float,
                 negative_prompt: str=None, image: torch.Tensor = None, image_denoise: float=None,
                 auth_token=None):
        validate_string(prompt, strip_whitespace=False)
        # prepare image binary if image present
        image_binary = None
        mode = Stability_SD3_5_GenerationMode.text_to_image
        if image is not None:
            image_binary = tensor_to_bytesio(image, total_pixels=1504*1504).read()
            mode = Stability_SD3_5_GenerationMode.image_to_image
            aspect_ratio = None
        else:
            image_denoise = None

        if not negative_prompt:
            negative_prompt = None
        if style_preset == "None":
            style_preset = None

        files = {
            "image": image_binary
        }

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/stability/v2beta/stable-image/generate/sd3",
                method=HttpMethod.POST,
                request_model=StabilityStable3_5Request,
                response_model=StabilityStableUltraResponse,
            ),
            request=StabilityStable3_5Request(
                prompt=prompt,
                negative_prompt=negative_prompt,
                aspect_ratio=aspect_ratio,
                seed=seed,
                strength=image_denoise,
                style_preset=style_preset,
                cfg_scale=cfg_scale,
                model=model,
                mode=mode,
            ),
            files=files,
            content_type="multipart/form-data",
            auth_token=auth_token,
        )
        response_api = operation.execute()

        if response_api.finish_reason != "SUCCESS":
            raise Exception(f"Stable Diffusion 3.5 Image generation failed: {response_api.finish_reason}.")

        image_data = base64.b64decode(response_api.image)
        returned_image = bytesio_to_image_tensor(BytesIO(image_data))

        return (returned_image,)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra)

[OpenAI GPT Image 1Node for generating images using OpenAI's GPT-4 Vision model  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-gpt-image1)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Notes](http://docs.comfy.org#notes)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image.md -->


<!-- BEGIN Development/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
  - Ideogram
  - Stability AI
    
    - [Stability Stable Image Ultra](http://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra)
    - [Stability AI SD 3.5 Image](http://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image)
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Stability Stable Image Ultra - ComfyUI Native Node Documentation

# Stability Stable Image Ultra - ComfyUI Native Node Documentation

A node that generates high-quality images using Stability AI’s ultra stable diffusion model

The Stability Stable Image Ultra node uses Stability AI’s Stable Diffusion Ultra API to generate high-quality images. It supports both text-to-image and image-to-image generation, creating detailed and artistic visuals from text prompts.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionpromptstring""Text description of what you want to generate. Better results come from clear, descriptive prompts that specify elements, colors and themes. You can control word importance using `(word:weight)` format, where weight is 0-1. For example: `The sky was (blue:0.3) and (green:0.8)` makes the sky more green than blue.aspect\_ratioselect”1:1”Width to height ratio of output imagestyle\_presetselect”None”Optional preset style for generated imageseedinteger0Random seed for noise generation (0-4294967294)

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDefaultDescriptionimageimage-Input image for image-to-image generationnegative\_promptstring""Describes what you don’t want in the output image. This is an advanced featureimage\_denoisefloat0.5Denoising strength for input image (0.0-1.0). 0.0 keeps input image unchanged, 1.0 is like having no input image

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image

## [​](http://docs.comfy.org#notes) Notes

- image\_denoise has no effect when no input image is provided
- No preset style is applied when style\_preset is “None”
- For image-to-image, input images are converted to the proper format before API submission

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class StabilityStableImageUltraNode:
    """
    Generates images synchronously based on prompt and resolution.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/stability"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "What you wish to see in the output image. A strong, descriptive prompt that clearly defines" +
                                    "What you wish to see in the output image. A strong, descriptive prompt that clearly defines" +
                                    "elements, colors, and subjects will lead to better results. " +
                                    "To control the weight of a given word use the format `(word:weight)`," +
                                    "where `word` is the word you'd like to control the weight of and `weight`" +
                                    "is a value between 0 and 1. For example: `The sky was a crisp (blue:0.3) and (green:0.8)`" +
                                    "would convey a sky that was blue and green, but more green than blue."
                    },
                ),
                "aspect_ratio": ([x.value for x in StabilityAspectRatio],
                    {
                        "default": StabilityAspectRatio.ratio_1_1,
                        "tooltip": "Aspect ratio of generated image.",
                    },
                ),
                "style_preset": (get_stability_style_presets(),
                    {
                        "tooltip": "Optional desired style of generated image.",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 4294967294,
                        "control_after_generate": True,
                        "tooltip": "The random seed used for creating the noise.",
                    },
                ),
            },
            "optional": {
                "image": (IO.IMAGE,),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "A blurb of text describing what you do not wish to see in the output image. This is an advanced feature."
                    },
                ),
                "image_denoise": (
                    IO.FLOAT,
                    {
                        "default": 0.5,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Denoise of input image; 0.0 yields image identical to input, 1.0 is as if no image was provided at all.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(self, prompt: str, aspect_ratio: str, style_preset: str, seed: int,
                 negative_prompt: str=None, image: torch.Tensor = None, image_denoise: float=None,
                 auth_token=None):
        # prepare image binary if image present
        image_binary = None
        if image is not None:
            image_binary = tensor_to_bytesio(image, 1504 * 1504).read()
        else:
            image_denoise = None

        if not negative_prompt:
            negative_prompt = None
        if style_preset == "None":
            style_preset = None

        files = {
            "image": image_binary
        }

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/stability/v2beta/stable-image/generate/ultra",
                method=HttpMethod.POST,
                request_model=StabilityStableUltraRequest,
                response_model=StabilityStableUltraResponse,
            ),
            request=StabilityStableUltraRequest(
                prompt=prompt,
                negative_prompt=negative_prompt,
                aspect_ratio=aspect_ratio,
                seed=seed,
                strength=image_denoise,
                style_preset=style_preset,
            ),
            files=files,
            content_type="multipart/form-data",
            auth_token=auth_token,
        )
        response_api = operation.execute()

        if response_api.finish_reason != "SUCCESS":
            raise Exception(f"Stable Image Ultra generation failed: {response_api.finish_reason}.")

        image_data = base64.b64decode(response_api.image)
        returned_image = bytesio_to_image_tensor(BytesIO(image_data))

        return (returned_image,)


```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v1)

[Stability AI SD 3.5 ImageA node that generates high-quality images using Stability AI Stable Diffusion 3.5 model  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Notes](http://docs.comfy.org#notes)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra.md -->


<!-- BEGIN Development/built-in-nodes/api-node/video/google/google-veo2-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
    
    - [Google Veo2 Video](http://docs.comfy.org/built-in-nodes/api-node/video/google/google-veo2-video)
  - Kling
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Google Veo2 Video - ComfyUI Native Node Documentation

# Google Veo2 Video - ComfyUI Native Node Documentation

A node that generates videos from text descriptions using Google’s Veo2 technology

The Google Veo2 Video node generates high-quality videos from text descriptions using Google’s Veo2 API technology, converting text prompts into dynamic video content.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text description of the video content to generateaspect\_ratioselect”16:9”Output video aspect ratio, “16:9” or “9:16”negative\_promptstring""Text describing what to avoid in the videoduration\_secondsinteger5Video duration, 5-8 secondsenhance\_promptbooleanTrueWhether to use AI to enhance the promptperson\_generationselect”ALLOW”Allow or block person generation, “ALLOW” or “BLOCK”seedinteger0Random seed, 0 means randomly generated

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDefaultDescriptionimageimageNoneOptional reference image to guide video creation

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOvideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class VeoVideoGenerationNode(ComfyNodeABC):
    """
    Generates videos from text prompts using Google's Veo API.

    This node can create videos from text descriptions and optional image inputs,
    with control over parameters like aspect ratio, duration, and more.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Text description of the video",
                    },
                ),
                "aspect_ratio": (
                    IO.COMBO,
                    {
                        "options": ["16:9", "9:16"],
                        "default": "16:9",
                        "tooltip": "Aspect ratio of the output video",
                    },
                ),
            },
            "optional": {
                "negative_prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Negative text prompt to guide what to avoid in the video",
                    },
                ),
                "duration_seconds": (
                    IO.INT,
                    {
                        "default": 5,
                        "min": 5,
                        "max": 8,
                        "step": 1,
                        "display": "number",
                        "tooltip": "Duration of the output video in seconds",
                    },
                ),
                "enhance_prompt": (
                    IO.BOOLEAN,
                    {
                        "default": True,
                        "tooltip": "Whether to enhance the prompt with AI assistance",
                    }
                ),
                "person_generation": (
                    IO.COMBO,
                    {
                        "options": ["ALLOW", "BLOCK"],
                        "default": "ALLOW",
                        "tooltip": "Whether to allow generating people in the video",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFF,
                        "step": 1,
                        "display": "number",
                        "control_after_generate": True,
                        "tooltip": "Seed for video generation (0 for random)",
                    },
                ),
                "image": (IO.IMAGE, {
                    "default": None,
                    "tooltip": "Optional reference image to guide video generation",
                }),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    RETURN_TYPES = (IO.VIDEO,)
    FUNCTION = "generate_video"
    CATEGORY = "api node/video/Veo"
    DESCRIPTION = "Generates videos from text prompts using Google's Veo API"
    API_NODE = True

    def generate_video(
        self,
        prompt,
        aspect_ratio="16:9",
        negative_prompt="",
        duration_seconds=5,
        enhance_prompt=True,
        person_generation="ALLOW",
        seed=0,
        image=None,
        auth_token=None,
    ):
        # Prepare the instances for the request
        instances = []

        instance = {
            "prompt": prompt
        }

        # Add image if provided
        if image is not None:
            image_base64 = convert_image_to_base64(image)
            if image_base64:
                instance["image"] = {
                    "bytesBase64Encoded": image_base64,
                    "mimeType": "image/png"
                }

        instances.append(instance)

        # Create parameters dictionary
        parameters = {
            "aspectRatio": aspect_ratio,
            "personGeneration": person_generation,
            "durationSeconds": duration_seconds,
            "enhancePrompt": enhance_prompt,
        }

        # Add optional parameters if provided
        if negative_prompt:
            parameters["negativePrompt"] = negative_prompt
        if seed > 0:
            parameters["seed"] = seed

        # Initial request to start video generation
        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/veo/generate",
                method=HttpMethod.POST,
                request_model=Veo2GenVidRequest,
                response_model=Veo2GenVidResponse
            ),
            request=Veo2GenVidRequest(
                instances=instances,
                parameters=parameters
            ),
            auth_token=auth_token
        )

        initial_response = initial_operation.execute()
        operation_name = initial_response.name

        logging.info(f"Veo generation started with operation name: {operation_name}")

        # Define status extractor function
        def status_extractor(response):
            # Only return "completed" if the operation is done, regardless of success or failure
            # We'll check for errors after polling completes
            return "completed" if response.done else "pending"

        # Define progress extractor function
        def progress_extractor(response):
            # Could be enhanced if the API provides progress information
            return None

        # Define the polling operation
        poll_operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path="/proxy/veo/poll",
                method=HttpMethod.POST,
                request_model=Veo2GenVidPollRequest,
                response_model=Veo2GenVidPollResponse
            ),
            completed_statuses=["completed"],
            failed_statuses=[],  # No failed statuses, we'll handle errors after polling
            status_extractor=status_extractor,
            progress_extractor=progress_extractor,
            request=Veo2GenVidPollRequest(
                operationName=operation_name
            ),
            auth_token=auth_token,
            poll_interval=5.0
        )

        # Execute the polling operation
        poll_response = poll_operation.execute()

        # Now check for errors in the final response
        # Check for error in poll response
        if hasattr(poll_response, 'error') and poll_response.error:
            error_message = f"Veo API error: {poll_response.error.message} (code: {poll_response.error.code})"
            logging.error(error_message)
            raise Exception(error_message)

        # Check for RAI filtered content
        if (hasattr(poll_response.response, 'raiMediaFilteredCount') and
            poll_response.response.raiMediaFilteredCount > 0):

            # Extract reason message if available
            if (hasattr(poll_response.response, 'raiMediaFilteredReasons') and
                poll_response.response.raiMediaFilteredReasons):
                reason = poll_response.response.raiMediaFilteredReasons[0]
                error_message = f"Content filtered by Google's Responsible AI practices: {reason} ({poll_response.response.raiMediaFilteredCount} videos filtered.)"
            else:
                error_message = f"Content filtered by Google's Responsible AI practices ({poll_response.response.raiMediaFilteredCount} videos filtered.)"

            logging.error(error_message)
            raise Exception(error_message)

        # Extract video data
        video_data = None
        if poll_response.response and hasattr(poll_response.response, 'videos') and poll_response.response.videos and len(poll_response.response.videos) > 0:
            video = poll_response.response.videos[0]

            # Check if video is provided as base64 or URL
            if hasattr(video, 'bytesBase64Encoded') and video.bytesBase64Encoded:
                # Decode base64 string to bytes
                video_data = base64.b64decode(video.bytesBase64Encoded)
            elif hasattr(video, 'gcsUri') and video.gcsUri:
                # Download from URL
                video_url = video.gcsUri
                video_response = requests.get(video_url)
                video_data = video_response.content
            else:
                raise Exception("Video returned but no data or URL was provided")
        else:
            raise Exception("Video generation completed but no video was returned")

        if not video_data:
            raise Exception("No video data was returned")

        logging.info("Video generation completed successfully")

        # Convert video data to BytesIO object
        video_io = io.BytesIO(video_data)

        # Return VideoFromFile object
        return (VideoFromFile(video_io),)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/google/google-veo2-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-text-to-video)

[Kling Camera ControlsA node that provides camera control parameters for Kling video generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/video/google/google-veo2-video.md -->


<!-- BEGIN Development/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
    
    - [Kling Camera Controls](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)
    - [Kling Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)
    - [Kling Image to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)
    - [Kling Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)
    - [Kling Start-End Frame to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)
    - [Kling Text to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Kling Image to Video (Camera Control) - ComfyUI Built-in Node

# Kling Image to Video (Camera Control) - ComfyUI Built-in Node

Image to video conversion node with camera control features

The Kling Image to Video (Camera Control) node converts static images into videos with professional camera movements. It supports camera controls like zoom, rotation, pan, tilt and first-person view while maintaining focus on the original image content.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionstart\_frameImage-Input image to convert to videopromptString""Text prompt describing video action and contentnegative\_promptString""Elements to avoid in the videocfg\_scaleFloat7.0Controls how closely to follow the promptaspect\_ratioSelect16:9Output video aspect ratio

### [​](http://docs.comfy.org#camera-control-parameters) Camera Control Parameters

ParameterTypeDescriptioncamera\_controlCameraControlCamera control config from Kling Camera Controls node

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class KlingCameraControlI2VNode(KlingImage2VideoNode):
    """
    Kling Image to Video Camera Control Node. This node is a image to video node, but it supports controlling the camera.
    Duration, mode, and model_name request fields are hard-coded because camera control is only supported in pro mode with the kling-v1-5 model at 5s duration as of 2025-05-02.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "start_frame": model_field_to_node_input(
                    IO.IMAGE, KlingImage2VideoRequest, "image"
                ),
                "prompt": model_field_to_node_input(
                    IO.STRING, KlingImage2VideoRequest, "prompt", multiline=True
                ),
                "negative_prompt": model_field_to_node_input(
                    IO.STRING,
                    KlingImage2VideoRequest,
                    "negative_prompt",
                    multiline=True,
                ),
                "cfg_scale": model_field_to_node_input(
                    IO.FLOAT, KlingImage2VideoRequest, "cfg_scale"
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "aspect_ratio",
                    enum_type=AspectRatio,
                ),
                "camera_control": (
                    "CAMERA_CONTROL",
                    {
                        "tooltip": "Can be created using the Kling Camera Controls node. Controls the camera movement and motion during the video generation.",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    DESCRIPTION = "Transform still images into cinematic videos with professional camera movements that simulate real-world cinematography. Control virtual camera actions including zoom, rotation, pan, tilt, and first-person view, while maintaining focus on your original image."

    def api_call(
        self,
        start_frame: torch.Tensor,
        prompt: str,
        negative_prompt: str,
        cfg_scale: float,
        aspect_ratio: str,
        camera_control: CameraControl,
        auth_token: Optional[str] = None,
    ):
        return super().api_call(
            model_name="kling-v1-5",
            start_frame=start_frame,
            cfg_scale=cfg_scale,
            mode="pro",
            aspect_ratio=aspect_ratio,
            duration="5",
            prompt=prompt,
            negative_prompt=negative_prompt,
            camera_control=camera_control,
            auth_token=auth_token,
        )



```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)

[Kling Image to VideoA node that converts static images to dynamic videos using Kling's AI technology  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Camera Control Parameters](http://docs.comfy.org#camera-control-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v.md -->


<!-- BEGIN Development/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
    
    - [Kling Camera Controls](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)
    - [Kling Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)
    - [Kling Image to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)
    - [Kling Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)
    - [Kling Start-End Frame to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)
    - [Kling Text to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Kling Text to Video (Camera Control) - ComfyUI Built-in Node

# Kling Text to Video (Camera Control) - ComfyUI Built-in Node

A text to video generation node with camera control features

The Kling Text to Video (Camera Control) node converts text into videos with professional camera movements. It extends the standard Kling Text to Video node by adding camera control capabilities.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptString""Text prompt describing video contentnegative\_promptString""Elements to avoid in the videocfg\_scaleFloat7.0Controls how closely to follow the promptaspect\_ratioSelect”16:9”Output video aspect ratiocamera\_controlCAMERA\_CONTROL-Camera settings from Kling Camera Controls node

### [​](http://docs.comfy.org#fixed-parameters) Fixed Parameters

Note: The following parameters are fixed and cannot be changed:

- Model: kling-v1-5
- Mode: pro
- Duration: 5 seconds

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class KlingCameraControlT2VNode(KlingTextToVideoNode):
    """
    Kling Text to Video Camera Control Node. This node is a text to video node, but it supports controlling the camera.
    Duration, mode, and model_name request fields are hard-coded because camera control is only supported in pro mode with the kling-v1-5 model at 5s duration as of 2025-05-02.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": model_field_to_node_input(
                    IO.STRING, KlingText2VideoRequest, "prompt", multiline=True
                ),
                "negative_prompt": model_field_to_node_input(
                    IO.STRING,
                    KlingText2VideoRequest,
                    "negative_prompt",
                    multiline=True,
                ),
                "cfg_scale": model_field_to_node_input(
                    IO.FLOAT, KlingText2VideoRequest, "cfg_scale"
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.COMBO,
                    KlingText2VideoRequest,
                    "aspect_ratio",
                    enum_type=AspectRatio,
                ),
                "camera_control": (
                    "CAMERA_CONTROL",
                    {
                        "tooltip": "Can be created using the Kling Camera Controls node. Controls the camera movement and motion during the video generation.",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    DESCRIPTION = "Transform text into cinematic videos with professional camera movements that simulate real-world cinematography. Control virtual camera actions including zoom, rotation, pan, tilt, and first-person view, while maintaining focus on your original text."

    def api_call(
        self,
        prompt: str,
        negative_prompt: str,
        cfg_scale: float,
        aspect_ratio: str,
        camera_control: Optional[CameraControl] = None,
        auth_token: Optional[str] = None,
    ):
        return super().api_call(
            model_name="kling-v1-5",
            cfg_scale=cfg_scale,
            mode="pro",
            aspect_ratio=aspect_ratio,
            duration="5",
            prompt=prompt,
            negative_prompt=negative_prompt,
            camera_control=camera_control,
            auth_token=auth_token,
        )



```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)

[Luma Text to VideoA node that converts text descriptions to videos using Luma AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Fixed Parameters](http://docs.comfy.org#fixed-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v.md -->


<!-- BEGIN Development/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
    
    - [Kling Camera Controls](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)
    - [Kling Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)
    - [Kling Image to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)
    - [Kling Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)
    - [Kling Start-End Frame to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)
    - [Kling Text to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Kling Camera Controls - ComfyUI Built-in Node Documentation

# Kling Camera Controls - ComfyUI Built-in Node Documentation

A node that provides camera control parameters for Kling video generation

The Kling Camera Controls node defines virtual camera behavior parameters to control camera movement and view changes during Kling video generation.

## [​](http://docs.comfy.org#parameters) Parameters

ParameterTypeDefaultDescriptioncamera\_control\_typeSelect”simple”Preset camera motion types. simple: Custom camera movement; down\_back: Camera moves down and back; forward\_up: Camera moves forward and up; right\_turn\_forward: Rotate right and move forward; left\_turn\_forward: Rotate left and move forwardhorizontal\_movementFloat0Controls camera movement on horizontal axis (x-axis). Negative values move left, positive values move rightvertical\_movementFloat0Controls camera movement on vertical axis (y-axis). Negative values move down, positive values move uppanFloat0.5Controls camera rotation in vertical plane (x-axis). Negative values rotate down, positive values rotate uptiltFloat0Controls camera rotation in horizontal plane (y-axis). Negative values rotate left, positive values rotate rightrollFloat0Controls camera roll amount (z-axis). Negative values rotate counterclockwise, positive values rotate clockwisezoomFloat0Controls camera focal length. Negative values narrow field of view, positive values widen it

**Note**: At least one non-zero camera control parameter is required for the effect to work.

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptioncamera\_controlCAMERA\_CONTROLConfiguration object with camera settings

**Note**: Not all model and mode combinations support camera control. Please check the Kling API documentation for details.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class KlingCameraControls(KlingNodeBase):
    """Kling Camera Controls Node"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "camera_control_type": (
                    IO.COMBO,
                    {
                        "options": [
                            camera_control_type.value
                            for camera_control_type in CameraType
                        ],
                        "default": "simple",
                        "tooltip": "Predefined camera movements type. simple: Customizable camera movement. down_back: Camera descends and moves backward. forward_up: Camera moves forward and tilts up. right_turn_forward: Rotate right and move forward. left_turn_forward: Rotate left and move forward.",
                    },
                ),
                "horizontal_movement": get_camera_control_input_config(
                    "Controls camera's movement along horizontal axis (x-axis). Negative indicates left, positive indicates right"
                ),
                "vertical_movement": get_camera_control_input_config(
                    "Controls camera's movement along vertical axis (y-axis). Negative indicates downward, positive indicates upward."
                ),
                "pan": get_camera_control_input_config(
                    "Controls camera's rotation in vertical plane (x-axis). Negative indicates downward rotation, positive indicates upward rotation.",
                    default=0.5,
                ),
                "tilt": get_camera_control_input_config(
                    "Controls camera's rotation in horizontal plane (y-axis). Negative indicates left rotation, positive indicates right rotation.",
                ),
                "roll": get_camera_control_input_config(
                    "Controls camera's rolling amount (z-axis). Negative indicates counterclockwise, positive indicates clockwise.",
                ),
                "zoom": get_camera_control_input_config(
                    "Controls change in camera's focal length. Negative indicates narrower field of view, positive indicates wider field of view.",
                ),
            }
        }

    DESCRIPTION = "Kling Camera Controls Node. Not all model and mode combinations support camera control. Please refer to the Kling API documentation for more information."
    RETURN_TYPES = ("CAMERA_CONTROL",)
    RETURN_NAMES = ("camera_control",)
    FUNCTION = "main"

    @classmethod
    def VALIDATE_INPUTS(
        cls,
        horizontal_movement: float,
        vertical_movement: float,
        pan: float,
        tilt: float,
        roll: float,
        zoom: float,
    ) -> bool | str:
        if not is_valid_camera_control_configs(
            [
                horizontal_movement,
                vertical_movement,
                pan,
                tilt,
                roll,
                zoom,
            ]
        ):
            return "Invalid camera control configs: at least one of the values must be non-zero"
        return True

    def main(
        self,
        camera_control_type: str,
        horizontal_movement: float,
        vertical_movement: float,
        pan: float,
        tilt: float,
        roll: float,
        zoom: float,
    ) -> tuple[CameraControl]:
        return (
            CameraControl(
                type=CameraType(camera_control_type),
                config=CameraConfig(
                    horizontal=horizontal_movement,
                    vertical=vertical_movement,
                    pan=pan,
                    roll=roll,
                    tilt=tilt,
                    zoom=zoom,
                ),
            ),
        )
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/google/google-veo2-video)

[Kling Text to VideoA node that converts text descriptions into videos using Kling's AI technology  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls.md -->


<!-- BEGIN Development/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
    
    - [Kling Camera Controls](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)
    - [Kling Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)
    - [Kling Image to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)
    - [Kling Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)
    - [Kling Start-End Frame to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)
    - [Kling Text to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Kling Image to Video - ComfyUI Built-in Node

# Kling Image to Video - ComfyUI Built-in Node

A node that converts static images to dynamic videos using Kling’s AI technology

The Kling Image to Video node converts static images into dynamic video content using Kling’s image-to-video API.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

All parameters below are required:

ParameterTypeDefaultDescriptionstart\_frameImage-Input source imagepromptString""Text prompt describing video action and contentnegative\_promptString""Elements to avoid in the videocfg\_scaleFloat7.0Controls how closely to follow the promptmodel\_nameSelect”kling-v1-5”Model type to useaspect\_ratioSelect”16:9”Output video aspect ratiodurationSelect”5s”Generated video durationmodeSelect”pro”Video generation mode

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated videovideo\_idStringUnique video identifierdurationStringActual video duration

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class KlingImage2VideoNode(KlingNodeBase):
    """Kling Image to Video Node"""

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "start_frame": model_field_to_node_input(
                    IO.IMAGE, KlingImage2VideoRequest, "image"
                ),
                "prompt": model_field_to_node_input(
                    IO.STRING, KlingImage2VideoRequest, "prompt", multiline=True
                ),
                "negative_prompt": model_field_to_node_input(
                    IO.STRING,
                    KlingImage2VideoRequest,
                    "negative_prompt",
                    multiline=True,
                ),
                "model_name": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "model_name",
                    enum_type=KlingVideoGenModelName,
                ),
                "cfg_scale": model_field_to_node_input(
                    IO.FLOAT, KlingImage2VideoRequest, "cfg_scale"
                ),
                "mode": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "mode",
                    enum_type=KlingVideoGenMode,
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "aspect_ratio",
                    enum_type=KlingVideoGenAspectRatio,
                ),
                "duration": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "duration",
                    enum_type=KlingVideoGenDuration,
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = ("VIDEO", "STRING", "STRING")
    RETURN_NAMES = ("VIDEO", "video_id", "duration")
    DESCRIPTION = "Kling Image to Video Node"

    def get_response(self, task_id: str, auth_token: str) -> KlingImage2VideoResponse:
        return poll_until_finished(
            auth_token,
            ApiEndpoint(
                path=f"{PATH_IMAGE_TO_VIDEO}/{task_id}",
                method=HttpMethod.GET,
                request_model=KlingImage2VideoRequest,
                response_model=KlingImage2VideoResponse,
            ),
        )

    def api_call(
        self,
        start_frame: torch.Tensor,
        prompt: str,
        negative_prompt: str,
        model_name: str,
        cfg_scale: float,
        mode: str,
        aspect_ratio: str,
        duration: str,
        camera_control: Optional[KlingCameraControl] = None,
        end_frame: Optional[torch.Tensor] = None,
        auth_token: Optional[str] = None,
    ) -> tuple[VideoFromFile]:
        validate_prompts(prompt, negative_prompt, MAX_PROMPT_LENGTH_I2V)
        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=PATH_IMAGE_TO_VIDEO,
                method=HttpMethod.POST,
                request_model=KlingImage2VideoRequest,
                response_model=KlingImage2VideoResponse,
            ),
            request=KlingImage2VideoRequest(
                model_name=KlingVideoGenModelName(model_name),
                image=tensor_to_base64_string(start_frame),
                image_tail=(
                    tensor_to_base64_string(end_frame)
                    if end_frame is not None
                    else None
                ),
                prompt=prompt,
                negative_prompt=negative_prompt if negative_prompt else None,
                cfg_scale=cfg_scale,
                mode=KlingVideoGenMode(mode),
                aspect_ratio=KlingVideoGenAspectRatio(aspect_ratio),
                duration=KlingVideoGenDuration(duration),
                camera_control=camera_control,
            ),
            auth_token=auth_token,
        )

        task_creation_response = initial_operation.execute()
        validate_task_creation_response(task_creation_response)
        task_id = task_creation_response.data.task_id

        final_response = self.get_response(task_id, auth_token)
        validate_video_result_response(final_response)

        video = get_video_from_response(final_response)
        return video_result_to_node_output(video)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)

[Kling Start-End Frame to VideoA node that creates smooth video transitions between start and end frames using Kling's AI technology  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video.md -->


<!-- BEGIN Development/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
    
    - [Kling Camera Controls](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)
    - [Kling Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)
    - [Kling Image to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)
    - [Kling Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)
    - [Kling Start-End Frame to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)
    - [Kling Text to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Kling Start-End Frame to Video - ComfyUI Built-in Node

# Kling Start-End Frame to Video - ComfyUI Built-in Node

A node that creates smooth video transitions between start and end frames using Kling’s AI technology

The Kling Start-End Frame to Video node lets you create smooth video transitions between two images. It automatically generates all the intermediate frames to produce a fluid transformation.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDescriptionstart\_frameImageStarting image for the videoend\_frameImageEnding image for the videopromptStringText describing video content and transitionnegative\_promptStringElements to avoid in the videocfg\_scaleFloatControls how closely to follow the promptaspect\_ratioSelectOutput video aspect ratiomodeSelectVideo generation settings (mode/duration/model)

### [​](http://docs.comfy.org#mode-options) Mode Options

Available mode combinations:

- standard mode / 5s duration / kling-v1
- standard mode / 5s duration / kling-v1-5
- pro mode / 5s duration / kling-v1
- pro mode / 5s duration / kling-v1-5
- pro mode / 5s duration / kling-v1-6
- pro mode / 10s duration / kling-v1-5
- pro mode / 10s duration / kling-v1-6

Default: “pro mode / 5s duration / kling-v1”

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#how-it-works) How It Works

The node analyzes the start and end frames to create a smooth transition sequence between them. It sends the images and parameters to Kling’s API server, which generates all necessary intermediate frames for a fluid transformation.

The transition style and content can be guided using prompts, while negative prompts help avoid unwanted elements.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python


class KlingStartEndFrameNode(KlingImage2VideoNode):
    """
    Kling First Last Frame Node. This node allows creation of a video from a first and last frame. It calls the normal image to video endpoint, but only allows the subset of input options that support the `image_tail` request field.
    """

    @staticmethod
    def get_mode_string_mapping() -> dict[str, tuple[str, str, str]]:
        """
        Returns a mapping of mode strings to their corresponding (mode, duration, model_name) tuples.
        Only includes config combos that support the `image_tail` request field.
        """
        return {
            "standard mode / 5s duration / kling-v1": ("std", "5", "kling-v1"),
            "standard mode / 5s duration / kling-v1-5": ("std", "5", "kling-v1-5"),
            "pro mode / 5s duration / kling-v1": ("pro", "5", "kling-v1"),
            "pro mode / 5s duration / kling-v1-5": ("pro", "5", "kling-v1-5"),
            "pro mode / 5s duration / kling-v1-6": ("pro", "5", "kling-v1-6"),
            "pro mode / 10s duration / kling-v1-5": ("pro", "10", "kling-v1-5"),
            "pro mode / 10s duration / kling-v1-6": ("pro", "10", "kling-v1-6"),
        }

    @classmethod
    def INPUT_TYPES(s):
        modes = list(KlingStartEndFrameNode.get_mode_string_mapping().keys())
        return {
            "required": {
                "start_frame": model_field_to_node_input(
                    IO.IMAGE, KlingImage2VideoRequest, "image"
                ),
                "end_frame": model_field_to_node_input(
                    IO.IMAGE, KlingImage2VideoRequest, "image_tail"
                ),
                "prompt": model_field_to_node_input(
                    IO.STRING, KlingImage2VideoRequest, "prompt", multiline=True
                ),
                "negative_prompt": model_field_to_node_input(
                    IO.STRING,
                    KlingImage2VideoRequest,
                    "negative_prompt",
                    multiline=True,
                ),
                "cfg_scale": model_field_to_node_input(
                    IO.FLOAT, KlingImage2VideoRequest, "cfg_scale"
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "aspect_ratio",
                    enum_type=AspectRatio,
                ),
                "mode": (
                    modes,
                    {
                        "default": modes[2],
                        "tooltip": "The configuration to use for the video generation following the format: mode / duration / model_name.",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    DESCRIPTION = "Generate a video sequence that transitions between your provided start and end images. The node creates all frames in between, producing a smooth transformation from the first frame to the last."

    def parse_inputs_from_mode(self, mode: str) -> tuple[str, str, str]:
        """Parses the mode input into a tuple of (model_name, duration, mode)."""
        return KlingStartEndFrameNode.get_mode_string_mapping()[mode]

    def api_call(
        self,
        start_frame: torch.Tensor,
        end_frame: torch.Tensor,
        prompt: str,
        negative_prompt: str,
        cfg_scale: float,
        aspect_ratio: str,
        mode: str,
        auth_token: Optional[str] = None,
    ):
        mode, duration, model_name = self.parse_inputs_from_mode(mode)
        return super().api_call(
            prompt=prompt,
            negative_prompt=negative_prompt,
            model_name=model_name,
            start_frame=start_frame,
            cfg_scale=cfg_scale,
            mode=mode,
            aspect_ratio=aspect_ratio,
            duration=duration,
            end_frame=end_frame,
            auth_token=auth_token,
        )


```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)

[Kling Text to Video (Camera Control)A text to video generation node with camera control features  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Mode Options](http://docs.comfy.org#mode-options)
- [Output](http://docs.comfy.org#output)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video.md -->


<!-- BEGIN Development/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
    
    - [Kling Camera Controls](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)
    - [Kling Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)
    - [Kling Image to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)
    - [Kling Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)
    - [Kling Start-End Frame to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)
    - [Kling Text to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Kling Text to Video - ComfyUI Built-in Node

# Kling Text to Video - ComfyUI Built-in Node

A node that converts text descriptions into videos using Kling’s AI technology

The Kling Text to Video node connects to Kling’s API service to generate videos from text descriptions. Users simply provide descriptive text to create corresponding video content.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionpromptString""Text prompt describing desired video contentnegative\_promptString""Elements to avoid in the videocfg\_scaleFloat7.0Controls how closely to follow the promptmodel\_nameSelect”kling-v2-master”Video generation model to useaspect\_ratioSelectAspectRatio enumOutput video aspect ratiodurationSelectDuration enumLength of generated videomodeSelectMode enumVideo generation mode

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated videoKling IDStringTask identifierDuration (sec)StringVideo length in seconds

## [​](http://docs.comfy.org#how-it-works) How It Works

The node sends text prompts to Kling’s API server, which processes and returns the generated video. The process includes initial request and status polling. When complete, the node downloads and outputs the video.

Users can control the generation by adjusting parameters like negative prompts, configuration scale, and video properties. The system validates prompt length to ensure API compliance.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class KlingTextToVideoNode(KlingNodeBase):
    """Kling Text to Video Node"""

    @staticmethod
    def poll_for_task_status(task_id: str, auth_token: str) -> KlingText2VideoResponse:
        """Polls the Kling API endpoint until the task reaches a terminal state."""
        polling_operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"{PATH_TEXT_TO_VIDEO}/{task_id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=KlingText2VideoResponse,
            ),
            completed_statuses=[
                TaskStatus.succeed.value,
            ],
            failed_statuses=[TaskStatus.failed.value],
            status_extractor=lambda response: (
                response.data.task_status.value
                if response.data and response.data.task_status
                else None
            ),
            auth_token=auth_token,
        )
        return polling_operation.execute()

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": model_field_to_node_input(
                    IO.STRING, KlingText2VideoRequest, "prompt", multiline=True
                ),
                "negative_prompt": model_field_to_node_input(
                    IO.STRING, KlingText2VideoRequest, "negative_prompt", multiline=True
                ),
                "model_name": model_field_to_node_input(
                    IO.COMBO,
                    KlingText2VideoRequest,
                    "model_name",
                    enum_type=ModelName,
                    default="kling-v2-master",
                ),
                "cfg_scale": model_field_to_node_input(
                    IO.FLOAT, KlingText2VideoRequest, "cfg_scale"
                ),
                "mode": model_field_to_node_input(
                    IO.COMBO, KlingText2VideoRequest, "mode", enum_type=Mode
                ),
                "duration": model_field_to_node_input(
                    IO.COMBO, KlingText2VideoRequest, "duration", enum_type=Duration
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.COMBO,
                    KlingText2VideoRequest,
                    "aspect_ratio",
                    enum_type=AspectRatio,
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = ("VIDEO", "STRING", "STRING")
    RETURN_NAMES = ("VIDEO", "Kling ID", "Duration (sec)")
    DESCRIPTION = "Kling Text to Video Node"

    def api_call(
        self,
        prompt: str,
        negative_prompt: str,
        model_name: str,
        cfg_scale: float,
        mode: str,
        duration: int,
        aspect_ratio: str,
        camera_control: Optional[CameraControl] = None,
        auth_token: Optional[str] = None,
    ) -> tuple[VideoFromFile, str, str]:
        validate_prompts(prompt, negative_prompt, MAX_PROMPT_LENGTH_T2V)
        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=PATH_TEXT_TO_VIDEO,
                method=HttpMethod.POST,
                request_model=KlingText2VideoRequest,
                response_model=KlingText2VideoResponse,
            ),
            request=KlingText2VideoRequest(
                prompt=prompt if prompt else None,
                negative_prompt=negative_prompt if negative_prompt else None,
                duration=Duration(duration),
                mode=Mode(mode),
                model_name=ModelName(model_name),
                cfg_scale=cfg_scale,
                aspect_ratio=AspectRatio(aspect_ratio),
                camera_control=camera_control,
            ),
            auth_token=auth_token,
        )

        initial_response = initial_operation.execute()
        if not is_valid_initial_response(initial_response):
            error_msg = f"Kling initial request failed. Code: {initial_response.code}, Message: {initial_response.message}, Data: {initial_response.data}"
            logging.error(error_msg)
            raise KlingApiError(error_msg)

        task_id = initial_response.data.task_id
        final_response = self.poll_for_task_status(task_id, auth_token)
        if not is_valid_video_response(final_response):
            error_msg = (
                f"Kling task {task_id} succeeded but no video data found in response."
            )
            logging.error(error_msg)
            raise KlingApiError(error_msg)

        video = final_response.data.task_result.videos[0]
        logging.debug("Kling task %s succeeded. Video URL: %s", task_id, video.url)
        return (
            download_url_to_video_output(video.url),
            str(video.id),
            str(video.duration),
        )

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)

[Kling Image to Video (Camera Control)Image to video conversion node with camera control features  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Output](http://docs.comfy.org#output)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video.md -->


<!-- BEGIN Development/built-in-nodes/api-node/video/luma/luma-concepts.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
    
    - [Luma Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video)
    - [Luma Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video)
    - [Luma Concepts](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Concepts - ComfyUI Native Node Documentation

# Luma Concepts - ComfyUI Native Node Documentation

A helper node that provides concept guidance for Luma image generation

The Luma Concepts node allows you to apply predefined camera concepts to the Luma generation process, providing precise control over camera angles and perspectives without complex prompt descriptions.

## [​](http://docs.comfy.org#node-function) Node Function

This node serves as a helper tool for Luma generation nodes, enabling users to select and apply predefined camera concepts. These concepts include different shooting angles (like overhead or low angle), camera distances (like close-up or long shot), and movement styles (like push-in or follow). It simplifies the creative workflow by providing an intuitive way to control camera effects in the generated output.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDescriptionconcept1selectFirst camera concept choice, includes various presets and “none”concept2selectSecond camera concept choice, includes various presets and “none”concept3selectThird camera concept choice, includes various presets and “none”concept4selectFourth camera concept choice, includes various presets and “none”

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionluma\_conceptsLUMA\_CONCEPTSOptional Camera Concepts to merge with selected concepts

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionluma\_conceptsLUMA\_CONCEPTCombined object containing all selected concepts

## [​](http://docs.comfy.org#usage-examples) Usage Examples

[**Luma Text to Video Workflow Example**  
\
Luma Text to Video Workflow Example](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video)

[**Luma Image to Video Workflow Example**  
\
Luma Image to Video Workflow Example](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video)

## [​](http://docs.comfy.org#how-it-works) How It Works

The Luma Concepts node offers a variety of predefined camera concepts including:

- Camera distances (close-up, medium shot, long shot)
- View angles (eye level, overhead, low angle)
- Movement types (push-in, follow, orbit)
- Special effects (handheld, stabilized, floating)

Users can select up to 4 concepts to use together. The node creates an object containing the selected camera concepts, which is then passed to Luma generation nodes. During generation, Luma AI uses these camera concepts to influence the viewpoint and composition of the output, ensuring the results reflect the chosen photographic effects.

By combining multiple camera concepts, users can create complex camera guidance without writing detailed prompt descriptions. This is particularly useful when specific camera angles or compositions are needed.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class LumaConceptsNode(ComfyNodeABC):
    """
    Holds one or more Camera Concepts for use with Luma Text to Video and Luma Image to Video nodes.
    """

    RETURN_TYPES = (LumaIO.LUMA_CONCEPTS,)
    RETURN_NAMES = ("luma_concepts",)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "create_concepts"
    CATEGORY = "api node/image/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "concept1": (get_luma_concepts(include_none=True),),
                "concept2": (get_luma_concepts(include_none=True),),
                "concept3": (get_luma_concepts(include_none=True),),
                "concept4": (get_luma_concepts(include_none=True),),
            },
            "optional": {
                "luma_concepts": (
                    LumaIO.LUMA_CONCEPTS,
                    {
                        "tooltip": "Optional Camera Concepts to add to the ones chosen here."
                    },
                ),
            },
        }

    def create_concepts(
        self,
        concept1: str,
        concept2: str,
        concept3: str,
        concept4: str,
        luma_concepts: LumaConceptChain = None,
    ):
        chain = LumaConceptChain(str_list=[concept1, concept2, concept3, concept4])
        if luma_concepts is not None:
            chain = luma_concepts.clone_and_merge(chain)
        return (chain,)


```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/luma/luma-concepts.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video)

[Pika 2.2 Text to VideoA node that converts text descriptions into videos using Pika AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-text-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Examples](http://docs.comfy.org#usage-examples)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/video/luma/luma-concepts.md -->


<!-- BEGIN Development/built-in-nodes/api-node/video/luma/luma-image-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
    
    - [Luma Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video)
    - [Luma Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video)
    - [Luma Concepts](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Image to Video - ComfyUI Native API Node Documentation

# Luma Image to Video - ComfyUI Native API Node Documentation

A node that converts static images to dynamic videos using Luma AI

The Luma Image to Video node uses Luma AI’s technology to transform static images into smooth, dynamic videos, bringing your images to life.

## [​](http://docs.comfy.org#node-function) Node Function

This node connects to Luma AI’s image-to-video API, allowing users to create dynamic videos from input images. It understands the image content and generates natural, coherent motion while maintaining the original visual style. Combined with text prompts, users can precisely control the video’s dynamic effects.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing video motion and contentmodelselect-Video generation model to useresolutionselect”540p”Output video resolutiondurationselect-Video length optionsloopbooleanFalseWhether to loop the videoseedinteger0Seed value for node rerun, results are nondeterministic

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionfirst\_imageimageFirst frame of video (required if no last\_image)last\_imageimageLast frame of video (required if no first\_image)luma\_conceptsLUMA\_CONCEPTSConcepts for controlling camera motion and shot style

### [​](http://docs.comfy.org#requirements) Requirements

- Either **first\_image** or **last\_image** must be provided
- Each image input (first\_image and last\_image) accepts only 1 image

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOvideoGenerated video

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Luma Image to Video Workflow Example**  
\
Luma Image to Video Workflow Tutorial](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class LumaImageToVideoGenerationNode(ComfyNodeABC):
    """
    Generates videos synchronously based on prompt, input images, and output_size.
    """

    RETURN_TYPES = (IO.VIDEO,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/video/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the video generation",
                    },
                ),
                "model": ([model.value for model in LumaVideoModel],),
                # "aspect_ratio": ([ratio.value for ratio in LumaAspectRatio], {
                #     "default": LumaAspectRatio.ratio_16_9,
                # }),
                "resolution": (
                    [resolution.value for resolution in LumaVideoOutputResolution],
                    {
                        "default": LumaVideoOutputResolution.res_540p,
                    },
                ),
                "duration": ([dur.value for dur in LumaVideoModelOutputDuration],),
                "loop": (
                    IO.BOOLEAN,
                    {
                        "default": False,
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "first_image": (
                    IO.IMAGE,
                    {"tooltip": "First frame of generated video."},
                ),
                "last_image": (IO.IMAGE, {"tooltip": "Last frame of generated video."}),
                "luma_concepts": (
                    LumaIO.LUMA_CONCEPTS,
                    {
                        "tooltip": "Optional Camera Concepts to dictate camera motion via the Luma Concepts node."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        model: str,
        resolution: str,
        duration: str,
        loop: bool,
        seed,
        first_image: torch.Tensor = None,
        last_image: torch.Tensor = None,
        luma_concepts: LumaConceptChain = None,
        auth_token=None,
        **kwargs,
    ):
        if first_image is None and last_image is None:
            raise Exception(
                "At least one of first_image and last_image requires an input."
            )
        keyframes = self._convert_to_keyframes(first_image, last_image, auth_token)
        duration = duration if model != LumaVideoModel.ray_1_6 else None
        resolution = resolution if model != LumaVideoModel.ray_1_6 else None

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/luma/generations",
                method=HttpMethod.POST,
                request_model=LumaGenerationRequest,
                response_model=LumaGeneration,
            ),
            request=LumaGenerationRequest(
                prompt=prompt,
                model=model,
                aspect_ratio=LumaAspectRatio.ratio_16_9,  # ignored, but still needed by the API for some reason
                resolution=resolution,
                duration=duration,
                loop=loop,
                keyframes=keyframes,
                concepts=luma_concepts.create_api_model() if luma_concepts else None,
            ),
            auth_token=auth_token,
        )
        response_api: LumaGeneration = operation.execute()

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/luma/generations/{response_api.id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=LumaGeneration,
            ),
            completed_statuses=[LumaState.completed],
            failed_statuses=[LumaState.failed],
            status_extractor=lambda x: x.state,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        vid_response = requests.get(response_poll.assets.video)
        return (VideoFromFile(BytesIO(vid_response.content)),)

    def _convert_to_keyframes(
        self,
        first_image: torch.Tensor = None,
        last_image: torch.Tensor = None,
        auth_token=None,
    ):
        if first_image is None and last_image is None:
            return None
        frame0 = None
        frame1 = None
        if first_image is not None:
            download_urls = upload_images_to_comfyapi(
                first_image, max_images=1, auth_token=auth_token
            )
            frame0 = LumaImageReference(type="image", url=download_urls[0])
        if last_image is not None:
            download_urls = upload_images_to_comfyapi(
                last_image, max_images=1, auth_token=auth_token
            )
            frame1 = LumaImageReference(type="image", url=download_urls[0])
        return LumaKeyframes(frame0=frame0, frame1=frame1)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/luma/luma-image-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video)

[Luma ConceptsA helper node that provides concept guidance for Luma image generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Requirements](http://docs.comfy.org#requirements)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/video/luma/luma-image-to-video.md -->


<!-- BEGIN Development/built-in-nodes/api-node/video/luma/luma-text-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
    
    - [Luma Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video)
    - [Luma Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video)
    - [Luma Concepts](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Text to Video - ComfyUI Native Node Documentation

# Luma Text to Video - ComfyUI Native Node Documentation

A node that converts text descriptions to videos using Luma AI

The Luma Text to Video node lets you create high-quality, smooth videos from text descriptions using Luma AI’s video generation technology.

## [​](http://docs.comfy.org#node-function) Node Function

This node connects to Luma AI’s text-to-video API, allowing users to generate dynamic video content from detailed text prompts.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing the video content to generatemodelselect-Video generation model to useaspect\_ratioselect”ratio\_16\_9”Video aspect ratioresolutionselect”res\_540p”Video resolutiondurationselect-Video length optionsloopbooleanFalseWhether to loop the videoseedinteger0Seed value for node rerun, results are nondeterministic

When using Ray 1.6 model, duration and resolution parameters will not take effect.

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionluma\_conceptsLUMA\_CONCEPTSCamera concepts to control motion via Luma Concepts node

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOvideoGenerated video

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Luma Text to Video Workflow Example**  
\
Luma Text to Video Workflow Example](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class LumaTextToVideoGenerationNode(ComfyNodeABC):
    """
    Generates videos synchronously based on prompt and output_size.
    """

    RETURN_TYPES = (IO.VIDEO,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/video/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the video generation",
                    },
                ),
                "model": ([model.value for model in LumaVideoModel],),
                "aspect_ratio": (
                    [ratio.value for ratio in LumaAspectRatio],
                    {
                        "default": LumaAspectRatio.ratio_16_9,
                    },
                ),
                "resolution": (
                    [resolution.value for resolution in LumaVideoOutputResolution],
                    {
                        "default": LumaVideoOutputResolution.res_540p,
                    },
                ),
                "duration": ([dur.value for dur in LumaVideoModelOutputDuration],),
                "loop": (
                    IO.BOOLEAN,
                    {
                        "default": False,
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "luma_concepts": (
                    LumaIO.LUMA_CONCEPTS,
                    {
                        "tooltip": "Optional Camera Concepts to dictate camera motion via the Luma Concepts node."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        model: str,
        aspect_ratio: str,
        resolution: str,
        duration: str,
        loop: bool,
        seed,
        luma_concepts: LumaConceptChain = None,
        auth_token=None,
        **kwargs,
    ):
        duration = duration if model != LumaVideoModel.ray_1_6 else None
        resolution = resolution if model != LumaVideoModel.ray_1_6 else None

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/luma/generations",
                method=HttpMethod.POST,
                request_model=LumaGenerationRequest,
                response_model=LumaGeneration,
            ),
            request=LumaGenerationRequest(
                prompt=prompt,
                model=model,
                resolution=resolution,
                aspect_ratio=aspect_ratio,
                duration=duration,
                loop=loop,
                concepts=luma_concepts.create_api_model() if luma_concepts else None,
            ),
            auth_token=auth_token,
        )
        response_api: LumaGeneration = operation.execute()

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/luma/generations/{response_api.id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=LumaGeneration,
            ),
            completed_statuses=[LumaState.completed],
            failed_statuses=[LumaState.failed],
            status_extractor=lambda x: x.state,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        vid_response = requests.get(response_poll.assets.video)
        return (VideoFromFile(BytesIO(vid_response.content)),)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/luma/luma-text-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)

[Luma Image to VideoA node that converts static images to dynamic videos using Luma AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/video/luma/luma-text-to-video.md -->


<!-- BEGIN Development/built-in-nodes/api-node/video/minimax/minimax-image-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
    
    - [MiniMax Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-image-to-video)
    - [MiniMax Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-text-to-video)
  - Google
  - Kling
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

MiniMax Image to Video - ComfyUI Native Node Documentation

# MiniMax Image to Video - ComfyUI Native Node Documentation

A node that converts static images to dynamic videos using MiniMax AI

The MiniMax Image to Video node uses MiniMax’s API to generate videos from input images and text prompts.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionimageimage-Input image used as the first frame of videoprompt\_textstring""Text prompt to guide video generationmodelselect”I2V-01”Available models: “I2V-01-Director”, “I2V-01”, “I2V-01-live”

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionseedintegerRandom seed for noise generation

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOvideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class MinimaxImageToVideoNode(MinimaxTextToVideoNode):
    """
    Generates videos synchronously based on an image and prompt, and optional parameters using Minimax's API.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (
                    IO.IMAGE,
                    {
                        "tooltip": "Image to use as first frame of video generation"
                    },
                ),
                "prompt_text": (
                    "STRING",
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Text prompt to guide the video generation",
                    },
                ),
                "model": (
                    [
                        "I2V-01-Director",
                        "I2V-01",
                        "I2V-01-live",
                    ],
                    {
                        "default": "I2V-01",
                        "tooltip": "Model to use for video generation",
                    },
                ),
            },
            "optional": {
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "The random seed used for creating the noise.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    RETURN_TYPES = ("VIDEO",)
    DESCRIPTION = "Generates videos from an image and prompts using Minimax's API"
    FUNCTION = "generate_video"
    CATEGORY = "api node/video/Minimax"
    API_NODE = True
    OUTPUT_NODE = True
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/minimax/minimax-image-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle3)

[MiniMax Text to VideoA node that converts text descriptions into videos using MiniMax AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-text-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/video/minimax/minimax-image-to-video.md -->


<!-- BEGIN Development/built-in-nodes/api-node/video/minimax/minimax-text-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
    
    - [MiniMax Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-image-to-video)
    - [MiniMax Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-text-to-video)
  - Google
  - Kling
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

MiniMax Text to Video - ComfyUI Native Node Documentation

# MiniMax Text to Video - ComfyUI Native Node Documentation

A node that converts text descriptions into videos using MiniMax AI

The MiniMax Text to Video node connects to MiniMax’s API to generate high-quality, smooth videos from text prompts. It supports different video generation models to create short video clips in various styles.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionprompt\_textString""Text prompt that guides the video generationmodelSelect”T2V-01”Video model to use, options are “T2V-01” and “T2V-01-Director”seedInteger0Random seed for noise generation, defaults to 0

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class MinimaxTextToVideoNode:
    """
    Generates videos synchronously based on a prompt, and optional parameters using Minimax's API.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt_text": (
                    "STRING",
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Text prompt to guide the video generation",
                    },
                ),
                "model": (
                    [
                        "T2V-01",
                        "T2V-01-Director",
                    ],
                    {
                        "default": "T2V-01",
                        "tooltip": "Model to use for video generation",
                    },
                ),
            },
            "optional": {
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "The random seed used for creating the noise.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    RETURN_TYPES = ("VIDEO",)
    DESCRIPTION = "Generates videos from prompts using Minimax's API"
    FUNCTION = "generate_video"
    CATEGORY = "api node/video/Minimax"
    API_NODE = True
    OUTPUT_NODE = True

    def generate_video(
        self,
        prompt_text,
        seed=0,
        model="T2V-01",
        image: torch.Tensor=None, # used for ImageToVideo
        subject: torch.Tensor=None, # used for SubjectToVideo
        auth_token=None,
    ):
        '''
        Function used between Minimax nodes - supports T2V, I2V, and S2V, based on provided arguments.
        '''
        # upload image, if passed in
        image_url = None
        if image is not None:
            image_url = upload_images_to_comfyapi(image, max_images=1, auth_token=auth_token)[0]

        # TODO: figure out how to deal with subject properly, API returns invalid params when using S2V-01 model
        subject_reference = None
        if subject is not None:
            subject_url = upload_images_to_comfyapi(subject, max_images=1, auth_token=auth_token)[0]
            subject_reference = [SubjectReferenceItem(image=subject_url)]


        video_generate_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/minimax/video_generation",
                method=HttpMethod.POST,
                request_model=MinimaxVideoGenerationRequest,
                response_model=MinimaxVideoGenerationResponse,
            ),
            request=MinimaxVideoGenerationRequest(
                model=Model(model),
                prompt=prompt_text,
                callback_url=None,
                first_frame_image=image_url,
                subject_reference=subject_reference,
                prompt_optimizer=None,
            ),
            auth_token=auth_token,
        )
        response = video_generate_operation.execute()

        task_id = response.task_id
        if not task_id:
            raise Exception(f"Minimax generation failed: {response.base_resp}")

        video_generate_operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path="/proxy/minimax/query/video_generation",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=MinimaxTaskResultResponse,
                query_params={"task_id": task_id},
            ),
            completed_statuses=["Success"],
            failed_statuses=["Fail"],
            status_extractor=lambda x: x.status.value,
            auth_token=auth_token,
        )
        task_result = video_generate_operation.execute()

        file_id = task_result.file_id
        if file_id is None:
            raise Exception("Request was not successful. Missing file ID.")
        file_retrieve_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/minimax/files/retrieve",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=MinimaxFileRetrieveResponse,
                query_params={"file_id": int(file_id)},
            ),
            request=EmptyRequest(),
            auth_token=auth_token,
        )
        file_result = file_retrieve_operation.execute()

        file_url = file_result.file.download_url
        if file_url is None:
            raise Exception(
                f"No video was found in the response. Full response: {file_result.model_dump()}"
            )
        logging.info(f"Generated video URL: {file_url}")

        video_io = download_url_to_bytesio(file_url)
        if video_io is None:
            error_msg = f"Failed to download video from {file_url}"
            logging.error(error_msg)
            raise Exception(error_msg)
        return (VideoFromFile(video_io),)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/minimax/minimax-text-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-image-to-video)

[Google Veo2 VideoA node that generates videos from text descriptions using Google's Veo2 technology  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/google/google-veo2-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/video/minimax/minimax-text-to-video.md -->


<!-- BEGIN Development/built-in-nodes/api-node/video/pika/pika-image-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
  - Pika
    
    - [Pika 2.2 Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-text-to-video)
    - [Pika 2.2 Scenes](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-scenes)
    - [Pika 2.2 Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-image-to-video)
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Pika 2.2 Image to Video - ComfyUI Native Node Documentation

# Pika 2.2 Image to Video - ComfyUI Native Node Documentation

A node that converts static images to dynamic videos using Pika AI

The Pika 2.2 Image to Video node connects to Pika’s latest 2.2 API to transform static images into dynamic videos. It preserves the visual features of the original image while adding natural motion based on text prompts.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionimageImage-Input image to convert to videoprompt\_textString""Text prompt describing video motion and contentnegative\_promptString""Elements to avoid in the videoseedInteger0Random seed for generationresolutionSelect”1080p”Output video resolutiondurationSelect”5s”Length of generated video

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#how-it-works) How It Works

The node sends the input image and parameters (prompts, resolution, duration, etc.) to Pika’s API server as multipart form data. The API processes this and returns the generated video. Users can control the output by adjusting the prompts, negative prompts, random seed and other parameters.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-05)]

```python

class PikaImageToVideoV2_2(PikaNodeBase):
    """Pika 2.2 Image to Video Node."""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": (
                    IO.IMAGE,
                    {"tooltip": "The image to convert to video"},
                ),
                **cls.get_base_inputs_types(PikaBodyGenerate22I2vGenerate22I2vPost),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    DESCRIPTION = "Sends an image and prompt to the Pika API v2.2 to generate a video."
    RETURN_TYPES = ("VIDEO",)

    def api_call(
        self,
        image: torch.Tensor,
        prompt_text: str,
        negative_prompt: str,
        seed: int,
        resolution: str,
        duration: int,
        auth_token: Optional[str] = None,
    ) -> tuple[VideoFromFile]:
        """API call for Pika 2.2 Image to Video."""
        # Convert image to BytesIO
        image_bytes_io = tensor_to_bytesio(image)
        image_bytes_io.seek(0)  # Reset stream position

        # Prepare file data for multipart upload
        pika_files = {"image": ("image.png", image_bytes_io, "image/png")}

        # Prepare non-file data using the Pydantic model
        pika_request_data = PikaBodyGenerate22I2vGenerate22I2vPost(
            promptText=prompt_text,
            negativePrompt=negative_prompt,
            seed=seed,
            resolution=resolution,
            duration=duration,
        )

        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=PATH_IMAGE_TO_VIDEO,
                method=HttpMethod.POST,
                request_model=PikaBodyGenerate22I2vGenerate22I2vPost,
                response_model=PikaGenerateResponse,
            ),
            request=pika_request_data,
            files=pika_files,
            content_type="multipart/form-data",
            auth_token=auth_token,
        )

        return self.execute_task(initial_operation, auth_token)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/pika/pika-image-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-scenes)

[PixVerse TemplateA helper node that provides preset templates for PixVerse video generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-template)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Output](http://docs.comfy.org#output)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/video/pika/pika-image-to-video.md -->


<!-- BEGIN Development/built-in-nodes/api-node/video/pika/pika-scenes.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
  - Pika
    
    - [Pika 2.2 Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-text-to-video)
    - [Pika 2.2 Scenes](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-scenes)
    - [Pika 2.2 Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-image-to-video)
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Pika 2.2 Scenes - ComfyUI Built-in Node Documentation

# Pika 2.2 Scenes - ComfyUI Built-in Node Documentation

A node that creates coherent scene videos from multiple images using Pika AI

The Pika 2.2 Scenes node allows you to upload multiple images and generate a high-quality video incorporating these elements. It uses Pika’s 2.2 API to create smooth scene transitions between the images.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionprompt\_textstring""Text prompt describing video content and scenesnegative\_promptstring""Elements to exclude from the videoseedinteger0Random seed for generationingredients\_modeselect”creative”Image combination moderesolutionselectAPI defaultOutput video resolutiondurationselectAPI defaultOutput video lengthaspect\_ratiofloat1.7777777777777777 (16:9)Video aspect ratio, range 0.4-2.5

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionimage\_ingredient\_1imageFirst scene imageimage\_ingredient\_2imageSecond scene imageimage\_ingredient\_3imageThird scene imageimage\_ingredient\_4imageFourth scene imageimage\_ingredient\_5imageFifth scene image

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOvideoGenerated video

## [​](http://docs.comfy.org#how-it-works) How It Works

The Pika 2.2 Scenes node analyzes all input images and creates a video containing these image elements. The node sends the images and parameters to Pika’s API server, which processes them and returns the generated video.

Users can guide the video style and content through prompts, and exclude unwanted elements using negative prompts. The node supports up to 5 input images as ingredients and generates the final video based on the specified combination mode, resolution, duration, and aspect ratio.

## [​](http://docs.comfy.org#source-code) Source Code

```python

class PikaScenesV2_2(PikaNodeBase):
    """Pika 2.2 Scenes Node."""

    @classmethod
    def INPUT_TYPES(cls):
        image_ingredient_input = (
            IO.IMAGE,
            {"tooltip": "Image that will be used as ingredient to create a video."},
        )
        return {
            "required": {
                **cls.get_base_inputs_types(
                    PikaBodyGenerate22C2vGenerate22PikascenesPost,
                ),
                "ingredients_mode": model_field_to_node_input(
                    IO.COMBO,
                    PikaBodyGenerate22C2vGenerate22PikascenesPost,
                    "ingredientsMode",
                    enum_type=IngredientsMode,
                    default="creative",
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.FLOAT,
                    PikaBodyGenerate22C2vGenerate22PikascenesPost,
                    "aspectRatio",
                    step=0.001,
                    min=0.4,
                    max=2.5,
                    default=1.7777777777777777,
                ),
            },
            "optional": {
                "image_ingredient_1": image_ingredient_input,
                "image_ingredient_2": image_ingredient_input,
                "image_ingredient_3": image_ingredient_input,
                "image_ingredient_4": image_ingredient_input,
                "image_ingredient_5": image_ingredient_input,
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    DESCRIPTION = "Combine your images to create a video with the objects in them. Upload multiple images as ingredients and generate a high-quality video that incorporates all of them."
    RETURN_TYPES = ("VIDEO",)

    def api_call(
        self,
        prompt_text: str,
        negative_prompt: str,
        seed: int,
        resolution: str,
        duration: int,
        ingredients_mode: str,
        aspect_ratio: float,
        image_ingredient_1: Optional[torch.Tensor] = None,
        image_ingredient_2: Optional[torch.Tensor] = None,
        image_ingredient_3: Optional[torch.Tensor] = None,
        image_ingredient_4: Optional[torch.Tensor] = None,
        image_ingredient_5: Optional[torch.Tensor] = None,
        auth_token: Optional[str] = None,
    ) -> tuple[VideoFromFile]:
        """API call for Pika Scenes 2.2."""
        all_image_bytes_io = []
        for image in [
            image_ingredient_1,
            image_ingredient_2,
            image_ingredient_3,
            image_ingredient_4,
            image_ingredient_5,
        ]:
            if image is not None:
                image_bytes_io = tensor_to_bytesio(image)
                image_bytes_io.seek(0)
                all_image_bytes_io.append(image_bytes_io)

        # Prepare files data for multipart upload
        pika_files = [
            ("images", (f"image_{i}.png", image_bytes_io, "image/png"))
            for i, image_bytes_io in enumerate(all_image_bytes_io)
        ]

        # Prepare non-file data using the Pydantic model
        pika_request_data = PikaBodyGenerate22C2vGenerate22PikascenesPost(
            ingredientsMode=ingredients_mode,
            promptText=prompt_text,
            negativePrompt=negative_prompt,
            seed=seed,
            resolution=resolution,
            duration=duration,
            aspectRatio=aspect_ratio,
        )

        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=PATH_PIKASCENES,
                method=HttpMethod.POST,
                request_model=PikaBodyGenerate22C2vGenerate22PikascenesPost,
                response_model=PikaGenerateResponse,
            ),
            request=pika_request_data,
            files=pika_files,
            content_type="multipart/form-data",
            auth_token=auth_token,
        )

        return self.execute_task(initial_operation, auth_token)


```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/pika/pika-scenes.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-text-to-video)

[Pika 2.2 Image to VideoA node that converts static images to dynamic videos using Pika AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-image-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/video/pika/pika-scenes.md -->


<!-- BEGIN Development/built-in-nodes/api-node/video/pika/pika-text-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
  - Pika
    
    - [Pika 2.2 Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-text-to-video)
    - [Pika 2.2 Scenes](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-scenes)
    - [Pika 2.2 Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-image-to-video)
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Pika 2.2 Text to Video - ComfyUI Native Node Documentation

# Pika 2.2 Text to Video - ComfyUI Native Node Documentation

A node that converts text descriptions into videos using Pika AI

The Pika 2.2 Text to Video node uses Pika’s 2.2 API to create videos from text descriptions. It connects to Pika’s text-to-video API, allowing users to generate videos using text prompts with various control parameters.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionprompt\_textString""Text prompt describing the video contentnegative\_promptString""Elements to exclude from the videoseedInteger0Random seed for generationresolutionSelect”1080p”Output video resolutiondurationSelect”5s”Length of generated videoaspect\_ratioFloat1.7777777777777777Video aspect ratio, range 0.4-2.5, step 0.001

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-05)]

```python

class PikaTextToVideoNodeV2_2(PikaNodeBase):
    """Pika 2.2 Text to Video Node."""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                **cls.get_base_inputs_types(PikaBodyGenerate22T2vGenerate22T2vPost),
                "aspect_ratio": model_field_to_node_input(
                    IO.FLOAT,
                    PikaBodyGenerate22T2vGenerate22T2vPost,
                    "aspectRatio",
                    step=0.001,
                    min=0.4,
                    max=2.5,
                    default=1.7777777777777777,
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    RETURN_TYPES = ("VIDEO",)
    DESCRIPTION = "Sends a text prompt to the Pika API v2.2 to generate a video."

    def api_call(
        self,
        prompt_text: str,
        negative_prompt: str,
        seed: int,
        resolution: str,
        duration: int,
        aspect_ratio: float,
        auth_token: Optional[str] = None,
    ) -> tuple[VideoFromFile]:
        """API call for Pika 2.2 Text to Video."""
        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=PATH_TEXT_TO_VIDEO,
                method=HttpMethod.POST,
                request_model=PikaBodyGenerate22T2vGenerate22T2vPost,
                response_model=PikaGenerateResponse,
            ),
            request=PikaBodyGenerate22T2vGenerate22T2vPost(
                promptText=prompt_text,
                negativePrompt=negative_prompt,
                seed=seed,
                resolution=resolution,
                duration=duration,
                aspectRatio=aspect_ratio,
            ),
            auth_token=auth_token,
            content_type="application/x-www-form-urlencoded",
        )

        return self.execute_task(initial_operation, auth_token)


```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/pika/pika-text-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)

[Pika 2.2 ScenesA node that creates coherent scene videos from multiple images using Pika AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-scenes)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/video/pika/pika-text-to-video.md -->


<!-- BEGIN Development/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
  - Pika
  - PixVerse
    
    - [PixVerse Template](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-template)
    - [PixVerse Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video)
    - [PixVerse Transition Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-transition-video)
    - [PixVerse Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

PixVerse Image to Video - ComfyUI Native Node Documentation

# PixVerse Image to Video - ComfyUI Native Node Documentation

A node that converts static images to dynamic videos using PixVerse AI

The PixVerse Image to Video node uses PixVerse’s API to transform static images into dynamic videos. It preserves the visual features of the original image while adding natural motion based on text prompts.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionimageImage-Input image to convert to videopromptString""Text prompt describing video motion/contentnegative\_promptString""Elements to avoid in the videoseedInteger-1Random seed (-1 for random)qualitySelect”high”Output video quality levelaspect\_ratioSelect”r16\_9”Output video aspect ratiodurationSelect”seconds\_4”Length of generated videomotion\_modeSelect”standard”Video motion style

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDefaultDescriptionpixverse\_templatePIXVERSE\_TEMPLATENoneOptional PixVerse template

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-05)]

```python
class PixverseImageToVideoNode(ComfyNodeABC):
    """
    Pixverse Image to Video

    Generates videos from an image and prompts.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": ("IMAGE",),
                "prompt": ("STRING", {"multiline": True, "default": ""}),
                "negative_prompt": ("STRING", {"multiline": True, "default": ""}),
                "seed": ("INT", {"default": -1, "min": -1, "max": 0xffffffffffffffff}),
                "quality": (list(PixverseQuality.__members__.keys()), {"default": "high"}),
                "aspect_ratio": (list(PixverseAspectRatio.__members__.keys()), {"default": "r16_9"}),
                "duration": (list(PixverseDuration.__members__.keys()), {"default": "seconds_4"}),
                "motion_mode": (list(PixverseMotionMode.__members__.keys()), {"default": "standard"}),
            },
            "optional": {
                "pixverse_template": ("PIXVERSE_TEMPLATE",),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    RETURN_TYPES = ("VIDEO",)
    DESCRIPTION = "Generates videos from an image and prompts using Pixverse's API"
    FUNCTION = "generate_video"
    CATEGORY = "api node/video/Pixverse"
    API_NODE = True
    OUTPUT_NODE = True
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video.mdx)

[Previous  
\
PixVerse Transition VideoCreate smooth transition videos between start and end frames using PixVerse AI](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-transition-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video.md -->


<!-- BEGIN Development/built-in-nodes/api-node/video/pixverse/pixverse-template.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
  - Pika
  - PixVerse
    
    - [PixVerse Template](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-template)
    - [PixVerse Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video)
    - [PixVerse Transition Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-transition-video)
    - [PixVerse Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

PixVerse Template - ComfyUI Native Node Documentation

# PixVerse Template - ComfyUI Native Node Documentation

A helper node that provides preset templates for PixVerse video generation

The PixVerse Template node lets you choose from predefined video generation templates to control the style and effects of PixVerse video generation nodes. This helper node connects to PixVerse video generation nodes, allowing users to quickly apply preset video styles without manually adjusting complex parameter combinations.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDescriptiontemplateSelectChoose a template from available video presets

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionpixverse\_templatePixverseIO.TEMPLATEConfiguration object containing the selected template ID

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-05)]

```python

class PixverseTemplateNode:
    """
    Select template for Pixverse Video generation.
    """

    RETURN_TYPES = (PixverseIO.TEMPLATE,)
    RETURN_NAMES = ("pixverse_template",)
    FUNCTION = "create_template"
    CATEGORY = "api node/video/Pixverse"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "template": (list(pixverse_templates.keys()), ),
            }
        }

    def create_template(self, template: str):
        template_id = pixverse_templates.get(template, None)
        if template_id is None:
            raise Exception(f"Template '{template}' is not recognized.")
        # just return the integer
        return (template_id,)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/pixverse/pixverse-template.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-image-to-video)

[PixVerse Text to VideoA node that converts text descriptions into videos using PixVerse AI technology  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/video/pixverse/pixverse-template.md -->


<!-- BEGIN Development/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
  - Pika
  - PixVerse
    
    - [PixVerse Template](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-template)
    - [PixVerse Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video)
    - [PixVerse Transition Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-transition-video)
    - [PixVerse Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

PixVerse Text to Video - ComfyUI Built-in Node Documentation

# PixVerse Text to Video - ComfyUI Built-in Node Documentation

A node that converts text descriptions into videos using PixVerse AI technology

The PixVerse Text to Video node connects to PixVerse’s text-to-video API, allowing users to generate high-quality videos from text descriptions. Users can customize their creations by adjusting various parameters like video quality, duration, and motion mode.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing the video contentaspect\_ratioselect-Output video aspect ratioqualityselectPixverseQuality.res\_540pVideo quality levelduration\_secondsselect-Video durationmotion\_modeselect-Video motion modeseedinteger0Random seed for consistent generation results

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDefaultDescriptionnegative\_promptstring""Elements to exclude from the videopixverse\_templatePIXVERSE\_TEMPLATENoneOptional template for style settings

### [​](http://docs.comfy.org#limitations) Limitations

- 1080p quality only supports normal motion mode with 5-second duration
- Non 5-second durations only support normal motion mode

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOvideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-05)]

```python

class PixverseTextToVideoNode(ComfyNodeABC):
    """
    Generates videos synchronously based on prompt and output_size.
    """

    RETURN_TYPES = (IO.VIDEO,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/video/Pixverse"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the video generation",
                    },
                ),
                "aspect_ratio": (
                    [ratio.value for ratio in PixverseAspectRatio],
                ),
                "quality": (
                    [resolution.value for resolution in PixverseQuality],
                    {
                        "default": PixverseQuality.res_540p,
                    },
                ),
                "duration_seconds": ([dur.value for dur in PixverseDuration],),
                "motion_mode": ([mode.value for mode in PixverseMotionMode],),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2147483647,
                        "control_after_generate": True,
                        "tooltip": "Seed for video generation.",
                    },
                ),
            },
            "optional": {
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
                "pixverse_template": (
                    PixverseIO.TEMPLATE,
                    {
                        "tooltip": "An optional template to influence style of generation, created by the Pixverse Template node."
                    }
                )
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        aspect_ratio: str,
        quality: str,
        duration_seconds: int,
        motion_mode: str,
        seed,
        negative_prompt: str=None,
        pixverse_template: int=None,
        auth_token=None,
        **kwargs,
    ):
        # 1080p is limited to 5 seconds duration
        # only normal motion_mode supported for 1080p or for non-5 second duration
        if quality == PixverseQuality.res_1080p:
            motion_mode = PixverseMotionMode.normal
            duration_seconds = PixverseDuration.dur_5
        elif duration_seconds != PixverseDuration.dur_5:
            motion_mode = PixverseMotionMode.normal

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/pixverse/video/text/generate",
                method=HttpMethod.POST,
                request_model=PixverseTextVideoRequest,
                response_model=PixverseVideoResponse,
            ),
            request=PixverseTextVideoRequest(
                prompt=prompt,
                aspect_ratio=aspect_ratio,
                quality=quality,
                duration=duration_seconds,
                motion_mode=motion_mode,
                negative_prompt=negative_prompt if negative_prompt else None,
                template_id=pixverse_template,
                seed=seed,
            ),
            auth_token=auth_token,
        )
        response_api = operation.execute()

        if response_api.Resp is None:
            raise Exception(f"Pixverse request failed: '{response_api.ErrMsg}'")

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/pixverse/video/result/{response_api.Resp.video_id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=PixverseGenerationStatusResponse,
            ),
            completed_statuses=[PixverseStatus.successful],
            failed_statuses=[PixverseStatus.contents_moderation, PixverseStatus.failed, PixverseStatus.deleted],
            status_extractor=lambda x: x.Resp.status,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        vid_response = requests.get(response_poll.Resp.url)
        return (VideoFromFile(BytesIO(vid_response.content)),)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-template)

[PixVerse Transition VideoCreate smooth transition videos between start and end frames using PixVerse AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-transition-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Limitations](http://docs.comfy.org#limitations)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video.md -->


<!-- BEGIN Development/built-in-nodes/api-node/video/pixverse/pixverse-transition-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
  - Pika
  - PixVerse
    
    - [PixVerse Template](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-template)
    - [PixVerse Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video)
    - [PixVerse Transition Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-transition-video)
    - [PixVerse Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

PixVerse Transition Video - ComfyUI Native Node Documentation

# PixVerse Transition Video - ComfyUI Native Node Documentation

Create smooth transition videos between start and end frames using PixVerse AI

The Pixverse Transition Video node connects to PixVerse’s API to generate smooth video transitions between two images. It automatically creates all intermediate frames to produce fluid transformations, perfect for morphing effects, scene transitions, and object evolution.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionfirst\_frameImage-Starting frame imagelast\_frameImage-Ending frame imagepromptString""Text prompt describing video and transitionqualitySelect”PixverseQuality.res\_540p”Output video qualityduration\_secondsSelect-Length of generated videomotion\_modeSelect-Video motion styleseedInteger0Random seed (range: 0-2147483647)

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDefaultDescriptionnegative\_promptString""Elements to avoid in videopixverse\_templatePIXVERSE\_TEMPLATENoneOptional style preset

### [​](http://docs.comfy.org#parameter-constraints) Parameter Constraints

- When quality is set to 1080p, motion\_mode is forced to normal and duration\_seconds to 5 seconds
- When duration\_seconds is not 5 seconds, motion\_mode is forced to normal

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

```python

class PixverseTransitionVideoNode(ComfyNodeABC):
    """
    Generates videos synchronously based on prompt and output_size.
    """

    RETURN_TYPES = (IO.VIDEO,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/video/Pixverse"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "first_frame": (
                    IO.IMAGE,
                ),
                "last_frame": (
                    IO.IMAGE,
                ),
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the video generation",
                    },
                ),
                "quality": (
                    [resolution.value for resolution in PixverseQuality],
                    {
                        "default": PixverseQuality.res_540p,
                    },
                ),
                "duration_seconds": ([dur.value for dur in PixverseDuration],),
                "motion_mode": ([mode.value for mode in PixverseMotionMode],),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2147483647,
                        "control_after_generate": True,
                        "tooltip": "Seed for video generation.",
                    },
                ),
            },
            "optional": {
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
                "pixverse_template": (
                    PixverseIO.TEMPLATE,
                    {
                        "tooltip": "An optional template to influence style of generation, created by the Pixverse Template node."
                    }
                )
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        first_frame: torch.Tensor,
        last_frame: torch.Tensor,
        prompt: str,
        quality: str,
        duration_seconds: int,
        motion_mode: str,
        seed,
        negative_prompt: str=None,
        pixverse_template: int=None,
        auth_token=None,
        **kwargs,
    ):
        first_frame_id = upload_image_to_pixverse(first_frame, auth_token=auth_token)
        last_frame_id = upload_image_to_pixverse(last_frame, auth_token=auth_token)

        # 1080p is limited to 5 seconds duration
        # only normal motion_mode supported for 1080p or for non-5 second duration
        if quality == PixverseQuality.res_1080p:
            motion_mode = PixverseMotionMode.normal
            duration_seconds = PixverseDuration.dur_5
        elif duration_seconds != PixverseDuration.dur_5:
            motion_mode = PixverseMotionMode.normal

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/pixverse/video/transition/generate",
                method=HttpMethod.POST,
                request_model=PixverseTransitionVideoRequest,
                response_model=PixverseVideoResponse,
            ),
            request=PixverseTransitionVideoRequest(
                first_frame_img=first_frame_id,
                last_frame_img=last_frame_id,
                prompt=prompt,
                quality=quality,
                duration=duration_seconds,
                motion_mode=motion_mode,
                negative_prompt=negative_prompt if negative_prompt else None,
                template_id=pixverse_template,
                seed=seed,
            ),
            auth_token=auth_token,
        )
        response_api = operation.execute()

        if response_api.Resp is None:
            raise Exception(f"Pixverse request failed: '{response_api.ErrMsg}'")

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/pixverse/video/result/{response_api.Resp.video_id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=PixverseGenerationStatusResponse,
            ),
            completed_statuses=[PixverseStatus.successful],
            failed_statuses=[PixverseStatus.contents_moderation, PixverseStatus.failed, PixverseStatus.deleted],
            status_extractor=lambda x: x.Resp.status,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        vid_response = requests.get(response_poll.Resp.url)
        return (VideoFromFile(BytesIO(vid_response.content)),)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/pixverse/pixverse-transition-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video)

[PixVerse Image to VideoA node that converts static images to dynamic videos using PixVerse AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Parameter Constraints](http://docs.comfy.org#parameter-constraints)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Development/built-in-nodes/api-node/video/pixverse/pixverse-transition-video.md -->


<!-- BEGIN Development/built-in-nodes/overview.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Built-in Nodes

# ComfyUI Built-in Nodes

Introduction to ComfyUI Built-in Nodes

Built-in nodes are ComfyUI’s default nodes. They are core functionalities of ComfyUI that you can use without installing any third-party custom node packages.

As we have just started updating this section, the content is not yet complete. We will gradually add more content in the future.

If you find any errors in the content, you can submit an issue or PR in this [repo](https://github.com/Comfy-Org/docs) to help us improve.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/overview.mdx)

[Flux pro ultra image  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/bfl/flux-pro-ultra-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

<!-- END Development/built-in-nodes/overview.md -->


<!-- BEGIN Development/comfy-cli/getting-started.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Getting Started

# Getting Started

### [​](http://docs.comfy.org#overview) Overview

`comfy-cli` is a [command line tool](https://github.com/Comfy-Org/comfy-cli) that makes it easier to install and manage Comfy.

### [​](http://docs.comfy.org#install-cli) Install CLI

pip

homebrew

```bash
pip install comfy-cli
```

To get shell completion hints:

```bash
comfy --install-completion
```

### [​](http://docs.comfy.org#install-comfyui) Install ComfyUI

Create a virtual environment with any Python version greater than 3.9.

conda

venv

```bash
conda create -n comfy-env python=3.11
conda activate comfy-env
```

Install ComfyUI

```bash
comfy install
```

You still need to install CUDA, or ROCm depending on your GPU.

### [​](http://docs.comfy.org#run-comfyui) Run ComfyUI

```bash
comfy launch
```

### [​](http://docs.comfy.org#manage-custom-nodes) Manage Custom Nodes

```bash
comfy node install <NODE_NAME>
```

We use `cm-cli` for installing custom nodes. See the [docs](https://github.com/ltdrdata/ComfyUI-Manager/blob/main/docs/en/cm-cli.md) for more information.

### [​](http://docs.comfy.org#manage-models) Manage Models

Downloading models with `comfy-cli` is easy. Just run:

```bash
comfy model download <url> models/checkpoints
```

### [​](http://docs.comfy.org#contributing) Contributing

We encourage contributions to comfy-cli! If you have suggestions, ideas, or bug reports, please open an issue on our [GitHub repository](https://github.com/Comfy-Org/comfy-cli/issues). If you want to contribute code, fork the repository and submit a pull request.

Refer to the [Dev Guide](https://github.com/Comfy-Org/comfy-cli/blob/main/DEV_README.md) for further details.

### [​](http://docs.comfy.org#analytics) Analytics

We track usage of the CLI to improve the user experience. You can disable this by running:

```bash
comfy tracking disable
```

To re-enable tracking, run:

```bash
comfy tracking enable
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/comfy-cli/getting-started.mdx)

[Previous](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

[Reference  
\
Next](http://docs.comfy.org/comfy-cli/reference)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Overview](http://docs.comfy.org#overview)
- [Install CLI](http://docs.comfy.org#install-cli)
- [Install ComfyUI](http://docs.comfy.org#install-comfyui)
- [Run ComfyUI](http://docs.comfy.org#run-comfyui)
- [Manage Custom Nodes](http://docs.comfy.org#manage-custom-nodes)
- [Manage Models](http://docs.comfy.org#manage-models)
- [Contributing](http://docs.comfy.org#contributing)
- [Analytics](http://docs.comfy.org#analytics)

<!-- END Development/comfy-cli/getting-started.md -->


<!-- BEGIN Development/comfy-cli/reference.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Reference

# Reference

# [​](http://docs.comfy.org#cli) CLI

## [​](http://docs.comfy.org#nodes) Nodes

**Usage**:

```plaintext
$ comfy node [OPTIONS] COMMAND [ARGS]...
```

**Options**:

- `--install-completion`: Install completion for the current shell.
- `--show-completion`: Show completion for the current shell, to copy it or customize the installation.
- `--help`: Show this message and exit.

**Commands**:

- `deps-in-workflow`
- `disable`
- `enable`
- `fix`
- `install`
- `install-deps`
- `reinstall`
- `restore-dependencies`
- `restore-snapshot`
- `save-snapshot`: Save a snapshot of the current ComfyUI…
- `show`
- `simple-show`
- `uninstall`
- `update`

### [​](http://docs.comfy.org#deps-in-workflow) `deps-in-workflow`

**Usage**:

```plaintext
$ deps-in-workflow [OPTIONS]
```

**Options**:

- `--workflow TEXT`: Workflow file (.json/.png) \[required]
- `--output TEXT`: Workflow file (.json/.png) \[required]
- `--channel TEXT`: Specify the operation mode
- `--mode TEXT`: \[remote|local|cache]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#disable) `disable`

**Usage**:

```plaintext
$ disable [OPTIONS] ARGS...
```

**Arguments**:

- `ARGS...`: disable custom nodes \[required]

**Options**:

- `--channel TEXT`: Specify the operation mode
- `--mode TEXT`: \[remote|local|cache]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#enable) `enable`

**Usage**:

```plaintext
$ enable [OPTIONS] ARGS...
```

**Arguments**:

- `ARGS...`: enable custom nodes \[required]

**Options**:

- `--channel TEXT`: Specify the operation mode
- `--mode TEXT`: \[remote|local|cache]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#fix) `fix`

**Usage**:

```plaintext
$ fix [OPTIONS] ARGS...
```

**Arguments**:

- `ARGS...`: fix dependencies for specified custom nodes \[required]

**Options**:

- `--channel TEXT`: Specify the operation mode
- `--mode TEXT`: \[remote|local|cache]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#install) `install`

**Usage**:

```plaintext
$ install [OPTIONS] ARGS...
```

**Arguments**:

- `ARGS...`: install custom nodes \[required]

**Options**:

- `--channel TEXT`: Specify the operation mode
- `--mode TEXT`: \[remote|local|cache]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#install-deps) `install-deps`

**Usage**:

```plaintext
$ install-deps [OPTIONS]
```

**Options**:

- `--deps TEXT`: Dependency spec file (.json)
- `--workflow TEXT`: Workflow file (.json/.png)
- `--channel TEXT`: Specify the operation mode
- `--mode TEXT`: \[remote|local|cache]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#reinstall) `reinstall`

**Usage**:

```plaintext
$ reinstall [OPTIONS] ARGS...
```

**Arguments**:

- `ARGS...`: reinstall custom nodes \[required]

**Options**:

- `--channel TEXT`: Specify the operation mode
- `--mode TEXT`: \[remote|local|cache]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#restore-dependencies) `restore-dependencies`

**Usage**:

```plaintext
$ restore-dependencies [OPTIONS]
```

**Options**:

- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#restore-snapshot) `restore-snapshot`

**Usage**:

```plaintext
$ restore-snapshot [OPTIONS] PATH
```

**Arguments**:

- `PATH`: \[required]

**Options**:

- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#save-snapshot) `save-snapshot`

Save a snapshot of the current ComfyUI environment

**Usage**:

```plaintext
$ save-snapshot [OPTIONS]
```

**Options**:

- `--output TEXT`: Specify the output file path. (.json/.yaml)
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#show) `show`

**Usage**:

```plaintext
$ show [OPTIONS] ARGS...
```

**Arguments**:

- `ARGS...`: \[installed|enabled|not-installed|disabled|all|snapshot|snapshot-list] \[required]

**Options**:

- `--channel TEXT`: Specify the operation mode
- `--mode TEXT`: \[remote|local|cache]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#simple-show) `simple-show`

**Usage**:

```plaintext
$ simple-show [OPTIONS] ARGS...
```

**Arguments**:

- `ARGS...`: \[installed|enabled|not-installed|disabled|all|snapshot|snapshot-list] \[required]

**Options**:

- `--channel TEXT`: Specify the operation mode
- `--mode TEXT`: \[remote|local|cache]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#uninstall) `uninstall`

**Usage**:

```plaintext
$ uninstall [OPTIONS] ARGS...
```

**Arguments**:

- `ARGS...`: uninstall custom nodes \[required]

**Options**:

- `--channel TEXT`: Specify the operation mode
- `--mode TEXT`: \[remote|local|cache]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#update) `update`

**Usage**:

```plaintext
$ update [OPTIONS] ARGS...
```

**Arguments**:

- `ARGS...`: update custom nodes \[required]

**Options**:

- `--channel TEXT`: Specify the operation mode
- `--mode TEXT`: \[remote|local|cache]
- `--help`: Show this message and exit.

## [​](http://docs.comfy.org#models) Models

**Usage**:

```plaintext
$ comfy model [OPTIONS] COMMAND [ARGS]...
```

**Options**:

- `--install-completion`: Install completion for the current shell.
- `--show-completion`: Show completion for the current shell, to copy it or customize the installation.
- `--help`: Show this message and exit.

**Commands**:

- `download`: Download a model to a specified relative…
- `list`: Display a list of all models currently…
- `remove`: Remove one or more downloaded models,…

### [​](http://docs.comfy.org#download) `download`

Download a model to a specified relative path if it is not already downloaded.

**Usage**:

```plaintext
$ download [OPTIONS]
```

**Options**:

- `--url TEXT`: The URL from which to download the model \[required]
- `--relative-path TEXT`: The relative path from the current workspace to install the model. \[default: models/checkpoints]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#list) `list`

Display a list of all models currently downloaded in a table format.

**Usage**:

```plaintext
$ list [OPTIONS]
```

**Options**:

- `--relative-path TEXT`: The relative path from the current workspace where the models are stored. \[default: models/checkpoints]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#remove) `remove`

Remove one or more downloaded models, either by specifying them directly or through an interactive selection.

**Usage**:

```plaintext
$ remove [OPTIONS]
```

**Options**:

- `--relative-path TEXT`: The relative path from the current workspace where the models are stored. \[default: models/checkpoints]
- `--model-names TEXT`: List of model filenames to delete, separated by spaces
- `--help`: Show this message and exit.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/comfy-cli/reference.mdx)

[Previous](http://docs.comfy.org/comfy-cli/getting-started)

[Overview  
\
Next](http://docs.comfy.org/custom-nodes/overview)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [CLI](http://docs.comfy.org#cli)
- [Nodes](http://docs.comfy.org#nodes)
- [deps-in-workflow](http://docs.comfy.org#deps-in-workflow)
- [disable](http://docs.comfy.org#disable)
- [enable](http://docs.comfy.org#enable)
- [fix](http://docs.comfy.org#fix)
- [install](http://docs.comfy.org#install)
- [install-deps](http://docs.comfy.org#install-deps)
- [reinstall](http://docs.comfy.org#reinstall)
- [restore-dependencies](http://docs.comfy.org#restore-dependencies)
- [restore-snapshot](http://docs.comfy.org#restore-snapshot)
- [save-snapshot](http://docs.comfy.org#save-snapshot)
- [show](http://docs.comfy.org#show)
- [simple-show](http://docs.comfy.org#simple-show)
- [uninstall](http://docs.comfy.org#uninstall)
- [update](http://docs.comfy.org#update)
- [Models](http://docs.comfy.org#models)
- [download](http://docs.comfy.org#download)
- [list](http://docs.comfy.org#list)
- [remove](http://docs.comfy.org#remove)

<!-- END Development/comfy-cli/reference.md -->


<!-- BEGIN Development/custom-nodes/backend/datatypes.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
  
  - [Properties](http://docs.comfy.org/custom-nodes/backend/server_overview)
  - [Lifecycle](http://docs.comfy.org/custom-nodes/backend/lifecycle)
  - [Publishing to the Manager](http://docs.comfy.org/custom-nodes/backend/manager)
  - [Datatypes](http://docs.comfy.org/custom-nodes/backend/datatypes)
  - [Images, Latents, and Masks](http://docs.comfy.org/custom-nodes/backend/images_and_masks)
  - [Hidden and Flexible inputs](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)
  - [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)
  - [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion)
  - [Data lists](http://docs.comfy.org/custom-nodes/backend/lists)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/backend/snippets)
  - [Working with torch.Tensor](http://docs.comfy.org/custom-nodes/backend/tensors)
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Datatypes

# Datatypes

These are the most important built in datatypes. You can also [define your own](http://docs.comfy.org/more_on_inputs#custom-datatypes).

Datatypes are used on the client side to prevent a workflow from passing the wrong form of data into a node - a bit like strong typing. The JavaScript client side code will generally not allow a node output to be connected to an input of a different datatype, although a few exceptions are noted below.

## [​](http://docs.comfy.org#comfy-datatypes) Comfy datatypes

### [​](http://docs.comfy.org#combo) COMBO

- No additional parameters in `INPUT_TYPES`
- Python datatype: defined as `list[str]`, output value is `str`

Represents a dropdown menu widget. Unlike other datatypes, `COMBO` it is not specified in `INPUT_TYPES` by a `str`, but by a `list[str]` corresponding to the options in the dropdown list, with the first option selected by default.

`COMBO` inputs are often dynamically generated at run time. For instance, in the built-in `CheckpointLoaderSimple` node, you find

```plaintext
"ckpt_name": (folder_paths.get_filename_list("checkpoints"), )
```

or they might just be a fixed list of options,

```plaintext
"play_sound": (["no","yes"], {}),
```

### [​](http://docs.comfy.org#primitive-and-reroute) Primitive and reroute

Primitive and reroute nodes only exist on the client side. They do not have an intrinsic datatype, but when connected they take on the datatype of the input or output to which they have been connected (which is why they can’t connect to a `*` input…)

## [​](http://docs.comfy.org#python-datatypes) Python datatypes

### [​](http://docs.comfy.org#int) INT

- Additional parameters in `INPUT_TYPES`:
  
  - `default` is required
  - `min` and `max` are optional
- Python datatype `int`

### [​](http://docs.comfy.org#float) FLOAT

- Additional parameters in `INPUT_TYPES`:
  
  - `default` is required
  - `min`, `max`, `step` are optional
- Python datatype `float`

### [​](http://docs.comfy.org#string) STRING

- Additional parameters in `INPUT_TYPES`:
  
  - `default` is required
- Python datatype `str`

### [​](http://docs.comfy.org#boolean) BOOLEAN

- Additional parameters in `INPUT_TYPES`:
  
  - `default` is required
- Python datatype `bool`

## [​](http://docs.comfy.org#tensor-datatypes) Tensor datatypes

### [​](http://docs.comfy.org#image) IMAGE

- No additional parameters in `INPUT_TYPES`
- Python datatype `torch.Tensor` with *shape* \[B,H,W,C]

A batch of `B` images, height `H`, width `W`, with `C` channels (generally `C=3` for `RGB`).

### [​](http://docs.comfy.org#latent) LATENT

- No additional parameters in `INPUT_TYPES`
- Python datatype `dict`, containing a `torch.Tensor` with *shape* \[B,C,H,W]

The `dict` passed contains the key `samples`, which is a `torch.Tensor` with *shape* \[B,C,H,W] representing a batch of `B` latents, with `C` channels (generally `C=4` for existing stable diffusion models), height `H`, width `W`.

The height and width are 1/8 of the corresponding image size (which is the value you set in the Empty Latent Image node).

Other entries in the dictionary contain things like latent masks.

### [​](http://docs.comfy.org#mask) MASK

- No additional parameters in `INPUT_TYPES`
- Python datatype `torch.Tensor` with *shape* \[H,W] or \[B,C,H,W]

### [​](http://docs.comfy.org#audio) AUDIO

- No additional parameters in `INPUT_TYPES`
- Python datatype `dict`, containing a `torch.Tensor` with *shape* \[B, C, T] and a sample rate.

The `dict` passed contains the key `waveform`, which is a `torch.Tensor` with *shape* \[B, C, T] representing a batch of `B` audio samples, with `C` channels (`C=2` for stereo and `C=1` for mono), and `T` time steps (i.e., the number of audio samples).

The `dict` contains another key `sample_rate`, which indicates the sampling rate of the audio.

## [​](http://docs.comfy.org#custom-sampling-datatypes) Custom Sampling datatypes

### [​](http://docs.comfy.org#noise) Noise

The `NOISE` datatype represents a *source* of noise (not the actual noise itself). It can be represented by any Python object that provides a method to generate noise, with the signature `generate_noise(self, input_latent:Tensor) -> Tensor`, and a property, `seed:Optional[int]`.

The `seed` is passed into `sample` guider in the `SamplerCustomAdvanced`, but does not appear to be used in any of the standard guiders. It is Optional, so you can generally set it to None.

When noise is to be added, the latent is passed into this method, which should return a `Tensor` of the same shape containing the noise.

See the [noise mixing example](http://docs.comfy.org/snippets#creating-noise-variations)

### [​](http://docs.comfy.org#sampler) Sampler

The `SAMPLER` datatype represents a sampler, which is represented as a Python object providing a `sample` method. Stable diffusion sampling is beyond the scope of this guide; see `comfy/samplers.py` if you want to dig into this part of the code.

### [​](http://docs.comfy.org#sigmas) Sigmas

The `SIGMAS` datatypes represents the values of sigma before and after each step in the sampling process, as produced by a scheduler. This is represented as a one-dimensional tensor, of length `steps+1`, where each element represents the noise expected to be present before the corresponding step, with the final value representing the noise present after the final step.

A `normal` scheduler, with 20 steps and denoise of 1, for an SDXL model, produces:

```plaintext
tensor([14.6146, 10.7468,  8.0815,  6.2049,  4.8557,  
         3.8654,  3.1238,  2.5572,  2.1157,  1.7648,  
         1.4806,  1.2458,  1.0481,  0.8784,  0.7297,  
         0.5964,  0.4736,  0.3555,  0.2322,  0.0292,  0.0000])
```

The starting value of sigma depends on the model, which is why a scheduler node requires a `MODEL` input to produce a SIGMAS output

### [​](http://docs.comfy.org#guider) Guider

A `GUIDER` is a generalisation of the denoising process, as ‘guided’ by a prompt or any other form of conditioning. In Comfy the guider is represented by a `callable` Python object providing a `__call__(*args, **kwargs)` method which is called by the sample.

The `__call__` method takes (in `args[0]`) a batch of noisy latents (tensor `[B,C,H,W]`), and returns a prediction of the noise (a tensor of the same shape).

## [​](http://docs.comfy.org#model-datatypes) Model datatypes

There are a number of more technical datatypes for stable diffusion models. The most significant ones are `MODEL`, `CLIP`, `VAE` and `CONDITIONING`. Working with these is (for the time being) beyond the scope of this guide!

## [​](http://docs.comfy.org#additional-parameters) Additional Parameters

Below is a list of officially supported keys that can be used in the ‘extra options’ portion of an input definition.

You can use additional keys for your own custom widgets, but should *not* reuse any of the keys below for other purposes.

KeyDescription`default`The default value of the widget`min`The minimum value of a number (`FLOAT` or `INT`)`max`The maximum value of a number (`FLOAT` or `INT`)`step`The amount to increment or decrement a widget`label_on`The label to use in the UI when the bool is `True` (`BOOL`)`label_off`The label to use in the UI when the bool is `False` (`BOOL`)`defaultInput`Defaults to an input socket rather than a supported widget`forceInput``defaultInput` and also don’t allow converting to a widget`multiline`Use a multiline text box (`STRING`)`placeholder`Placeholder text to display in the UI when empty (`STRING`)`dynamicPrompts`Causes the front-end to evaluate dynamic prompts`lazy`Declares that this input uses [Lazy Evaluation](http://docs.comfy.org/lazy_evaluation)`rawLink`When a link exists, rather than receiving the evaluated value, you will receive the link (i.e. `["nodeId", <outputIndex>]`). Primarily useful when your node uses [Node Expansion](http://docs.comfy.org/expansion).

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/backend/datatypes.mdx)

[Previous](http://docs.comfy.org/custom-nodes/backend/manager)

[Images, Latents, and Masks  
\
Next](http://docs.comfy.org/custom-nodes/backend/images_and_masks)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Comfy datatypes](http://docs.comfy.org#comfy-datatypes)
- [COMBO](http://docs.comfy.org#combo)
- [Primitive and reroute](http://docs.comfy.org#primitive-and-reroute)
- [Python datatypes](http://docs.comfy.org#python-datatypes)
- [INT](http://docs.comfy.org#int)
- [FLOAT](http://docs.comfy.org#float)
- [STRING](http://docs.comfy.org#string)
- [BOOLEAN](http://docs.comfy.org#boolean)
- [Tensor datatypes](http://docs.comfy.org#tensor-datatypes)
- [IMAGE](http://docs.comfy.org#image)
- [LATENT](http://docs.comfy.org#latent)
- [MASK](http://docs.comfy.org#mask)
- [AUDIO](http://docs.comfy.org#audio)
- [Custom Sampling datatypes](http://docs.comfy.org#custom-sampling-datatypes)
- [Noise](http://docs.comfy.org#noise)
- [Sampler](http://docs.comfy.org#sampler)
- [Sigmas](http://docs.comfy.org#sigmas)
- [Guider](http://docs.comfy.org#guider)
- [Model datatypes](http://docs.comfy.org#model-datatypes)
- [Additional Parameters](http://docs.comfy.org#additional-parameters)

<!-- END Development/custom-nodes/backend/datatypes.md -->


<!-- BEGIN Development/custom-nodes/backend/expansion.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
  
  - [Properties](http://docs.comfy.org/custom-nodes/backend/server_overview)
  - [Lifecycle](http://docs.comfy.org/custom-nodes/backend/lifecycle)
  - [Publishing to the Manager](http://docs.comfy.org/custom-nodes/backend/manager)
  - [Datatypes](http://docs.comfy.org/custom-nodes/backend/datatypes)
  - [Images, Latents, and Masks](http://docs.comfy.org/custom-nodes/backend/images_and_masks)
  - [Hidden and Flexible inputs](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)
  - [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)
  - [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion)
  - [Data lists](http://docs.comfy.org/custom-nodes/backend/lists)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/backend/snippets)
  - [Working with torch.Tensor](http://docs.comfy.org/custom-nodes/backend/tensors)
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Node Expansion

# Node Expansion

## [​](http://docs.comfy.org#node-expansion) Node Expansion

Normally, when a node is executed, that execution function immediately returns the output results of that node. “Node Expansion” is a relatively advanced technique that allows nodes to return a new subgraph of nodes that should take its place in the graph. This technique is what allows custom nodes to implement loops.

### [​](http://docs.comfy.org#a-simple-example) A Simple Example

First, here’s a simple example of what node expansion looks like:

We highly recommend using the `GraphBuilder` class when creating subgraphs. It isn’t mandatory, but it prevents you from making many easy mistakes.

```python
def load_and_merge_checkpoints(self, checkpoint_path1, checkpoint_path2, ratio):
    from comfy_execution.graph_utils import GraphBuilder # Usually at the top of the file
    graph = GraphBuilder()
    checkpoint_node1 = graph.node("CheckpointLoaderSimple", checkpoint_path=checkpoint_path1)
    checkpoint_node2 = graph.node("CheckpointLoaderSimple", checkpoint_path=checkpoint_path2)
    merge_model_node = graph.node("ModelMergeSimple", model1=checkpoint_node1.out(0), model2=checkpoint_node2.out(0), ratio=ratio)
    merge_clip_node = graph.node("ClipMergeSimple", clip1=checkpoint_node1.out(1), clip2=checkpoint_node2.out(1), ratio=ratio)
    return {
        # Returning (MODEL, CLIP, VAE) outputs
        "result": (merge_model_node.out(0), merge_clip_node.out(0), checkpoint_node1.out(2)),
        "expand": graph.finalize(),
    }
```

While this same node could previously be implemented by manually calling into ComfyUI internals, using expansion means that each subnode will be cached separately (so if you change `model2`, you don’t have to reload `model1`).

### [​](http://docs.comfy.org#requirements) Requirements

In order to perform node expansion, a node must return a dictionary with the following keys:

1. `result`: A tuple of the outputs of the node. This may be a mix of finalized values (like you would return from a normal node) and node outputs.
2. `expand`: The finalized graph to perform expansion on. See below if you are not using the `GraphBuilder`.

#### [​](http://docs.comfy.org#additional-requirements-if-not-using-graphbuilder) Additional Requirements if not using GraphBuilder

The format expected from the `expand` key is the same as the ComfyUI API format. The following requirements are handled by the `GraphBuilder`, but must be handled manually if you choose to forego it:

1. Node IDs must be unique across the entire graph. (This includes between multiple executions of the same node due to the use of lists.)
2. Node IDs must be deterministic and consistent between multiple executions of the graph (including partial executions due to caching).

Even if you don’t want to use the `GraphBuilder` for actually building the graph (e.g. because you’re loading the raw json of the graph from a file), you can use the `GraphBuilder.alloc_prefix()` function to generate a prefix and `comfy.graph_utils.add_graph_prefix` to fix existing graphs to meet these requirements.

### [​](http://docs.comfy.org#efficient-subgraph-caching) Efficient Subgraph Caching

While you can pass non-literal inputs to nodes within the subgraph (like torch tensors), this can inhibit caching *within* the subgraph. When possible, you should pass links to subgraph objects rather than the node itself. (You can declare an input as a `rawLink` within the input’s [Additional Parameters](http://docs.comfy.org/datatypes#additional-parameters) to do this easily.)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/backend/expansion.mdx)

[Previous](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)

[Data lists  
\
Next](http://docs.comfy.org/custom-nodes/backend/lists)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Expansion](http://docs.comfy.org#node-expansion)
- [A Simple Example](http://docs.comfy.org#a-simple-example)
- [Requirements](http://docs.comfy.org#requirements)
- [Additional Requirements if not using GraphBuilder](http://docs.comfy.org#additional-requirements-if-not-using-graphbuilder)
- [Efficient Subgraph Caching](http://docs.comfy.org#efficient-subgraph-caching)

<!-- END Development/custom-nodes/backend/expansion.md -->


<!-- BEGIN Development/custom-nodes/backend/images_and_masks.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
  
  - [Properties](http://docs.comfy.org/custom-nodes/backend/server_overview)
  - [Lifecycle](http://docs.comfy.org/custom-nodes/backend/lifecycle)
  - [Publishing to the Manager](http://docs.comfy.org/custom-nodes/backend/manager)
  - [Datatypes](http://docs.comfy.org/custom-nodes/backend/datatypes)
  - [Images, Latents, and Masks](http://docs.comfy.org/custom-nodes/backend/images_and_masks)
  - [Hidden and Flexible inputs](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)
  - [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)
  - [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion)
  - [Data lists](http://docs.comfy.org/custom-nodes/backend/lists)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/backend/snippets)
  - [Working with torch.Tensor](http://docs.comfy.org/custom-nodes/backend/tensors)
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Images, Latents, and Masks

# Images, Latents, and Masks

When working with these datatypes, you will need to know about the `torch.Tensor` class. Complete documentation is [here](https://pytorch.org/docs/stable/tensors.html), or an introduction to the key concepts required for Comfy [here](http://docs.comfy.org/tensors).

If your node has a single output which is a tensor, remember to return `(image,)` not `(image)`

Most of the concepts below are illustrated in the [example code snippets](http://docs.comfy.org/snippets).

## [​](http://docs.comfy.org#images) Images

An IMAGE is a `torch.Tensor` with shape `[B,H,W,C]`, `C=3`. If you are going to save or load images, you will need to convert to and from `PIL.Image` format - see the code snippets below! Note that some `pytorch` operations offer (or expect) `[B,C,H,W]`, known as ‘channel first’, for reasons of computational efficiency. Just be careful.

### [​](http://docs.comfy.org#working-with-pil-image) Working with PIL.Image

If you want to load and save images, you’ll want to use PIL:

```python
from PIL import Image, ImageOps
```

## [​](http://docs.comfy.org#masks) Masks

A MASK is a `torch.Tensor` with shape `[B,H,W]`. In many contexts, masks have binary values (0 or 1), which are used to indicate which pixels should undergo specific operations. In some cases values between 0 and 1 are used indicate an extent of masking, (for instance, to alter transparency, adjust filters, or composite layers).

### [​](http://docs.comfy.org#masks-from-the-load-image-node) Masks from the Load Image Node

The `LoadImage` node uses an image’s alpha channel (the “A” in “RGBA”) to create MASKs. The values from the alpha channel are normalized to the range \[0,1] (torch.float32) and then inverted. The `LoadImage` node always produces a MASK output when loading an image. Many images (like JPEGs) don’t have an alpha channel. In these cases, `LoadImage` creates a default mask with the shape `[1, 64, 64]`.

### [​](http://docs.comfy.org#understanding-mask-shapes) Understanding Mask Shapes

In libraries like `numpy`, `PIL`, and many others, single-channel images (like masks) are typically represented as 2D arrays, shape `[H,W]`. This means the `C` (channel) dimension is implicit, and thus unlike IMAGE types, batches of MASKs have only three dimensions: `[B, H, W]`. It is not uncommon to encounter a mask which has had the `B` dimension implicitly squeezed, giving a tensor `[H,W]`.

To use a MASK, you will often have to match shapes by unsqueezing to produce a shape `[B,H,W,C]` with `C=1` To unsqueezing the `C` dimension, so you should `unsqueeze(-1)`, to unsqueeze `B`, you `unsqueeze(0)`. If your node receives a MASK as input, you would be wise to always check `len(mask.shape)`.

## [​](http://docs.comfy.org#latents) Latents

A LATENT is a `dict`; the latent sample is referenced by the key `samples` and has shape `[B,C,H,W]`, with `C=4`.

LATENT is channel first, IMAGE is channel last

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/backend/images_and_masks.mdx)

[Previous](http://docs.comfy.org/custom-nodes/backend/datatypes)

[Hidden and Flexible inputs  
\
Next](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Images](http://docs.comfy.org#images)
- [Working with PIL.Image](http://docs.comfy.org#working-with-pil-image)
- [Masks](http://docs.comfy.org#masks)
- [Masks from the Load Image Node](http://docs.comfy.org#masks-from-the-load-image-node)
- [Understanding Mask Shapes](http://docs.comfy.org#understanding-mask-shapes)
- [Latents](http://docs.comfy.org#latents)

<!-- END Development/custom-nodes/backend/images_and_masks.md -->


<!-- BEGIN Development/custom-nodes/backend/lazy_evaluation.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
  
  - [Properties](http://docs.comfy.org/custom-nodes/backend/server_overview)
  - [Lifecycle](http://docs.comfy.org/custom-nodes/backend/lifecycle)
  - [Publishing to the Manager](http://docs.comfy.org/custom-nodes/backend/manager)
  - [Datatypes](http://docs.comfy.org/custom-nodes/backend/datatypes)
  - [Images, Latents, and Masks](http://docs.comfy.org/custom-nodes/backend/images_and_masks)
  - [Hidden and Flexible inputs](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)
  - [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)
  - [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion)
  - [Data lists](http://docs.comfy.org/custom-nodes/backend/lists)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/backend/snippets)
  - [Working with torch.Tensor](http://docs.comfy.org/custom-nodes/backend/tensors)
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Lazy Evaluation

# Lazy Evaluation

## [​](http://docs.comfy.org#lazy-evaluation) Lazy Evaluation

By default, all `required` and `optional` inputs are evaluated before a node can be run. Sometimes, however, an input won’t necessarily be used and evaluating it would result in unnecessary processing. Here are some examples of nodes where lazy evaluation may be beneficial:

1. A `ModelMergeSimple` node where the ratio is either `0.0` (in which case the first model doesn’t need to be loaded) or `1.0` (in which case the second model doesn’t need to be loaded).
2. Interpolation between two images where the ratio (or mask) is either entirely `0.0` or entirely `1.0`.
3. A Switch node where one input determines which of the other inputs will be passed through.

There is very little cost in making an input lazy. If it’s something you can do, you generally should.

### [​](http://docs.comfy.org#creating-lazy-inputs) Creating Lazy Inputs

There are two steps to making an input a “lazy” input. They are:

1. Mark the input as lazy in the dictionary returned by `INPUT_TYPES`
2. Define a method named `check_lazy_status` (note: *not* a class method) that will be called prior to evaluation to determine if any more inputs are necessary.

To demonstrate these, we’ll make a “MixImages” node that interpolates between two images according to a mask. If the entire mask is `0.0`, we don’t need to evaluate any part of the tree leading up to the second image. If the entire mask is `1.0`, we can skip evaluating the first image.

#### [​](http://docs.comfy.org#defining-input-types) Defining `INPUT_TYPES`

Declaring that an input is lazy is as simple as adding a `lazy: True` key-value pair to the input’s options dictionary.

```python
@classmethod
def INPUT_TYPES(cls):
    return {
        "required": {
            "image1": ("IMAGE",{"lazy": True}),
            "image2": ("IMAGE",{"lazy": True}),
            "mask": ("MASK",),
        },
    }
```

In this example, `image1` and `image2` are both marked as lazy inputs, but `mask` will always be evaluated.

#### [​](http://docs.comfy.org#defining-check-lazy-status) Defining `check_lazy_status`

A `check_lazy_status` method is called if there are one or more lazy inputs that are not yet available. This method receives the same arguments as the standard execution function. All available inputs are passed in with their final values while unavailable lazy inputs have a value of `None`.

The responsibility of the `check_lazy_status` function is to return a list of the names of any lazy inputs that are needed to proceed. If all lazy inputs are available, the function should return an empty list.

Note that `check_lazy_status` may be called multiple times. (For example, you might find after evaluating one lazy input that you need to evaluate another.)

Note that because the function uses actual input values, it is *not* a class method.

```python
def check_lazy_status(self, mask, image1, image2):
    mask_min = mask.min()
    mask_max = mask.max()
    needed = []
    if image1 is None and (mask_min != 1.0 or mask_max != 1.0):
        needed.append("image1")
    if image2 is None and (mask_min != 0.0 or mask_max != 0.0):
        needed.append("image2")
    return needed
```

### [​](http://docs.comfy.org#full-example) Full Example

```python
class LazyMixImages:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image1": ("IMAGE",{"lazy": True}),
                "image2": ("IMAGE",{"lazy": True}),
                "mask": ("MASK",),
            },
        }

    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "mix"

    CATEGORY = "Examples"

    def check_lazy_status(self, mask, image1, image2):
        mask_min = mask.min()
        mask_max = mask.max()
        needed = []
        if image1 is None and (mask_min != 1.0 or mask_max != 1.0):
            needed.append("image1")
        if image2 is None and (mask_min != 0.0 or mask_max != 0.0):
            needed.append("image2")
        return needed

    # Not trying to handle different batch sizes here just to keep the demo simple
    def mix(self, mask, image1, image2):
        mask_min = mask.min()
        mask_max = mask.max()
        if mask_min == 0.0 and mask_max == 0.0:
            return (image1,)
        elif mask_min == 1.0 and mask_max == 1.0:
            return (image2,)

        result = image1 * (1. - mask) + image2 * mask,
        return (result[0],)
```

## [​](http://docs.comfy.org#execution-blocking) Execution Blocking

While Lazy Evaluation is the recommended way to “disable” part of a graph, there are times when you want to disable an `OUTPUT` node that doesn’t implement lazy evaluation itself. If it’s an output node that you developed yourself, you should just add lazy evaluation as follows:

1. Add a required (if this is a new node) or optional (if you care about backward compatibility) input for `enabled` that defaults to `True`
2. Make all other inputs `lazy` inputs
3. Only evaluate the other inputs if `enabled` is `True`

If it’s not a node you control, you can make use of a `comfy_execution.graph.ExecutionBlocker`. This special object can be returned as an output from any socket. Any nodes which receive an `ExecutionBlocker` as input will skip execution and return that `ExecutionBlocker` for any outputs.

**There is intentionally no way to stop an ExecutionBlocker from propagating forward.** If you think you want this, you should really be using Lazy Evaluation.

### [​](http://docs.comfy.org#usage) Usage

There are two ways to construct and use an `ExecutionBlocker`

1. Pass `None` into the constructor to silently block execution. This is useful for cases where blocking execution is part of a successful run — like disabling an output.

```python
def silent_passthrough(self, passthrough, blocked):
    if blocked:
        return (ExecutionBlocker(None),)
    else:
        return (passthrough,)
```

2. Pass a string into the constructor to display an error message when a node is blocked due to receiving the object. This can be useful if you want to display a meaningful error message if someone uses a meaningless output — for example, the `VAE` output when loading a model that doesn’t contain VAEs.

```python
def load_checkpoint(self, ckpt_name):
    ckpt_path = folder_paths.get_full_path("checkpoints", ckpt_name)
    model, clip, vae = load_checkpoint(ckpt_path)
    if vae is None:
        # This error is more useful than a "'NoneType' has no attribute" error
        # in a later node
        vae = ExecutionBlocker(f"No VAE contained in the loaded model {ckpt_name}")
    return (model, clip, vae)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/backend/lazy_evaluation.mdx)

[Previous](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)

[Node Expansion  
\
Next](http://docs.comfy.org/custom-nodes/backend/expansion)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Lazy Evaluation](http://docs.comfy.org#lazy-evaluation)
- [Creating Lazy Inputs](http://docs.comfy.org#creating-lazy-inputs)
- [Defining INPUT\_TYPES](http://docs.comfy.org#defining-input-types)
- [Defining check\_lazy\_status](http://docs.comfy.org#defining-check-lazy-status)
- [Full Example](http://docs.comfy.org#full-example)
- [Execution Blocking](http://docs.comfy.org#execution-blocking)
- [Usage](http://docs.comfy.org#usage)

<!-- END Development/custom-nodes/backend/lazy_evaluation.md -->


<!-- BEGIN Development/custom-nodes/backend/lifecycle.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
  
  - [Properties](http://docs.comfy.org/custom-nodes/backend/server_overview)
  - [Lifecycle](http://docs.comfy.org/custom-nodes/backend/lifecycle)
  - [Publishing to the Manager](http://docs.comfy.org/custom-nodes/backend/manager)
  - [Datatypes](http://docs.comfy.org/custom-nodes/backend/datatypes)
  - [Images, Latents, and Masks](http://docs.comfy.org/custom-nodes/backend/images_and_masks)
  - [Hidden and Flexible inputs](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)
  - [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)
  - [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion)
  - [Data lists](http://docs.comfy.org/custom-nodes/backend/lists)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/backend/snippets)
  - [Working with torch.Tensor](http://docs.comfy.org/custom-nodes/backend/tensors)
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Lifecycle

# Lifecycle

## [​](http://docs.comfy.org#how-comfy-loads-custom-nodes) How Comfy loads custom nodes

When Comfy starts, it scans the directory `custom_nodes` for Python modules, and attempts to load them. If the module exports `NODE_CLASS_MAPPINGS`, it will be treated as a custom node.

A python module is a directory containing an `__init__.py` file. The module exports whatever is listed in the `__all__` attribute defined in `__init__.py`.

### [​](http://docs.comfy.org#init-py) **init**.py

`__init__.py` is executed when Comfy attempts to import the module. For a module to be recognized as containing custom node definitions, it needs to export `NODE_CLASS_MAPPINGS`. If it does (and if nothing goes wrong in the import), the nodes defined in the module will be available in Comfy. If there is an error in your code, Comfy will continue, but will report the module as having failed to load. So check the Python console!

A very simple `__init__.py` file would look like this:

```python
from .python_file import MyCustomNode
NODE_CLASS_MAPPINGS = { "My Custom Node" : MyCustomNode }
__all__ = ["NODE_CLASS_MAPPINGS"]
```

#### [​](http://docs.comfy.org#node-class-mappings) NODE\_CLASS\_MAPPINGS

`NODE_CLASS_MAPPINGS` must be a `dict` mapping custom node names (unique across the Comfy install) to the corresponding node class.

#### [​](http://docs.comfy.org#node-display-name-mappings) NODE\_DISPLAY\_NAME\_MAPPINGS

`__init__.py` may also export `NODE_DISPLAY_NAME_MAPPINGS`, which maps the same unique name to a display name for the node. If `NODE_DISPLAY_NAME_MAPPINGS` is not provided, Comfy will use the unique name as the display name.

#### [​](http://docs.comfy.org#web-directory) WEB\_DIRECTORY

If you are deploying client side code, you will also need to export the path, relative to the module, in which the JavaScript files are to be found. It is conventional to place these in a subdirectory of your custom node named `js`.

*Only* `.js` files will be served; you can’t deploy `.css` or other types in this way

In previous versions of Comfy, `__init__.py` was required to copy the JavaScript files into the main Comfy web subdirectory. You will still see code that does this. Don’t.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/backend/lifecycle.mdx)

[Previous](http://docs.comfy.org/custom-nodes/backend/server_overview)

[Publishing to the Manager  
\
Next](http://docs.comfy.org/custom-nodes/backend/manager)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [How Comfy loads custom nodes](http://docs.comfy.org#how-comfy-loads-custom-nodes)
- [init.py](http://docs.comfy.org#init-py)
- [NODE\_CLASS\_MAPPINGS](http://docs.comfy.org#node-class-mappings)
- [NODE\_DISPLAY\_NAME\_MAPPINGS](http://docs.comfy.org#node-display-name-mappings)
- [WEB\_DIRECTORY](http://docs.comfy.org#web-directory)

<!-- END Development/custom-nodes/backend/lifecycle.md -->


<!-- BEGIN Development/custom-nodes/backend/lists.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
  
  - [Properties](http://docs.comfy.org/custom-nodes/backend/server_overview)
  - [Lifecycle](http://docs.comfy.org/custom-nodes/backend/lifecycle)
  - [Publishing to the Manager](http://docs.comfy.org/custom-nodes/backend/manager)
  - [Datatypes](http://docs.comfy.org/custom-nodes/backend/datatypes)
  - [Images, Latents, and Masks](http://docs.comfy.org/custom-nodes/backend/images_and_masks)
  - [Hidden and Flexible inputs](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)
  - [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)
  - [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion)
  - [Data lists](http://docs.comfy.org/custom-nodes/backend/lists)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/backend/snippets)
  - [Working with torch.Tensor](http://docs.comfy.org/custom-nodes/backend/tensors)
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Data lists

# Data lists

## [​](http://docs.comfy.org#length-one-processing) Length one processing

Internally, the Comfy server represents data flowing from one node to the next as a Python `list`, normally length 1, of the relevant datatype. In normal operation, when a node returns an output, each element in the output `tuple` is separately wrapped in a list (length 1); then when the next node is called, the data is unwrapped and passed to the main function.

You generally don’t need to worry about this, since Comfy does the wrapping and unwrapping.

This isn’t about batches. A batch (of, for instance, latents, or images) is a *single entry* in the list (see [tensor datatypes](http://docs.comfy.org/images_and_masks))

## [​](http://docs.comfy.org#list-processing) List processing

In some circumstance, multiple data instances are processed in a single workflow, in which case the internal data will be a list containing the data instances. An example of this might be processing a series of images one at a time to avoid running out of VRAM, or handling images of different sizes.

By default, Comfy will process the values in the list sequentially:

- if the inputs are `list`s of different lengths, the shorter ones are padded by repeating the last value
- the main method is called once for each value in the input lists
- the outputs are `list`s, each of which is the same length as the longest input

The relevant code can be found in the method `map_node_over_list` in `execution.py`.

However, as Comfy wraps node outputs into a `list` of length one, if the `tuple` returned by a custom node contains a `list`, that `list` will be wrapped, and treated as a single piece of data. In order to tell Comfy that the list being returned should not be wrapped, but treated as a series of data for sequential processing, the node should provide a class attribute `OUTPUT_IS_LIST`, which is a `tuple[bool]`, of the same length as `RETURN_TYPES`, specifying which outputs which should be so treated.

A node can also override the default input behaviour and receive the whole list in a single call. This is done by setting a class attribute `INPUT_IS_LIST` to `True`.

Here’s a (lightly annotated) example from the built in nodes - `ImageRebatch` takes one or more batches of images (received as a list, because `INPUT_IS_LIST - True`) and rebatches them into batches of the requested size.

`INPUT_IS_LIST` is node level - all inputs get the same treatment. So the value of the `batch_size` widget is given by `batch_size[0]`.

```python

class ImageRebatch:
    @classmethod
    def INPUT_TYPES(s):
        return {"required": { "images": ("IMAGE",),
                              "batch_size": ("INT", {"default": 1, "min": 1, "max": 4096}) }}
    RETURN_TYPES = ("IMAGE",)
    INPUT_IS_LIST = True
    OUTPUT_IS_LIST = (True, )
    FUNCTION = "rebatch"
    CATEGORY = "image/batch"

    def rebatch(self, images, batch_size):
        batch_size = batch_size[0]    # everything comes as a list, so batch_size is list[int]

        output_list = []
        all_images = []
        for img in images:                    # each img is a batch of images
            for i in range(img.shape[0]):     # each i is a single image
                all_images.append(img[i:i+1])

        for i in range(0, len(all_images), batch_size): # take batch_size chunks and turn each into a new batch
            output_list.append(torch.cat(all_images[i:i+batch_size], dim=0))  # will die horribly if the image batches had different width or height!

        return (output_list,)
```

#### [​](http://docs.comfy.org#input-is-list) INPUT\_IS\_LIST

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/backend/lists.mdx)

[Previous](http://docs.comfy.org/custom-nodes/backend/expansion)

[Annotated Examples  
\
Next](http://docs.comfy.org/custom-nodes/backend/snippets)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Length one processing](http://docs.comfy.org#length-one-processing)
- [List processing](http://docs.comfy.org#list-processing)
- [INPUT\_IS\_LIST](http://docs.comfy.org#input-is-list)

<!-- END Development/custom-nodes/backend/lists.md -->


<!-- BEGIN Development/custom-nodes/backend/manager.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
  
  - [Properties](http://docs.comfy.org/custom-nodes/backend/server_overview)
  - [Lifecycle](http://docs.comfy.org/custom-nodes/backend/lifecycle)
  - [Publishing to the Manager](http://docs.comfy.org/custom-nodes/backend/manager)
  - [Datatypes](http://docs.comfy.org/custom-nodes/backend/datatypes)
  - [Images, Latents, and Masks](http://docs.comfy.org/custom-nodes/backend/images_and_masks)
  - [Hidden and Flexible inputs](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)
  - [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)
  - [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion)
  - [Data lists](http://docs.comfy.org/custom-nodes/backend/lists)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/backend/snippets)
  - [Working with torch.Tensor](http://docs.comfy.org/custom-nodes/backend/tensors)
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Publishing to the Manager

# Publishing to the Manager

### [​](http://docs.comfy.org#using-comfyui-manager) Using ComfyUI Manager

To make your custom node available through **ComfyUI Manager** you need to save it as a git repository (generally at `github.com`) and then submit a Pull Request on the **ComfyUI Manager** git, in which you have edited `custom-node-list.json` to add your node. [More details](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#how-to-register-your-custom-node-into-comfyui-manager).

When a user installs the node, **ComfyUI Manager** will:

1

Git Clone

git clone the repository,

2

Install Python Dependencies

install the pip dependencies listed in the custom node repository under `requirements.txt` (if present),

```plaintext
pip install -r requirements.txt
```

As is always the case with `pip`, it is possible that your node requirements will be in conflict with other custom nodes. Don’t make your `requirements.txt` any more restrictive than they need to be.

3

Run Install Script

execute `install.py`, if it is present in the custom node repository.

`install.py` is executed from the root path of the custom node

### [​](http://docs.comfy.org#comfyui-manager-files) ComfyUI Manager files

As indicated above, there are a number of files and scripts that **ComfyUI Manager** will use to manage the lifecycle of a custom node. These are all optional.

- `requirements.txt` - Python dependencies as mentioned above
- `install.py`, `uninstall.py` - executed when the custom node is installed or uninstalled
  
  Users can just delete the directory, so you can’t rely on `uninstall.py` being run
- `disable.py`, `enable.py` - executed when a custom node is disabled or re-enabled
  
  `enable.py` is only run when a disabled node is re-enabled - it should just reverse anything done in `disable.py`
  
  Disabled custom node subdirectory have `.disabled` appended to their names, and Comfy ignores these modules
- `node_list.json` - only required if the custom nodes pattern of NODE\_CLASS\_MAPPINGS is not conventional.

See the [ComfyUI Manager guide](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#custom-node-support-guide) for official details.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/backend/manager.mdx)

[Previous](http://docs.comfy.org/custom-nodes/backend/lifecycle)

[Datatypes  
\
Next](http://docs.comfy.org/custom-nodes/backend/datatypes)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Using ComfyUI Manager](http://docs.comfy.org#using-comfyui-manager)
- [ComfyUI Manager files](http://docs.comfy.org#comfyui-manager-files)

<!-- END Development/custom-nodes/backend/manager.md -->


<!-- BEGIN Development/custom-nodes/backend/more_on_inputs.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
  
  - [Properties](http://docs.comfy.org/custom-nodes/backend/server_overview)
  - [Lifecycle](http://docs.comfy.org/custom-nodes/backend/lifecycle)
  - [Publishing to the Manager](http://docs.comfy.org/custom-nodes/backend/manager)
  - [Datatypes](http://docs.comfy.org/custom-nodes/backend/datatypes)
  - [Images, Latents, and Masks](http://docs.comfy.org/custom-nodes/backend/images_and_masks)
  - [Hidden and Flexible inputs](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)
  - [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)
  - [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion)
  - [Data lists](http://docs.comfy.org/custom-nodes/backend/lists)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/backend/snippets)
  - [Working with torch.Tensor](http://docs.comfy.org/custom-nodes/backend/tensors)
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Hidden and Flexible inputs

# Hidden and Flexible inputs

## [​](http://docs.comfy.org#hidden-inputs) Hidden inputs

Alongside the `required` and `optional` inputs, which create corresponding inputs or widgets on the client-side, there are three `hidden` input options which allow the custom node to request certain information from the server.

These are accessed by returning a value for `hidden` in the `INPUT_TYPES` `dict`, with the signature `dict[str,str]`, containing one or more of `PROMPT`, `EXTRA_PNGINFO`, or `UNIQUE_ID`

```python
@classmethod
def INPUT_TYPES(s):
    return {
        "required": {...},
        "optional": {...},
        "hidden": {
            "unique_id": "UNIQUE_ID",
            "prompt": "PROMPT", 
            "extra_pnginfo": "EXTRA_PNGINFO",
        }
    }
```

### [​](http://docs.comfy.org#unique-id) UNIQUE\_ID

`UNIQUE_ID` is the unique identifier of the node, and matches the `id` property of the node on the client side. It is commonly used in client-server communications (see [messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages#getting-node-id)).

### [​](http://docs.comfy.org#prompt) PROMPT

`PROMPT` is the complete prompt sent by the client to the server. See [the prompt object](http://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking#prompt) for a full description.

### [​](http://docs.comfy.org#extra-pnginfo) EXTRA\_PNGINFO

`EXTRA_PNGINFO` is a dictionary that will be copied into the metadata of any `.png` files saved. Custom nodes can store additional information in this dictionary for saving (or as a way to communicate with a downstream node).

Note that if Comfy is started with the `disable_metadata` option, this data won’t be saved.

### [​](http://docs.comfy.org#dynprompt) DYNPROMPT

`DYNPROMPT` is an instance of `comfy_execution.graph.DynamicPrompt`. It differs from `PROMPT` in that it may mutate during the course of execution in response to [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion).

`DYNPROMPT` should only be used for advanced cases (like implementing loops in custom nodes).

## [​](http://docs.comfy.org#flexible-inputs) Flexible inputs

### [​](http://docs.comfy.org#custom-datatypes) Custom datatypes

If you want to pass data between your own custom nodes, you may find it helpful to define a custom datatype. This is (almost) as simple as just choosing a name for the datatype, which should be a unique string in upper case, such as `CHEESE`.

You can then use `CHEESE` in your node `INPUT_TYPES` and `RETURN_TYPES`, and the Comfy client will only allow `CHEESE` outputs to connect to a `CHEESE` input. `CHEESE` can be any python object.

The only point to note is that because the Comfy client doesn’t know about `CHEESE` you need (unless you define a custom widget for `CHEESE`, which is a topic for another day), to force it to be an input rather than a widget. This can be done with the `forceInput` option in the input options dictionary:

```python
@classmethod
def INPUT_TYPES(s):
    return {
        "required": { "my_cheese": ("CHEESE", {"forceInput":True}) }
    }
```

### [​](http://docs.comfy.org#wildcard-inputs) Wildcard inputs

```python
@classmethod
def INPUT_TYPES(s):
    return {
        "required": { "anything": ("*",{})},
    }

@classmethod
def VALIDATE_INPUTS(s, input_types):
    return True
```

The frontend allows `*` to indicate that an input can be connected to any source. Because this is not officially supported by the backend, you can skip the backend validation of types by accepting a parameter named `input_types` in your `VALIDATE_INPUTS` function. (See [VALIDATE\_INPUTS](http://docs.comfy.org/server_overview#validate-inputs) for more information.) It’s up to the node to make sense of the data that is passed.

### [​](http://docs.comfy.org#dynamically-created-inputs) Dynamically created inputs

If inputs are dynamically created on the client side, they can’t be defined in the Python source code. In order to access this data we need an `optional` dictionary that allows Comfy to pass data with arbitrary names. Since the Comfy server

```python
class ContainsAnyDict(dict):
    def __contains__(self, key):
        return True
...

@classmethod
def INPUT_TYPES(s):
    return {
        "required": {},
        "optional": ContainsAnyDict()
    }
...

def main_method(self, **kwargs):
    # the dynamically created input data will be in the dictionary kwargs

```

Hat tip to rgthree for this pythonic trick!

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/backend/more_on_inputs.mdx)

[Previous](http://docs.comfy.org/custom-nodes/backend/images_and_masks)

[Lazy Evaluation  
\
Next](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Hidden inputs](http://docs.comfy.org#hidden-inputs)
- [UNIQUE\_ID](http://docs.comfy.org#unique-id)
- [PROMPT](http://docs.comfy.org#prompt)
- [EXTRA\_PNGINFO](http://docs.comfy.org#extra-pnginfo)
- [DYNPROMPT](http://docs.comfy.org#dynprompt)
- [Flexible inputs](http://docs.comfy.org#flexible-inputs)
- [Custom datatypes](http://docs.comfy.org#custom-datatypes)
- [Wildcard inputs](http://docs.comfy.org#wildcard-inputs)
- [Dynamically created inputs](http://docs.comfy.org#dynamically-created-inputs)

<!-- END Development/custom-nodes/backend/more_on_inputs.md -->


<!-- BEGIN Development/custom-nodes/backend/server_overview.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
  
  - [Properties](http://docs.comfy.org/custom-nodes/backend/server_overview)
  - [Lifecycle](http://docs.comfy.org/custom-nodes/backend/lifecycle)
  - [Publishing to the Manager](http://docs.comfy.org/custom-nodes/backend/manager)
  - [Datatypes](http://docs.comfy.org/custom-nodes/backend/datatypes)
  - [Images, Latents, and Masks](http://docs.comfy.org/custom-nodes/backend/images_and_masks)
  - [Hidden and Flexible inputs](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)
  - [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)
  - [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion)
  - [Data lists](http://docs.comfy.org/custom-nodes/backend/lists)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/backend/snippets)
  - [Working with torch.Tensor](http://docs.comfy.org/custom-nodes/backend/tensors)
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Properties

# Properties

Properties of a custom node

### [​](http://docs.comfy.org#simple-example) Simple Example

Here’s the code for the Invert Image Node, which gives an overview of the key concepts in custom node development.

```python
class InvertImageNode:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": { "image_in" : ("IMAGE", {}) },
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image_out",)
    CATEGORY = "examples"
    FUNCTION = "invert"

    def invert(self, image_in):
        image_out = 1 - image_in
        return (image_out,)
```

### [​](http://docs.comfy.org#main-properties) Main properties

Every custom node is a Python class, with the following key properties:

#### [​](http://docs.comfy.org#input-types) INPUT\_TYPES

`INPUT_TYPES`, as the name suggests, defines the inputs for the node. The method returns a `dict` which *must* contain the key `required`, and *may* also include the keys `optional` and/or `hidden`. The only difference between `required` and `optional` inputs is that `optional` inputs can be left unconnected. For more information on `hidden` inputs, see [Hidden Inputs](http://docs.comfy.org/more_on_inputs#hidden-inputs).

Each key has, as its value, another `dict`, in which key-value pairs specify the names and types of the inputs. The types are defined by a `tuple`, the first element of which defines the data type, and the second element of which is a `dict` of additional parameters.

Here we have just one required input, named `image_in`, of type `IMAGE`, with no additional parameters.

Note that unlike the next few attributes, this `INPUT_TYPES` is a `@classmethod`. This is so that the options in dropdown widgets (like the name of the checkpoint to be loaded) can be computed by Comfy at run time. We’ll go into this more later.

#### [​](http://docs.comfy.org#return-types) RETURN\_TYPES

A `tuple` of `str` defining the data types returned by the node. If the node has no outputs this must still be provided `RETURN_TYPES = ()`

If you have exactly one output, remember the trailing comma: `RETURN_TYPES = ("IMAGE",)`. This is required for Python to make it a `tuple`

#### [​](http://docs.comfy.org#return-names) RETURN\_NAMES

The names to be used to label the outputs. This is optional; if omitted, the names are simply the `RETURN_TYPES` in lowercase.

#### [​](http://docs.comfy.org#category) CATEGORY

Where the node will be found in the ComfyUI **Add Node** menu. Submenus can be specified as a path, eg. `examples/trivial`.

#### [​](http://docs.comfy.org#function) FUNCTION

The name of the Python function in the class that should be called when the node is executed.

The function is called with named arguments. All `required` (and `hidden`) inputs will be included; `optional` inputs will be included only if they are connected, so you should provide default values for them in the function definition (or capture them with `**kwargs`).

The function returns a tuple corresponding to the `RETURN_TYPES`. This is required even if nothing is returned (`return ()`). Again, if you only have one output, remember that trailing comma `return (image_out,)`!

### [​](http://docs.comfy.org#execution-control-extras) Execution Control Extras

A great feature of Comfy is that it caches outputs, and only executes nodes that might produce a different result than the previous run. This can greatly speed up lots of workflows.

In essence this works by identifying which nodes produce an output (these, notably the Image Preview and Save Image nodes, are always executed), and then working backwards to identify which nodes provide data that might have changed since the last run.

Two optional features of a custom node assist in this process.

#### [​](http://docs.comfy.org#output-node) OUTPUT\_NODE

By default, a node is not considered an output. Set `OUTPUT_NODE = True` to specify that it is.

#### [​](http://docs.comfy.org#is-changed) IS\_CHANGED

By default, Comfy considers that a node has changed if any of its inputs or widgets have changed. This is normally correct, but you may need to override this if, for instance, the node uses a random number (and does not specify a seed - it’s best practice to have a seed input in this case so that the user can control reproducability and avoid unecessary execution), or loads an input that may have changed externally, or sometimes ignores inputs (so doesn’t need to execute just because those inputs changed).

Despite the name, IS\_CHANGED should not return a `bool`

`IS_CHANGED` is passed the same arguments as the main function defined by `FUNCTION`, and can return any Python object. This object is compared with the one returned in the previous run (if any) and the node will be considered to have changed if `is_changed != is_changed_old` (this code is in `execution.py` if you need to dig).

Since `True == True`, a node that returns `True` to say it has changed will be considered not to have! I’m sure this would be changed in the Comfy code if it wasn’t for the fact that it might break existing nodes to do so.

To specify that your node should always be considered to have changed (which you should avoid if possible, since it stops Comfy optimising what gets run), `return float("NaN")`. This returns a `NaN` value, which is not equal to anything, even another `NaN`.

A good example of actually checking for changes is the code from the built-in LoadImage node, which loads the image and returns a hash

```python
    @classmethod
    def IS_CHANGED(s, image):
        image_path = folder_paths.get_annotated_filepath(image)
        m = hashlib.sha256()
        with open(image_path, 'rb') as f:
            m.update(f.read())
        return m.digest().hex()
```

### [​](http://docs.comfy.org#other-attributes) Other attributes

There are three other attributes that can be used to modify the default Comfy treatment of a node.

#### [​](http://docs.comfy.org#input-is-list%2C-output-is-list) INPUT\_IS\_LIST, OUTPUT\_IS\_LIST

These are used to control sequential processing of data, and are described [later](http://docs.comfy.org/lists.mdx).

### [​](http://docs.comfy.org#validate-inputs) VALIDATE\_INPUTS

If a class method `VALIDATE_INPUTS` is defined, it will be called before the workflow begins execution. `VALIDATE_INPUTS` should return `True` if the inputs are valid, or a message (as a `str`) describing the error (which will prevent execution).

#### [​](http://docs.comfy.org#validating-constants) Validating Constants

Note that `VALIDATE_INPUTS` will only receive inputs that are defined as constants within the workflow. Any inputs that are received from other nodes will *not* be available in `VALIDATE_INPUTS`.

`VALIDATE_INPUTS` is called with only the inputs that its signature requests (those returned by `inspect.getfullargspec(obj_class.VALIDATE_INPUTS).args`). Any inputs which are received in this way will *not* run through the default validation rules. For example, in the following snippet, the front-end will use the specified `min` and `max` values of the `foo` input, but the back-end will not enforce it.

```python
class CustomNode:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": { "foo" : ("INT", {"min": 0, "max": 10}) },
        }

    @classmethod
    def VALIDATE_INPUTS(cls, foo):
        # YOLO, anything goes!
        return True
```

Additionally, if the function takes a `**kwargs` input, it will receive *all* available inputs and all of them will skip validation as if specified explicitly.

#### [​](http://docs.comfy.org#validating-types) Validating Types

If the `VALIDATE_INPUTS` method receives an argument named `input_types`, it will be passed a dictionary in which the key is the name of each input which is connected to an output from another node and the value is the type of that output.

When this argument is present, all default validation of input types is skipped. Here’s an example making use of the fact that the front-end allows for the specification of multiple types:

```python
class AddNumbers:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "input1" : ("INT,FLOAT", {"min": 0, "max": 1000})
                "input2" : ("INT,FLOAT", {"min": 0, "max": 1000})
            },
        }

    @classmethod
    def VALIDATE_INPUTS(cls, input_types):
        # The min and max of input1 and input2 are still validated because
        # we didn't take `input1` or `input2` as arguments
        if input_types["input1"] not in ("INT", "FLOAT"):
            return "input1 must be an INT or FLOAT type"
        if input_types["input2"] not in ("INT", "FLOAT"):
            return "input2 must be an INT or FLOAT type"
        return True
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/backend/server_overview.mdx)

[Previous](http://docs.comfy.org/custom-nodes/walkthrough)

[Lifecycle  
\
Next](http://docs.comfy.org/custom-nodes/backend/lifecycle)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Simple Example](http://docs.comfy.org#simple-example)
- [Main properties](http://docs.comfy.org#main-properties)
- [INPUT\_TYPES](http://docs.comfy.org#input-types)
- [RETURN\_TYPES](http://docs.comfy.org#return-types)
- [RETURN\_NAMES](http://docs.comfy.org#return-names)
- [CATEGORY](http://docs.comfy.org#category)
- [FUNCTION](http://docs.comfy.org#function)
- [Execution Control Extras](http://docs.comfy.org#execution-control-extras)
- [OUTPUT\_NODE](http://docs.comfy.org#output-node)
- [IS\_CHANGED](http://docs.comfy.org#is-changed)
- [Other attributes](http://docs.comfy.org#other-attributes)
- [INPUT\_IS\_LIST, OUTPUT\_IS\_LIST](http://docs.comfy.org#input-is-list%2C-output-is-list)
- [VALIDATE\_INPUTS](http://docs.comfy.org#validate-inputs)
- [Validating Constants](http://docs.comfy.org#validating-constants)
- [Validating Types](http://docs.comfy.org#validating-types)

<!-- END Development/custom-nodes/backend/server_overview.md -->


<!-- BEGIN Development/custom-nodes/backend/snippets.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
  
  - [Properties](http://docs.comfy.org/custom-nodes/backend/server_overview)
  - [Lifecycle](http://docs.comfy.org/custom-nodes/backend/lifecycle)
  - [Publishing to the Manager](http://docs.comfy.org/custom-nodes/backend/manager)
  - [Datatypes](http://docs.comfy.org/custom-nodes/backend/datatypes)
  - [Images, Latents, and Masks](http://docs.comfy.org/custom-nodes/backend/images_and_masks)
  - [Hidden and Flexible inputs](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)
  - [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)
  - [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion)
  - [Data lists](http://docs.comfy.org/custom-nodes/backend/lists)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/backend/snippets)
  - [Working with torch.Tensor](http://docs.comfy.org/custom-nodes/backend/tensors)
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Annotated Examples

# Annotated Examples

A growing collection of fragments of example code…

## [​](http://docs.comfy.org#images-and-masks) Images and Masks

### [​](http://docs.comfy.org#load-an-image) Load an image

Load an image into a batch of size 1 (based on `LoadImage` source code in `nodes.py`)

```python
i = Image.open(image_path)
i = ImageOps.exif_transpose(i)
if i.mode == 'I':
    i = i.point(lambda i: i * (1 / 255))
image = i.convert("RGB")
image = np.array(image).astype(np.float32) / 255.0
image = torch.from_numpy(image)[None,]
```

### [​](http://docs.comfy.org#save-an-image-batch) Save an image batch

Save a batch of images (based on `SaveImage` source code in `nodes.py`)

```python
for (batch_number, image) in enumerate(images):
    i = 255. * image.cpu().numpy()
    img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))
    filepath = # some path that takes the batch number into account
    img.save(filepath)
```

### [​](http://docs.comfy.org#invert-a-mask) Invert a mask

Inverting a mask is a straightforward process. Since masks are normalised to the range \[0,1]:

```python
mask = 1.0 - mask
```

### [​](http://docs.comfy.org#convert-a-mask-to-image-shape) Convert a mask to Image shape

```python
# We want [B,H,W,C] with C = 1
if len(mask.shape)==2: # we have [H,W], so insert B and C as dimension 1
    mask = mask[None,:,:,None]
elif len(mask.shape)==3 and mask.shape[2]==1: # we have [H,W,C]
    mask = mask[None,:,:,:]
elif len(mask.shape)==3:                      # we have [B,H,W]
    mask = mask[:,:,:,None]
```

### [​](http://docs.comfy.org#using-masks-as-transparency-layers) Using Masks as Transparency Layers

When used for tasks like inpainting or segmentation, the MASK’s values will eventually be rounded to the nearest integer so that they are binary — 0 indicating regions to be ignored and 1 indicating regions to be targeted. However, this doesn’t happen until the MASK is passed to those nodes. This flexibility allows you to use MASKs as as you would in digital photography contexts as a transparency layer:

```python
# Invert mask back to original transparency layer
mask = 1.0 - mask

# Unsqueeze the `C` (channels) dimension
mask = mask.unsqueeze(-1)

# Concatenate ("cat") along the `C` dimension
rgba_image = torch.cat((rgb_image, mask), dim=-1)
```

## [​](http://docs.comfy.org#noise) Noise

### [​](http://docs.comfy.org#creating-noise-variations) Creating noise variations

Here’s an example of creating a noise object which mixes the noise from two sources. This could be used to create slight noise variations by varying `weight2`.

```python
class Noise_MixedNoise:
    def __init__(self, nosie1, noise2, weight2):
        self.noise1  = noise1
        self.noise2  = noise2
        self.weight2 = weight2

    @property
    def seed(self): return self.noise1.seed

    def generate_noise(self, input_latent:torch.Tensor) -> torch.Tensor:
        noise1 = self.noise1.generate_noise(input_latent)
        noise2 = self.noise2.generate_noise(input_latent)
        return noise1 * (1.0-self.weight2) + noise2 * (self.weight2)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/backend/snippets.mdx)

[Previous](http://docs.comfy.org/custom-nodes/backend/lists)

[Working with torch.Tensor  
\
Next](http://docs.comfy.org/custom-nodes/backend/tensors)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Images and Masks](http://docs.comfy.org#images-and-masks)
- [Load an image](http://docs.comfy.org#load-an-image)
- [Save an image batch](http://docs.comfy.org#save-an-image-batch)
- [Invert a mask](http://docs.comfy.org#invert-a-mask)
- [Convert a mask to Image shape](http://docs.comfy.org#convert-a-mask-to-image-shape)
- [Using Masks as Transparency Layers](http://docs.comfy.org#using-masks-as-transparency-layers)
- [Noise](http://docs.comfy.org#noise)
- [Creating noise variations](http://docs.comfy.org#creating-noise-variations)

<!-- END Development/custom-nodes/backend/snippets.md -->


<!-- BEGIN Development/custom-nodes/backend/tensors.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
  
  - [Properties](http://docs.comfy.org/custom-nodes/backend/server_overview)
  - [Lifecycle](http://docs.comfy.org/custom-nodes/backend/lifecycle)
  - [Publishing to the Manager](http://docs.comfy.org/custom-nodes/backend/manager)
  - [Datatypes](http://docs.comfy.org/custom-nodes/backend/datatypes)
  - [Images, Latents, and Masks](http://docs.comfy.org/custom-nodes/backend/images_and_masks)
  - [Hidden and Flexible inputs](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)
  - [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)
  - [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion)
  - [Data lists](http://docs.comfy.org/custom-nodes/backend/lists)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/backend/snippets)
  - [Working with torch.Tensor](http://docs.comfy.org/custom-nodes/backend/tensors)
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Working with torch.Tensor

# Working with torch.Tensor

## [​](http://docs.comfy.org#pytorch%2C-tensors%2C-and-torch-tensor) pytorch, tensors, and torch.Tensor

All the core number crunching in Comfy is done by [pytorch](https://pytorch.org/). If your custom nodes are going to get into the guts of stable diffusion you will need to become familiar with this library, which is way beyond the scope of this introduction.

However, many custom nodes will need to manipulate images, latents and masks, each of which are represented internally as `torch.Tensor`, so you’ll want to bookmark the [documentation for torch.Tensor](https://pytorch.org/docs/stable/tensors.html).

### [​](http://docs.comfy.org#what-is-a-tensor%3F) What is a Tensor?

`torch.Tensor` represents a tensor, which is the mathematical generalization of a vector or matrix to any number of dimensions. A tensor’s *rank* is the number of dimensions it has (so a vector has *rank* 1, a matrix *rank* 2); its *shape* describes the size of each dimension.

So an RGB image (of height H and width W) might be thought of as three arrays (one for each color channel), each measuring H x W, which could be represented as a tensor with *shape* `[H,W,3]`. In Comfy images almost always come in a batch (even if the batch only contains a single image). `torch` always places the batch dimension first, so Comfy images have *shape* `[B,H,W,3]`, generally written as `[B,H,W,C]` where C stands for Channels.

### [​](http://docs.comfy.org#squeeze%2C-unsqueeze%2C-and-reshape) squeeze, unsqueeze, and reshape

If a tensor has a dimension of size 1 (known as a collapsed dimension), it is equivalent to the same tensor with that dimension removed (a batch with 1 image is just an image). Removing such a collapsed dimension is referred to as squeezing, and inserting one is known as unsqueezing.

Some torch code, and some custom node authors, will return a squeezed tensor when a dimension is collapsed - such as when a batch has only one member. This is a common cause of bugs!

To represent the same data in a different shape is referred to as reshaping. This often requires you to know the underlying data structure, so handle with care!

### [​](http://docs.comfy.org#important-notation) Important notation

`torch.Tensor` supports most Python slice notation, iteration, and other common list-like operations. A tensor also has a `.shape` attribute which returns its size as a `torch.Size` (which is a subclass of `tuple` and can be treated as such).

There are some other important bits of notation you’ll often see (several of these are less common standard Python notation, seen much more frequently when dealing with tensors)

- `torch.Tensor` supports the use of `None` in slice notation to indicate the insertion of a dimension of size 1.
- `:` is frequently used when slicing a tensor; this simply means ‘keep the whole dimension’. It’s like using `a[start:end]` in Python, but omitting the start point and end point.
- `...` represents ‘the whole of an unspecified number of dimensions’. So `a[0, ...]` would extract the first item from a batch regardless of the number of dimensions.
- in methods which require a shape to be passed, it is often passed as a `tuple` of the dimensions, in which a single dimension can be given the size `-1`, indicating that the size of this dimension should be calculated based on the total size of the data.

```python
>>> a = torch.Tensor((1,2))
>>> a.shape
torch.Size([2])
>>> a[:,None].shape 
torch.Size([2, 1])
>>> a.reshape((1,-1)).shape
torch.Size([1, 2])
```

### [​](http://docs.comfy.org#elementwise-operations) Elementwise operations

Many binary on `torch.Tensor` (including ’+’, ’-’, ’\*’, ’/’ and ’==’) are applied elementwise (independantly applied to each element). The operands must be *either* two tensors of the same shape, *or* a tensor and a scalar. So:

```python
>>> import torch
>>> a = torch.Tensor((1,2))
>>> b = torch.Tensor((3,2))
>>> a*b
tensor([3., 4.])
>>> a/b
tensor([0.3333, 1.0000])
>>> a==b
tensor([False,  True])
>>> a==1
tensor([ True, False])
>>> c = torch.Tensor((3,2,1)) 
>>> a==c
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0
```

### [​](http://docs.comfy.org#tensor-truthiness) Tensor truthiness

The ‘truthiness’ value of a Tensor is not the same as that of Python lists.

You may be familiar with the truthy value of a Python list as `True` for any non-empty list, and `False` for `None` or `[]`. By contrast A `torch.Tensor` (with more than one elements) does not have a defined truthy value. Instead you need to use `.all()` or `.any()` to combine the elementwise truthiness:

```python
>>> a = torch.Tensor((1,2))
>>> print("yes" if a else "no")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
RuntimeError: Boolean value of Tensor with more than one value is ambiguous
>>> a.all()
tensor(False)
>>> a.any()
tensor(True)
```

This also means that you need to use `if a is not None:` not `if a:` to determine if a tensor variable has been set.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/backend/tensors.mdx)

[Previous](http://docs.comfy.org/custom-nodes/backend/snippets)

[Javascript Extensions  
\
Next](http://docs.comfy.org/custom-nodes/js/javascript_overview)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [pytorch, tensors, and torch.Tensor](http://docs.comfy.org#pytorch%2C-tensors%2C-and-torch-tensor)
- [What is a Tensor?](http://docs.comfy.org#what-is-a-tensor%3F)
- [squeeze, unsqueeze, and reshape](http://docs.comfy.org#squeeze%2C-unsqueeze%2C-and-reshape)
- [Important notation](http://docs.comfy.org#important-notation)
- [Elementwise operations](http://docs.comfy.org#elementwise-operations)
- [Tensor truthiness](http://docs.comfy.org#tensor-truthiness)

<!-- END Development/custom-nodes/backend/tensors.md -->


<!-- BEGIN Development/custom-nodes/js/javascript_examples.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
  
  - [Javascript Extensions](http://docs.comfy.org/custom-nodes/js/javascript_overview)
  - [Comfy Hooks](http://docs.comfy.org/custom-nodes/js/javascript_hooks)
  - [Comfy Objects](http://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking)
  - [Settings](http://docs.comfy.org/custom-nodes/js/javascript_settings)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/js/javascript_examples)
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Annotated Examples

# Annotated Examples

A growing collection of fragments of example code…

## [​](http://docs.comfy.org#right-click-menus) Right click menus

### [​](http://docs.comfy.org#background-menu) Background menu

The main background menu (right-click on the canvas) is generated by a call to  
`LGraph.getCanvasMenuOptions`. One way to add your own menu options is to hijack this call:

```javascript
/* in setup() */
    const original_getCanvasMenuOptions = LGraphCanvas.prototype.getCanvasMenuOptions;
    LGraphCanvas.prototype.getCanvasMenuOptions = function () {
        // get the basic options 
        const options = original_getCanvasMenuOptions.apply(this, arguments);
        options.push(null); // inserts a divider
        options.push({
            content: "The text for the menu",
            callback: async () => {
                // do whatever
            }
        })
        return options;
    }
```

### [​](http://docs.comfy.org#node-menu) Node menu

When you right click on a node, the menu is similarly generated by `node.getExtraMenuOptions`. But instead of returning an options object, this one gets it passed in…

```javascript
/* in beforeRegisterNodeDef() */
if (nodeType?.comfyClass=="MyNodeClass") { 
    const original_getExtraMenuOptions = nodeType.prototype.getExtraMenuOptions;
    nodeType.prototype.getExtraMenuOptions = function(_, options) {
        original_getExtraMenuOptions?.apply(this, arguments);
        options.push({
            content: "Do something fun",
            callback: async () => {
                // fun thing
            }
        })
    }   
}
```

### [​](http://docs.comfy.org#submenus) Submenus

If you want a submenu, provide a callback which uses `LiteGraph.ContextMenu` to create it:

```javascript
function make_submenu(value, options, e, menu, node) {
    const submenu = new LiteGraph.ContextMenu(
        ["option 1", "option 2", "option 3"],
        { 
            event: e, 
            callback: function (v) { 
                // do something with v (=="option x")
            }, 
            parentMenu: menu, 
            node:node
        }
    )
}

/* ... */
    options.push(
        {
            content: "Menu with options",
            has_submenu: true,
            callback: make_submenu,
        }
    )
```

## [​](http://docs.comfy.org#capture-ui-events) Capture UI events

This works just like you’d expect - find the UI element in the DOM and add an eventListener. `setup()` is a good place to do this, since the page has fully loaded. For instance, to detect a click on the ‘Queue’ button:

```javascript
function queue_button_pressed() { console.log("Queue button was pressed!") }
document.getElementById("queue-button").addEventListener("click", queue_button_pressed);
```

## [​](http://docs.comfy.org#detect-when-a-workflow-starts) Detect when a workflow starts

This is one of many `api` events:

```javascript
import { api } from "../../scripts/api.js";
/* in setup() */
    function on_execution_start() { 
        /* do whatever */
    }
    api.addEventListener("execution_start", on_execution_start);
```

## [​](http://docs.comfy.org#detect-an-interrupted-workflow) Detect an interrupted workflow

A simple example of hijacking the api:

```javascript
import { api } from "../../scripts/api.js";
/* in setup() */
    const original_api_interrupt = api.interrupt;
    api.interrupt = function () {
        /* Do something before the original method is called */
        original_api_interrupt.apply(this, arguments);
        /* Or after */
    }
```

## [​](http://docs.comfy.org#catch-clicks-on-your-node) Catch clicks on your node

`node` has a mouseDown method you can hijack. This time we’re careful to pass on any return value.

```javascript
async nodeCreated(node) {
    if (node?.comfyClass === "My Node Name") {
        const original_onMouseDown = node.onMouseDown;
        node.onMouseDown = function( e, pos, canvas ) {
            alert("ouch!");
            return original_onMouseDown?.apply(this, arguments);
        }        
    }
}
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/js/javascript_examples.mdx)

[Previous](http://docs.comfy.org/custom-nodes/js/javascript_settings)

[Workflow templates  
\
Next](http://docs.comfy.org/custom-nodes/workflow_templates)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Right click menus](http://docs.comfy.org#right-click-menus)
- [Background menu](http://docs.comfy.org#background-menu)
- [Node menu](http://docs.comfy.org#node-menu)
- [Submenus](http://docs.comfy.org#submenus)
- [Capture UI events](http://docs.comfy.org#capture-ui-events)
- [Detect when a workflow starts](http://docs.comfy.org#detect-when-a-workflow-starts)
- [Detect an interrupted workflow](http://docs.comfy.org#detect-an-interrupted-workflow)
- [Catch clicks on your node](http://docs.comfy.org#catch-clicks-on-your-node)

<!-- END Development/custom-nodes/js/javascript_examples.md -->


<!-- BEGIN Development/custom-nodes/js/javascript_hooks.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
  
  - [Javascript Extensions](http://docs.comfy.org/custom-nodes/js/javascript_overview)
  - [Comfy Hooks](http://docs.comfy.org/custom-nodes/js/javascript_hooks)
  - [Comfy Objects](http://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking)
  - [Settings](http://docs.comfy.org/custom-nodes/js/javascript_settings)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/js/javascript_examples)
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Comfy Hooks

# Comfy Hooks

## [​](http://docs.comfy.org#extension-hooks) Extension hooks

At various points during Comfy execution, the application calls `#invokeExtensionsAsync` or `#invokeExtensions` with the name of a hook. These invoke, on all registered extensions, the appropriately named method (if present), such as `setup` in the example above.

Comfy provides a variety of hooks for custom extension code to use to modify client behavior.

These hooks are called during creation and modification of the Comfy client side elements.  
Events during workflow execution are handled by the `apiUpdateHandlers`

A few of the most significant hooks are described below. As Comfy is being actively developed, from time to time additional hooks are added, so search for `#invokeExtensions` in `app.js` to find all available hooks.

See also the [sequence](http://docs.comfy.org/_sites/docs.comfy.org/custom-nodes/js/javascript_hooks#call-sequences) in which hooks are invoked.

### [​](http://docs.comfy.org#commonly-used-hooks) Commonly used hooks

Start with `beforeRegisterNodeDef`, which is used by the majority of extensions, and is often the only one needed.

#### [​](http://docs.comfy.org#beforeregisternodedef) beforeRegisterNodeDef()

Called once for each node type (the list of nodes available in the `AddNode` menu), and is used to modify the bahaviour of the node.

```javascript
async beforeRegisterNodeDef(nodeType, nodeData, app) 
```

The object passed in the `nodeType` parameter essentially serves as a template for all nodes that will be created of this type, so modifications made to `nodeType.prototype` will apply to all nodes of this type. `nodeData` is an encapsulation of aspects of the node defined in the Python code, such as its category, inputs, and outputs. `app` is a reference to the main Comfy app object (which you have already imported anyway!)

This method is called, on each registered extension, for *every* node type, not just the ones added by that extension.

The usual idiom is to check `nodeType.ComfyClass`, which holds the Python class name corresponding to this node, to see if you need to modify the node. Often this means modifying the custom nodes that you have added, although you may sometimes need to modify the behavior of other nodes (or other custom nodes might modify yours!), in which case care should be taken to ensure interoperability.

Since other extensions may also modify nodes, aim to write code that makes as few assumptions as possible. And play nicely - isolate your changes wherever possible.

A very common idiom in `beforeRegisterNodeDef` is to ‘hijack’ an existing method:

```javascript
async beforeRegisterNodeDef(nodeType, nodeData, app) {
	if (nodeType.comfyClass=="MyNodeClass") { 
		const onConnectionsChange = nodeType.prototype.onConnectionsChange;
		nodeType.prototype.onConnectionsChange = function (side,slot,connect,link_info,output) {     
			const r = onConnectionsChange?.apply(this, arguments);   
			console.log("Someone changed my connection!");
			return r;
		}
	}
}
```

In this idiom the existing prototype method is stored, and then replaced. The replacement calls the original method (the `?.apply` ensures that if there wasn’t one this is still safe) and then performs additional operations. Depending on your code logic, you may need to place the `apply` elsewhere in your replacement code, or even make calling it conditional.

When hijacking a method in this way, you will want to look at the core comfy code (breakpoints are your friend) to check and conform with the method signature.

#### [​](http://docs.comfy.org#nodecreated) nodeCreated()

```javascript
async nodeCreated(node)
```

Called when a specific instance of a node gets created (right at the end of the `ComfyNode()` function on `nodeType` which serves as a constructor). In this hook you can make modifications to individual instances of your node.

Changes that apply to all instances are better added to the prototype in `beforeRegisterNodeDef` as described above.

#### [​](http://docs.comfy.org#init) init()

```javascript
async init()
```

Called when the Comfy webpage is loaded (or reloaded). The call is made after the graph object has been created, but before any nodes are registered or created. It can be used to modify core Comfy behavior by hijacking methods of the app, or of the graph (a `LiteGraph` object). This is discussed further in [Comfy Objects](http://docs.comfy.org/javascript_objects_and_hijacking).

With great power comes great responsibility. Hijacking core behavior makes it more likely your nodes will be incompatible with other custom nodes, or future Comfy updates

#### [​](http://docs.comfy.org#setup) setup()

```javascript
async setup()
```

Called at the end of the startup process. A good place to add event listeners (either for Comfy events, or DOM events), or adding to the global menus, both of which are discussed elsewhere.

To do something when a workflow has loaded, use `afterConfigureGraph`, not `setup`

### [​](http://docs.comfy.org#call-sequences) Call sequences

These sequences were obtained by insert logging code into the Comfy `app.js` file. You may find similar code helpful in understanding the execution flow.

```javascript
/* approx line 220 at time of writing: */
	#invokeExtensions(method, ...args) {
		console.log(`invokeExtensions      ${method}`) // this line added
		// ...
	}
/* approx line 250 at time of writing: */
	async #invokeExtensionsAsync(method, ...args) {
		console.log(`invokeExtensionsAsync ${method}`) // this line added
		// ...
	}
```

#### [​](http://docs.comfy.org#web-page-load) Web page load

```plaintext
invokeExtensionsAsync init
invokeExtensionsAsync addCustomNodeDefs
invokeExtensionsAsync getCustomWidgets
invokeExtensionsAsync beforeRegisterNodeDef    [repeated multiple times]
invokeExtensionsAsync registerCustomNodes
invokeExtensionsAsync beforeConfigureGraph
invokeExtensionsAsync nodeCreated
invokeExtensions      loadedGraphNode
invokeExtensionsAsync afterConfigureGraph
invokeExtensionsAsync setup
```

#### [​](http://docs.comfy.org#loading-workflow) Loading workflow

```plaintext
invokeExtensionsAsync beforeConfigureGraph
invokeExtensionsAsync beforeRegisterNodeDef   [zero, one, or multiple times]
invokeExtensionsAsync nodeCreated             [repeated multiple times]
invokeExtensions      loadedGraphNode         [repeated multiple times]
invokeExtensionsAsync afterConfigureGraph
```

#### [​](http://docs.comfy.org#adding-new-node) Adding new node

```plaintext
invokeExtensionsAsync nodeCreated
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/js/javascript_hooks.mdx)

[Previous](http://docs.comfy.org/custom-nodes/js/javascript_overview)

[Comfy Objects  
\
Next](http://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Extension hooks](http://docs.comfy.org#extension-hooks)
- [Commonly used hooks](http://docs.comfy.org#commonly-used-hooks)
- [beforeRegisterNodeDef()](http://docs.comfy.org#beforeregisternodedef)
- [nodeCreated()](http://docs.comfy.org#nodecreated)
- [init()](http://docs.comfy.org#init)
- [setup()](http://docs.comfy.org#setup)
- [Call sequences](http://docs.comfy.org#call-sequences)
- [Web page load](http://docs.comfy.org#web-page-load)
- [Loading workflow](http://docs.comfy.org#loading-workflow)
- [Adding new node](http://docs.comfy.org#adding-new-node)

<!-- END Development/custom-nodes/js/javascript_hooks.md -->


<!-- BEGIN Development/custom-nodes/js/javascript_objects_and_hijacking.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
  
  - [Javascript Extensions](http://docs.comfy.org/custom-nodes/js/javascript_overview)
  - [Comfy Hooks](http://docs.comfy.org/custom-nodes/js/javascript_hooks)
  - [Comfy Objects](http://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking)
  - [Settings](http://docs.comfy.org/custom-nodes/js/javascript_settings)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/js/javascript_examples)
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Comfy Objects

# Comfy Objects

## [​](http://docs.comfy.org#litegraph) LiteGraph

The Comfy UI is built on top of [LiteGraph](https://github.com/jagenjo/litegraph.js). Much of the Comfy functionality is provided by LiteGraph, so if developing more complex nodes you will probably find it helpful to clone that repository and browse the documentation, which can be found at `doc/index.html`.

## [​](http://docs.comfy.org#comfyapp) ComfyApp

The `app` object (always accessible by `import { app } from "../../scripts/app.js";`) represents the Comfy application running in the browser, and contains a number of useful properties and functions, some of which are listed below.

Hijacking functions on `app` is not recommended, as Comfy is under constant development, and core behavior may change.

### [​](http://docs.comfy.org#properties) Properties

Important properties of `app` include (this is not an exhaustive list):

propertycontents`canvas`An LGraphCanvas object, representing the current user interface. It contains some potentially interesting properties, such as `node_over` and `selected_nodes`.`canvasEl`The DOM `<canvas>` element`graph`A reference to the LGraph object describing the current graph`runningNodeId`During execution, the node currently being executed`ui`Provides access to some UI elements, such as the queue, menu, and dialogs

`canvas` (for graphical elements) and `graph` (for logical connections) are probably the ones you are most likely to want to access.

### [​](http://docs.comfy.org#functions) Functions

Again, there are many. A few significant ones are:

functionnotesgraphToPromptConvert the graph into a prompt that can be sent to the Python serverloadGraphDataLoad a graphqueuePromptSubmit a prompt to the queueregisterExtensionYou’ve seen this one - used to add an extension

## [​](http://docs.comfy.org#lgraph) LGraph

The `LGraph` object is part of the LiteGraph framework, and represents the current logical state of the graph (nodes and links). If you want to manipulate the graph, the LiteGraph documentation (at `doc/index.html` if you clone `https://github.com/jagenjo/litegraph.js`) describes the functions you will need.

You can use `graph` to obtain details of nodes and links, for example:

```javascript
const ComfyNode_object_for_my_node = app.graph._nodes_by_id(my_node_id) 
ComfyNode_object_for_my_node.inputs.forEach(input => {
    const link_id = input.link;
    if (link_id) {
        const LLink_object = app.graph.links[link_id]
        const id_of_upstream_node = LLink_object.origin_id
        // etc
    }
});
```

## [​](http://docs.comfy.org#llink) LLink

The `LLink` object, accessible through `graph.links`, represents a single link in the graph, from node `link.origin_id` output slot `link.origin_slot` to node `link.target_id` slot `link.target_slot`. It also has a string representing the data type, in `link.type`, and `link.id`.

`LLink`s are created in the `connect` method of a `LGraphNode` (of which `ComfyNode` is a subclass).

Avoid creating your own LLink objects - use the LiteGraph functions instead.

## [​](http://docs.comfy.org#comfynode) ComfyNode

`ComfyNode` is a subclass of `LGraphNode`, and the LiteGraph documentation is therefore helpful for more generic operations. However, Comfy has significantly extended the LiteGraph core behavior, and also does not make use of all LiteGraph functionality.

The description that follows applies to a normal node. Group nodes, primitive nodes, notes, and redirect nodes have different properties.

A `ComfyNode` object represents a node in the current workflow. It has a number of important properties that you may wish to make use of, a very large number of functions that you may wish to use, or hijack to modify behavior.

To get a more complete sense of the node object, you may find it helpful to insert the following code into your extension and place a breakpoint on the `console.log` command. When you then create a new node you can use your favorite debugger to interrogate the node.

```javascript
async nodeCreated(node) {
    console.log("nodeCreated")
}
```

### [​](http://docs.comfy.org#properties-2) Properties

propertycontents`bgcolor`The background color of the node, or undefined for the default`comfyClass`The Python class representing the node`flags`A dictionary that may contain flags related to the state of the node. In particular, `flags.collapsed` is true for collapsed nodes.`graph`A reference to the LGraph object`id`A unique id`input_type`A list of the input types (eg “STRING”, “MODEL”, “CLIP” etc). Generally matches the Python INPUT\_TYPES`inputs`A list of inputs (discussed below)`mode`Normally 0, set to 2 if the node is muted and 4 if the node is bypassed. Values of 1 and 3 are not used by Comfy`order`The node’s position in the execution order. Set by `LGraph.computeExecutionOrder()` when the prompt is submitted`pos`The \[x,y] position of the node on the canvas`properties`A dictionary containing `"Node name for S&R"`, used by LiteGraph`properties_info`The type and default value of entries in `properties``size`The width and height of the node on the canvas`title`Display Title`type`The unique name (from Python) of the node class`widgets`A list of widgets (discussed below)`widgets_values`A list of the current values of widgets

### [​](http://docs.comfy.org#functions-2) Functions

There are a very large number of functions (85, last time I counted). A selection are listed below. Most of these functions are unmodified from the LiteGraph core code.

#### [​](http://docs.comfy.org#inputs%2C-outputs%2C-widgets) Inputs, Outputs, Widgets

functionnotesInputs / OutputsMost have output methods with the equivalent names: s/In/Out/`addInput`Create a new input, defined by name and type`addInputs`Array version of `addInput``findInputSlot`Find the slot index from the input name`findInputSlotByType`Find an input matching the type. Options to prefer, or only use, free slots`removeInput`By slot index`getInputNode`Get the node connected to this input. The output equivalent is `getOutputNodes` and returns a list`getInputLink`Get the LLink connected to this input. No output equivalentWidgets`addWidget`Add a standard Comfy widget`addCustomWidget`Add a custom widget (defined in the `getComfyWidgets` hook)`addDOMWidget`Add a widget defined by a DOM element`convertWidgetToInput`Convert a widget to an input if allowed by `isConvertableWidget` (in `widgetInputs.js`)

#### [​](http://docs.comfy.org#connections) Connections

functionnotes`connect`Connect this node’s output to another node’s input`connectByType`Connect output to another node by specifying the type - connects to first available matching slot`connectByTypeOutput`Connect input to another node output by type`disconnectInput`Remove any link into the input (specified by name or index)`disconnectOutput`Disconnect an output from a specified node’s input`onConnectionChange`Called on each node. `side==1` if it’s an input on this node`onConnectInput`Called *before* a connection is made. If this returns `false`, the connection is refused

#### [​](http://docs.comfy.org#display) Display

functionnotes`setDirtyCanvas`Specify that the foreground (nodes) and/or background (links and images) need to be redrawn`onDrawBackground`Called with a `CanvasRenderingContext2D` object to draw the background. Used by Comfy to render images`onDrawForeground`Called with a `CanvasRenderingContext2D` object to draw the node.`getTitle`The title to be displayed.`collapse`Toggles the collapsed state of the node.

`collapse` is badly named; it *toggles* the collapsed state. It takes a boolean parameter, which can be used to override `node.collapsable === false`.

#### [​](http://docs.comfy.org#other) Other

functionnotes`changeMode`Use to set the node to bypassed (`mode == 4`) or not (`mode == 0`)

## [​](http://docs.comfy.org#inputs-and-widgets) Inputs and Widgets

Inputs and Widgets represent the two ways that data can be fed into a node. In general a widget can be converted to an input, but not all inputs can be converted to a widget (as many datatypes can’t be entered through a UI element).

`node.inputs` is a list of the current inputs (colored dots on the left hand side of the node), specifying their `.name`, `.type`, and `.link` (a reference to the connected `LLink` in `app.graph.links`).

If an input is a widget which has been converted, it also holds a reference to the, now inactive, widget in `.widget`.

`node.widgets` is a list of all widgets, whether or not they have been converted to an input. A widget has:

property/functionnotes`callback`A function called when the widget value is changed`last_y`The vertical position of the widget in the node`name`The (unique within a node) widget name`options`As specified in the Python code (such as default, min, and max)`type`The name of the widget type (see below) in lowercase`value`The current widget value. This is a property with `get` and `set` methods

### [​](http://docs.comfy.org#widget-types) Widget Types

`app.widgets` is a dictionary of currently registered widget types, keyed in the UPPER CASE version of the name of the type. Build in Comfy widgets types include the self explanatory `BOOLEAN`, `INT`, and `FLOAT`, as well as `STRING` (which comes in two flavours, single line and multiline), `COMBO` for dropdown selection from a list, and `IMAGEUPLOAD`, used in Load Image nodes.

Custom widget types can be added by providing a `getCustomWidgets` method in your extension.

### [​](http://docs.comfy.org#linked-widgets) Linked widgets

Widgets can also be linked - the built in behavior of `seed` and `control_after_generate`, for example. A linked widget has `.type = 'base_widget_type:base_widget_name'`; so `control_after_generate` may have type `int:seed`.

## [​](http://docs.comfy.org#prompt) Prompt

When you press the `Queue Prompt` button in Comfy, the `app.graphToPrompt()` method is called to convert the current graph into a prompt that can be sent to the server.

`app.graphToPrompt` returns an object (refered to herein as `prompt`) with two properties, `output` and `workflow`.

### [​](http://docs.comfy.org#output) output

`prompt.output` maps from the `node_id` of each node in the graph to an object with two properties.

- `prompt.output[node_id].class_type`, the unique name of the custom node class, as defined in the Python code
- `prompt.output[node_id].inputs`, which contains the value of each input (or widget) as a map from the input name to:
  
  - the selected value, if it is a widget, or
  - an array containing (`upstream_node_id`, `upstream_node_output_slot`) if there is a link connected to the input, or
  - undefined, if it is a widget that has been converted to an input and is not connected
  - other unconnected inputs are not included in `.inputs`

Note that the `upstream_node_id` in the array describing a connected input is represented as a string, not an integer.

### [​](http://docs.comfy.org#workflow) workflow

`prompt.workflow` contains the following properties:

- `config` - a dictionary of additional configuration options (empty by default)
- `extra` - a dictionary containing extra information about the workflow. By default it contains:
  
  - `extra.ds` - describes the current view of the graph (`scale` and `offset`)
- `groups` - all groups in the workflow
- `last_link_id` - the id of the last link added
- `last_node_id` - the id of the last node added
- `links` - a list of all links in the graph. Each entry is an array of five integers and one string:
  
  - (`link_id`, `upstream_node_id`, `upstream_node_output_slot`, `downstream_node_id`, `downstream_node_input_slot`, `data type`)
- `nodes` - a list of all nodes in the graph. Each entry is a map of a subset of the properties of the node as described [above](http://docs.comfy.org/_sites/docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking#comfynode)
  
  - The following properties are included: `flags`, `id`, `inputs`, `mode`, `order`, `pos`, `properties`, `size`, `type`, `widgets_values`
  - In addition, unless a node has no outputs, there is an `outputs` property, which is a list of the outputs of the node, each of which contains:
    
    - `name` - the name of the output
    - `type` - the data type of the output
    - `links` - a list of the `link_id` of all links from this output (if there are no connections, may be an empty list, or null),
    - `shape` - the shape used to draw the output (default 3 for a dot)
    - `slot_index` - the slot number of the output
- `version` - the LiteGraph version number (at time of writing, `0.4`)

`nodes.output` is absent for nodes with no outputs, not an empty list.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/js/javascript_objects_and_hijacking.mdx)

[Previous](http://docs.comfy.org/custom-nodes/js/javascript_hooks)

[Settings  
\
Next](http://docs.comfy.org/custom-nodes/js/javascript_settings)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [LiteGraph](http://docs.comfy.org#litegraph)
- [ComfyApp](http://docs.comfy.org#comfyapp)
- [Properties](http://docs.comfy.org#properties)
- [Functions](http://docs.comfy.org#functions)
- [LGraph](http://docs.comfy.org#lgraph)
- [LLink](http://docs.comfy.org#llink)
- [ComfyNode](http://docs.comfy.org#comfynode)
- [Properties](http://docs.comfy.org#properties-2)
- [Functions](http://docs.comfy.org#functions-2)
- [Inputs, Outputs, Widgets](http://docs.comfy.org#inputs%2C-outputs%2C-widgets)
- [Connections](http://docs.comfy.org#connections)
- [Display](http://docs.comfy.org#display)
- [Other](http://docs.comfy.org#other)
- [Inputs and Widgets](http://docs.comfy.org#inputs-and-widgets)
- [Widget Types](http://docs.comfy.org#widget-types)
- [Linked widgets](http://docs.comfy.org#linked-widgets)
- [Prompt](http://docs.comfy.org#prompt)
- [output](http://docs.comfy.org#output)
- [workflow](http://docs.comfy.org#workflow)

<!-- END Development/custom-nodes/js/javascript_objects_and_hijacking.md -->


<!-- BEGIN Development/custom-nodes/js/javascript_overview.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
  
  - [Javascript Extensions](http://docs.comfy.org/custom-nodes/js/javascript_overview)
  - [Comfy Hooks](http://docs.comfy.org/custom-nodes/js/javascript_hooks)
  - [Comfy Objects](http://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking)
  - [Settings](http://docs.comfy.org/custom-nodes/js/javascript_settings)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/js/javascript_examples)
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Javascript Extensions

# Javascript Extensions

## [​](http://docs.comfy.org#extending-the-comfy-client) Extending the Comfy Client

Comfy can be modified through an extensions mechanism. To add an extension you need to:

- Export `WEB_DIRECTORY` from your Python module,
- Place one or more `.js` files into that directory,
- Use `app.registerExtension` to register your extension.

These three steps are below. Once you know how to add an extension, look through the [hooks](http://docs.comfy.org/javascript_hooks) available to get your code called, a description of various [Comfy objects](http://docs.comfy.org/javascript_objects_and_hijacking) you might need, or jump straight to some [example code snippets](http://docs.comfy.org/javascript_examples).

### [​](http://docs.comfy.org#exporting-web-directory) Exporting `WEB_DIRECTORY`

The Comfy web client can be extended by creating a subdirectory in your custom node directory, conventionally called `js`, and exporting `WEB_DIRECTORY` - so your `__init_.py` will include something like:

```python
WEB_DIRECTORY = "./js"
__all__ = ["NODE_CLASS_MAPPINGS", "NODE_DISPLAY_NAME_MAPPINGS", "WEB_DIRECTORY"]
```

### [​](http://docs.comfy.org#including-js-files) Including `.js` files

All Javascript `.js` files will be loaded by the browser as the Comfy webpage loads. You don’t need to specify the file your extension is in.

*Only* `.js` files will be added to the webpage. Other resources (such as `.css` files) can be accessed at `extensions/custom_node_subfolder/the_file.css` and added programmatically.

That path does *not* include the name of the subfolder. The value of `WEB_DIRECTORY` is inserted by the server.

### [​](http://docs.comfy.org#registering-an-extension) Registering an extension

The basic structure of an extension follows is to import the main Comfy `app` object, and call `app.registerExtension`, passing a dictionary that contains a unique `name`, and one or more functions to be called by hooks in the Comfy code.

A complete, trivial, and annoying, extension might look like this:

```javascript
import { app } from "../../scripts/app.js";
app.registerExtension({ 
	name: "a.unique.name.for.a.useless.extension",
	async setup() { 
		alert("Setup complete!")
	},
})
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/js/javascript_overview.mdx)

[Previous](http://docs.comfy.org/custom-nodes/backend/tensors)

[Comfy Hooks  
\
Next](http://docs.comfy.org/custom-nodes/js/javascript_hooks)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Extending the Comfy Client](http://docs.comfy.org#extending-the-comfy-client)
- [Exporting WEB\_DIRECTORY](http://docs.comfy.org#exporting-web-directory)
- [Including .js files](http://docs.comfy.org#including-js-files)
- [Registering an extension](http://docs.comfy.org#registering-an-extension)

<!-- END Development/custom-nodes/js/javascript_overview.md -->


<!-- BEGIN Development/custom-nodes/js/javascript_settings.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
  
  - [Javascript Extensions](http://docs.comfy.org/custom-nodes/js/javascript_overview)
  - [Comfy Hooks](http://docs.comfy.org/custom-nodes/js/javascript_hooks)
  - [Comfy Objects](http://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking)
  - [Settings](http://docs.comfy.org/custom-nodes/js/javascript_settings)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/js/javascript_examples)
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Settings

# Settings

You can provide a settings object to ComfyUI that will show up when the user opens the ComfyUI settings panel.

## [​](http://docs.comfy.org#basic-operation) Basic operation

### [​](http://docs.comfy.org#add-a-setting) Add a setting

```javascript
import { app } from "../../scripts/app.js";

app.registerExtension({
    name: "My Extension",
    settings: [
        {
            id: "example.boolean",
            name: "Example boolean setting",
            type: "boolean",
            defaultValue: false,
        },
    ],
});
```

The `id` must be unique across all extensions and will be used to fetch values.

If you do not [provide a category](http://docs.comfy.org/_sites/docs.comfy.org/custom-nodes/js/javascript_settings#categories), then the `id` will be split by `.` to determine where it appears in the settings panel.

- If your `id` doesn’t contain any `.` then it will appear in the “Other” category and your `id` will be used as the section heading.
- If your `id` contains at least one `.` then the leftmost part will be used as the setting category and the second part will be used as the section heading. Any further parts are ignored.

### [​](http://docs.comfy.org#read-a-setting) Read a setting

```javascript
import { app } from "../../scripts/app.js";

if (app.extensionManager.setting.get('example.boolean')) {
    console.log("Setting is enabled.");
} else {
    console.log("Setting is disabled.");
}
```

### [​](http://docs.comfy.org#react-to-changes) React to changes

The `onChange()` event handler will be called as soon as the user changes the setting in the settings panel.

This will also be called when the extension is registered, on every page load.

```javascript
{
    id: "example.boolean",
    name: "Example boolean setting",
    type: "boolean",
    defaultValue: false,
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### [​](http://docs.comfy.org#write-a-setting) Write a setting

```javascript
import { app } from "../../scripts/app.js";

try {
    await app.extensionManager.setting.set("example.boolean", true);
} catch (error) {
    console.error(`Error changing setting: ${error}`);
}
```

### [​](http://docs.comfy.org#extra-configuration) Extra configuration

The setting types are based on [PrimeVue](https://primevue.org/) components. Props described in the PrimeVue documentation can be defined for ComfyUI settings by adding them in an `attrs` field.

For instance, this adds increment/decrement buttons to a number input:

```javascript
{
    id: "example.number",
    name: "Example number setting",
    type: "number",
    defaultValue: 0,
    attrs: {
        showButtons: true,
    },
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

## [​](http://docs.comfy.org#types) Types

### [​](http://docs.comfy.org#boolean) Boolean

This shows an on/off toggle.

Based on the [ToggleSwitch PrimeVue component](https://primevue.org/toggleswitch/).

```javascript
{
    id: "example.boolean",
    name: "Example boolean setting",
    type: "boolean",
    defaultValue: false,
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### [​](http://docs.comfy.org#text) Text

This is freeform text.

Based on the [InputText PrimeVue component](https://primevue.org/inputtext/).

```javascript
{
    id: "example.text",
    name: "Example text setting",
    type: "text",
    defaultValue: "Foo",
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### [​](http://docs.comfy.org#number) Number

This for entering numbers.

To allow decimal places, set the `maxFractionDigits` attribute to a number greater than zero.

Based on the [InputNumber PrimeVue component](https://primevue.org/inputnumber/).

```javascript
{
    id: "example.number",
    name: "Example number setting",
    type: "number",
    defaultValue: 42,
    attrs: {
        showButtons: true,
        maxFractionDigits: 1,
    },
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### [​](http://docs.comfy.org#slider) Slider

This lets the user enter a number directly or via a slider.

Based on the [Slider PrimeVue component](https://primevue.org/slider/). Ranges are not supported.

```javascript
{
    id: "example.slider",
    name: "Example slider setting",
    type: "slider",
    attrs: {
        min: -10,
        max: 10,
        step: 0.5,
    },
    defaultValue: 0,
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### [​](http://docs.comfy.org#combo) Combo

This lets the user pick from a drop-down list of values.

You can provide options either as a plain string or as an object with `text` and `value` fields. If you only provide a plain string, then it will be used for both.

You can let the user enter freeform text by supplying the `editable: true` attribute, or search by supplying the `filter: true` attribute.

Based on the [Select PrimeVue component](https://primevue.org/select/). Groups are not supported.

```javascript
{
    id: "example.combo",
    name: "Example combo setting",
    type: "combo",
    defaultValue: "first",
    options: [
        { text: "My first option", value: "first" },
        "My second option",
    ],
    attrs: {
        editable: true,
        filter: true,
    },
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### [​](http://docs.comfy.org#color) Color

This lets the user select a color from a color picker or type in a hex reference.

Note that the format requires six full hex digits - three digit shorthand does not work.

Based on the [ColorPicker PrimeVue component](https://primevue.org/colorpicker/).

```javascript
{
    id: "example.color",
    name: "Example color setting",
    type: "color",
    defaultValue: "ff0000",
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### [​](http://docs.comfy.org#image) Image

This lets the user upload an image.

The setting will be saved as a [data URL](https://developer.mozilla.org/en-US/docs/Web/URI/Schemes/data).

Based on the [FileUpload PrimeVue component](https://primevue.org/fileupload/).

```javascript
{
    id: "example.image",
    name: "Example image setting",
    type: "image",
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### [​](http://docs.comfy.org#hidden) Hidden

Hidden settings aren’t displayed in the settings panel, but you can read and write to them from your code.

```javascript
{
    id: "example.hidden",
    name: "Example hidden setting",
    type: "hidden",
}
```

## [​](http://docs.comfy.org#other) Other

### [​](http://docs.comfy.org#categories) Categories

You can specify the categorisation of your setting separately to the `id`. This means you can change the categorisation and naming without changing the `id` and losing the values that have already been set by users.

```javascript
{
    id: "example.boolean",
    name: "Example boolean setting",
    type: "boolean",
    defaultValue: false,
    category: ["Category name", "Section heading", "Setting label"],
}
```

### [​](http://docs.comfy.org#tooltips) Tooltips

You can add extra contextual help with the `tooltip` field. This adds a small ℹ︎ icon after the field name that will show the help text when the user hovers over it.

```javascript
{
    id: "example.boolean",
    name: "Example boolean setting",
    type: "boolean",
    defaultValue: false,
    tooltip: "This is some helpful information",
}
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/js/javascript_settings.mdx)

[Previous](http://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking)

[Annotated Examples  
\
Next](http://docs.comfy.org/custom-nodes/js/javascript_examples)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Basic operation](http://docs.comfy.org#basic-operation)
- [Add a setting](http://docs.comfy.org#add-a-setting)
- [Read a setting](http://docs.comfy.org#read-a-setting)
- [React to changes](http://docs.comfy.org#react-to-changes)
- [Write a setting](http://docs.comfy.org#write-a-setting)
- [Extra configuration](http://docs.comfy.org#extra-configuration)
- [Types](http://docs.comfy.org#types)
- [Boolean](http://docs.comfy.org#boolean)
- [Text](http://docs.comfy.org#text)
- [Number](http://docs.comfy.org#number)
- [Slider](http://docs.comfy.org#slider)
- [Combo](http://docs.comfy.org#combo)
- [Color](http://docs.comfy.org#color)
- [Image](http://docs.comfy.org#image)
- [Hidden](http://docs.comfy.org#hidden)
- [Other](http://docs.comfy.org#other)
- [Categories](http://docs.comfy.org#categories)
- [Tooltips](http://docs.comfy.org#tooltips)

<!-- END Development/custom-nodes/js/javascript_settings.md -->


<!-- BEGIN Development/custom-nodes/overview.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Overview

# Overview

Custom nodes allow you to implement new features and share them with the wider community.

A custom node is like any Comfy node: it takes input, does something to it, and produces an output. While some custom nodes perform highly complex tasks, many just do one thing. Here’s an example of a simple node that takes an image and inverts it.

## [​](http://docs.comfy.org#client-server-model) Client-Server Model

Comfy runs in a client-server model. The server, written in Python, handles all the real work: data-processing, models, image diffusion etc. The client, written in Javascript, handles the user interface.

Comfy can also be used in API mode, in which a workflow is sent to the server by a non-Comfy client (such as another UI, or a command line script).

Custom nodes can be placed into one of four categories:

### [​](http://docs.comfy.org#server-side-only) Server side only

The majority of Custom Nodes run purely on the server side, by defining a Python class that specifies the input and output types, and provides a function that can be called to process inputs and produce an output.

### [​](http://docs.comfy.org#client-side-only) Client side only

A few Custom Nodes provide a modification to the client UI, but do not add core functionality. Despite the name, they may not even add new nodes to the system.

### [​](http://docs.comfy.org#independent-client-and-server) Independent Client and Server

Custom nodes may provide additional server features, and additional (related) UI features (such as a new widget to deal with a new data type). In most cases, communication between the client and server can be handled by the Comfy data flow control.

### [​](http://docs.comfy.org#connected-client-and-server) Connected Client and Server

In a small number of cases, the UI features and the server need to interact with each other directly.

Any node that requires Client-Server communication will not be compatible with use through the API.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/overview.mdx)

[Previous](http://docs.comfy.org/comfy-cli/reference)

[Getting Started  
\
Next](http://docs.comfy.org/custom-nodes/walkthrough)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Client-Server Model](http://docs.comfy.org#client-server-model)
- [Server side only](http://docs.comfy.org#server-side-only)
- [Client side only](http://docs.comfy.org#client-side-only)
- [Independent Client and Server](http://docs.comfy.org#independent-client-and-server)
- [Connected Client and Server](http://docs.comfy.org#connected-client-and-server)

<!-- END Development/custom-nodes/overview.md -->


<!-- BEGIN Development/custom-nodes/tips.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Tips

# Tips

### [​](http://docs.comfy.org#recommended-development-lifecycle) Recommended Development Lifecycle

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/tips.mdx)

[Previous](http://docs.comfy.org/custom-nodes/workflow_templates)

[Overview  
\
Next](http://docs.comfy.org/registry/overview)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Recommended Development Lifecycle](http://docs.comfy.org#recommended-development-lifecycle)

<!-- END Development/custom-nodes/tips.md -->


<!-- BEGIN Development/custom-nodes/walkthrough.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Getting Started

# Getting Started

This page will take you step-by-step through the process of creating a custom node.

Our example will take a batch of images, and return one of the images. Initially, the node will return the image which is, on average, the lightest in color; we’ll then extend it to have a range of selection criteria, and then finally add some client side code.

This page assumes very little knowledge of Python or Javascript.

After this walkthrough, dive into the details of [backend code](http://docs.comfy.org/backend/server_overview), and [frontend code](http://docs.comfy.org/backend/server_overview).

## [​](http://docs.comfy.org#write-a-basic-node) Write a basic node

### [​](http://docs.comfy.org#prerequisites) Prerequisites

- A working ComfyUI [installation](http://docs.comfy.org/installation/manual_install). For development, we recommend installing ComfyUI manually.
- A working comfy-cli [installation](http://docs.comfy.org/comfy-cli/getting-started).

### [​](http://docs.comfy.org#setting-up) Setting up

```bash
cd ComfyUI/custom_nodes
comfy node scaffold
```

After answering a few questions, you’ll have a new directory set up.

```bash
 ~  % comfy node scaffold
You've downloaded .cookiecutters/cookiecutter-comfy-extension before. Is it okay to delete and re-download it? [y/n] (y): y
  [1/9] full_name (): Comfy
  [2/9] email (you@gmail.com): me@comfy.org
  [3/9] github_username (your_github_username): comfy
  [4/9] project_name (My Custom Nodepack): FirstComfyNode
  [5/9] project_slug (firstcomfynode): 
  [6/9] project_short_description (A collection of custom nodes for ComfyUI): 
  [7/9] version (0.0.1): 
  [8/9] Select open_source_license
    1 - GNU General Public License v3
    2 - MIT license
    3 - BSD license
    4 - ISC license
    5 - Apache Software License 2.0
    6 - Not open source
    Choose from [1/2/3/4/5/6] (1): 1
  [9/9] include_web_directory_for_custom_javascript [y/n] (n): y
Initialized empty Git repository in firstcomfynode/.git/
✓ Custom node project created successfully!
```

### [​](http://docs.comfy.org#defining-the-node) Defining the node

Add the following code to the end of `src/nodes.py`:

src/nodes.py

```python
class ImageSelector:
    CATEGORY = "example"
    @classmethod    
    def INPUT_TYPES(s):
        return { "required":  { "images": ("IMAGE",), } }
    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "choose_image"
```

The basic structure of a custom node is described in detail [here](http://docs.comfy.org/custom-nodes/backend/server_overview).

A custom node is defined using a Python class, which must include these four things: `CATEGORY`, which specifies where in the add new node menu the custom node will be located, `INPUT_TYPES`, which is a class method defining what inputs the node will take (see [later](http://docs.comfy.org/custom-nodes/backend/server_overview#input-types) for details of the dictionary returned), `RETURN_TYPES`, which defines what outputs the node will produce, and `FUNCTION`, the name of the function that will be called when the node is executed.

Notice that the data type for input and output is `IMAGE` (singular) even though we expect to receive a batch of images, and return just one. In Comfy, `IMAGE` means image batch, and a single image is treated as a batch of size 1.

### [​](http://docs.comfy.org#the-main-function) The main function

The main function, `choose_image`, receives named arguments as defined in `INPUT_TYPES`, and returns a `tuple` as defined in `RETURN_TYPES`. Since we’re dealing with images, which are internally stored as `torch.Tensor`,

```python
import torch
```

Then add the function to your class. The datatype for image is `torch.Tensor` with shape `[B,H,W,C]`, where `B` is the batch size and `C` is the number of channels - 3, for RGB. If we iterate over such a tensor, we will get a series of `B` tensors of shape `[H,W,C]`. The `.flatten()` method turns this into a one dimensional tensor, of length `H*W*C`, `torch.mean()` takes the mean, and `.item()` turns a single value tensor into a Python float.

```python
def choose_image(self, images):
    brightness = list(torch.mean(image.flatten()).item() for image in images)
    brightest = brightness.index(max(brightness))
    result = images[brightest].unsqueeze(0)
    return (result,)
```

Notes on those last two lines:

- `images[brightest]` will return a Tensor of shape `[H,W,C]`. `unsqueeze` is used to insert a (length 1) dimension at, in this case, dimension zero, to give us `[B,H,W,C]` with `B=1`: a single image.
- in `return (result,)`, the trailing comma is essential to ensure you return a tuple.

### [​](http://docs.comfy.org#register-the-node) Register the node

To make Comfy recognize the new node, it must be available at the package level. Modify the `NODE_CLASS_MAPPINGS` variable at the end of `src/nodes.py`. You must restart ComfyUI to see any changes.

src/nodes.py

```python

NODE_CLASS_MAPPINGS = {
    "Example" : Example,
    "Image Selector" : ImageSelector,
}

# Optionally, you can rename the node in the `NODE_DISPLAY_NAME_MAPPINGS` dictionary.
NODE_DISPLAY_NAME_MAPPINGS = {
    "Example": "Example Node",
    "Image Selector": "Image Selector",
}
```

For a detailed explanation of how ComfyUI discovers and loads custom nodes, see the [node lifecycle documentation](http://docs.comfy.org/custom-nodes/backend/lifecycle).

## [​](http://docs.comfy.org#add-some-options) Add some options

That node is maybe a bit boring, so we might add some options; a widget that allows you to choose the brightest image, or the reddest, bluest, or greenest. Edit your `INPUT_TYPES` to look like:

```python
@classmethod    
def INPUT_TYPES(s):
    return { "required":  { "images": ("IMAGE",), 
                            "mode": (["brightest", "reddest", "greenest", "bluest"],)} }
```

Then update the main function. We’ll use a fairly naive definition of ‘reddest’ as being the average `R` value of the pixels divided by the average of all three colors. So:

```python
def choose_image(self, images, mode):
    batch_size = images.shape[0]
    brightness = list(torch.mean(image.flatten()).item() for image in images)
    if (mode=="brightest"):
        scores = brightness
    else:
        channel = 0 if mode=="reddest" else (1 if mode=="greenest" else 2)
        absolute = list(torch.mean(image[:,:,channel].flatten()).item() for image in images)
        scores = list( absolute[i]/(brightness[i]+1e-8) for i in range(batch_size) )
    best = scores.index(max(scores))
    result = images[best].unsqueeze(0)
    return (result,)
```

## [​](http://docs.comfy.org#tweak-the-ui) Tweak the UI

Maybe we’d like a bit of visual feedback, so let’s send a little text message to be displayed.

### [​](http://docs.comfy.org#send-a-message-from-server) Send a message from server

This requires two lines to be added to the Python code:

```python
from server import PromptServer
```

and, at the end of the `choose_image` method, add a line to send a message to the front end (`send_sync` takes a message type, which should be unique, and a dictionary)

```python
PromptServer.instance.send_sync("example.imageselector.textmessage", {"message":f"Picked image {best+1}"})
return (result,)
```

### [​](http://docs.comfy.org#write-a-client-extension) Write a client extension

To add some Javascript to the client, create a subdirectory, `web/js` in your custom node directory, and modify the end of `__init__.py` to tell Comfy about it by exporting `WEB_DIRECTORY`:

```python
WEB_DIRECTORY = "./web/js"
__all__ = ['NODE_CLASS_MAPPINGS', 'WEB_DIRECTORY']
```

The client extension is saved as a `.js` file in the `web/js` subdirectory, so create `image_selector/web/js/imageSelector.js` with the code below. (For more, see [client side coding](http://docs.comfy.org/js/javascript_overview)).

```javascript
app.registerExtension({
	name: "example.imageselector",
    async setup() {
        function messageHandler(event) { alert(event.detail.message); }
        app.api.addEventListener("example.imageselector.textmessage", messageHandler);
    },
})
```

All we’ve done is register an extension and add a listener for the message type we are sending in the `setup()` method. This reads the dictionary we sent (which is stored in `event.detail`).

Stop the Comfy server, start it again, reload the webpage, and run your workflow.

### [​](http://docs.comfy.org#the-complete-example) The complete example

The complete example is available [here](https://gist.github.com/robinjhuang/fbf54b7715091c7b478724fc4dffbd03). You can download the example workflow [JSON file](https://github.com/Comfy-Org/docs/blob/main/public/workflow.json) or view it below:

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/walkthrough.mdx)

[Previous](http://docs.comfy.org/custom-nodes/overview)

[PropertiesProperties of a custom node  
\
Next](http://docs.comfy.org/custom-nodes/backend/server_overview)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Write a basic node](http://docs.comfy.org#write-a-basic-node)
- [Prerequisites](http://docs.comfy.org#prerequisites)
- [Setting up](http://docs.comfy.org#setting-up)
- [Defining the node](http://docs.comfy.org#defining-the-node)
- [The main function](http://docs.comfy.org#the-main-function)
- [Register the node](http://docs.comfy.org#register-the-node)
- [Add some options](http://docs.comfy.org#add-some-options)
- [Tweak the UI](http://docs.comfy.org#tweak-the-ui)
- [Send a message from server](http://docs.comfy.org#send-a-message-from-server)
- [Write a client extension](http://docs.comfy.org#write-a-client-extension)
- [The complete example](http://docs.comfy.org#the-complete-example)

<!-- END Development/custom-nodes/walkthrough.md -->


<!-- BEGIN Development/custom-nodes/workflow_templates.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Workflow templates

# Workflow templates

If you have example workflow files associated with your custom nodes then ComfyUI can show these to the user in the template browser (`Workflow`/`Browse Templates` menu). Workflow templates are a great way to support people getting started with your nodes.

All you have to do as a node developer is to create an `example_workflows` folder and place the `json` files there. Optionally you can place `jpg` files with the same name to be shown as the template thumbnail.

Under the hood ComfyUI statically serves these files along with an endpoint (`/api/workflow_templates`) that returns the collection of workflow templates.

## [​](http://docs.comfy.org#example) Example

Under `ComfyUI-MyCustomNodeModule/example_workflows/` directory:

- `My_example_workflow_1.json`
- `My_example_workflow_1.jpg`
- `My_example_workflow_2.json`

In this example ComfyUI’s template browser shows a category called `ComfyUI-MyCustomNodeModule` with two items, one of which has a thumbnail.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/workflow_templates.mdx)

[Previous](http://docs.comfy.org/custom-nodes/js/javascript_examples)

[Tips  
\
Next](http://docs.comfy.org/custom-nodes/tips)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Example](http://docs.comfy.org#example)

<!-- END Development/custom-nodes/workflow_templates.md -->


<!-- BEGIN Development/essentials/comfyui-server/comms_messages.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Messages

# Messages

## [​](http://docs.comfy.org#messages) Messages

During execution (or when the state of the queue changes), the `PromptExecutor` sends messages back to the client through the `send_sync` method of `PromptServer`.

These messages are received by a socket event listener defined in `api.js` (at time of writing around line 90, or search for `this.socket.addEventListener`), which creates a `CustomEvent` object for any known message type, and dispatches it to any registered listeners.

An extension can register to receive events (normally done in the `setup()` function) following the standard Javascript idiom:

```javascript
api.addEventListener(message_type, messageHandler);
```

If the `message_type` is not one of the built in ones, it will be added to the list of known message types automatically. The message `messageHandler` will be called with a `CustomEvent` object, which extends the event raised by the socket to add a `.detail` property, which is a dictionary of the data sent by the server. So usage is generally along the lines of:

```javascript
function messageHandler(event) {
    if (event.detail.node == aNodeIdThatIsInteresting) {
        // do something with event.detail.other_things
    }
}
```

### [​](http://docs.comfy.org#built-in-message-types) Built in message types

During execution (or when the state of the queue changes), the `PromptExecutor` sends the following messages back to the client through the `send_sync` method of `PromptServer`. An extension can register as a listener for any of these.

eventwhendata`execution_start`When a prompt is about to run`prompt_id``execution_error`When an error occurs during execution`prompt_id`, plus additional information`execution_interrupted`When execution is stopped by a node raising `InterruptProcessingException``prompt_id`, `node_id`, `node_type` and `executed` (a list of executed nodes)`execution_cached`At the start of execution`prompt_id`, `nodes` (a list of nodes which are being skipped because their cached outputs can be used)`executing`When a new node is about to be executed`node` (node id or `None` to indicate completion), `prompt_id``executed`When a node returns a ui element`node` (node id), `prompt_id`, `output``progress`During execution of a node that implements the required hook`node` (node id), `prompt_id`, `value`, `max``status`When the state of the queue changes`exec_info`, a dictionary holding `queue_remaining`, the number of entries in the queue

### [​](http://docs.comfy.org#using-executed) Using executed

Despite the name, an `executed` message is not sent whenever a node completes execution (unlike `executing`), but only when the node returns a ui update.

To do this, the main function needs to return a dictionary instead of a tuple:

```python
# at the end of my main method
        return { "ui":a_new_dictionary, "result": the_tuple_of_output_values }
```

`a_new_dictionary` will then be sent as the value of `output` in an `executed` message. The `result` key can be omitted if the node has no outputs (see, for instance, the code for `SaveImage` in `nodes.py`)

### [​](http://docs.comfy.org#custom-message-types) Custom message types

As indicated above, on the client side, a custom message type can be added simply by registering as a listener for a unique message type name.

```javascript
api.addEventListener("my.custom.message", messageHandler);
```

On the server, the code is equally simple:

```python
from server import PromptServer
# then, in your main execution function (normally)
        PromptServer.instance.send_sync("my.custom.message", a_dictionary)
```

#### [​](http://docs.comfy.org#getting-node-id) Getting node\_id

Most of the built-in messages include the current node id in the value of `node`. It’s likely that you will want to do the same.

The node\_id is available on the server side through a hidden input, which is obtained with the `hidden` key in the `INPUT_TYPES` dictionary:

```python
    @classmethod    
    def INPUT_TYPES(s):
        return {"required" : { }, # whatever your required inputs are 
                "hidden": { "node_id": "UNIQUE_ID" } } # Add the hidden key

    def my_main_function(self, required_inputs, node_id):
        # do some things
        PromptServer.instance.send_sync("my.custom.message", {"node": node_id, "other_things": etc})
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/comfyui-server/comms_messages.mdx)

[Previous](http://docs.comfy.org/essentials/comfyui-server/comms_overview)

[Execution Model Inversion Guide  
\
Next](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Messages](http://docs.comfy.org#messages)
- [Built in message types](http://docs.comfy.org#built-in-message-types)
- [Using executed](http://docs.comfy.org#using-executed)
- [Custom message types](http://docs.comfy.org#custom-message-types)
- [Getting node\_id](http://docs.comfy.org#getting-node-id)

<!-- END Development/essentials/comfyui-server/comms_messages.md -->


<!-- BEGIN Development/essentials/comfyui-server/comms_overview.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Server Overview

# Server Overview

## [​](http://docs.comfy.org#overview) Overview

The Comfy server runs on top of the [aiohttp framework](https://docs.aiohttp.org/), which in turn uses [asyncio](https://pypi.org/project/asyncio/).

Messages from the server to the client are sent by socket messages through the `send_sync` method of the server, which is an instance of `PromptServer` (defined in `server.py`). They are processed by a socket event listener registered in `api.js`. See [messages](http://docs.comfy.org/comms_messages).

Messages from the client to the server are sent by the `api.fetchApi()` method defined in `api.js`, and are handled by http routes defined by the server. See [routes](http://docs.comfy.org/comms_routes).

The client submits the whole workflow (widget values and all) when you queue a request. The server does not receive any changes you make after you send a request to the queue. If you want to modify server behavior during execution, you’ll need routes.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/comfyui-server/comms_overview.mdx)

[Messages  
\
Next](http://docs.comfy.org/essentials/comfyui-server/comms_messages)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Overview](http://docs.comfy.org#overview)

<!-- END Development/essentials/comfyui-server/comms_overview.md -->


<!-- BEGIN Development/essentials/comfyui-server/execution_model_inversion_guide.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Execution Model Inversion Guide

# Execution Model Inversion Guide

[PR #2666](https://github.com/comfyanonymous/ComfyUI/pull/2666) inverts the execution model from a back-to-front recursive model to a front-to-back topological sort. While most custom nodes should continue to “just work”, this page is intended to serve as a guide for custom node creators to the things that *could* break.

## [​](http://docs.comfy.org#breaking-changes) Breaking Changes

### [​](http://docs.comfy.org#monkey-patching) Monkey Patching

Any code that monkey patched the execution model is likely to stop working. Note that the performance of execution with this PR exceeds that with the most popular monkey patches, so many of them will be unnecessary.

### [​](http://docs.comfy.org#optional-input-validation) Optional Input Validation

Prior to this PR, only nodes that were connected to outputs exclusively through a string of `"required"` inputs were actually validated. If you had custom nodes that were only ever connected to `"optional"` inputs, you previously wouldn’t have been seeing that they failed validation.

If your nodes’ outputs could already be connected to `"required"` inputs, it is unlikely that anything in this section applies to you. It will primarily apply to custom node authors who use custom types and exclusively use `"optional"` inputs.

Here are some of the things that could cause you to fail validation along with recommended solutions:

- Use of reserved [Additional Parameters](http://docs.comfy.org/custom-nodes/backend/datatypes#additional-parameters) like `min` and `max` on types that aren’t comparable (e.g. dictionaries) in order to configure custom widgets.
  
  - Change the additional parameters used to non-reserved keys like `uiMin` and `uiMax`. *(Recommended Solution)*
    
    ```python
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "my_size": ("VEC2", {"uiMin": 0.0, "uiMax": 1.0}),
            }
        }
    ```
  - Define a custom [VALIDATE\_INPUTS](http://docs.comfy.org/custom-nodes/backend/server_overview#validate-inputs) function with this input so validation of it is skipped. *(Quick Solution)*
    
    ```python
    @classmethod
    def VALIDATE_INPUTS(cls, my_size):
        return True
    ```
- Use of composite types (e.g. `CUSTOM_A,CUSTOM_B`)
  
  - (When used as output) Define and use a wrapper like `MakeSmartType` [seen here in the PR’s unit tests](https://github.com/comfyanonymous/ComfyUI/pull/2666/files#diff-714643f1fdb6f8798c45f77ab10d212ca7f41dd71bbe55069f1f9f146a8f0cb9R2)
    
    ```python
    class MyCustomNode:
    
        @classmethod
        def INPUT_TYPES(cls):
            return {
                "required": {
                    "input": (MakeSmartType("FOO,BAR"), {}),
                }
            }
    
        RETURN_TYPES = (MakeSmartType("FOO,BAR"),)
    
        # ...
    ```
  - (When used as input) Define a custom[VALIDATE\_INPUTS](http://docs.comfy.org/custom-nodes/backend/server_overview#validate-inputs) function that takes a `input_types` argument so type validation is skipped.
    
    ```python
    @classmethod
    def VALIDATE_INPUTS(cls, input_types):
        return True
    ```
  - (Supports both, convenient) Define and use the `@VariantSupport` decorator [seen here in the PR’s unit tests](https://github.com/comfyanonymous/ComfyUI/pull/2666/files#diff-714643f1fdb6f8798c45f77ab10d212ca7f41dd71bbe55069f1f9f146a8f0cb9R15)
    
    ```python
    @VariantSupport
    class MyCustomNode:
    
        @classmethod
        def INPUT_TYPES(cls):
            return {
                "required": {
                    "input": ("FOO,BAR", {}),
                }
            }
        
        RETURN_TYPES = (MakeSmartType("FOO,BAR"),)
    
        # ...
    ```
- The use of lists (e.g. `[1, 2, 3]`) as constants in the graph definition (e.g. to represent a const `VEC3` input). This would have required a front-end extension before. Previously, lists of size exactly `2` would have failed anyway — they would have been treated as broken links.
  
  - Wrap the lists in a dictionary like `{ "value": [1, 2, 3] }`

### [​](http://docs.comfy.org#execution-order) Execution Order

Execution order has always changed depending on which nodes happen to have which IDs, but it may now change depending on which values are cached as well. In general, the execution order should be considered non-deterministic and subject to change (beyond what is enforced by the graph’s structure).

Don’t rely on the execution order.

*HIC SUNT DRACONES*

## [​](http://docs.comfy.org#new-functionality) New Functionality

### [​](http://docs.comfy.org#validation-changes) Validation Changes

A number of features were added to the `VALIDATE_INPUTS` function in order to lessen the impact of the [Optional Input Validation](http://docs.comfy.org/_sites/docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide#optional-input-validation) mentioned above.

- Default validation will now be skipped for inputs which are received by the `VALIDATE_INPUTS` function.
- The `VALIDATE_INPUTS` function can now take `**kwargs` which causes all inputs to be treated as validated by the node creator.
- The `VALIDATE_INPUTS` function can take an input named `input_types`. This input will be a dict mapping each input (connected via a link) to the type of the connected output. When this argument exists, type validation for the node’s inputs is skipped.

You can read more at [VALIDATE\_INPUTS](http://docs.comfy.org/custom-nodes/backend/server_overview#validate-inputs).

### [​](http://docs.comfy.org#lazy-evaluation) Lazy Evaluation

Inputs can be evaluated lazily (i.e. you can wait to see if they are needed before evaluating the attached node and all its ancestors). See [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation) for more information.

### [​](http://docs.comfy.org#node-expansion) Node Expansion

At runtime, nodes can expand into a subgraph of nodes. This is what allows loops to be implemented (via tail-recursion). See [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion) for more information.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/comfyui-server/execution_model_inversion_guide.mdx)

[Previous](http://docs.comfy.org/essentials/comfyui-server/comms_messages)

[Getting Started  
\
Next](http://docs.comfy.org/comfy-cli/getting-started)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Breaking Changes](http://docs.comfy.org#breaking-changes)
- [Monkey Patching](http://docs.comfy.org#monkey-patching)
- [Optional Input Validation](http://docs.comfy.org#optional-input-validation)
- [Execution Order](http://docs.comfy.org#execution-order)
- [New Functionality](http://docs.comfy.org#new-functionality)
- [Validation Changes](http://docs.comfy.org#validation-changes)
- [Lazy Evaluation](http://docs.comfy.org#lazy-evaluation)
- [Node Expansion](http://docs.comfy.org#node-expansion)

<!-- END Development/essentials/comfyui-server/execution_model_inversion_guide.md -->


<!-- BEGIN Development/essentials/core-concepts/dependencies.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Dependencies

# Dependencies

Understand dependencies in ComfyUI

## [​](http://docs.comfy.org#a-workflow-file-depends-on-other-files) A workflow file depends on other files

We often obtain various workflow files from the community, but frequently find that the workflow cannot run directly after loading. This is because a workflow file depends on other files besides the workflow itself, such as media asset inputs, models, custom nodes, related Python dependencies, etc. ComfyUI workflows can only run normally when all relevant dependencies are satisfied.

ComfyUI workflow dependencies mainly fall into the following categories:

- Assets (media files including audio, video, images, and other inputs)
- Custom nodes
- Python dependencies
- Models (such as Stable Diffusion models, etc.)

## [​](http://docs.comfy.org#assets) Assets

An AI model is an example of an ***asset***. In media production, an asset is some media file that supplies input data. For example, a video editing program operates on movie files stored on disk. The editing program’s project file holds links to these movie file assets, allowing non-destructive editing that doesn’t alter the original movie files.

ComfyUI works the same way. A workflow can only run if all of the required assets are found and loaded. Generative AI models, images, movies, and sounds are some examples of assets that a workflow might depend upon. These are therefore known as ***dependent assets*** or ***asset dependencies***.

## [​](http://docs.comfy.org#custom-nodes) Custom Nodes

Custom nodes are an important component of ComfyUI that extend its functionality. They are created by the community and can be installed to add new capabilities to your workflows.

## [​](http://docs.comfy.org#python-dependencies) Python Dependencies

ComfyUI is a Python-based project. We build a standalone Python environment to run ComfyUI, and all related dependencies are installed in this isolated Python environment.

### [​](http://docs.comfy.org#comfyui-dependencies) ComfyUI Dependencies

You can view ComfyUI’s current dependencies in the [requirements.txt](https://github.com/comfyanonymous/ComfyUI/blob/master/requirements.txt) file:

```text
comfyui-frontend-package==1.14.5
torch
torchsde
torchvision
torchaudio
numpy>=1.25.0
einops
transformers>=4.28.1
tokenizers>=0.13.3
sentencepiece
safetensors>=0.4.2
aiohttp>=3.11.8
yarl>=1.18.0
pyyaml
Pillow
scipy
tqdm
psutil

#non essential dependencies:
kornia>=0.7.1
spandrel
soundfile
av
```

As ComfyUI evolves, we may adjust dependencies accordingly, such as adding new dependencies or removing ones that are no longer needed. So if you use Git to update ComfyUI, you need to run the following command in the corresponding environment after pulling the latest updates:

```bash
pip install -r requirements.txt
```

This ensures that ComfyUI’s dependencies are up to date for proper operation. You can also modify specific package dependency versions to upgrade or downgrade certain dependencies.

Additionally, ComfyUI’s frontend [ComfyUI\_frontend](https://github.com/Comfy-Org/ComfyUI_frontend) is currently maintained as a separate project. We update the `comfyui-frontend-package` dependency version after the corresponding version stabilizes. If you need to switch to a different frontend version, you can check the version information [here](https://pypi.org/project/comfyui-frontend-package/#history).

### [​](http://docs.comfy.org#custom-node-dependencies) Custom Node Dependencies

Thanks to the efforts of many authors in the ComfyUI community, we can extend ComfyUI’s functionality by using different custom nodes, enabling impressive creativity.

Typically, each custom node has its own dependencies and a separate `requirements.txt` file. If you use [ComfyUI Manager](https://github.com/ltdrdata/ComfyUI-Manager) to install custom nodes, ComfyUI Manager will usually automatically install the corresponding dependencies.

There are also cases where you need to install dependencies manually. Currently, all custom nodes are installed in the `ComfyUI/custom_nodes` directory.

You need to navigate to the corresponding plugin directory in your ComfyUI Python environment and run `pip install -r requirements.txt` to install the dependencies.

If you’re using the [Windows Portable version](http://docs.comfy.org/installation/comfyui_portable_windows), you can use the following command in the `ComfyUI_windows_portable` directory:

```plaintext
python_embeded\python.exe -m pip install -r ComfyUI\custom_nodes\<custom_node_name>\requirements.txt
```

to install the dependencies for the corresponding node.

### [​](http://docs.comfy.org#dependency-conflicts) Dependency Conflicts

Dependency conflicts are a common issue when using ComfyUI. You might find that after installing or updating a custom node, previously installed custom nodes can no longer be found in ComfyUI’s node library, or error pop-ups appear. One possible reason is dependency conflicts.

There can be many reasons for dependency conflicts, such as:

1. Custom node version locking

Some plugins may fix the exact version of a dependency library (e.g., `open_clip_torch==2.26.1`), while other plugins may require a higher version (e.g., `open_clip_torch>=2.29.0`), making it impossible to satisfy both version requirements simultaneously.

**Solution**: You can try changing the fixed version dependency to a range constraint, such as `open_clip_torch>=2.26.1`, and then reinstall the dependencies to resolve these issues.

2. Environment pollution

During the installation of custom node dependencies, it may overwrite versions of libraries already installed by other plugins. For example, multiple plugins may depend on `PyTorch` but require different CUDA versions, and the later installed plugin will break the existing environment.

**Solutions**:

- You can try manually installing specific versions of dependencies in the Python virtual environment to resolve such issues.
- Or create different Python virtual environments for different plugins to resolve these issues.
- Try installing plugins one by one, restarting ComfyUI after each installation to observe if dependency conflicts occur.

<!--THE END-->

3. Custom node dependency versions incompatible with ComfyUI dependency versions

These types of dependency conflicts may be more difficult to resolve, and you may need to upgrade/downgrade ComfyUI or change the dependency versions of custom nodes to resolve these issues.

**Solution**: These types of dependency conflicts may be more difficult to resolve, and you may need to upgrade/downgrade ComfyUI or change the dependency versions of custom nodes to resolve these issues.

## [​](http://docs.comfy.org#models) Models

Models are a significant asset dependency for ComfyUI. Various custom nodes and workflows are built around specific models, such as the Stable Diffusion series, Flux series, Ltxv, and others. These models are an essential foundation for creation with ComfyUI, so we need to ensure that the models we use are properly available. Typically, our models are saved in the corresponding directory under `ComfyUI/models/`. Of course, you can also create an [extra\_model\_paths.yaml](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) by modifying the template to make additional model paths recognized by ComfyUI. This allows multiple ComfyUI instances to share the same model library, reducing disk usage.

## [​](http://docs.comfy.org#software) Software

An advanced application like ComfyUI also has ***software dependencies***. These are libraries of programming code and data that are required for the application to run. Custom nodes are examples of software dependencies. On an even more fundamental level, the Python programming environment is the ultimate dependency for ComfyUI. The correct version of Python is required to run a particular version of ComfyUI. Updates to Python, ComfyUI, and custom nodes can all be handled from the **ComfyUI Manager** window.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/core-concepts/dependencies.mdx)

[Previous](http://docs.comfy.org/essentials/core-concepts/models)

[ShortcutsKeyboard and mouse shortcuts for ComfyUI and related settings  
\
Next](http://docs.comfy.org/interface/shortcuts)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [A workflow file depends on other files](http://docs.comfy.org#a-workflow-file-depends-on-other-files)
- [Assets](http://docs.comfy.org#assets)
- [Custom Nodes](http://docs.comfy.org#custom-nodes)
- [Python Dependencies](http://docs.comfy.org#python-dependencies)
- [ComfyUI Dependencies](http://docs.comfy.org#comfyui-dependencies)
- [Custom Node Dependencies](http://docs.comfy.org#custom-node-dependencies)
- [Dependency Conflicts](http://docs.comfy.org#dependency-conflicts)
- [Models](http://docs.comfy.org#models)
- [Software](http://docs.comfy.org#software)

<!-- END Development/essentials/core-concepts/dependencies.md -->


<!-- BEGIN Development/essentials/core-concepts/links.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Links

# Links

Understand connection links in ComfyUI

As ComfyUI is still in rapid iteration and development, we are continuously improving it every day. Therefore, some operations mentioned in this article may change or be omitted. Please refer to the actual interface. If you find changes in actual operations, it may be due to our iterative updates. You can also fork [this repo](https://github.com/Comfy-Org/docs) and help us improve this documentation.

## [​](http://docs.comfy.org#links-connect-nodes) Links connect nodes

In the terminology of ComfyUI, the lines or curves between nodes are called ***links***. They’re also known as ***connections*** or wires. Links can be displayed in several ways, such as curves, right angles, straight lines, or completely hidden.

You can modify the link style in **Setup Menu** —&gt; **Display (Lite Graph)** —&gt; **Graph** —&gt; **Link Render Mode**.

You can also temporarily hide links in the **Canvas Menu**.

Link display is crucial. Depending on the situation, it may be necessary to see all links. Especially when learning, sharing, or even just understanding workflows, the visibility of links enables users to follow the flow of data through the graph. For packaged workflows that aren’t intended to be altered, it might make sense to hide the links to reduce clutter.

### [​](http://docs.comfy.org#reroute-node) Reroute node

If legibility of the graph structure is important, then link wires can be manually routed in the 2D space of the graph with a tiny node called **Reroute**. Its purpose is to position the beginning and/or end points of link wires to ensure visibility. We can design a workflow so that link wires don’t pass behind nodes, don’t cross other link wires, and so on.

We are also continuously improving the native reroute functionality in litegraph. We recommend using this feature in the future to reorganize connections.

## [​](http://docs.comfy.org#color-coding) Color-coding

The data type of node properties is indicated by color coding of input/output ports and link connection wires. We can always tell which inputs and outputs can be connected to one another by their color. Ports can only be connected to other ports of the same color to ensure matching data types.

Common data types:

Data typeColordiffusion modellavenderCLIP modelyellowVAE modelroseconditioningorangelatent imagepinkpixel imagebluemaskgreennumber (integer or float)light greenmeshbright green

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/core-concepts/links.mdx)

[Previous](http://docs.comfy.org/essentials/core-concepts/properties)

[Mask EditorLearn how to use the Mask Editor in ComfyUI, including settings and usage instructions  
\
Next](http://docs.comfy.org/interface/maskeditor)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Links connect nodes](http://docs.comfy.org#links-connect-nodes)
- [Reroute node](http://docs.comfy.org#reroute-node)
- [Color-coding](http://docs.comfy.org#color-coding)

<!-- END Development/essentials/core-concepts/links.md -->


<!-- BEGIN Development/essentials/core-concepts/models.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Models

# Models

## [​](http://docs.comfy.org#models-are-essential) Models are essential

Models are essential building blocks for media generation workflows. They can be combined and mixed to achieve different creative effects.

The word ***model*** has many different meanings. Here, it means a data file carrying information that is required for a node graph to do its work. Specifically, it’s a data structure that *models* some function. As a verb, to model something means to represent it or provide an example.

The primary example of a model data file in ComfyUI is an AI ***diffusion model***. This is a large set of data that represents the complex relationships among text strings and images, making it possible to translate words into pictures or vice versa. Other examples of common models used for image generation are language models such as CLIP, and upscaling models such as RealESRGAN.

## [​](http://docs.comfy.org#model-files) Model files

Model files are absolutely required for generative media production. Nothing can happen in a workflow if the model files are not found. Models are not included in the ComfyUI installation, but ComfyUI can often automatically download and install missing model files. Many models can be downloaded and installed from the **ComfyUI Manager** window. Models can also be found at websites such as [huggingface.co](https://huggingface.co), [civitai.green](https://civitai.green), and [github.com](https://github.com).

### [​](http://docs.comfy.org#using-models-in-comfyui) Using Models in ComfyUI

1. Download and place them in the ComfyUI program directory
   
   1. Within the **models** folder, you’ll find subfolders for various types of models, such as **checkpoints**
   2. The **ComfyUI Manager** helps to automate the process of searching, downloading, and installing
   3. Restart ComfyUI if it’s running
2. In your workflow, create the node appropriate to the model type, e.g. **Load Checkpoint**, **Load LoRA**, **Load VAE**
3. In the loader node, choose the model you wish to use
4. Connect the loader node to other nodes in your workflow

### [​](http://docs.comfy.org#file-size) File size

Models can be extremely large files relative to image files. A typical uncompressed image may require a few megabytes of disk storage. Generative AI models can be tens of thousands of times larger, up to tens of gigabytes per model. They take up a great deal of disk space and take a long time to transfer over a network.

## [​](http://docs.comfy.org#model-training-and-refinement) Model training and refinement

A generative AI model is created by training a machine learning program on a very large set of data, such as pairs of images and text descriptions. An AI model doesn’t store the training data explicitly, but rather it stores the correlations that are implicit within the data.

Organizations and companies such as Stability AI and Black Forest Labs release “base” models that carry large amounts of generic information. These are general purpose generative AI models. Commonly, the base models need to be ***refined*** in order to get high quality generative outputs. A dedicated community of people work to refine the base models. The new, refined models produce better output, provide new or different functionality, and/or use fewer resources. Refined models can usually be run on systems with less computing power and/or memory.

## [​](http://docs.comfy.org#auxiliary-models) Auxiliary models

Model functionality can be extended with auxiliary models. For example, art directing a text-to-image workflow to achieve a specific result may be difficult or impossible using a diffusion model alone. Additional models can refine a diffusion model within the workflow graph to produce desired results. Examples include **LoRA** (Low Rank Adaptation), a small model that is trained on a specific subject; **ControlNet**, a model that helps control composition using a guide image; and **Inpainting**, a model that allows certain diffusion models to generate new content within an existing image.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/core-concepts/models.mdx)

[Previous](http://docs.comfy.org/interface/maskeditor)

[DependenciesUnderstand dependencies in ComfyUI  
\
Next](http://docs.comfy.org/essentials/core-concepts/dependencies)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Models are essential](http://docs.comfy.org#models-are-essential)
- [Model files](http://docs.comfy.org#model-files)
- [Using Models in ComfyUI](http://docs.comfy.org#using-models-in-comfyui)
- [File size](http://docs.comfy.org#file-size)
- [Model training and refinement](http://docs.comfy.org#model-training-and-refinement)
- [Auxiliary models](http://docs.comfy.org#auxiliary-models)

<!-- END Development/essentials/core-concepts/models.md -->


<!-- BEGIN Development/essentials/core-concepts/nodes.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Nodes

# Nodes

Understand the concept of a node in ComfyUI.

In ComfyUI, nodes are the fundamental building blocks for executing tasks. Each node is an independently built module, whether it’s a **Comfy Core** node or a **Custom Node**, with its own unique functionality. Nodes connect to each other through links, allowing us to build complex functionality like assembling LEGO blocks. The combinations of different nodes create the unlimited possibilities of ComfyUI.

For example, in the K-Sampler node, you can see it has multiple inputs and outputs, and also includes multiple parameter settings. These parameters determine the logic of node execution. Behind each node is well-written Python logic, allowing you to achieve corresponding functionality without having to write code yourself.

As ComfyUI is still in rapid iteration and development, we are continuously improving it every day. Therefore, some operations mentioned in this article may change or be omitted. Please refer to the actual interface. If you find changes in actual operations, it may be due to our iterative updates. You can also fork [this repo](https://github.com/Comfy-Org/docs) and help us improve this documentation.

## [​](http://docs.comfy.org#nodes-perform-operations) Nodes perform operations

In computer science, a ***node*** is a container for information, usually including programmed instructions to perform some task. Nodes almost never exist in isolation, they’re almost always connected to other nodes in a networked graph. In ComfyUI, nodes take the visual form of boxes that are connected to each other.

ComfyUI nodes are usually ***function operators***. This means that they operate on some data to perform a function. A function is a process that accepts input data, performs some operation on it, and produces output data. In other words, nodes do some work, contributing to the completion of a task such as generating an image. So ComfyUI nodes almost always have at least one input or output, and usually have multiple inputs and outputs.

## [​](http://docs.comfy.org#different-node-states) Different Node States

In ComfyUI, nodes have multiple states. Here are some common node states:

1. **Normal State**: The default state
2. **Running State**: The running state, typically displayed when a node is executing after you start running the workflow
3. **Error State**: Node error, typically displayed after running the workflow if there’s a problem with the node’s input, indicated by red marking of the erroneous input node. You need to fix the problematic input to ensure the workflow runs correctly
4. **Missing State**: This state usually appears after importing workflows, with two possibilities:
   
   - Comfy Core native node missing: This usually happens because ComfyUI has been updated, but you’re using an older version of ComfyUI. You need to update ComfyUI to resolve this issue
   - Custom node missing: The workflow uses custom nodes developed by third-party authors, but your local ComfyUI version doesn’t have these custom nodes installed. You can use [ComfyUI-Manager](https://github.com/Comfy-Org/ComfyUI-Manager) to find and install the missing custom nodes

## [​](http://docs.comfy.org#connections-between-nodes) Connections Between Nodes

In ComfyUI, nodes are connected through [links](http://docs.comfy.org/essentials/core-concepts/links), allowing data of the same type to flow between different processing units to achieve the final result.

Each node receives some input, processes it through its module, and converts it to corresponding output. Connections between different nodes must conform to the data type requirements. In ComfyUI, we use different colors to distinguish node data types. Below are some basic data types:

Data typeColordiffusion modellavenderCLIP modelyellowVAE modelroseconditioningorangelatent imagepinkpixel imagebluemaskgreennumber (integer or float)light greenmeshbright green

As ComfyUI evolves, we may expand to more data types to meet the needs of more scenarios.

### [​](http://docs.comfy.org#connecting-and-disconnecting-nodes) Connecting and Disconnecting Nodes

**Connecting**: Drag from the output point of one node to the input of the same color on another node to connect them **Disconnecting**: Click on the input endpoint and drag the mouse left button to disconnect, or cancel the connection through the midpoint menu of the link

## [​](http://docs.comfy.org#node-appearance) Node Appearance

We provide various style settings for you to customize the appearance of nodes:

- Modify styles
- Double-click the node title to modify the node name
- Switch node inputs between input sockets and widgets through the context menu
- Resize the node using the bottom right corner

### [​](http://docs.comfy.org#node-badges) Node Badges

We provide multiple node badge display features, such as:

- Node ID
- Node source

Currently, **Comfy Core nodes** use a fox icon for display, while custom nodes use their names. This way you can quickly understand which node package a node comes from.

You can set the corresponding display in the menu:

## [​](http://docs.comfy.org#node-context-menus) Node Context Menus

Node context menus are mainly divided into two types:

- Context menu for the node itself
- Context menu for inputs/outputs

### [​](http://docs.comfy.org#node-context-menu) Node Context Menu

By right-clicking on a node, you can expand the corresponding node context menu:

In the node’s right-click context menu, you can:

- Adjust the node’s color style
- Modify the title
- Clone, copy, or delete the node
- Set the node’s mode

In this menu, besides appearance-related settings, the following menu operations are important:

- **Mode**: Set the node’s mode: Always, Never, Bypass
- **Toggle between Widget and Input mode for node inputs**: Switch between widget and input mode for node inputs

#### [​](http://docs.comfy.org#mode) Mode

For modes, you may notice that we currently provide: Always, Never, On Event, On Trigger - four modes, but actually only **Always** and **Never** are effective. **On Event** and **On Trigger** are currently ineffective as we haven’t fully implemented this feature. Additionally, you can understand **Bypass** as a mode. Below is an explanation of the available modes:

- **Always**: The default node mode. The node will execute whenever it runs for the first time or when any of its inputs change since the last execution
- **Never**: The node will never execute under any circumstances, as if it’s been deleted. Subsequent nodes cannot read or receive any data from it
- **Bypass**: The node will never execute under any circumstances, but subsequent nodes can still try to obtain data that hasn’t been processed by this node

Below is a comparison of the `Never` and `Bypass` modes:

In this comparison example, you can see that both workflows apply two LoRA models simultaneously, with the difference being that one `Load LoRA` node is set to `Never` mode while the other is set to `Bypass` mode.

- The node set to `Never` mode causes subsequent nodes to show errors because they don’t receive any input data
- The node set to `Bypass` mode still allows subsequent nodes to receive unprocessed data, so they load the output data from the first `Load LoRA` node, allowing the subsequent workflow to continue running normally

#### [​](http://docs.comfy.org#switching-between-widget-and-input-mode-for-node-inputs) Switching Between Widget and Input Mode for Node Inputs

In some cases, we need to use output results from other nodes as input. In this case, we can switch between widget and input mode for node inputs.

Here’s a very simple example:

By switching the K-Sampler’s Seed from widget to input mode, multiple nodes can share the same seed, achieving variable uniformity across multiple samplers. Comparing the first node with the subsequent two nodes, you can see that the seed in the latter two nodes is in input mode. You can also convert it back to widget mode:

After frontend version v1.16.0, we improved this feature. Now you only need to directly connect the input line to the corresponding widget to complete this process

> Say goodbye to annoying widget &lt;&gt; socket conversion starting from frontend version v1.16.0! Now each widget just always have an associated input socket by default [#ComfyUI](https://twitter.com/hashtag/ComfyUI?src=hash&ref_src=twsrc%5Etfw) [pic.twitter.com/sP9HHKyGYW](https://t.co/sP9HHKyGYW)
> 
> — Chenlei Hu (@HclHno3) [April 7, 2025](https://twitter.com/HclHno3/status/1909059259536375961?ref_src=twsrc%5Etfw)

### [​](http://docs.comfy.org#input%2Foutput-context-menu) Input/Output Context Menu

This context menu is mainly related to the data type of the corresponding input/output:

When dragging the input/output of a node, if a connection appears but you haven’t connected to another node’s input or output, releasing the mouse will pop up a context menu for the input/output, used to quickly add related types of nodes. You can adjust the number of node suggestions in the settings:

## [​](http://docs.comfy.org#node-selection-toolbox) Node Selection Toolbox

The **Node Selection Toolbox** is a floating tool that provides quick operations for nodes. When you select a node, it hovers above the selected node. Through this toolbox, you can:

- Change the node’s color
- Quickly set the node to Bypass mode (not execute during runtime)
- Lock the node
- Delete the node

Of course, these functions can also be found in the right-click menu of the corresponding node. The node selection toolbox just provides a shortcut operation. If you want to disable this feature, you can turn it off in the settings.

## [​](http://docs.comfy.org#node-groups) Node Groups

In ComfyUI, you can select multiple parts of a workflow simultaneously, then use the right-click menu to merge them into a node group, making that part a reusable module that can be repeatedly called in your ComfyUI.

## [​](http://docs.comfy.org#custom-nodes) Custom Nodes

ComfyUI includes many powerful nodes in the base installation package. These are known as **Comfy Core** nodes. Additionally, the ComfyUI community has created an amazing array of [***custom nodes***](https://registry.comfy.org) to perform a wide variety of functions.

## [​](http://docs.comfy.org#comfyui-manager) ComfyUI Manager

The **ComfyUI Manager** window makes it easy to perform custom node management tasks such as search, install, update, disable, and uninstall. The Manager is included in the ComfyUI desktop application, but not in the ComfyUI server application.

### [​](http://docs.comfy.org#installing-the-comfyui-manager) Installing the ComfyUI Manager

If you’re running the ComfyUI server application, you need to install the Manager. If ComfyUI is running, shut it down before proceeding.

The first step is to install Git, a command-line application for software version control. Git will download the ComfyUI Manager from [github.com](https://github.com). Download Git from [git-scm.com](https://git-scm.com/) and install it.

Once Git is installed, navigate to the ComfyUI server program directory, to the folder labeled **custom\_nodes**. Open up a command window or terminal. Make sure that the command line displays the current directory path as **custom\_nodes**. Enter the following command. This will download the Manager. Technically, this is known as *cloning a Git repository*.

```bash
git clone https://github.com/ltdrdata/ComfyUI-Manager.git
```

For details or special cases, see [ComfyUI Manager Install](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#installation).

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/core-concepts/nodes.mdx)

[Previous](http://docs.comfy.org/essentials/core-concepts/workflow)

[Properties  
\
Next](http://docs.comfy.org/essentials/core-concepts/properties)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Nodes perform operations](http://docs.comfy.org#nodes-perform-operations)
- [Different Node States](http://docs.comfy.org#different-node-states)
- [Connections Between Nodes](http://docs.comfy.org#connections-between-nodes)
- [Connecting and Disconnecting Nodes](http://docs.comfy.org#connecting-and-disconnecting-nodes)
- [Node Appearance](http://docs.comfy.org#node-appearance)
- [Node Badges](http://docs.comfy.org#node-badges)
- [Node Context Menus](http://docs.comfy.org#node-context-menus)
- [Node Context Menu](http://docs.comfy.org#node-context-menu)
- [Mode](http://docs.comfy.org#mode)
- [Switching Between Widget and Input Mode for Node Inputs](http://docs.comfy.org#switching-between-widget-and-input-mode-for-node-inputs)
- [Input/Output Context Menu](http://docs.comfy.org#input%2Foutput-context-menu)
- [Node Selection Toolbox](http://docs.comfy.org#node-selection-toolbox)
- [Node Groups](http://docs.comfy.org#node-groups)
- [Custom Nodes](http://docs.comfy.org#custom-nodes)
- [ComfyUI Manager](http://docs.comfy.org#comfyui-manager)
- [Installing the ComfyUI Manager](http://docs.comfy.org#installing-the-comfyui-manager)

<!-- END Development/essentials/core-concepts/nodes.md -->


<!-- BEGIN Development/essentials/core-concepts/properties.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Properties

# Properties

## [​](http://docs.comfy.org#nodes-are-containers-for-properties) Nodes are containers for properties

Nodes usually have ***properties***. Also known as ***parameters*** or ***attributes***, node properties are variables that can be changed. Some properties can be adjusted manually by the user, using a data entry field called a ***widget***. Other properties can be driven automatically by other nodes connected to the property ***input slot*** or port. Usually, a property can be converted from widget to input and vice versa, allowing users to control property values manually or automatically.

Properties can take many forms and hold many different types of information. For example, a **Load Checkpoint** node has a single property:  the file path to the generative model checkpoint file. A **KSampler** node has multiple properties such as the number of sampling **steps**, **CFG** scale, **sampler\_name**, etc.

## [​](http://docs.comfy.org#data-types) Data types

Information can come in many different forms, called ***data types***. For example, alphanumeric text is known as a ***string***, a whole number is an ***integer***, and a number with a decimal point is known as a ***floating point*** number or ***float***. New data types are always being added to ComfyUI.

ComfyUI is written in the Python scripting language, which is very forgiving about data types. By contrast, the ComfyUI environment is very ***strongly typed***. This means that different data types can’t be mixed up. For example, we can’t connect an image output to an integer input. This is a huge benefit to users, guiding them to proper workflow construction and preventing program errors.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/core-concepts/properties.mdx)

[Previous](http://docs.comfy.org/essentials/core-concepts/nodes)

[LinksUnderstand connection links in ComfyUI  
\
Next](http://docs.comfy.org/essentials/core-concepts/links)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Nodes are containers for properties](http://docs.comfy.org#nodes-are-containers-for-properties)
- [Data types](http://docs.comfy.org#data-types)

<!-- END Development/essentials/core-concepts/properties.md -->


<!-- BEGIN Development/essentials/core-concepts/workflow.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Workflow

# Workflow

## [​](http://docs.comfy.org#a-graph-of-nodes) A graph of nodes

ComfyUI is an environment for building and running generative content ***workflows***. In this context, a workflow is defined as a collection of program objects called ***nodes*** that are connected to each other, forming a network. This network is also known as a ***graph***.

A ComfyUI workflow can generate any type of media: image, video, audio, AI model, AI agent, and so on.

## [​](http://docs.comfy.org#sample-workflows) Sample workflows

To get started, try out some of the [official workflows](https://comfyanonymous.github.io/ComfyUI_examples). These use only the Core nodes included in the ComfyUI installation. A thriving community of developers has created a rich [ecosystem](https://registry.comfy.org) of custom nodes to extend the functionality of ComfyUI.

### [​](http://docs.comfy.org#simple-example) Simple Example

## [​](http://docs.comfy.org#visual-programming) Visual programming

A node-based computer program like ComfyUI provides a level of power and flexibility that can’t be achieved with traditional menu- and button-driven applications. The ComfyUI node graph is not limited by the tools provided in a traditional computer application. It’s a high-level ***visual programming environment*** allowing users to design complex systems without needing to write program code or understand advanced mathematics.

Many other computer applications use this same node graph paradigm. Examples include the compositing application called Nuke, the 3D programs Maya and Blender, the Unreal real-time graphics engine, and the interactive media authoring program called Max.

### [​](http://docs.comfy.org#more-complex-example) More Complex Example

## [​](http://docs.comfy.org#procedural-framework) Procedural framework

Another term used to describe a node-based application is ***procedural framework***. Procedural means generative: some procedure or algorithm is employed to generate content such as a 3D model or a musical composition.

ComfyUI is all of these things: a node graph, a visual programming environment, and a procedural framework. What makes ComfyUI different (and amazing!) is that its radically open structure allows us to generate any type of media asset such as picture, movie, sound, 3D model, AI model, etc.

In the context of ComfyUI, the term ***workflow*** is a synonym for the node network or graph. It corresponds to the ***scene graph*** in a 3D or multimedia program: the network of all of the nodes within a particular disk file. 3D programs call this a ***scene file***. Video editing, compositing, and multimedia programs usually call it a ***project file***.

## [​](http://docs.comfy.org#saving-workflows) Saving workflows

The ComfyUI workflow is automatically saved in the metadata of any generated image, allowing users to open and use the graph that generated the image. A workflow can also be stored in a human-readable text file that follows the JSON data format. This is necessary for media formats that don’t support metadata. ComfyUI workflows stored as JSON files are very small, allowing convenient versioning, archiving, and sharing of graphs, independently of any generated media.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/core-concepts/workflow.mdx)

[Previous](http://docs.comfy.org/interface/credits)

[NodesUnderstand the concept of a node in ComfyUI.  
\
Next](http://docs.comfy.org/essentials/core-concepts/nodes)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [A graph of nodes](http://docs.comfy.org#a-graph-of-nodes)
- [Sample workflows](http://docs.comfy.org#sample-workflows)
- [Simple Example](http://docs.comfy.org#simple-example)
- [Visual programming](http://docs.comfy.org#visual-programming)
- [More Complex Example](http://docs.comfy.org#more-complex-example)
- [Procedural framework](http://docs.comfy.org#procedural-framework)
- [Saving workflows](http://docs.comfy.org#saving-workflows)

<!-- END Development/essentials/core-concepts/workflow.md -->


<!-- BEGIN Development/get_started/first_generation.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Getting Started with AI Image Generation

# Getting Started with AI Image Generation

This tutorial will guide you through your first image generation with ComfyUI, covering basic interface operations like workflow loading, model installation, and image generation

This guide aims to help you understand ComfyUI’s basic operations and complete your first image generation. We’ll cover:

1. Loading example workflows
   
   - Loading from ComfyUI’s workflow templates
   - Loading from images with workflow metadata
2. Model installation guidance
   
   - Automatic model installation
   - Manual model installation
   - Using ComfyUI Manager for model installation
3. Completing your first text-to-image generation

## [​](http://docs.comfy.org#about-text-to-image) About Text-to-Image

Text-to-Image is a fundamental AI drawing feature that generates images from text descriptions. It’s one of the most commonly used functions in AI art generation. You can think of the process as telling your requirements (positive and negative prompts) to an artist (the drawing model), who will then create what you want. Detailed explanations about text-to-image will be covered in the [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image) chapter.

## [​](http://docs.comfy.org#comfyui-text-to-image-workflow-tutorial) ComfyUI Text-to-Image Workflow Tutorial

### [​](http://docs.comfy.org#1-launch-comfyui) 1. Launch ComfyUI

Make sure you’ve followed the installation guide to start ComfyUI and can successfully enter the ComfyUI interface.

If you have not installed ComfyUI, please choose a suitable version to install based on your device.

ComfyUI Desktop (Recommended)

ComfyUI Desktop currently supports standalone installation for **Windows and MacOS (ARM)**, currently in Beta

- Code is open source on [Github](https://github.com/Comfy-Org/desktop)

You can choose the appropriate installation for your system and hardware below

- Windows
- MacOS(Apple Silicon)
- Linux

[**ComfyUI Desktop (Windows) Installation Guide**  
\
Suitable for **Windows** version with **Nvidia** GPU](http://docs.comfy.org/installation/desktop/windows)

[**ComfyUI Desktop (Windows) Installation Guide**  
\
Suitable for **Windows** version with **Nvidia** GPU](http://docs.comfy.org/installation/desktop/windows)

[**ComfyUI Desktop (MacOS) Installation Guide**  
\
Suitable for MacOS with **Apple Silicon**](http://docs.comfy.org/installation/desktop/macos)

ComfyUI Desktop **currently has no Linux prebuilds**, please visit the [Manual Installation](http://docs.comfy.org/installation/manual_install) section to install ComfyUI

ComfyUI Portable (Windows)

[**ComfyUI Portable (Windows) Installation Guide**  
\
Supports **Windows** ComfyUI version running on **Nvidia GPUs** or **CPU-only**, always use the latest commits and completely portable.](http://docs.comfy.org/installation/comfyui_portable_windows)

Manual Installation

[**ComfyUI Manual Installation Guide**  
\
Supports all system types and GPU types (Nvidia, AMD, Intel, Apple Silicon, Ascend NPU, Cambricon MLU)](http://docs.comfy.org/installation/manual_install)

### [​](http://docs.comfy.org#2-load-default-text-to-image-workflow) 2. Load Default Text-to-Image Workflow

ComfyUI usually loads the default text-to-image workflow automatically when launched. However, you can try different methods to load workflows to familiarize yourself with ComfyUI’s basic operations:

- Load from Workflow Template
- Load from Images with Metadata
- Load from workflow.json

Follow the numbered steps in the image:

1. Click the **Fit View** button in the bottom right to ensure any loaded workflow isn’t hidden
2. Click the **folder icon (workflows)** in the sidebar
3. Click the **Browse example workflows** button at the top of the Workflows panel

Continue with:

4. Select the first default workflow **Image Generation** to load it

Alternatively, you can select **Browse workflow templates** from the workflow menu

Follow the numbered steps in the image:

1. Click the **Fit View** button in the bottom right to ensure any loaded workflow isn’t hidden
2. Click the **folder icon (workflows)** in the sidebar
3. Click the **Browse example workflows** button at the top of the Workflows panel

Continue with:

4. Select the first default workflow **Image Generation** to load it

Alternatively, you can select **Browse workflow templates** from the workflow menu

All images generated by ComfyUI contain metadata including workflow information. You can load workflows by:

- Dragging and dropping a ComfyUI-generated image into the interface
- Using menu **Workflows** -&gt; **Open** to open an image

Try loading the workflow using this example image:

ComfyUI workflows can be stored in JSON format. You can export workflows using menu **Workflows** -&gt; **Export**.

Try downloading and loading this example workflow:

[Download text-to-image.json](https://github.com/Comfy-Org/docs/blob/main/public/text-to-image.json)

After downloading, use menu **Workflows** -&gt; **Open** to load the JSON file.

### [​](http://docs.comfy.org#3-model-installation) 3. Model Installation

Most ComfyUI installations don’t include base models by default. After loading the workflow, if you don’t have the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) model installed, you’ll see this prompt:

All models are stored in `<your ComfyUI installation>/ComfyUI/models/` with subfolders like `checkpoints`, `embeddings`, `vae`, `lora`, `upscale_model`, etc. ComfyUI detects models in these folders and paths configured in `extra_models_config.yaml` at startup.

You can install models through:

- Automatic Download
- ComfyUI Manager
- Manual Installation

After you click the **Download** button, ComfyUI will execute the download, and different behaviors will be performed depending on the version you are using.

- ComfyUI Desktop
- ComfyUI Portable

The desktop version will automatically complete the model download and save it to the `<your ComfyUI installation location>/ComfyUI/models/checkpoints` directory. You can wait for the installation to complete or view the installation progress in the model panel on the sidebar.

If everything goes smoothly, the model should be able to download locally. If the download fails for a long time, please try other installation methods.

The desktop version will automatically complete the model download and save it to the `<your ComfyUI installation location>/ComfyUI/models/checkpoints` directory. You can wait for the installation to complete or view the installation progress in the model panel on the sidebar.

If everything goes smoothly, the model should be able to download locally. If the download fails for a long time, please try other installation methods.

The browser will execute file downloads. Please save the file to the `<your ComfyUI installation location>/ComfyUI_windows_portable/ComfyUI/models/checkpoints` directory after the download is complete.

After you click the **Download** button, ComfyUI will execute the download, and different behaviors will be performed depending on the version you are using.

- ComfyUI Desktop
- ComfyUI Portable

The desktop version will automatically complete the model download and save it to the `<your ComfyUI installation location>/ComfyUI/models/checkpoints` directory. You can wait for the installation to complete or view the installation progress in the model panel on the sidebar.

If everything goes smoothly, the model should be able to download locally. If the download fails for a long time, please try other installation methods.

The desktop version will automatically complete the model download and save it to the `<your ComfyUI installation location>/ComfyUI/models/checkpoints` directory. You can wait for the installation to complete or view the installation progress in the model panel on the sidebar.

If everything goes smoothly, the model should be able to download locally. If the download fails for a long time, please try other installation methods.

The browser will execute file downloads. Please save the file to the `<your ComfyUI installation location>/ComfyUI_windows_portable/ComfyUI/models/checkpoints` directory after the download is complete.

ComfyUI Manager is a tool for managing custom nodes, models, and plugins.

1

Open ComfyUI Manager

Click the `Manager` button to open ComfyUI Manager

2

Open Model Manager

Click `Model Manager`

3

Search and Install Model

1. Search for `v1-5-pruned-emaonly.ckpt`
2. Click `install` on the desired model

Visit [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) and follow this guide:

Save the downloaded file to:

- ComfyUI Desktop
- ComfyUI Portable

Save to `<your ComfyUI installation>/ComfyUI/models/checkpoints`

Save to `<your ComfyUI installation>/ComfyUI/models/checkpoints`

Save to `ComfyUI_windows_portable/ComfyUI/models/checkpoints`

Refresh or restart ComfyUI after saving.

### [​](http://docs.comfy.org#4-load-model-and-generate-your-first-image) 4. Load Model and Generate Your First Image

After installing the model:

1. In the **Load Checkpoint** node, ensure **v1-5-pruned-emaonly-fp16.safetensors** is selected
2. Click `Queue` or press `Ctrl + Enter` to generate

The result will appear in the **Save Image** node. Right-click to save locally.

For detailed text-to-image instructions, see our comprehensive guide:

[**ComfyUI Text-to-Image Workflow Guide**  
\
Click here for detailed text-to-image workflow instructions](http://docs.comfy.org/tutorials/basic/text-to-image)

## [​](http://docs.comfy.org#troubleshooting) Troubleshooting

### [​](http://docs.comfy.org#model-loading-issues) Model Loading Issues

If the `Load Checkpoint` node shows no models or displays “null”, verify your model installation location and try refreshing or restarting ComfyUI.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/get_started/first_generation.mdx)

[Previous](http://docs.comfy.org/installation/manual_install)

[Interface OverviewIn this article, we will briefly introduce the basic user interface of ComfyUI, familiarizing you with the various parts of the ComfyUI interface.  
\
Next](http://docs.comfy.org/interface/overview)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [About Text-to-Image](http://docs.comfy.org#about-text-to-image)
- [ComfyUI Text-to-Image Workflow Tutorial](http://docs.comfy.org#comfyui-text-to-image-workflow-tutorial)
- [1. Launch ComfyUI](http://docs.comfy.org#1-launch-comfyui)
- [2. Load Default Text-to-Image Workflow](http://docs.comfy.org#2-load-default-text-to-image-workflow)
- [3. Model Installation](http://docs.comfy.org#3-model-installation)
- [4. Load Model and Generate Your First Image](http://docs.comfy.org#4-load-model-and-generate-your-first-image)
- [Troubleshooting](http://docs.comfy.org#troubleshooting)
- [Model Loading Issues](http://docs.comfy.org#model-loading-issues)

<!-- END Development/get_started/first_generation.md -->


<!-- BEGIN Development/get_started/introduction.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Introduction

# Introduction

Official documentation for ComfyUI. Contribute [here](https://github.com/Comfy-Org/docs).

## [​](http://docs.comfy.org#comfyui) [ComfyUI](https://github.com/comfyanonymous/ComfyUI)

The most powerful and modular stable diffusion GUI and backend. Written by [comfyanonymous](https://github.com/comfyanonymous) and other [contributors](https://github.com/comfyanonymous/ComfyUI/graphs/contributors).

- **ComfyUI** is a node-based interface and inference engine for generative AI
- Users can combine various AI models and operations through nodes to achieve highly customizable and controllable content generation
- ComfyUI is completely open source and can run on your local device

## [​](http://docs.comfy.org#getting-started-with-comfyui) Getting Started with ComfyUI

### [​](http://docs.comfy.org#comfyui-installation) ComfyUI Installation

ComfyUI currently offers multiple installation methods, supporting Windows, MacOS, and Linux systems:

ComfyUI Desktop (Recommended)

ComfyUI Desktop currently supports standalone installation for **Windows and MacOS (ARM)**, currently in Beta

- Code is open source on [Github](https://github.com/Comfy-Org/desktop)

You can choose the appropriate installation for your system and hardware below

- Windows
- MacOS(Apple Silicon)
- Linux

[**ComfyUI Desktop (Windows) Installation Guide**  
\
Suitable for **Windows** version with **Nvidia** GPU](http://docs.comfy.org/installation/desktop/windows)

[**ComfyUI Desktop (Windows) Installation Guide**  
\
Suitable for **Windows** version with **Nvidia** GPU](http://docs.comfy.org/installation/desktop/windows)

[**ComfyUI Desktop (MacOS) Installation Guide**  
\
Suitable for MacOS with **Apple Silicon**](http://docs.comfy.org/installation/desktop/macos)

ComfyUI Desktop **currently has no Linux prebuilds**, please visit the [Manual Installation](http://docs.comfy.org/installation/manual_install) section to install ComfyUI

ComfyUI Portable (Windows)

[**ComfyUI Portable (Windows) Installation Guide**  
\
Supports **Windows** ComfyUI version running on **Nvidia GPUs** or **CPU-only**, always use the latest commits and completely portable.](http://docs.comfy.org/installation/comfyui_portable_windows)

Manual Installation

[**ComfyUI Manual Installation Guide**  
\
Supports all system types and GPU types (Nvidia, AMD, Intel, Apple Silicon, Ascend NPU, Cambricon MLU)](http://docs.comfy.org/installation/manual_install)

## [​](http://docs.comfy.org#contributing-to-comfyui-ecosystem) Contributing to ComfyUI Ecosystem

If you’re planning to develop ComfyUI custom nodes (plugins), please read the following section.

[**Custom Node Development Guide**  
\
Learn how to build a custom node (plugin) for ComfyUI](http://docs.comfy.org/custom-nodes/overview)

## [​](http://docs.comfy.org#contributing-to-documentation) Contributing to Documentation

Fork the documentation [repo](https://github.com/comfyanonymous/ComfyUI) on Github and submit a PR to us

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/get_started/introduction.mdx)

[System RequirementsThis guide introduces some system requirements for ComfyUI, including hardware and software requirements  
\
Next](http://docs.comfy.org/installation/system_requirements)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [ComfyUI](http://docs.comfy.org#comfyui)
- [Getting Started with ComfyUI](http://docs.comfy.org#getting-started-with-comfyui)
- [ComfyUI Installation](http://docs.comfy.org#comfyui-installation)
- [Contributing to ComfyUI Ecosystem](http://docs.comfy.org#contributing-to-comfyui-ecosystem)
- [Contributing to Documentation](http://docs.comfy.org#contributing-to-documentation)

<!-- END Development/get_started/introduction.md -->


<!-- BEGIN Development/installation/comfyui_portable_windows.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
  
  - [System Requirements](http://docs.comfy.org/installation/system_requirements)
  - Desktop(recommended)
  - [ComfyUI(portable) Windows](http://docs.comfy.org/installation/comfyui_portable_windows)
  - [Manual Installation](http://docs.comfy.org/installation/manual_install)
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI(portable) Windows

# ComfyUI(portable) Windows

This tutorial will guide you on how to download and start using ComfyUI Portable and run the corresponding programs

For Nvidia 50 series (Blackwell) GPUs, please refer to the [System Requirements](http://docs.comfy.org/installation/nvidia-50-series) section to ensure your system meets the requirements for ComfyUI.

**ComfyUI Portable** is a standalone packaged complete ComfyUI Windows version that has integrated an independent **Python (python\_embeded)** required for ComfyUI to run. You only need to extract it to use it. Currently, the portable version supports running through **Nvidia GPU** or **CPU**.

This guide section will walk you through installing ComfyUI Portable.

## [​](http://docs.comfy.org#download-comfyui-portable) Download ComfyUI Portable

You can get the latest ComfyUI Portable download link by clicking the link below

[Download ComfyUI Portable](https://github.com/comfyanonymous/ComfyUI/releases/latest/download/ComfyUI_windows_portable_nvidia.7z)

After downloading, you can use decompression software like [7-ZIP](https://7-zip.org/) to extract the compressed package

The file structure and description after extracting the portable version are as follows:

```plaintext
ComfyUI_windows_portable
├── 📂ComfyUI                   // ComfyUI main program
├── 📂python_embeded            // Independent Python environment
├── 📂update                    // Batch scripts for upgrading portable version
├── README_VERY_IMPORTANT.txt   // ComfyUI Portable usage instructions in English
├── run_cpu.bat                 // Double click to start ComfyUI (CPU only)
└── run_nvidia_gpu.bat          // Double click to start ComfyUI (Nvidia GPU)
```

## [​](http://docs.comfy.org#how-to-launch-comfyui) How to Launch ComfyUI

Double click either `run_nvidia_gpu.bat` or `run_cpu.bat` depending on your computer’s configuration to launch ComfyUI. You will see the command running as shown in the image below

When you see something similar to the image

```plaintext
To see the GUI go to: http://127.0.0.1:8188
```

At this point, your ComfyUI service has started. Normally, ComfyUI will automatically open your default browser and navigate to `http://127.0.0.1:8188`. If it doesn’t open automatically, please manually open your browser and visit this address.

During use, please do not close the corresponding command line window, otherwise ComfyUI will stop running

## [​](http://docs.comfy.org#first-image-generation) First Image Generation

After successful installation, you can refer to the section below to start your ComfyUI journey~

[**First Image Generation**  
\
This tutorial will guide you through your first model installation and text-to-image generation](http://docs.comfy.org/get_started/first_generation)

## [​](http://docs.comfy.org#additional-comfyui-portable-instructions) Additional ComfyUI Portable Instructions

### [​](http://docs.comfy.org#1-upgrading-comfyui-portable) 1. Upgrading ComfyUI Portable

You can use the batch commands in the update folder to upgrade your ComfyUI Portable version

```plaintext
ComfyUI_windows_portable
└─ 📂update
   ├── update.py
   ├── update_comfyui.bat                          // Update ComfyUI to the latest commit version
   ├── update_comfyui_and_python_dependencies.bat  // Only use when you have issues with your runtime environment
   └── update_comfyui_stable.bat                   // Update ComfyUI to the latest stable version
```

### [​](http://docs.comfy.org#2-comfyui-model-sharing-and-custom-model-directory-configuration) 2. ComfyUI Model Sharing and Custom Model Directory Configuration

If you are also using [A1111](https://github.com/AUTOMATIC1111/stable-diffusion-webui) or want to customize your model storage location, you can modify the following file to complete the configuration

```plaintext
ComfyUI_windows_portable
└─ 📂ComfyUI
  └── extra_model_paths.yaml.example  // This file is the configuration template
```

Please copy and rename the `extra_model_paths.yaml.example` to `extra_model_paths.yaml`.

Below is the original configuration file content, which you can modify according to your needs

```yaml
#Rename this to extra_model_paths.yaml and ComfyUI will load it


#config for a1111 ui
#all you have to do is change the base_path to where yours is installed
a111:
    base_path: path/to/stable-diffusion-webui/

    checkpoints: models/Stable-diffusion
    configs: models/Stable-diffusion
    vae: models/VAE
    loras: |
         models/Lora
         models/LyCORIS
    upscale_models: |
                  models/ESRGAN
                  models/RealESRGAN
                  models/SwinIR
    embeddings: embeddings
    hypernetworks: models/hypernetworks
    controlnet: models/ControlNet

#config for comfyui
#your base path should be either an existing comfy install or a central folder where you store all of your models, loras, etc.

#comfyui:
#     base_path: path/to/comfyui/
#     # You can use is_default to mark that these folders should be listed first, and used as the default dirs for eg downloads
#     #is_default: true
#     checkpoints: models/checkpoints/
#     clip: models/clip/
#     clip_vision: models/clip_vision/
#     configs: models/configs/
#     controlnet: models/controlnet/
#     diffusion_models: |
#                  models/diffusion_models
#                  models/unet
#     embeddings: models/embeddings/
#     loras: models/loras/
#     upscale_models: models/upscale_models/
#     vae: models/vae/

#other_ui:
#    base_path: path/to/ui
#    checkpoints: models/checkpoints
#    gligen: models/gligen
#    custom_nodes: path/custom_nodes

```

For example, if your WebUI is located at `D:\stable-diffusion-webui\`, you can modify the corresponding configuration to

```yaml
a111:
    base_path: D:\stable-diffusion-webui\
    checkpoints: models/Stable-diffusion
    configs: models/Stable-diffusion
    vae: models/VAE
    loras: |
         models/Lora
         models/LyCORIS
    upscale_models: |
                  models/ESRGAN
                  models/RealESRGAN
                  models/SwinIR
    embeddings: embeddings
    hypernetworks: models/hypernetworks
    controlnet: models/ControlNet
```

This way, models under paths like `D:\stable-diffusion-webui\models\Stable-diffusion\` can be detected and used by ComfyUI. Similarly, you can add other custom model location configurations

### [​](http://docs.comfy.org#3-setting-up-lan-access-for-comfyui-portable) 3. Setting Up LAN Access for ComfyUI Portable

If your ComfyUI is running on a local network and you want other devices to access ComfyUI, you can modify the `run_nvidia_gpu.bat` or `run_cpu.bat` file using Notepad to complete the configuration. This is mainly done by adding `--listen` to specify the listening address. Below is an example of the `run_nvidia_gpu.bat` file command with the `--listen` parameter added

```plaintext
.\python_embeded\python.exe -s ComfyUI\main.py --listen --windows-standalone-build
pause
```

After enabling ComfyUI, you will notice the final running address will become

```plaintext
Starting server

To see the GUI go to: http://0.0.0.0:8188
To see the GUI go to: http://[::]:8188
```

You can press `WIN + R` and type `cmd` to open the command prompt, then enter `ipconfig` to view your local IP address. Other devices can then access ComfyUI by entering `http://your-local-IP:8188` in their browser.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/installation/comfyui_portable_windows.mdx)

[Previous](http://docs.comfy.org/installation/desktop/linux)

[Manual Installation  
\
Next](http://docs.comfy.org/installation/manual_install)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Download ComfyUI Portable](http://docs.comfy.org#download-comfyui-portable)
- [How to Launch ComfyUI](http://docs.comfy.org#how-to-launch-comfyui)
- [First Image Generation](http://docs.comfy.org#first-image-generation)
- [Additional ComfyUI Portable Instructions](http://docs.comfy.org#additional-comfyui-portable-instructions)
- [1. Upgrading ComfyUI Portable](http://docs.comfy.org#1-upgrading-comfyui-portable)
- [2. ComfyUI Model Sharing and Custom Model Directory Configuration](http://docs.comfy.org#2-comfyui-model-sharing-and-custom-model-directory-configuration)
- [3. Setting Up LAN Access for ComfyUI Portable](http://docs.comfy.org#3-setting-up-lan-access-for-comfyui-portable)

<!-- END Development/installation/comfyui_portable_windows.md -->


<!-- BEGIN Development/installation/desktop/linux.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
  
  - [System Requirements](http://docs.comfy.org/installation/system_requirements)
  - Desktop(recommended)
    
    - [Windows Desktop Version](http://docs.comfy.org/installation/desktop/windows)
    - [MacOS Desktop Version](http://docs.comfy.org/installation/desktop/macos)
    - [Linux Desktop Version](http://docs.comfy.org/installation/desktop/linux)
  - [ComfyUI(portable) Windows](http://docs.comfy.org/installation/comfyui_portable_windows)
  - [Manual Installation](http://docs.comfy.org/installation/manual_install)
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Linux Desktop Version

# Linux Desktop Version

This article introduces how to download, install and use ComfyUI Desktop for Linux

Linux pre-built packages are not yet available. Please try [manual installation.](http://docs.comfy.org/installation/manual_install)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/installation/desktop/linux.mdx)

[Previous](http://docs.comfy.org/installation/desktop/macos)

[ComfyUI(portable) WindowsThis tutorial will guide you on how to download and start using ComfyUI Portable and run the corresponding programs  
\
Next](http://docs.comfy.org/installation/comfyui_portable_windows)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

<!-- END Development/installation/desktop/linux.md -->


<!-- BEGIN Development/installation/desktop/macos.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
  
  - [System Requirements](http://docs.comfy.org/installation/system_requirements)
  - Desktop(recommended)
    
    - [Windows Desktop Version](http://docs.comfy.org/installation/desktop/windows)
    - [MacOS Desktop Version](http://docs.comfy.org/installation/desktop/macos)
    - [Linux Desktop Version](http://docs.comfy.org/installation/desktop/linux)
  - [ComfyUI(portable) Windows](http://docs.comfy.org/installation/comfyui_portable_windows)
  - [Manual Installation](http://docs.comfy.org/installation/manual_install)
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

MacOS Desktop Version

# MacOS Desktop Version

This article introduces how to download, install and use ComfyUI Desktop for MacOS

**ComfyUI Desktop** is a standalone installation version that can be installed like regular software. It supports quick installation and automatic configuration of the **Python environment and dependencies**, and supports one-click import of existing ComfyUI settings, models, workflows, and files.

ComfyUI Desktop is an open source project, please visit the full code [here](https://github.com/Comfy-Org/desktop).

ComfyUI Desktop (MacOS) only supports Apple Silicon

This tutorial will guide you through the software installation process and explain related configuration details.

As **ComfyUI Desktop** is still in **Beta** status, the actual installation process may change

## [​](http://docs.comfy.org#comfyui-desktop-macos-download) ComfyUI Desktop (MacOS) Download

Please click the button below to download the installation package for MacOS **ComfyUI Desktop**

[Download for MacOS](https://download.comfy.org/mac/dmg/arm64)

## [​](http://docs.comfy.org#comfyui-desktop-installation-steps) ComfyUI Desktop Installation Steps

Double-click the downloaded installation package file. As shown in the image, drag the **ComfyUI** application into the **Applications** folder following the arrow

If your folder shows as below with a prohibition sign on the icon after opening the installation package, it means your current system version is not compatible with **ComfyUI Desktop**

Then find the **ComfyUI icon** in **Launchpad** and click it to enter ComfyUI initialization settings

## [​](http://docs.comfy.org#comfyui-desktop-initialization-process) ComfyUI Desktop Initialization Process

1

Start Screen

- Normal Start
- Maintenance Page

Click **Get Started** to begin initialization

Click **Get Started** to begin initialization

There are many reasons you might have issues installing ComfyUI. Maybe a network connection failed when installing pytorch (15 GB). Or you don’t have git installed. The maintenance page automatically opens when it detects an issue and provides a way to resolve the issue.

You can use it to resolve most issues:

- Create a python virtual environment
- Reinstall all missing core dependencies to your Python virtual environment that’s managed by Desktop
- Install git, VC redis
- Choose a new install location

The default maintenance page displays the current error content

Clicking `All` allows you to view all the content that can be operated on currently

2

Select GPU

The three options are:

1. **MPS (Recommended):** Metal Performance Shaders (MPS) is an Apple framework that uses GPUs to accelerate computing and machine learning tasks on Apple devices, supporting frameworks like PyTorch.
2. **Manual Configuration:** You need to manually install and configure the python runtime environment. Don’t select this unless you know how to configure
3. **Enable CPU Mode:** For developers and special cases only. Don’t select this unless you’re sure you need it

Unless there are special circumstances, please select **MPS** as shown and click **Next** to proceed

3

Install location

In this step, you will select the installation location for the following related content of ComfyUI:

- **Python Environment**
- **Models Model Files**
- **Custom Nodes Custom Nodes**

Recommendations:

- Please create a separate empty folder as the installation directory for ComfyUI
- Please ensure that the disk has at least **5G** of disk space to ensure the normal installation of **ComfyUI Desktop**

Not all files are installed in this directory, some files will be located in the MacOS system directory, you can refer to the uninstallation section of this guide to complete the uninstallation of the ComfyUI desktop version

4

Migrate from Existing Installation (Optional)

In this step you can migrate your existing ComfyUI installation content to ComfyUI Desktop. Select your existing ComfyUI installation directory, and the installer will automatically recognize:

- **User Files**
- **Models:** Will not be copied, only linked with desktop version
- **Custom Nodes:** Nodes will be reinstalled

Don’t worry, this step won’t copy model files. You can check or uncheck options as needed. Click **Next** to continue

5

Desktop Settings

These are preference settings:

1. **Automatic Updates:** Whether to set automatic updates when ComfyUI updates are available
2. **Usage Metrics:** If enabled, we will collect **anonymous usage data** to help improve ComfyUI
3. **Mirror Settings:** Since the program needs internet access to download Python and complete environment installation, if you see a red ❌ during installation indicating this may cause installation failure, please follow the steps below

Expand the mirror settings to find the specific failing mirror. In this screenshot the error is **Python Install Mirror** failure.

For different mirror errors, you can refer to the following content to try to manually find different mirrors and replace them

The following cases mainly apply to users in China.

#### [​](http://docs.comfy.org#python-installation-mirror) Python Installation Mirror

If the default mirror is unavailable, please try using the mirror below.

```plaintext
https://python-standalone.org/mirror/astral-sh/python-build-standalone
```

If you need to find other alternative GitHub mirror addresses, please look for and construct a mirror address pointing to the releases of the `python-build-standalone` repository.

```plaintext
https://github.com/astral-sh/python-build-standalone/releases/download
```

Build a link in the following pattern

```plaintext
https://xxx/astral-sh/python-build-standalone/releases/download
```

#### [​](http://docs.comfy.org#pypi-mirror) PyPI Mirror

- Alibaba Cloud: [https://mirrors.aliyun.com/pypi/simple/](https://mirrors.aliyun.com/pypi/simple/)
- Tencent Cloud: [https://mirrors.cloud.tencent.com/pypi/simple/](https://mirrors.cloud.tencent.com/pypi/simple/)
- University of Science and Technology of China: [https://pypi.mirrors.ustc.edu.cn/simple/](https://pypi.mirrors.ustc.edu.cn/simple/)
- Shanghai Jiao Tong University: [https://pypi.sjtu.edu.cn/simple/](https://pypi.sjtu.edu.cn/simple/)

#### [​](http://docs.comfy.org#torch-mirror) Torch Mirror

- Aliyun: [https://mirrors.aliyun.com/pytorch-wheels/cu121/](https://mirrors.aliyun.com/pytorch-wheels/cu121/)

6

Complete the installation

If everything is correct, the installer will complete and automatically enter the ComfyUI Desktop interface, then the installation is successful

## [​](http://docs.comfy.org#first-image-generation) First Image Generation

After successful installation, you can refer to the section below to start your ComfyUI journey~

[**First Image Generation**  
\
This tutorial will guide you through your first model installation and text-to-image generation](http://docs.comfy.org/get_started/first_generation)

## [​](http://docs.comfy.org#how-to-update-comfyui-desktop) How to Update ComfyUI Desktop

Currently, ComfyUI Desktop updates use automatic detection updates, please ensure that automatic updates are enabled in the settings

You can also choose to manually check for available updates in the `Menu` —&gt; `Help` —&gt; `Check for Updates`

## [​](http://docs.comfy.org#how-to-uninstall-comfyui-desktop) How to Uninstall ComfyUI Desktop

For **ComfyUI Desktop**, you can directly delete **ComfyUI** from the **Applications** folder

If you want to completely remove all **ComfyUI Desktop** files, you can manually delete these folders:

- /Users/Library/Application Support/ComfyUI

The above operations will not delete your following folders. If you need to delete corresponding files, please delete manually:

- models files
- custom nodes
- input/output directories

## [​](http://docs.comfy.org#troubleshooting) Troubleshooting

### [​](http://docs.comfy.org#%E2%80%8Berror-identification%E2%80%8B) ​Error identification​

If installation fails, you should see the following screen

It is recommended to take these steps to find the error cause:

1. Click `Show Terminal` to view error output
2. Click `Open Logs` to view installation logs
3. Visit official forum to search for error reports
4. Click `Reinstall` to try reinstalling

Before submitting feedback, it’s recommended to provide the **error output** and **log files** to tools like **GPT**

As shown above, ask the GPT for the cause of the corresponding error, or remove ComfyUI completely and retry the installation.

### [​](http://docs.comfy.org#feedback-installation-failure) Feedback Installation Failure

If you encounter any errors during installation, please check if there are similar error reports or submit errors to us through:

- Github Issues: [https://github.com/Comfy-Org/desktop/issues](https://github.com/Comfy-Org/desktop/issues)
- Comfy Official Forum: [https://forum.comfy.org/](https://forum.comfy.org/)

When submitting error reports, please ensure you include the following logs and configuration files to help us locate and investigate the issue:

1. Log Files

FilenameDescriptionLocationmain.logContains logs related to desktop application and server startup from the Electron processcomfyui.logContains logs related to ComfyUI normal operation, such as core ComfyUI process terminal output

2. Configuration Files

FilenameDescriptionLocationextra\_models\_config.yamlContains additional paths where ComfyUI will search for models and custom nodesconfig.jsonContains application configuration. This file should not be edited directly

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/installation/desktop/macos.mdx)

[Previous](http://docs.comfy.org/installation/desktop/windows)

[Linux Desktop VersionThis article introduces how to download, install and use ComfyUI Desktop for Linux  
\
Next](http://docs.comfy.org/installation/desktop/linux)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [ComfyUI Desktop (MacOS) Download](http://docs.comfy.org#comfyui-desktop-macos-download)
- [ComfyUI Desktop Installation Steps](http://docs.comfy.org#comfyui-desktop-installation-steps)
- [ComfyUI Desktop Initialization Process](http://docs.comfy.org#comfyui-desktop-initialization-process)
- [First Image Generation](http://docs.comfy.org#first-image-generation)
- [How to Update ComfyUI Desktop](http://docs.comfy.org#how-to-update-comfyui-desktop)
- [How to Uninstall ComfyUI Desktop](http://docs.comfy.org#how-to-uninstall-comfyui-desktop)
- [Troubleshooting](http://docs.comfy.org#troubleshooting)
- [​Error identification​](http://docs.comfy.org#%E2%80%8Berror-identification%E2%80%8B)
- [Feedback Installation Failure](http://docs.comfy.org#feedback-installation-failure)

<!-- END Development/installation/desktop/macos.md -->


<!-- BEGIN Development/installation/desktop/windows.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
  
  - [System Requirements](http://docs.comfy.org/installation/system_requirements)
  - Desktop(recommended)
    
    - [Windows Desktop Version](http://docs.comfy.org/installation/desktop/windows)
    - [MacOS Desktop Version](http://docs.comfy.org/installation/desktop/macos)
    - [Linux Desktop Version](http://docs.comfy.org/installation/desktop/linux)
  - [ComfyUI(portable) Windows](http://docs.comfy.org/installation/comfyui_portable_windows)
  - [Manual Installation](http://docs.comfy.org/installation/manual_install)
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Windows Desktop Version

# Windows Desktop Version

This article introduces how to download, install and use ComfyUI Desktop for Windows

**ComfyUI Desktop** is a standalone installation version that can be installed like regular software. It supports quick installation and automatic configuration of the **Python environment and dependencies**, and supports one-click import of existing ComfyUI settings, models, workflows, and files. You can quickly migrate from an existing [ComfyUI Portable version](http://docs.comfy.org/installation/comfyui_portable_windows) to the Desktop version.

ComfyUI Desktop is an open source project, please visit the full code [here](https://github.com/Comfy-Org/desktop)

ComfyUI Desktop hardware requirements:

- NVIDIA GPU

This tutorial will guide you through the software installation process and explain related configuration details.

As **ComfyUI Desktop** is still in **Beta** status, the actual installation process may change

## [​](http://docs.comfy.org#comfyui-desktop-windows-download) ComfyUI Desktop (Windows) Download

Please click the button below to download the installation package for Windows **ComfyUI Desktop**

[Download for Windows (NVIDIA)](https://download.comfy.org/windows/nsis/x64)

## [​](http://docs.comfy.org#comfyui-desktop-installation-steps) ComfyUI Desktop Installation Steps

Double-click the downloaded installation package file, which will first perform an automatic installation and create a **ComfyUI Desktop** shortcut on the desktop

Double-click the corresponding shortcut to enter ComfyUI initialization settings

### [​](http://docs.comfy.org#comfyui-desktop-initialization-process) ComfyUI Desktop Initialization Process

1

Start Screen

- Normal Start
- Maintenance Page

Click **Get Started** to begin initialization

Click **Get Started** to begin initialization

There are many reasons you might have issues installing ComfyUI. Maybe a network connection failed when installing pytorch (15 GB). Or you don’t have git installed. The maintenance page automatically opens when it detects an issue and provides a way to resolve the issue.

You can use it to resolve most issues:

- Create a python virtual environment
- Reinstall all missing core dependencies to your Python virtual environment that’s managed by Desktop
- Install git, VC redis
- Choose a new install location

The default maintenance page displays the current error content

Clicking `All` allows you to view all the content that can be operated on currently

2

Select GPU

The three options are:

1. **Nvidia GPU (Recommended):** Direct support for pytorch and CUDA
2. **Manual Configuration:** You need to manually install and configure the python runtime environment. Don’t select this unless you know how to configure
3. **Enable CPU Mode:** For developers and special cases only. Don’t select this unless you’re sure you need it

Unless there are special circumstances, please select **NVIDIA** as shown and click **Next** to proceed

3

Install location

In this step, you will select the installation location for the following ComfyUI content:

- **Python Environment**
- **Models Model Files**
- **Custom Nodes Custom Nodes**

Recommendations:

- Please select a **solid-state drive** as the installation location, which will increase ComfyUI’s performance when accessing models.
- Please create a separate empty folder as the ComfyUI installation directory
- Please ensure that the corresponding disk has at least around **15G** of disk space to ensure the installation of ComfyUI Desktop

Not all files are installed in this directory, some files will still be installed on the C drive, and if you need to uninstall in the future, you can refer to the uninstallation section of this guide to complete the full uninstallation of ComfyUI Desktop

After completing this step, click **Next** to proceed to the next step

4

Migrate from Existing Installation (Optional)

In this step you can migrate your existing ComfyUI installation content to ComfyUI Desktop. As shown, I selected my original **D:\\ComfyUI\_windows\_portable\\ComfyUI** installation directory. The installer will automatically recognize:

- **User Files**
- **Models:** Will not be copied, only linked with desktop version
- **Custom Nodes:** Nodes will be reinstalled

Don’t worry, this step won’t copy model files. You can check or uncheck options as needed. Click **Next** to continue

5

Desktop Settings

These are preference settings:

1. **Automatic Updates:** Whether to set automatic updates when ComfyUI updates are available
2. **Usage Metrics:** If enabled, we will collect **anonymous usage data** to help improve ComfyUI
3. **Mirror Settings:** Since the program needs internet access to download Python and complete environment installation, if you see a red ❌ during installation indicating this may cause installation failure, please follow the steps below

Expand the mirror settings to find the specific failing mirror. In this screenshot the error is **Python Install Mirror** failure.

For different mirror errors, you can refer to the following content to try to manually find different mirrors and replace them

The following cases mainly apply to users in China.

#### [​](http://docs.comfy.org#python-installation-mirror) Python Installation Mirror

If the default mirror is unavailable, please try using the mirror below.

```plaintext
https://python-standalone.org/mirror/astral-sh/python-build-standalone
```

If you need to find other alternative GitHub mirror addresses, please look for and construct a mirror address pointing to the releases of the `python-build-standalone` repository.

```plaintext
https://github.com/astral-sh/python-build-standalone/releases/download
```

Build a link in the following pattern

```plaintext
https://xxx/astral-sh/python-build-standalone/releases/download
```

#### [​](http://docs.comfy.org#pypi-mirror) PyPI Mirror

- Alibaba Cloud: [https://mirrors.aliyun.com/pypi/simple/](https://mirrors.aliyun.com/pypi/simple/)
- Tencent Cloud: [https://mirrors.cloud.tencent.com/pypi/simple/](https://mirrors.cloud.tencent.com/pypi/simple/)
- University of Science and Technology of China: [https://pypi.mirrors.ustc.edu.cn/simple/](https://pypi.mirrors.ustc.edu.cn/simple/)
- Shanghai Jiao Tong University: [https://pypi.sjtu.edu.cn/simple/](https://pypi.sjtu.edu.cn/simple/)

#### [​](http://docs.comfy.org#torch-mirror) Torch Mirror

- Aliyun: [https://mirrors.aliyun.com/pytorch-wheels/cu121/](https://mirrors.aliyun.com/pytorch-wheels/cu121/)

6

Complete the installation

If everything is correct, the installer will complete and automatically enter the ComfyUI Desktop interface, then the installation is successful

## [​](http://docs.comfy.org#first-image-generation) First Image Generation

After successful installation, you can refer to the section below to start your ComfyUI journey~

[**First Image Generation**  
\
This tutorial will guide you through your first model installation and text-to-image generation](http://docs.comfy.org/get_started/first_generation)

## [​](http://docs.comfy.org#how-to-update-comfyui-desktop) How to Update ComfyUI Desktop

Currently, ComfyUI Desktop updates use automatic detection updates, please ensure that automatic updates are enabled in the settings

You can also choose to manually check for available updates in the `Menu` —&gt; `Help` —&gt; `Check for Updates`

## [​](http://docs.comfy.org#how-to-uninstall-comfyui-desktop) How to Uninstall ComfyUI Desktop

For **ComfyUI Desktop** you can use the system uninstall function in Windows Settings to complete software uninstallation

If you want to completely remove all **ComfyUI Desktop** files, you can manually delete these folders:

- C:\\Users&lt;your username&gt;\\AppData\\Local@comfyorgcomfyui-electron-updater
- C:\\Users&lt;your username&gt;\\AppData\\Local\\Programs@comfyorgcomfyui-electron
- C:\\Users&lt;your username&gt;\\AppData\\Roaming\\ComfyUI

The above operations will not delete your following folders. If you need to delete corresponding files, please delete manually:

- models files
- custom nodes
- input/output directories

## [​](http://docs.comfy.org#troubleshooting) Troubleshooting

### [​](http://docs.comfy.org#display-unsupported-devices) Display unsupported devices

Since ComfyUI Desktop (Windows) only supports **NVIDIA GPUs with CUDA**, you may see this screen if your device is not supported

- Please switch to a supported device
- Or consider using [ComfyUI Portable](http://docs.comfy.org/installation/comfyui_portable_windows) or through [manual installation](http://docs.comfy.org/installation/manual_install) to use ComfyUI

### [​](http://docs.comfy.org#%E2%80%8Berror-identification%E2%80%8B) ​Error identification​

If installation fails, you should see the following screen

It is recommended to take these steps to find the error cause:

1. Click `Show Terminal` to view error output
2. Click `Open Logs` to view installation logs
3. Visit official forum to search for error reports
4. Click `Reinstall` to try reinstalling

Before submitting feedback, it’s recommended to provide the **error output** and **log files** to tools like **GPT**

As shown above, ask the GPT for the cause of the corresponding error, or remove ComfyUI completely and retry the installation.

### [​](http://docs.comfy.org#feedback-installation-failure) Feedback Installation Failure

If you encounter any errors during installation, please check if there are similar error reports or submit errors to us through:

- Github Issues: [https://github.com/Comfy-Org/desktop/issues](https://github.com/Comfy-Org/desktop/issues)
- Comfy Official Forum: [https://forum.comfy.org/](https://forum.comfy.org/)

When submitting error reports, please ensure you include the following logs and configuration files to help us locate and investigate the issue:

1. Log Files

FilenameDescriptionLocationmain.logContains logs related to desktop application and server startup from the Electron processcomfyui.logContains logs related to ComfyUI normal operation, such as core ComfyUI process terminal output

2. Configuration Files

FilenameDescriptionLocationextra\_models\_config.yamlContains additional paths where ComfyUI will search for models and custom nodesconfig.jsonContains application configuration. This file should not be edited directly

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/installation/desktop/windows.mdx)

[Previous](http://docs.comfy.org/installation/system_requirements)

[MacOS Desktop VersionThis article introduces how to download, install and use ComfyUI Desktop for MacOS  
\
Next](http://docs.comfy.org/installation/desktop/macos)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [ComfyUI Desktop (Windows) Download](http://docs.comfy.org#comfyui-desktop-windows-download)
- [ComfyUI Desktop Installation Steps](http://docs.comfy.org#comfyui-desktop-installation-steps)
- [ComfyUI Desktop Initialization Process](http://docs.comfy.org#comfyui-desktop-initialization-process)
- [First Image Generation](http://docs.comfy.org#first-image-generation)
- [How to Update ComfyUI Desktop](http://docs.comfy.org#how-to-update-comfyui-desktop)
- [How to Uninstall ComfyUI Desktop](http://docs.comfy.org#how-to-uninstall-comfyui-desktop)
- [Troubleshooting](http://docs.comfy.org#troubleshooting)
- [Display unsupported devices](http://docs.comfy.org#display-unsupported-devices)
- [​Error identification​](http://docs.comfy.org#%E2%80%8Berror-identification%E2%80%8B)
- [Feedback Installation Failure](http://docs.comfy.org#feedback-installation-failure)

<!-- END Development/installation/desktop/windows.md -->


<!-- BEGIN Development/installation/manual_install.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
  
  - [System Requirements](http://docs.comfy.org/installation/system_requirements)
  - Desktop(recommended)
  - [ComfyUI(portable) Windows](http://docs.comfy.org/installation/comfyui_portable_windows)
  - [Manual Installation](http://docs.comfy.org/installation/manual_install)
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Manual Installation

# Manual Installation

For Nvidia 50 series (Blackwell) GPUs, please refer to the [System Requirements](http://docs.comfy.org/installation/nvidia-50-series) section to ensure your system meets the requirements for ComfyUI.

- Windows
- Linux
- MacOS

### [​](http://docs.comfy.org#clone-the-repository) Clone the repository

```bash
git clone git@github.com:comfyanonymous/ComfyUI.git
```

More [info](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) on git clone.

If you have not installed Microsoft Visual C++ Redistributable, please install it [here.](https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist?view=msvc-170)

### [​](http://docs.comfy.org#install-dependencies) Install Dependencies

1. [Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI.
   
   Create an environment with Conda.
   
   ```plaintext
   conda create -n comfyenv
   conda activate comfyenv
   ```
2. Install GPU Dependencies
   
   Nvidia
   
   ```plaintext
   conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
   ```
   
   Alternatively, you can install the nightly version of PyTorch.
   
   Install Nightly
   
   Install Nightly version (might be more risky)
   
   ```plaintext
   conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch-nightly -c nvidia
   ```
   
   AMD
   
   ```plaintext
   pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
   ```
   
   Alternatively, you can install the nightly version of PyTorch.
   
   Install Nightly
   
   Install Nightly version (might be more risky)
   
   ```plaintext
   pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0
   ```
   
   Mac ARM Silicon
   
   ```bash
   conda install pytorch-nightly::pytorch torchvision torchaudio -c pytorch-nightly
   ```
3. ```bash
   cd ComfyUI
   pip install -r requirements.txt
   ```
4. Start the application
   
   ```plaintext
   cd ComfyUI
   python main.py
   ```

### [​](http://docs.comfy.org#clone-the-repository) Clone the repository

```bash
git clone git@github.com:comfyanonymous/ComfyUI.git
```

More [info](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) on git clone.

If you have not installed Microsoft Visual C++ Redistributable, please install it [here.](https://learn.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist?view=msvc-170)

### [​](http://docs.comfy.org#install-dependencies) Install Dependencies

1. [Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI.
   
   Create an environment with Conda.
   
   ```plaintext
   conda create -n comfyenv
   conda activate comfyenv
   ```
2. Install GPU Dependencies
   
   Nvidia
   
   ```plaintext
   conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
   ```
   
   Alternatively, you can install the nightly version of PyTorch.
   
   Install Nightly
   
   Install Nightly version (might be more risky)
   
   ```plaintext
   conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch-nightly -c nvidia
   ```
   
   AMD
   
   ```plaintext
   pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
   ```
   
   Alternatively, you can install the nightly version of PyTorch.
   
   Install Nightly
   
   Install Nightly version (might be more risky)
   
   ```plaintext
   pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0
   ```
   
   Mac ARM Silicon
   
   ```bash
   conda install pytorch-nightly::pytorch torchvision torchaudio -c pytorch-nightly
   ```
3. ```bash
   cd ComfyUI
   pip install -r requirements.txt
   ```
4. Start the application
   
   ```plaintext
   cd ComfyUI
   python main.py
   ```

### [​](http://docs.comfy.org#clone-the-repository-2) Clone the repository

```bash
git clone git@github.com:comfyanonymous/ComfyUI.git
```

More [info](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) on git clone.

### [​](http://docs.comfy.org#install-dependencies-2) Install Dependencies

1. [Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI.
   
   Create an environment with Conda.
   
   ```plaintext
   conda create -n comfyenv
   conda activate comfyenv
   ```
2. Install GPU Dependencies
   
   Nvidia
   
   ```plaintext
   conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
   ```
   
   Alternatively, you can install the nightly version of PyTorch.
   
   Install Nightly
   
   Install Nightly version (might be more risky)
   
   ```plaintext
   conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch-nightly -c nvidia
   ```
   
   AMD
   
   ```plaintext
   pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
   ```
   
   Alternatively, you can install the nightly version of PyTorch.
   
   Install Nightly
   
   Install Nightly version (might be more risky)
   
   ```plaintext
   pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0
   ```
   
   Mac ARM Silicon
   
   ```bash
   conda install pytorch-nightly::pytorch torchvision torchaudio -c pytorch-nightly
   ```
3. ```bash
   cd ComfyUI
   pip install -r requirements.txt
   ```
4. Start the application
   
   ```plaintext
   cd ComfyUI
   python main.py
   ```

### [​](http://docs.comfy.org#clone-the-repository-3) Clone the repository

Open [Terminal application](https://support.apple.com/guide/terminal/open-or-quit-terminal-apd5265185d-f365-44cb-8b09-71a064a42125/mac).

```bash
git clone git@github.com:comfyanonymous/ComfyUI.git
```

More [info](https://docs.github.com/en/repositories/creating-and-managing-repositories/cloning-a-repository) on git clone.

### [​](http://docs.comfy.org#install-dependencies-3) Install Dependencies

1. [Install Miniconda](https://docs.anaconda.com/free/miniconda/index.html#latest-miniconda-installer-links). This will help you install the correct versions of Python and other libraries needed by ComfyUI.
   
   Create an environment with Conda.
   
   ```plaintext
   conda create -n comfyenv
   conda activate comfyenv
   ```
2. Install GPU Dependencies
   
   Nvidia
   
   ```plaintext
   conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
   ```
   
   Alternatively, you can install the nightly version of PyTorch.
   
   Install Nightly
   
   Install Nightly version (might be more risky)
   
   ```plaintext
   conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch-nightly -c nvidia
   ```
   
   AMD
   
   ```plaintext
   pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm6.0
   ```
   
   Alternatively, you can install the nightly version of PyTorch.
   
   Install Nightly
   
   Install Nightly version (might be more risky)
   
   ```plaintext
   pip3 install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/rocm6.0
   ```
   
   Mac ARM Silicon
   
   ```bash
   conda install pytorch-nightly::pytorch torchvision torchaudio -c pytorch-nightly
   ```
3. ```bash
   cd ComfyUI
   pip install -r requirements.txt
   ```
4. Start the application
   
   ```plaintext
   cd ComfyUI
   python main.py
   ```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/installation/manual_install.mdx)

[Previous](http://docs.comfy.org/installation/comfyui_portable_windows)

[First Image GenerationThis tutorial will guide you through your first image generation with ComfyUI, covering basic interface operations like workflow loading, model installation, and image generation  
\
Next](http://docs.comfy.org/get_started/first_generation)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

<!-- END Development/installation/manual_install.md -->


<!-- BEGIN Development/installation/system_requirements.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
  
  - [System Requirements](http://docs.comfy.org/installation/system_requirements)
  - Desktop(recommended)
  - [ComfyUI(portable) Windows](http://docs.comfy.org/installation/comfyui_portable_windows)
  - [Manual Installation](http://docs.comfy.org/installation/manual_install)
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

System Requirements

# System Requirements

This guide introduces some system requirements for ComfyUI, including hardware and software requirements

In this guide, we will introduce the system requirements for installing ComfyUI. Due to frequent updates of ComfyUI, this document may not be updated in a timely manner. Please refer to the relevant instructions in [ComfyUI](https://github.com/comfyanonymous/ComfyUI).

Regardless of which version of ComfyUI you use, it runs in a separate Python environment.

You can refer to the following sections to learn about the installation methods for different systems and versions of ComfyUI. In the installation of different versions, we have simply described the system requirements.

ComfyUI Desktop (Recommended)

ComfyUI Desktop currently supports standalone installation for **Windows and MacOS (ARM)**, currently in Beta

- Code is open source on [Github](https://github.com/Comfy-Org/desktop)

You can choose the appropriate installation for your system and hardware below

- Windows
- MacOS(Apple Silicon)
- Linux

[**ComfyUI Desktop (Windows) Installation Guide**  
\
Suitable for **Windows** version with **Nvidia** GPU](http://docs.comfy.org/installation/desktop/windows)

[**ComfyUI Desktop (Windows) Installation Guide**  
\
Suitable for **Windows** version with **Nvidia** GPU](http://docs.comfy.org/installation/desktop/windows)

[**ComfyUI Desktop (MacOS) Installation Guide**  
\
Suitable for MacOS with **Apple Silicon**](http://docs.comfy.org/installation/desktop/macos)

ComfyUI Desktop **currently has no Linux prebuilds**, please visit the [Manual Installation](http://docs.comfy.org/installation/manual_install) section to install ComfyUI

ComfyUI Portable (Windows)

[**ComfyUI Portable (Windows) Installation Guide**  
\
Supports **Windows** ComfyUI version running on **Nvidia GPUs** or **CPU-only**, always use the latest commits and completely portable.](http://docs.comfy.org/installation/comfyui_portable_windows)

Manual Installation

[**ComfyUI Manual Installation Guide**  
\
Supports all system types and GPU types (Nvidia, AMD, Intel, Apple Silicon, Ascend NPU, Cambricon MLU)](http://docs.comfy.org/installation/manual_install)

## [​](http://docs.comfy.org#nvidia-50-series-gpu-requirements) Nvidia 50 Series GPU Requirements

To make your Nvidia 50 series GPU (Blackwell architecture) work properly with ComfyUI, you need a PyTorch version that supports CUDA 12.8 or newer. Currently (March 2025), the stable version of PyTorch does not yet support the Blackwell architecture, so you need to use a nightly build version.

Related discussions on this issue are concentrated [here](https://github.com/comfyanonymous/ComfyUI/discussions/6643).

### [​](http://docs.comfy.org#for-windows-users) For Windows Users

**Recommended Option:** Download the standalone ComfyUI Portable version with nightly pytorch 2.7 cu128:

- [Click here to download](https://github.com/comfyanonymous/ComfyUI/releases/download/latest/ComfyUI_windows_portable_nvidia_or_cpu_nightly_pytorch.7z)

**Other Options:** Older torch 2.6 Windows package:

- [Click here to download the standalone ComfyUI package with a cuda 12.8 torch build](https://github.com/comfyanonymous/ComfyUI/releases/download/latest/ComfyUI_cu128_50XX.7z)

### [​](http://docs.comfy.org#manual-installation) Manual Installation

Windows and Linux users can install the PyTorch nightly version using the following command:

```bash
pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu128
```

### [​](http://docs.comfy.org#docker-container-alternative) Docker Container Alternative

You can try the PyTorch container provided by Nvidia, which might offer better performance.

Container address: [https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/pytorch)

Usage method:

```bash
docker run -p 8188:8188 --gpus all -it --rm nvcr.io/nvidia/pytorch:25.01-py3
```

Inside the Docker container, execute the following commands:

```bash
git clone https://github.com/comfyanonymous/ComfyUI
cd ComfyUI
grep -v 'torchaudio\|torchvision' requirements.txt > temp_requirements.txt
pip install -r temp_requirements.txt
python main.py --listen
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/installation/system_requirements.mdx)

[Previous](http://docs.comfy.org/get_started/introduction)

[Windows Desktop VersionThis article introduces how to download, install and use ComfyUI Desktop for Windows  
\
Next](http://docs.comfy.org/installation/desktop/windows)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Nvidia 50 Series GPU Requirements](http://docs.comfy.org#nvidia-50-series-gpu-requirements)
- [For Windows Users](http://docs.comfy.org#for-windows-users)
- [Manual Installation](http://docs.comfy.org#manual-installation)
- [Docker Container Alternative](http://docs.comfy.org#docker-container-alternative)

<!-- END Development/installation/system_requirements.md -->


<!-- BEGIN Development/interface/credits.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Credits Management

# Credits Management

In this article, we will introduce ComfyUI’s credit management features, including how to obtain, use, and view credits.

The credit system was added to support the `API Nodes`, as calling closed-source AI models requires token consumption, so proper credit management is necessary. By default, the credits interface is not displayed. Please first log in to your ComfyUI account in `Settings` -&gt; `User`, and then you can view your associated account’s credit information in `Settings` -&gt; `Credits`.

ComfyUI will always remain fully open-source and free for local users.

## [​](http://docs.comfy.org#how-to-purchase-credits%3F) How to Purchase Credits?

Below is a demonstration video for purchasing credits:

Detailed steps are as follows:

1

Log in to your ComfyUI account

Log in to your ComfyUI account in `Settings` -&gt; `User`

2

Go to \`Settings\` -&gt; \`Credits\` to purchase credits

After logging in, you should see the `Credits` option added to the menu

Go to `Settings` -&gt; `Credits` to purchase credits

3

Set the amount of credits to purchase

In the popup, set the purchase amount and click the `Buy` button

4

Make payment through Stripe

On the payment page, please follow these steps:

1. Select the currency for payment
2. Confirm that the email is the same as your ComfyUI registration email
3. Choose your payment method

<!--THE END-->

- Credit Card
- WeChat (only supported when paying in USD)

<!--THE END-->

4. Click the `Pay` button or the `Generate QR Code` button to complete the payment process

5

Complete payment and check your credit balance

After completing the payment, please return to `Menu` -&gt; `Credits` to check if your balance has been updated. Try refreshing the interface or restarting if necessary

## [​](http://docs.comfy.org#frequently-asked-questions) Frequently Asked Questions

Can credits go negative?

No, when your credit balance is negative, you will not be able to run API Nodes

Can I get a refund for unused credits?

Currently, we do not support refunds

How do I check my current balance and usage?

Click on `Settings` -&gt; `Credits` to see your current balance and access the `Credit History` entry

Can I share my credits with other users?

You can log into the same account on multiple devices, but we do not support sharing credits with other users

How do I know how many credits I've consumed each time?

Due to different image sizes and generation quantities, the `Tokens` and `Credits` consumed each time vary. In `Settings` -&gt; `Credits`, you can see the credits consumed each time and the corresponding credit history

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/interface/credits.mdx)

[Previous](http://docs.comfy.org/interface/user)

[Workflow  
\
Next](http://docs.comfy.org/essentials/core-concepts/workflow)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [How to Purchase Credits?](http://docs.comfy.org#how-to-purchase-credits%3F)
- [Frequently Asked Questions](http://docs.comfy.org#frequently-asked-questions)

<!-- END Development/interface/credits.md -->


<!-- BEGIN Development/interface/maskeditor.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Mask Editor - Create and Edit Masks in ComfyUI

# Mask Editor - Create and Edit Masks in ComfyUI

Learn how to use the Mask Editor in ComfyUI, including settings and usage instructions

The Mask Editor is a very useful feature in ComfyUI that allows users to create and edit masks within images without needing to use other applications.

The Mask Editor is currently triggered through the `Load Image` node. After uploading an image, you can right-click on the node and select `Open in MaskEditor` from the menu to open the Mask Editor.

You can then click with your mouse on the image to create and edit masks.

## [​](http://docs.comfy.org#demo-video) Demo Video

Your browser does not support the video tag.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/interface/maskeditor.mdx)

[Previous](http://docs.comfy.org/essentials/core-concepts/links)

[Models  
\
Next](http://docs.comfy.org/essentials/core-concepts/models)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Demo Video](http://docs.comfy.org#demo-video)

<!-- END Development/interface/maskeditor.md -->


<!-- BEGIN Development/interface/overview.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Interface Overview

# ComfyUI Interface Overview

In this article, we will briefly introduce the basic user interface of ComfyUI, familiarizing you with the various parts of the ComfyUI interface.

The visual interface is currently the way most users utilize ComfyUI to call the [ComfyUI Server](http://docs.comfy.org/essentials/comfyui-server/comms_overview) to generate corresponding media resources. It provides a visual interface for users to operate and organize workflows, debug workflows, and create amazing works.

Typically, when you start the ComfyUI server, you will see an interface like this:

If you are an earlier user, you may have seen the previous menu interface like this:

Currently, the [ComfyUI frontend](https://github.com/Comfy-Org/ComfyUI_frontend) is a separate project, released and maintained as an independent pip package. If you want to contribute, you can fork this [repository](https://github.com/Comfy-Org/ComfyUI_frontend) and submit a pull request.

## [​](http://docs.comfy.org#localization-support) Localization Support

Currently, ComfyUI supports: English, Chinese, Russian, French, Japanese, and Korean. If you need to switch the interface language to your preferred language, you can click the **Settings gear icon** and then select your desired language under `Comfy` —&gt; `Locale`.

## [​](http://docs.comfy.org#new-menu-interface) New Menu Interface

### [​](http://docs.comfy.org#workspace-areas) Workspace Areas

Below are the main interface areas of ComfyUI and a brief introduction to each part.

Currently, apart from the main workflow interface, the ComfyUI interface is mainly divided into the following parts:

1. Menu Bar: Provides workflow, editing, help menus, workflow execution, ComfyUI Manager entry, etc.
2. Sidebar Panel Switch Buttons: Used to switch between workflow history queue, node library, model library, local user workflow browsing, etc.
3. Theme Switch Button: Quickly switch between ComfyUI’s default dark theme and light theme
4. Settings: Click to open the settings button
5. Canvas Menu: Provides zoom in, zoom out, and auto-fit operations for the ComfyUI canvas

### [​](http://docs.comfy.org#menu-bar-functions) Menu Bar Functions

The image above shows the corresponding functions of the top menu bar, including common features, which we will explain in detail in the specific function usage section.

### [​](http://docs.comfy.org#sidebar-panel-buttons) Sidebar Panel Buttons

In the current ComfyUI, we provide four side panels with the following functions:

1. Workflow History Queue (Queue): All queue information for ComfyUI executing media content generation
2. Node Library: All nodes in ComfyUI, including `Comfy Core` and your installed custom nodes, can be found here
3. Model Library: Models in your local `ComfyUI/models` directory can be found here
4. Local User Workflows (Workflows): Your locally saved workflows can be found here

## [​](http://docs.comfy.org#old-menu-version) Old Menu Version

Currently, ComfyUI enables the new interface by default. If you prefer to use the old interface, you can click the **Settings gear icon** and then set `Use new menu` to `disabled` under `Comfy` —&gt; `Menu` to switch to the old menu version.

The old menu interface only supports English.

The function annotations for the old menu interface are explained below:

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/interface/overview.mdx)

[Previous](http://docs.comfy.org/get_started/first_generation)

[Account ManagementIn this document, we will introduce the account management features of ComfyUI, including account login, registration, and logout operations.  
\
Next](http://docs.comfy.org/interface/user)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Localization Support](http://docs.comfy.org#localization-support)
- [New Menu Interface](http://docs.comfy.org#new-menu-interface)
- [Workspace Areas](http://docs.comfy.org#workspace-areas)
- [Menu Bar Functions](http://docs.comfy.org#menu-bar-functions)
- [Sidebar Panel Buttons](http://docs.comfy.org#sidebar-panel-buttons)
- [Old Menu Version](http://docs.comfy.org#old-menu-version)

<!-- END Development/interface/overview.md -->


<!-- BEGIN Development/interface/shortcuts.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Keyboard Shortcuts and Custom Settings

# ComfyUI Keyboard Shortcuts and Custom Settings

Keyboard and mouse shortcuts for ComfyUI and related settings

Currently, ComfyUI supports custom keyboard shortcuts. You can set the shortcuts by clicking on `Settings (gear icon)` —&gt; `Keybinding`.

In the corresponding menu, you can see all the current shortcut settings for ComfyUI. Click the `edit icon` before the corresponding command to customize the shortcut.

Below is the current list of shortcuts for ComfyUI, which you can customize as needed.

- Windows/Linux
- MacOS

ShortcutCommandCtrl + EnterQueue up current graph for generationCtrl + Shift + EnterQueue up current graph as first for generationCtrl + Z / Ctrl + YUndo/RedoCtrl + SSave workflowCtrl + OLoad workflowCtrl + ASelect all nodesAlt + CCollapse/uncollapse selected nodesCtrl + MMute/unmute selected nodesCtrl + BBypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)Delete  
BackspaceDelete selected nodesCtrl + Delete  
Ctrl + BackspaceDelete the current graphSpaceMove the canvas around when held and moving the cursorCtrl + Click  
Shift + ClickAdd clicked node to selectionCtrl + C/Ctrl + VCopy and paste selected nodes (without maintaining connections to outputs of unselected nodes)Ctrl + C/Ctrl + Shift + VCopy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes)Shift + DragMove multiple selected nodes at the same timeCtrl + DLoad default graphQToggle visibility of the queueHToggle visibility of historyRRefresh graphDouble-Click LMBQuick search for nodes to add

ShortcutCommandCtrl + EnterQueue up current graph for generationCtrl + Shift + EnterQueue up current graph as first for generationCtrl + Z / Ctrl + YUndo/RedoCtrl + SSave workflowCtrl + OLoad workflowCtrl + ASelect all nodesAlt + CCollapse/uncollapse selected nodesCtrl + MMute/unmute selected nodesCtrl + BBypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)Delete  
BackspaceDelete selected nodesCtrl + Delete  
Ctrl + BackspaceDelete the current graphSpaceMove the canvas around when held and moving the cursorCtrl + Click  
Shift + ClickAdd clicked node to selectionCtrl + C/Ctrl + VCopy and paste selected nodes (without maintaining connections to outputs of unselected nodes)Ctrl + C/Ctrl + Shift + VCopy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes)Shift + DragMove multiple selected nodes at the same timeCtrl + DLoad default graphQToggle visibility of the queueHToggle visibility of historyRRefresh graphDouble-Click LMBQuick search for nodes to add

KeybindExplanationCmd ⌘ + EnterQueue up current graph for generationCmd ⌘ + Shift + EnterQueue up current graph as first for generationCmd ⌘ + Z/Cmd ⌘ + YUndo/RedoCmd ⌘ + SSave workflowCmd ⌘ + OLoad workflowCmd ⌘ + ASelect all nodesOpt ⌥ + CCollapse/uncollapse selected nodesCmd ⌘ + MMute/unmute selected nodesCmd ⌘ + BBypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)Delete  
BackspaceDelete selected nodesCmd ⌘ + Delete  
Cmd ⌘ + BackspaceDelete the current graphSpaceMove the canvas around when held and moving the cursorCmd ⌘ + Click  
Shift + ClickAdd clicked node to selectionCmd ⌘ + C / Cmd ⌘ + VCopy and paste selected nodes (without maintaining connections to outputs of unselected nodes)Cmd ⌘ + C / Cmd ⌘ + Shift + VCopy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes)Shift + DragMove multiple selected nodes at the same timeCmd ⌘ + DLoad default graphQToggle visibility of the queueHToggle visibility of historyRRefresh graphDouble-Click LMBQuick search for nodes to add

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/interface/shortcuts.mdx)

[Previous](http://docs.comfy.org/essentials/core-concepts/dependencies)

[Text to ImageThis guide will help you understand the concept of text-to-image in AI art generation and complete a text-to-image workflow in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/basic/text-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

<!-- END Development/interface/shortcuts.md -->


<!-- BEGIN Development/interface/user.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Account Management

# Account Management

In this document, we will introduce the account management features of ComfyUI, including account login, registration, and logout operations.

The account system was added to support `API Nodes`, which enable calls to closed-source model APIs, greatly expanding the possibilities of ComfyUI. Since these API calls consume tokens, we have added a corresponding user system.

Currently, we support the following login methods:

- Email login
- Google login
- Github login
- API Key login (for non-whitelisted site authorization)

We will provide relevant login requirements and explanations in this document.

## [​](http://docs.comfy.org#comfyui-version-requirements) ComfyUI Version Requirements

You may need to use at least [ComfyUI v0.3.0](https://github.com/comfyanonymous/ComfyUI/releases/tag/v0.3.30) to use the account system. Ensure that the corresponding frontend version is at least `1.17.11`. Sometimes the frontend may fail to install and revert to an older version, so please check if the frontend version is greater than `1.17.11` in `Settings` -&gt; `About`.

In some regions, network restrictions may prevent normal access to the login API, causing timeouts or failures. Before logging in, please **ensure that your network environment does not restrict access to the corresponding API**, and make sure you can access sites like Google or Github.

As we are still rapidly iterating and updating, related features may change. If there are no special circumstances, please try to update to the latest version to access the relevant features.

## [​](http://docs.comfy.org#network-requirements) Network Requirements

To login to ComfyUI account, you must be in a secure network environment:

- Only allow access from `127.0.0.1` or `localhost`.
- Do not support using the `--listen` parameter to access the API node through a local network.
- If you are using a non-SSL certificate or a site that does not start with `https`, you may not be able to successfully log in.
- You may not be able to log in on a site that is not in our whitelist (but you can log in using an API Key now).
- Ensure you can connect to our service normally (some regions may require a proxy).

## [​](http://docs.comfy.org#how-to-log-in) How to Log In

Log in via `Settings` -&gt; `User`:

## [​](http://docs.comfy.org#login-methods) Login Methods

If this is your first login, please create an account first.

## [​](http://docs.comfy.org#logging-in-with-an-api-key) Logging in with an API Key

Since not all ComfyUI deployments are on our domain authorization whitelist, we have provided API Key login in a recent update (2025-05-10) for logging in through non-whitelisted sites. Below are the steps for logging in with an API Key:

- Have an API Key
- No API Key, Apply for an API Key First

1

Select Comfy API Key Login on the Login Screen

Select `Comfy API Key` login in the login popup

2

Enter Your API Key

1. Enter your API Key and save it
2. If you don’t have an API Key, click the `Get one here` link to go to [https://platform.comfy.org/login](https://platform.comfy.org/login) and log in to obtain it.

3

Login Successful

After a successful login, you can see the corresponding API Key login information in the settings menu

1

Select Comfy API Key Login on the Login Screen

Select `Comfy API Key` login in the login popup

2

Enter Your API Key

1. Enter your API Key and save it
2. If you don’t have an API Key, click the `Get one here` link to go to [https://platform.comfy.org/login](https://platform.comfy.org/login) and log in to obtain it.

3

Login Successful

After a successful login, you can see the corresponding API Key login information in the settings menu

Please refer to the following steps to apply for and obtain an API Key:

1

Visit https://platform.comfy.org/login and Log In

Please visit [https://platform.comfy.org/login](https://platform.comfy.org/login) and log in with the corresponding account

2

Click \`+ New\` in API Keys to Create an API Key

Click `+ New` in API Keys to create an API Key

3

Enter API Key Name

1. (Required) Enter the API Key name,
2. Click `Generate` to create

4

Save the Obtained API Key

Since the API Key is only visible upon first creation, please save it immediately after creation. It cannot be viewed later, so please keep it safe.

5

API Key Management

For unused API Keys or those at risk of being leaked, you can click `Delete` to remove them to prevent unnecessary losses.

6

(Optional) Log Out

If you have obtained an API Key and are logged in on a public device, please log out promptly.

## [​](http://docs.comfy.org#post-login-status) Post-Login Status

After logging in, a login button is displayed in the top menu bar of the ComfyUI interface. You can open the corresponding login interface through this button and log out of the corresponding account in the settings menu.

## [​](http://docs.comfy.org#frequently-asked-questions) Frequently Asked Questions

Are there any login device restrictions?

We do not restrict login devices. You can log in to your account on any device, but please note that your account information may be accessed by other devices, so do not log in to your account on public devices.

Why can't I log in with a LAN IP?

We do not currently support logging in via LAN IP because the security of the LAN environment is uncontrollable. Therefore, the current version does not fully support LAN IP login. We may consider handling LAN situations in the future.

Why can't I log in on some websites?

Our login service has a whitelist, so you may not be able to log in to ComfyUI deployed on some servers.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/interface/user.mdx)

[Previous](http://docs.comfy.org/interface/overview)

[Credits ManagementIn this article, we will introduce ComfyUI's credit management features, including how to obtain, use, and view credits.  
\
Next](http://docs.comfy.org/interface/credits)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [ComfyUI Version Requirements](http://docs.comfy.org#comfyui-version-requirements)
- [Network Requirements](http://docs.comfy.org#network-requirements)
- [How to Log In](http://docs.comfy.org#how-to-log-in)
- [Login Methods](http://docs.comfy.org#login-methods)
- [Logging in with an API Key](http://docs.comfy.org#logging-in-with-an-api-key)
- [Post-Login Status](http://docs.comfy.org#post-login-status)
- [Frequently Asked Questions](http://docs.comfy.org#frequently-asked-questions)

<!-- END Development/interface/user.md -->


<!-- BEGIN Development/registry/cicd.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Custom Node CI/CD

# Custom Node CI/CD

## [​](http://docs.comfy.org#introduction) Introduction

When making changes to custom nodes, it’s not uncommon to break things in Comfy or other custom nodes. It is often unrealistic to test on every operating system and different configurations of Pytorch.

### [​](http://docs.comfy.org#run-comfy-workflows-using-github-actions) Run Comfy Workflows using Github Actions

[Comfy-Action](https://github.com/Comfy-Org/comfy-action) allows you to run a Comfy workflow.json file on Github Actions. It supports downloading models, custom nodes, and runs on Linux/Mac/Windows.

### [​](http://docs.comfy.org#results) Results

Output files are uploaded to the [CI/CD Dashboard](https://comfyci.org) and can be viewed as a last step before commiting new changes or publishing new versions of the custom node.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/registry/cicd.mdx)

[Previous](http://docs.comfy.org/registry/standards)

[pyproject.toml  
\
Next](http://docs.comfy.org/registry/specifications)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Introduction](http://docs.comfy.org#introduction)
- [Run Comfy Workflows using Github Actions](http://docs.comfy.org#run-comfy-workflows-using-github-actions)
- [Results](http://docs.comfy.org#results)

<!-- END Development/registry/cicd.md -->


<!-- BEGIN Development/registry/overview.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Overview

# Overview

## [​](http://docs.comfy.org#introduction) Introduction

The Registry is a public collection of custom nodes. Developers can publish, version, deprecate, and track metrics related to their custom nodes. ComfyUI users can discover, install, and rate custom nodes from the registry.

## [​](http://docs.comfy.org#why-use-the-registry%3F) Why use the Registry?

The Comfy Registry helps the community by standardizing the development of custom nodes:

  **Node Versioning:** Developers frequently publish new versions of their custom nodes which often break workflows that rely on them. With registry nodes being [semantically versioned](https://semver.org/), users can now choose to safely upgrade, deprecate, or lock their node versions in place, knowing in advance how their actions will impact their workflows. The workflow JSON will store the version of the node used, so you can always reliably reproduce your workflows.

  **Node Security:** The registry will serve as a backend for the [ComfyUI-manager](https://github.com/ltdrdata/ComfyUI-Manager). All nodes will be scanned for malicious behaviour such as custom pip wheels, arbitrary system calls, etc. Nodes that pass these checks will have a verification flag () beside their name on the UI-manager. For a list of security standards, see the [standards](http://docs.comfy.org/registry/standards).

  **Search:** Search across all nodes on the Registry to find existing nodes for your workflow.x

## [​](http://docs.comfy.org#publishing-nodes) Publishing Nodes

Get started publishing your first node by following the [tutorial](http://docs.comfy.org/registry/publishing).

## [​](http://docs.comfy.org#frequently-asked-questions) Frequently Asked Questions

Do registry nodes have unique identifiers?

Yes, a custom node on the Registry has a globally unique name and this allows Comfy Workflow JSON files to uniquely identify any custom node without collisions.

Are there any restrictions on what I can publish?

Check the [standards](http://docs.comfy.org/registry/standards) for more information.

How do you ensure node stability?

Once a custom node version is published, it cannot be changed. This ensures that users can rely on the stability of the custom node over time.

How are nodes versioned?

Custom nodes are versioned using [semantic versioning](https://semver.org/). This allows users to understand the impact of upgrading to a new version.

How do I deprecate a node version?

You can deprecate a version in the Comfy Registry website by clicking **More Actions &gt; Deprecate**. Users who installed this version will be shown the deprecation message and be encouraged to upgrade to a newer version.

Deprecating versions is useful when an issue is discovered after publishing.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/registry/overview.mdx)

[Previous](http://docs.comfy.org/custom-nodes/tips)

[Publishing Nodes  
\
Next](http://docs.comfy.org/registry/publishing)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Introduction](http://docs.comfy.org#introduction)
- [Why use the Registry?](http://docs.comfy.org#why-use-the-registry%3F)
- [Publishing Nodes](http://docs.comfy.org#publishing-nodes)
- [Frequently Asked Questions](http://docs.comfy.org#frequently-asked-questions)

<!-- END Development/registry/overview.md -->


<!-- BEGIN Development/registry/publishing.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Publishing Nodes

# Publishing Nodes

## [​](http://docs.comfy.org#set-up-a-registry-account) Set up a Registry Account

Follow the steps below to set up a registry account and publish your first node.

### [​](http://docs.comfy.org#watch-a-tutorial) Watch a Tutorial

### [​](http://docs.comfy.org#create-a-publisher) Create a Publisher

A publisher is an identity that can publish custom nodes to the registry. Every custom node needs to include a publisher identifier in the pyproject.toml [file](http://docs.comfy.org).

Go to [Comfy Registry](https://registry.comfy.org), and create a publisher account. Your publisher id is globally unique, and cannot be changed later because it is used in the URL of your custom node.

Your publisher id is found after the `@` symbol on your profile page.

### [​](http://docs.comfy.org#create-an-api-key-for-publishing) Create an API Key for publishing

Go [here](https://registry.comfy.org/nodes) and click on the publisher you want to create an API key for. This will be used to publish a custom node via the CLI.

Name the API key and save it somewhere safe. If you lose it, you’ll have to create a new key.

### [​](http://docs.comfy.org#add-metadata) Add Metadata

Have you installed the comfy-cli? [Do that first](http://docs.comfy.org/comfy-cli/getting-started).

```bash
comfy node init
```

This command will generate the following metadata:

```toml
# pyproject.toml
[project]
name = "" # Unique identifier for your node. Immutable after creation.
description = ""
version = "1.0.0" # Custom Node version. Must be semantically versioned.
license = { file = "LICENSE.txt" }
dependencies  = [] # Filled in from requirements.txt

[project.urls]
Repository = "https://github.com/..."

[tool.comfy]
PublisherId = "" # TODO (fill in Publisher ID from Comfy Registry Website).
DisplayName = "" # Display name for the Custom Node. Can be changed later.
Icon = "https://example.com/icon.png" # SVG, PNG, JPG or GIF (MAX. 800x400px)
```

Add this file to your repository. Check the [specifications](http://docs.comfy.org/registry/specifications) for more information on the pyproject.toml file.

## [​](http://docs.comfy.org#publish-to-the-registry) Publish to the Registry

### [​](http://docs.comfy.org#option-1%3A-comfy-cli) Option 1: Comfy CLI

Run the command below to manually publish your node to the registry.

```bash
comfy node publish
```

You’ll be prompted for the API key.

```bash
API Key for publisher '<publisher id>': ****************************************************

...Version 1.0.0 Published. 
See it here: https://registry.comfy.org/publisherId/your-node
```

Keep in mind that the API key is hidden by default.

When copy-pasting, your API key might have an additional \\x16 at the back when using CTRL+V (for Windows), eg: \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\\x16.

It is recommended to copy-paste your API key via right-clicking instead.

### [​](http://docs.comfy.org#option-2%3A-github-actions) Option 2: Github Actions

Automatically publish your node through github actions.

1

Set up a Github Secret

Go to Settings -&gt; Secrets and Variables -&gt; Actions -&gt; Under Secrets Tab and Repository secrets -&gt; New Repository Secret.

Create a secret called `REGISTRY_ACCESS_TOKEN` and store your API key as the value.

2

Create a Github Action

Copy the code below and paste it here `/.github/workflows/publish_action.yml`

```bash
name: Publish to Comfy registry
on:
  workflow_dispatch:
  push:
    branches:
      - main
    paths:
      - "pyproject.toml"

jobs:
  publish-node:
    name: Publish Custom Node to registry
    runs-on: ubuntu-latest
    steps:
      - name: Check out code
        uses: actions/checkout@v4
      - name: Publish Custom Node
        uses: Comfy-Org/publish-node-action@main
        with:
          personal_access_token: ${{ secrets.REGISTRY_ACCESS_TOKEN }} ## Add your own personal access token to your Github Repository secrets and reference it here.
```

If your working branch is named something besides `main`, such as `master`, add the name under the branches section.

3

Test the Github Action

Push an update to your `pyproject.toml`’s version number. You should see your updated node on the registry.

The github action will automatically run every time you push an update to your `pyproject.toml` file

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/registry/publishing.mdx)

[Previous](http://docs.comfy.org/registry/overview)

[StandardsSecurity and other standards for publishing to the Registry  
\
Next](http://docs.comfy.org/registry/standards)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Set up a Registry Account](http://docs.comfy.org#set-up-a-registry-account)
- [Watch a Tutorial](http://docs.comfy.org#watch-a-tutorial)
- [Create a Publisher](http://docs.comfy.org#create-a-publisher)
- [Create an API Key for publishing](http://docs.comfy.org#create-an-api-key-for-publishing)
- [Add Metadata](http://docs.comfy.org#add-metadata)
- [Publish to the Registry](http://docs.comfy.org#publish-to-the-registry)
- [Option 1: Comfy CLI](http://docs.comfy.org#option-1%3A-comfy-cli)
- [Option 2: Github Actions](http://docs.comfy.org#option-2%3A-github-actions)

<!-- END Development/registry/publishing.md -->


<!-- BEGIN Development/registry/specifications.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

pyproject.toml

# pyproject.toml

# [​](http://docs.comfy.org#node-id) Node ID

The node id (“name” field in toml file) uniquely identifies the custom node, and will be used in URLs from the registry. Users can also install the node by referencing the name.

`comfy node install <node-id>`

The node id must be less than 100 characters and can only contain alphanumeric characters, hyphens, underscores, and periods. There should not be consecutive special characters and the id cannot start with a number or special character.

Comparison of node ids is case-insensitive. See the official [python documentation](https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#name) for more information.

We recommend using a short name for your node id, and don’t include “ComfyUI” in the name.

# [​](http://docs.comfy.org#version-number) Version Number

The registry uses [semantic versioning](https://semver.org/) which indicates the specific release of a custom node through a three-digit version number X.Y.Z.

X - **MAJOR** change that breaks previous updates

Y - **MINOR** change that adds new features and is backwards compatible

Z - **PATCH** change that fixes a bug

# [​](http://docs.comfy.org#license) License

An optional field that expects a relative path to your license file (usually named `LICENSE` or `LICENSE.txt`).

- `license = { file = "LICENSE" }` ✅
- `license = "LICENSE"` ❌

Alternatively, it can also be referenced by name. Common licenses include [MIT](https://opensource.org/license/mit), [GPL](https://www.gnu.org/licenses/gpl-3.0.en.html), or [Apache](https://www.apache.org/licenses/LICENSE-2.0).

- `license = {text = "MIT License"}` ✅
- `license = "MIT LICENSE"` ❌

Read up more on toml file standards [here](https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license)

# [​](http://docs.comfy.org#publisher-id) Publisher ID

The publisher id uniquely identifies a publisher and is ideally the same as your github username. It is also referred to as the username on the registry and can be found after the `@` symbol on the profile page.

# [​](http://docs.comfy.org#icon) Icon

An optional field that expects a icon URL. It accepts extensions SVG, PNG, JPG or GIF with a maximum resolution of 800px by 400px.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/registry/specifications.mdx)

[Previous](http://docs.comfy.org/registry/cicd)

[Workflow JSONJSON schema for a ComfyUI workflow.  
\
Next](http://docs.comfy.org/specs/workflow_json)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node ID](http://docs.comfy.org#node-id)
- [Version Number](http://docs.comfy.org#version-number)
- [License](http://docs.comfy.org#license)
- [Publisher ID](http://docs.comfy.org#publisher-id)
- [Icon](http://docs.comfy.org#icon)

<!-- END Development/registry/specifications.md -->


<!-- BEGIN Development/registry/standards.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Standards

# Standards

Security and other standards for publishing to the Registry

## [​](http://docs.comfy.org#base-standards) Base Standards

### [​](http://docs.comfy.org#1-community-value) 1. Community Value

Custom nodes must provide valuable functionality to the ComfyUI community

Avoid:

- Excessive self-promotion
- Impersonation or misleading behavior
- Malicious behavior
- Self-promotion is permitted only within your designated settings menu section
- Top and side menus should contain only useful functionality

### [​](http://docs.comfy.org#2-node-compatibility) 2. Node Compatibility

Do not interfere with other custom nodes’ operations (installation, updates, removal)

- For dependencies on other custom nodes:
  
  - Display clear warnings when dependent functionality is used
  - Provide example workflows demonstrating required nodes

### [​](http://docs.comfy.org#3-legal-compliance) 3. Legal Compliance

Must comply with all applicable laws and regulations

### [​](http://docs.comfy.org#5-quality-requirements) 5. Quality Requirements

Nodes must be fully functional, well documented, and actively maintained.

### [​](http://docs.comfy.org#6-fork-guidelines) 6. Fork Guidelines

Forked nodes must:

- Have clearly distinct names from original
- Provide significant differences in functionality or code

Below are standards that must be met to publish custom nodes to the registry.

## [​](http://docs.comfy.org#security-standards) Security Standards

Custom nodes should be secure. We will start working with custom nodes that violate these standards to be rewritten. If there is some major functionality that should be exposed by core, please request it in the [rfcs repo](https://github.com/comfy-org/rfcs).

### [​](http://docs.comfy.org#eval%2Fexec-calls) eval/exec Calls

#### [​](http://docs.comfy.org#policy) Policy

The use of `eval` and `exec` functions is prohibited in custom nodes due to security concerns.

#### [​](http://docs.comfy.org#reasoning) Reasoning

These functions can enable arbitrary code execution, creating potential Remote Code Execution (RCE) vulnerabilities when processing user inputs. Workflows containing nodes that pass user inputs into `eval` or `exec` could be exploited for various cyberattacks, including:

- Keylogging
- Ransomware
- Other malicious code execution

### [​](http://docs.comfy.org#subprocess-for-pip-install) subprocess for pip install

#### [​](http://docs.comfy.org#policy-2) Policy

Runtime package installation through subprocess calls is not permitted.

#### [​](http://docs.comfy.org#reasoning-2) Reasoning

- First item ComfyUI manager will ship with ComfyUI and lets the user install dependencies
- Centralized dependency management improves security and user experience
- Helps prevent potential supply chain attacks
- Eliminates need for multiple ComfyUI reloads

### [​](http://docs.comfy.org#code-obfuscation) Code Obfuscation

#### [​](http://docs.comfy.org#policy-3) Policy

Code obfuscation is prohibited in custom nodes.

#### [​](http://docs.comfy.org#reasoning-3) Reasoning

Obfuscated code:

- Impossible to review and likely to be malicious

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/registry/standards.mdx)

[Previous](http://docs.comfy.org/registry/publishing)

[Custom Node CI/CD  
\
Next](http://docs.comfy.org/registry/cicd)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Base Standards](http://docs.comfy.org#base-standards)
- [1. Community Value](http://docs.comfy.org#1-community-value)
- [2. Node Compatibility](http://docs.comfy.org#2-node-compatibility)
- [3. Legal Compliance](http://docs.comfy.org#3-legal-compliance)
- [5. Quality Requirements](http://docs.comfy.org#5-quality-requirements)
- [6. Fork Guidelines](http://docs.comfy.org#6-fork-guidelines)
- [Security Standards](http://docs.comfy.org#security-standards)
- [eval/exec Calls](http://docs.comfy.org#eval%2Fexec-calls)
- [Policy](http://docs.comfy.org#policy)
- [Reasoning](http://docs.comfy.org#reasoning)
- [subprocess for pip install](http://docs.comfy.org#subprocess-for-pip-install)
- [Policy](http://docs.comfy.org#policy-2)
- [Reasoning](http://docs.comfy.org#reasoning-2)
- [Code Obfuscation](http://docs.comfy.org#code-obfuscation)
- [Policy](http://docs.comfy.org#policy-3)
- [Reasoning](http://docs.comfy.org#reasoning-3)

<!-- END Development/registry/standards.md -->


<!-- BEGIN Development/specs/nodedef_json.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions
  
  - [Node Definition JSON](http://docs.comfy.org/specs/nodedef_json)
  - [Node Definition JSON 1.0](http://docs.comfy.org/specs/nodedef_json_1_0)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Node Definition JSON

# Node Definition JSON

JSON schema for a ComfyUI Node.

The node definition JSON is defined using [JSON Schema](https://json-schema.org/). Changes to this schema will be discussed in the [rfcs repo](https://github.com/comfy-org/rfcs).

## [​](http://docs.comfy.org#v2-0-latest) v2.0 (Latest)

Node Definition v2.0

```json
{
  "$ref": "#/definitions/ComfyNodeDefV2",
  "definitions": {
    "ComfyNodeDefV2": {
      "type": "object",
      "properties": {
        "inputs": {
          "type": "object",
          "additionalProperties": {
            "anyOf": [
              {
                "type": "object",
                "properties": {
                  "default": {
                    "anyOf": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "array",
                        "items": {
                          "type": "number"
                        }
                      }
                    ]
                  },
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "min": {
                    "type": "number"
                  },
                  "max": {
                    "type": "number"
                  },
                  "step": {
                    "type": "number"
                  },
                  "display": {
                    "type": "string",
                    "enum": [
                      "slider",
                      "number",
                      "knob"
                    ]
                  },
                  "control_after_generate": {
                    "type": "boolean"
                  },
                  "type": {
                    "type": "string",
                    "const": "INT"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {
                    "anyOf": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "array",
                        "items": {
                          "type": "number"
                        }
                      }
                    ]
                  },
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "min": {
                    "type": "number"
                  },
                  "max": {
                    "type": "number"
                  },
                  "step": {
                    "type": "number"
                  },
                  "display": {
                    "type": "string",
                    "enum": [
                      "slider",
                      "number",
                      "knob"
                    ]
                  },
                  "round": {
                    "anyOf": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "boolean",
                        "const": false
                      }
                    ]
                  },
                  "type": {
                    "type": "string",
                    "const": "FLOAT"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {
                    "type": "boolean"
                  },
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "label_on": {
                    "type": "string"
                  },
                  "label_off": {
                    "type": "string"
                  },
                  "type": {
                    "type": "string",
                    "const": "BOOLEAN"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {
                    "type": "string"
                  },
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "multiline": {
                    "type": "boolean"
                  },
                  "dynamicPrompts": {
                    "type": "boolean"
                  },
                  "defaultVal": {
                    "type": "string"
                  },
                  "placeholder": {
                    "type": "string"
                  },
                  "type": {
                    "type": "string",
                    "const": "STRING"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {},
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "control_after_generate": {
                    "type": "boolean"
                  },
                  "image_upload": {
                    "type": "boolean"
                  },
                  "image_folder": {
                    "type": "string",
                    "enum": [
                      "input",
                      "output",
                      "temp"
                    ]
                  },
                  "allow_batch": {
                    "type": "boolean"
                  },
                  "video_upload": {
                    "type": "boolean"
                  },
                  "remote": {
                    "type": "object",
                    "properties": {
                      "route": {
                        "anyOf": [
                          {
                            "type": "string",
                            "format": "uri"
                          },
                          {
                            "type": "string",
                            "pattern": "^\\/"
                          }
                        ]
                      },
                      "refresh": {
                        "anyOf": [
                          {
                            "type": "number",
                            "minimum": -9007199254740991,
                            "maximum": 9007199254740991
                          },
                          {
                            "type": "number",
                            "maximum": 9007199254740991,
                            "minimum": -9007199254740991
                          }
                        ]
                      },
                      "response_key": {
                        "type": "string"
                      },
                      "query_params": {
                        "type": "object",
                        "additionalProperties": {
                          "type": "string"
                        }
                      },
                      "refresh_button": {
                        "type": "boolean"
                      },
                      "control_after_refresh": {
                        "type": "string",
                        "enum": [
                          "first",
                          "last"
                        ]
                      },
                      "timeout": {
                        "type": "number",
                        "minimum": 0
                      },
                      "max_retries": {
                        "type": "number",
                        "minimum": 0
                      }
                    },
                    "required": [
                      "route"
                    ],
                    "additionalProperties": false
                  },
                  "options": {
                    "type": "array",
                    "items": {
                      "type": [
                        "string",
                        "number"
                      ]
                    }
                  },
                  "type": {
                    "type": "string",
                    "const": "COMBO"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {},
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "type": {
                    "type": "string"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              }
            ]
          }
        },
        "outputs": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "index": {
                "type": "number"
              },
              "name": {
                "type": "string"
              },
              "type": {
                "type": "string"
              },
              "is_list": {
                "type": "boolean"
              },
              "options": {
                "type": "array"
              },
              "tooltip": {
                "type": "string"
              }
            },
            "required": [
              "index",
              "name",
              "type",
              "is_list"
            ],
            "additionalProperties": false
          }
        },
        "hidden": {
          "type": "object",
          "additionalProperties": {}
        },
        "name": {
          "type": "string"
        },
        "display_name": {
          "type": "string"
        },
        "description": {
          "type": "string"
        },
        "category": {
          "type": "string"
        },
        "output_node": {
          "type": "boolean"
        },
        "python_module": {
          "type": "string"
        },
        "deprecated": {
          "type": "boolean"
        },
        "experimental": {
          "type": "boolean"
        }
      },
      "required": [
        "inputs",
        "outputs",
        "name",
        "display_name",
        "description",
        "category",
        "output_node",
        "python_module"
      ],
      "additionalProperties": false
    }
  },
  "$schema": "http://json-schema.org/draft-07/schema#"
}
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/specs/nodedef_json.mdx)

[Previous](http://docs.comfy.org/specs/workflow_json_0.4)

[Node Definition JSON 1.0JSON schema for a ComfyUI Node.  
\
Next](http://docs.comfy.org/specs/nodedef_json_1_0)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [v2.0 (Latest)](http://docs.comfy.org#v2-0-latest)

<!-- END Development/specs/nodedef_json.md -->


<!-- BEGIN Development/specs/nodedef_json_1_0.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions
  
  - [Node Definition JSON](http://docs.comfy.org/specs/nodedef_json)
  - [Node Definition JSON 1.0](http://docs.comfy.org/specs/nodedef_json_1_0)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Node Definition JSON 1.0

# Node Definition JSON 1.0

JSON schema for a ComfyUI Node.

## [​](http://docs.comfy.org#v1-0) v1.0

Node Definition v1.0

```json
{
  "$ref": "#/definitions/ComfyNodeDefV1",
  "definitions": {
    "ComfyNodeDefV1": {
      "type": "object",
      "properties": {
        "input": {
          "type": "object",
          "properties": {
            "required": {
              "type": "object",
              "additionalProperties": {
                "anyOf": [
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "INT"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "array",
                                    "items": {
                                      "type": "number"
                                    }
                                  }
                                ]
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "min": {
                                "type": "number"
                              },
                              "max": {
                                "type": "number"
                              },
                              "step": {
                                "type": "number"
                              },
                              "display": {
                                "type": "string",
                                "enum": [
                                  "slider",
                                  "number",
                                  "knob"
                                ]
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "FLOAT"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "array",
                                    "items": {
                                      "type": "number"
                                    }
                                  }
                                ]
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "min": {
                                "type": "number"
                              },
                              "max": {
                                "type": "number"
                              },
                              "step": {
                                "type": "number"
                              },
                              "display": {
                                "type": "string",
                                "enum": [
                                  "slider",
                                  "number",
                                  "knob"
                                ]
                              },
                              "round": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "boolean",
                                    "const": false
                                  }
                                ]
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "BOOLEAN"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "type": "boolean"
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "label_on": {
                                "type": "string"
                              },
                              "label_off": {
                                "type": "string"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "STRING"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "type": "string"
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "multiline": {
                                "type": "boolean"
                              },
                              "dynamicPrompts": {
                                "type": "boolean"
                              },
                              "defaultVal": {
                                "type": "string"
                              },
                              "placeholder": {
                                "type": "string"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "array",
                        "items": {
                          "type": [
                            "string",
                            "number"
                          ]
                        }
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              },
                              "image_upload": {
                                "type": "boolean"
                              },
                              "image_folder": {
                                "type": "string",
                                "enum": [
                                  "input",
                                  "output",
                                  "temp"
                                ]
                              },
                              "allow_batch": {
                                "type": "boolean"
                              },
                              "video_upload": {
                                "type": "boolean"
                              },
                              "remote": {
                                "type": "object",
                                "properties": {
                                  "route": {
                                    "anyOf": [
                                      {
                                        "type": "string",
                                        "format": "uri"
                                      },
                                      {
                                        "type": "string",
                                        "pattern": "^\\/"
                                      }
                                    ]
                                  },
                                  "refresh": {
                                    "anyOf": [
                                      {
                                        "type": "number",
                                        "minimum": -9007199254740991,
                                        "maximum": 9007199254740991
                                      },
                                      {
                                        "type": "number",
                                        "maximum": 9007199254740991,
                                        "minimum": -9007199254740991
                                      }
                                    ]
                                  },
                                  "response_key": {
                                    "type": "string"
                                  },
                                  "query_params": {
                                    "type": "object",
                                    "additionalProperties": {
                                      "type": "string"
                                    }
                                  },
                                  "refresh_button": {
                                    "type": "boolean"
                                  },
                                  "control_after_refresh": {
                                    "type": "string",
                                    "enum": [
                                      "first",
                                      "last"
                                    ]
                                  },
                                  "timeout": {
                                    "type": "number",
                                    "minimum": 0
                                  },
                                  "max_retries": {
                                    "type": "number",
                                    "minimum": 0
                                  }
                                },
                                "required": [
                                  "route"
                                ],
                                "additionalProperties": false
                              },
                              "options": {
                                "type": "array",
                                "items": {
                                  "type": [
                                    "string",
                                    "number"
                                  ]
                                }
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "COMBO"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              },
                              "image_upload": {
                                "type": "boolean"
                              },
                              "image_folder": {
                                "type": "string",
                                "enum": [
                                  "input",
                                  "output",
                                  "temp"
                                ]
                              },
                              "allow_batch": {
                                "type": "boolean"
                              },
                              "video_upload": {
                                "type": "boolean"
                              },
                              "remote": {
                                "type": "object",
                                "properties": {
                                  "route": {
                                    "anyOf": [
                                      {
                                        "type": "string",
                                        "format": "uri"
                                      },
                                      {
                                        "type": "string",
                                        "pattern": "^\\/"
                                      }
                                    ]
                                  },
                                  "refresh": {
                                    "anyOf": [
                                      {
                                        "type": "number",
                                        "minimum": -9007199254740991,
                                        "maximum": 9007199254740991
                                      },
                                      {
                                        "type": "number",
                                        "maximum": 9007199254740991,
                                        "minimum": -9007199254740991
                                      }
                                    ]
                                  },
                                  "response_key": {
                                    "type": "string"
                                  },
                                  "query_params": {
                                    "type": "object",
                                    "additionalProperties": {
                                      "type": "string"
                                    }
                                  },
                                  "refresh_button": {
                                    "type": "boolean"
                                  },
                                  "control_after_refresh": {
                                    "type": "string",
                                    "enum": [
                                      "first",
                                      "last"
                                    ]
                                  },
                                  "timeout": {
                                    "type": "number",
                                    "minimum": 0
                                  },
                                  "max_retries": {
                                    "type": "number",
                                    "minimum": 0
                                  }
                                },
                                "required": [
                                  "route"
                                ],
                                "additionalProperties": false
                              },
                              "options": {
                                "type": "array",
                                "items": {
                                  "type": [
                                    "string",
                                    "number"
                                  ]
                                }
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  }
                ]
              }
            },
            "optional": {
              "type": "object",
              "additionalProperties": {
                "anyOf": [
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "INT"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "array",
                                    "items": {
                                      "type": "number"
                                    }
                                  }
                                ]
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "min": {
                                "type": "number"
                              },
                              "max": {
                                "type": "number"
                              },
                              "step": {
                                "type": "number"
                              },
                              "display": {
                                "type": "string",
                                "enum": [
                                  "slider",
                                  "number",
                                  "knob"
                                ]
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "FLOAT"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "array",
                                    "items": {
                                      "type": "number"
                                    }
                                  }
                                ]
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "min": {
                                "type": "number"
                              },
                              "max": {
                                "type": "number"
                              },
                              "step": {
                                "type": "number"
                              },
                              "display": {
                                "type": "string",
                                "enum": [
                                  "slider",
                                  "number",
                                  "knob"
                                ]
                              },
                              "round": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "boolean",
                                    "const": false
                                  }
                                ]
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "BOOLEAN"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "type": "boolean"
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "label_on": {
                                "type": "string"
                              },
                              "label_off": {
                                "type": "string"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "STRING"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "type": "string"
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "multiline": {
                                "type": "boolean"
                              },
                              "dynamicPrompts": {
                                "type": "boolean"
                              },
                              "defaultVal": {
                                "type": "string"
                              },
                              "placeholder": {
                                "type": "string"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "array",
                        "items": {
                          "type": [
                            "string",
                            "number"
                          ]
                        }
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              },
                              "image_upload": {
                                "type": "boolean"
                              },
                              "image_folder": {
                                "type": "string",
                                "enum": [
                                  "input",
                                  "output",
                                  "temp"
                                ]
                              },
                              "allow_batch": {
                                "type": "boolean"
                              },
                              "video_upload": {
                                "type": "boolean"
                              },
                              "remote": {
                                "type": "object",
                                "properties": {
                                  "route": {
                                    "anyOf": [
                                      {
                                        "type": "string",
                                        "format": "uri"
                                      },
                                      {
                                        "type": "string",
                                        "pattern": "^\\/"
                                      }
                                    ]
                                  },
                                  "refresh": {
                                    "anyOf": [
                                      {
                                        "type": "number",
                                        "minimum": -9007199254740991,
                                        "maximum": 9007199254740991
                                      },
                                      {
                                        "type": "number",
                                        "maximum": 9007199254740991,
                                        "minimum": -9007199254740991
                                      }
                                    ]
                                  },
                                  "response_key": {
                                    "type": "string"
                                  },
                                  "query_params": {
                                    "type": "object",
                                    "additionalProperties": {
                                      "type": "string"
                                    }
                                  },
                                  "refresh_button": {
                                    "type": "boolean"
                                  },
                                  "control_after_refresh": {
                                    "type": "string",
                                    "enum": [
                                      "first",
                                      "last"
                                    ]
                                  },
                                  "timeout": {
                                    "type": "number",
                                    "minimum": 0
                                  },
                                  "max_retries": {
                                    "type": "number",
                                    "minimum": 0
                                  }
                                },
                                "required": [
                                  "route"
                                ],
                                "additionalProperties": false
                              },
                              "options": {
                                "type": "array",
                                "items": {
                                  "type": [
                                    "string",
                                    "number"
                                  ]
                                }
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "COMBO"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              },
                              "image_upload": {
                                "type": "boolean"
                              },
                              "image_folder": {
                                "type": "string",
                                "enum": [
                                  "input",
                                  "output",
                                  "temp"
                                ]
                              },
                              "allow_batch": {
                                "type": "boolean"
                              },
                              "video_upload": {
                                "type": "boolean"
                              },
                              "remote": {
                                "type": "object",
                                "properties": {
                                  "route": {
                                    "anyOf": [
                                      {
                                        "type": "string",
                                        "format": "uri"
                                      },
                                      {
                                        "type": "string",
                                        "pattern": "^\\/"
                                      }
                                    ]
                                  },
                                  "refresh": {
                                    "anyOf": [
                                      {
                                        "type": "number",
                                        "minimum": -9007199254740991,
                                        "maximum": 9007199254740991
                                      },
                                      {
                                        "type": "number",
                                        "maximum": 9007199254740991,
                                        "minimum": -9007199254740991
                                      }
                                    ]
                                  },
                                  "response_key": {
                                    "type": "string"
                                  },
                                  "query_params": {
                                    "type": "object",
                                    "additionalProperties": {
                                      "type": "string"
                                    }
                                  },
                                  "refresh_button": {
                                    "type": "boolean"
                                  },
                                  "control_after_refresh": {
                                    "type": "string",
                                    "enum": [
                                      "first",
                                      "last"
                                    ]
                                  },
                                  "timeout": {
                                    "type": "number",
                                    "minimum": 0
                                  },
                                  "max_retries": {
                                    "type": "number",
                                    "minimum": 0
                                  }
                                },
                                "required": [
                                  "route"
                                ],
                                "additionalProperties": false
                              },
                              "options": {
                                "type": "array",
                                "items": {
                                  "type": [
                                    "string",
                                    "number"
                                  ]
                                }
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  }
                ]
              }
            },
            "hidden": {
              "type": "object",
              "additionalProperties": {}
            }
          },
          "additionalProperties": false
        },
        "output": {
          "type": "array",
          "items": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "array",
                "items": {
                  "type": [
                    "string",
                    "number"
                  ]
                }
              }
            ]
          }
        },
        "output_is_list": {
          "type": "array",
          "items": {
            "type": "boolean"
          }
        },
        "output_name": {
          "type": "array",
          "items": {
            "type": "string"
          }
        },
        "output_tooltips": {
          "type": "array",
          "items": {
            "type": "string"
          }
        },
        "name": {
          "type": "string"
        },
        "display_name": {
          "type": "string"
        },
        "description": {
          "type": "string"
        },
        "category": {
          "type": "string"
        },
        "output_node": {
          "type": "boolean"
        },
        "python_module": {
          "type": "string"
        },
        "deprecated": {
          "type": "boolean"
        },
        "experimental": {
          "type": "boolean"
        }
      },
      "required": [
        "name",
        "display_name",
        "description",
        "category",
        "output_node",
        "python_module"
      ],
      "additionalProperties": false
    }
  },
  "$schema": "http://json-schema.org/draft-07/schema#"
}
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/specs/nodedef_json_1_0.mdx)

[Previous  
\
Node Definition JSONJSON schema for a ComfyUI Node.](http://docs.comfy.org/specs/nodedef_json)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [v1.0](http://docs.comfy.org#v1-0)

<!-- END Development/specs/nodedef_json_1_0.md -->


<!-- BEGIN Development/specs/workflow_json.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
  
  - [Workflow JSON](http://docs.comfy.org/specs/workflow_json)
  - [Workflow JSON 0.4](http://docs.comfy.org/specs/workflow_json_0.4)
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Workflow JSON

# Workflow JSON

JSON schema for a ComfyUI workflow.

The workflow JSON is defined using [JSON Schema](https://json-schema.org/). Changes to this schema will be discussed in the [rfcs repo](https://github.com/comfy-org/rfcs).

## [​](http://docs.comfy.org#version-1-0-latest) Version 1.0 (Latest)

ComfyUI Workflow v1.0

```json
{
  "$ref": "#/definitions/ComfyWorkflow1_0",
  "definitions": {
    "ComfyWorkflow1_0": {
      "type": "object",
      "properties": {
        "version": {
          "type": "number",
          "const": 1
        },
        "config": {
          "anyOf": [
            {
              "anyOf": [
                {
                  "not": {}
                },
                {
                  "type": "object",
                  "properties": {
                    "links_ontop": {
                      "type": "boolean"
                    },
                    "align_to_grid": {
                      "type": "boolean"
                    }
                  },
                  "additionalProperties": true
                }
              ]
            },
            {
              "type": "null"
            }
          ]
        },
        "state": {
          "type": "object",
          "properties": {
            "lastGroupid": {
              "type": "number"
            },
            "lastNodeId": {
              "type": "number"
            },
            "lastLinkId": {
              "type": "number"
            },
            "lastRerouteId": {
              "type": "number"
            }
          },
          "additionalProperties": true
        },
        "groups": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "title": {
                "type": "string"
              },
              "bounding": {
                "type": "array",
                "minItems": 4,
                "maxItems": 4,
                "items": [
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  }
                ]
              },
              "color": {
                "type": "string"
              },
              "font_size": {
                "type": "number"
              },
              "locked": {
                "type": "boolean"
              }
            },
            "required": [
              "title",
              "bounding"
            ],
            "additionalProperties": true
          }
        },
        "nodes": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "id": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "type": {
                "type": "string"
              },
              "pos": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "size": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "flags": {
                "type": "object",
                "properties": {
                  "collapsed": {
                    "type": "boolean"
                  },
                  "pinned": {
                    "type": "boolean"
                  },
                  "allow_interaction": {
                    "type": "boolean"
                  },
                  "horizontal": {
                    "type": "boolean"
                  },
                  "skip_repeated_outputs": {
                    "type": "boolean"
                  }
                },
                "additionalProperties": true
              },
              "order": {
                "type": "number"
              },
              "mode": {
                "type": "number"
              },
              "inputs": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "type": {
                      "anyOf": [
                        {
                          "type": "string"
                        },
                        {
                          "type": "array",
                          "items": {
                            "type": "string"
                          }
                        },
                        {
                          "type": "number"
                        }
                      ]
                    },
                    "link": {
                      "type": [
                        "number",
                        "null"
                      ]
                    },
                    "slot_index": {
                      "anyOf": [
                        {
                          "type": "integer"
                        },
                        {
                          "type": "string"
                        }
                      ]
                    }
                  },
                  "required": [
                    "name",
                    "type"
                  ],
                  "additionalProperties": true
                }
              },
              "outputs": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "type": {
                      "anyOf": [
                        {
                          "type": "string"
                        },
                        {
                          "type": "array",
                          "items": {
                            "type": "string"
                          }
                        },
                        {
                          "type": "number"
                        }
                      ]
                    },
                    "links": {
                      "anyOf": [
                        {
                          "type": "array",
                          "items": {
                            "type": "number"
                          }
                        },
                        {
                          "type": "null"
                        }
                      ]
                    },
                    "slot_index": {
                      "anyOf": [
                        {
                          "type": "integer"
                        },
                        {
                          "type": "string"
                        }
                      ]
                    }
                  },
                  "required": [
                    "name",
                    "type"
                  ],
                  "additionalProperties": true
                }
              },
              "properties": {
                "type": "object",
                "properties": {
                  "Node name for S&R": {
                    "type": "string"
                  }
                },
                "additionalProperties": true
              },
              "widgets_values": {
                "anyOf": [
                  {
                    "type": "array"
                  },
                  {
                    "type": "object",
                    "additionalProperties": {}
                  }
                ]
              },
              "color": {
                "type": "string"
              },
              "bgcolor": {
                "type": "string"
              }
            },
            "required": [
              "id",
              "type",
              "pos",
              "size",
              "flags",
              "order",
              "mode",
              "properties"
            ],
            "additionalProperties": true
          }
        },
        "links": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "id": {
                "type": "number"
              },
              "origin_id": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "origin_slot": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "target_id": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "target_slot": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "type": {
                "anyOf": [
                  {
                    "type": "string"
                  },
                  {
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  {
                    "type": "number"
                  }
                ]
              },
              "parentId": {
                "type": "number"
              }
            },
            "required": [
              "id",
              "origin_id",
              "origin_slot",
              "target_id",
              "target_slot",
              "type"
            ],
            "additionalProperties": true
          }
        },
        "reroutes": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "id": {
                "type": "number"
              },
              "parentId": {
                "type": "number"
              },
              "pos": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "linkIds": {
                "anyOf": [
                  {
                    "type": "array",
                    "items": {
                      "type": "number"
                    }
                  },
                  {
                    "type": "null"
                  }
                ]
              }
            },
            "required": [
              "id",
              "pos"
            ],
            "additionalProperties": true
          }
        },
        "extra": {
          "anyOf": [
            {
              "anyOf": [
                {
                  "not": {}
                },
                {
                  "type": "object",
                  "properties": {
                    "ds": {
                      "type": "object",
                      "properties": {
                        "scale": {
                          "type": "number"
                        },
                        "offset": {
                          "anyOf": [
                            {
                              "type": "object",
                              "properties": {
                                "0": {
                                  "type": "number"
                                },
                                "1": {
                                  "type": "number"
                                }
                              },
                              "required": [
                                "0",
                                "1"
                              ],
                              "additionalProperties": true
                            },
                            {
                              "type": "array",
                              "minItems": 2,
                              "maxItems": 2,
                              "items": [
                                {
                                  "type": "number"
                                },
                                {
                                  "type": "number"
                                }
                              ]
                            }
                          ]
                        }
                      },
                      "required": [
                        "scale",
                        "offset"
                      ],
                      "additionalProperties": true
                    },
                    "info": {
                      "type": "object",
                      "properties": {
                        "name": {
                          "type": "string"
                        },
                        "author": {
                          "type": "string"
                        },
                        "description": {
                          "type": "string"
                        },
                        "version": {
                          "type": "string"
                        },
                        "created": {
                          "type": "string"
                        },
                        "modified": {
                          "type": "string"
                        },
                        "software": {
                          "type": "string"
                        }
                      },
                      "required": [
                        "name",
                        "author",
                        "description",
                        "version",
                        "created",
                        "modified",
                        "software"
                      ],
                      "additionalProperties": true
                    },
                    "linkExtensions": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "id": {
                            "type": "number"
                          },
                          "parentId": {
                            "type": "number"
                          }
                        },
                        "required": [
                          "id",
                          "parentId"
                        ],
                        "additionalProperties": true
                      }
                    },
                    "reroutes": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "id": {
                            "type": "number"
                          },
                          "parentId": {
                            "type": "number"
                          },
                          "pos": {
                            "anyOf": [
                              {
                                "type": "object",
                                "properties": {
                                  "0": {
                                    "type": "number"
                                  },
                                  "1": {
                                    "type": "number"
                                  }
                                },
                                "required": [
                                  "0",
                                  "1"
                                ],
                                "additionalProperties": true
                              },
                              {
                                "type": "array",
                                "minItems": 2,
                                "maxItems": 2,
                                "items": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "number"
                                  }
                                ]
                              }
                            ]
                          },
                          "linkIds": {
                            "anyOf": [
                              {
                                "type": "array",
                                "items": {
                                  "type": "number"
                                }
                              },
                              {
                                "type": "null"
                              }
                            ]
                          }
                        },
                        "required": [
                          "id",
                          "pos"
                        ],
                        "additionalProperties": true
                      }
                    }
                  },
                  "additionalProperties": true
                }
              ]
            },
            {
              "type": "null"
            }
          ]
        },
        "models": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "name": {
                "type": "string"
              },
              "url": {
                "type": "string",
                "format": "uri"
              },
              "hash": {
                "type": "string"
              },
              "hash_type": {
                "type": "string"
              },
              "directory": {
                "type": "string"
              }
            },
            "required": [
              "name",
              "url",
              "directory"
            ],
            "additionalProperties": false
          }
        }
      },
      "required": [
        "version",
        "state",
        "nodes"
      ],
      "additionalProperties": true
    }
  },
  "$schema": "http://json-schema.org/draft-07/schema#"
}
```

## [​](http://docs.comfy.org#older-versions) Older versions

- [0.4](http://docs.comfy.org/workflow_json_0.4.mdx)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/specs/workflow_json.mdx)

[Previous](http://docs.comfy.org/registry/specifications)

[Workflow JSON 0.4JSON schema for a ComfyUI workflow.  
\
Next](http://docs.comfy.org/specs/workflow_json_0.4)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Version 1.0 (Latest)](http://docs.comfy.org#version-1-0-latest)
- [Older versions](http://docs.comfy.org#older-versions)

<!-- END Development/specs/workflow_json.md -->


<!-- BEGIN Development/specs/workflow_json_0.4.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
  
  - [Workflow JSON](http://docs.comfy.org/specs/workflow_json)
  - [Workflow JSON 0.4](http://docs.comfy.org/specs/workflow_json_0.4)
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Workflow JSON 0.4

# Workflow JSON 0.4

JSON schema for a ComfyUI workflow.

## [​](http://docs.comfy.org#v0-4) v0.4

```json
{
  "$ref": "#/definitions/ComfyWorkflow0_4",
  "definitions": {
    "ComfyWorkflow0_4": {
      "type": "object",
      "properties": {
        "last_node_id": {
          "anyOf": [
            {
              "type": "integer"
            },
            {
              "type": "string"
            }
          ]
        },
        "last_link_id": {
          "type": "number"
        },
        "nodes": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "id": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "type": {
                "type": "string"
              },
              "pos": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "size": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "flags": {
                "type": "object",
                "properties": {
                  "collapsed": {
                    "type": "boolean"
                  },
                  "pinned": {
                    "type": "boolean"
                  },
                  "allow_interaction": {
                    "type": "boolean"
                  },
                  "horizontal": {
                    "type": "boolean"
                  },
                  "skip_repeated_outputs": {
                    "type": "boolean"
                  }
                },
                "additionalProperties": true
              },
              "order": {
                "type": "number"
              },
              "mode": {
                "type": "number"
              },
              "inputs": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "type": {
                      "anyOf": [
                        {
                          "type": "string"
                        },
                        {
                          "type": "array",
                          "items": {
                            "type": "string"
                          }
                        },
                        {
                          "type": "number"
                        }
                      ]
                    },
                    "link": {
                      "type": [
                        "number",
                        "null"
                      ]
                    },
                    "slot_index": {
                      "anyOf": [
                        {
                          "type": "integer"
                        },
                        {
                          "type": "string"
                        }
                      ]
                    }
                  },
                  "required": [
                    "name",
                    "type"
                  ],
                  "additionalProperties": true
                }
              },
              "outputs": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "type": {
                      "anyOf": [
                        {
                          "type": "string"
                        },
                        {
                          "type": "array",
                          "items": {
                            "type": "string"
                          }
                        },
                        {
                          "type": "number"
                        }
                      ]
                    },
                    "links": {
                      "anyOf": [
                        {
                          "type": "array",
                          "items": {
                            "type": "number"
                          }
                        },
                        {
                          "type": "null"
                        }
                      ]
                    },
                    "slot_index": {
                      "anyOf": [
                        {
                          "type": "integer"
                        },
                        {
                          "type": "string"
                        }
                      ]
                    }
                  },
                  "required": [
                    "name",
                    "type"
                  ],
                  "additionalProperties": true
                }
              },
              "properties": {
                "type": "object",
                "properties": {
                  "Node name for S&R": {
                    "type": "string"
                  }
                },
                "additionalProperties": true
              },
              "widgets_values": {
                "anyOf": [
                  {
                    "type": "array"
                  },
                  {
                    "type": "object",
                    "additionalProperties": {}
                  }
                ]
              },
              "color": {
                "type": "string"
              },
              "bgcolor": {
                "type": "string"
              }
            },
            "required": [
              "id",
              "type",
              "pos",
              "size",
              "flags",
              "order",
              "mode",
              "properties"
            ],
            "additionalProperties": true
          }
        },
        "links": {
          "type": "array",
          "items": {
            "type": "array",
            "minItems": 6,
            "maxItems": 6,
            "items": [
              {
                "type": "number"
              },
              {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              {
                "anyOf": [
                  {
                    "type": "string"
                  },
                  {
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  {
                    "type": "number"
                  }
                ]
              }
            ]
          }
        },
        "groups": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "title": {
                "type": "string"
              },
              "bounding": {
                "type": "array",
                "minItems": 4,
                "maxItems": 4,
                "items": [
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  }
                ]
              },
              "color": {
                "type": "string"
              },
              "font_size": {
                "type": "number"
              },
              "locked": {
                "type": "boolean"
              }
            },
            "required": [
              "title",
              "bounding"
            ],
            "additionalProperties": true
          }
        },
        "config": {
          "anyOf": [
            {
              "anyOf": [
                {
                  "not": {}
                },
                {
                  "type": "object",
                  "properties": {
                    "links_ontop": {
                      "type": "boolean"
                    },
                    "align_to_grid": {
                      "type": "boolean"
                    }
                  },
                  "additionalProperties": true
                }
              ]
            },
            {
              "type": "null"
            }
          ]
        },
        "extra": {
          "anyOf": [
            {
              "anyOf": [
                {
                  "not": {}
                },
                {
                  "type": "object",
                  "properties": {
                    "ds": {
                      "type": "object",
                      "properties": {
                        "scale": {
                          "type": "number"
                        },
                        "offset": {
                          "anyOf": [
                            {
                              "type": "object",
                              "properties": {
                                "0": {
                                  "type": "number"
                                },
                                "1": {
                                  "type": "number"
                                }
                              },
                              "required": [
                                "0",
                                "1"
                              ],
                              "additionalProperties": true
                            },
                            {
                              "type": "array",
                              "minItems": 2,
                              "maxItems": 2,
                              "items": [
                                {
                                  "type": "number"
                                },
                                {
                                  "type": "number"
                                }
                              ]
                            }
                          ]
                        }
                      },
                      "required": [
                        "scale",
                        "offset"
                      ],
                      "additionalProperties": true
                    },
                    "info": {
                      "type": "object",
                      "properties": {
                        "name": {
                          "type": "string"
                        },
                        "author": {
                          "type": "string"
                        },
                        "description": {
                          "type": "string"
                        },
                        "version": {
                          "type": "string"
                        },
                        "created": {
                          "type": "string"
                        },
                        "modified": {
                          "type": "string"
                        },
                        "software": {
                          "type": "string"
                        }
                      },
                      "required": [
                        "name",
                        "author",
                        "description",
                        "version",
                        "created",
                        "modified",
                        "software"
                      ],
                      "additionalProperties": true
                    },
                    "linkExtensions": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "id": {
                            "type": "number"
                          },
                          "parentId": {
                            "type": "number"
                          }
                        },
                        "required": [
                          "id",
                          "parentId"
                        ],
                        "additionalProperties": true
                      }
                    },
                    "reroutes": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "id": {
                            "type": "number"
                          },
                          "parentId": {
                            "type": "number"
                          },
                          "pos": {
                            "anyOf": [
                              {
                                "type": "object",
                                "properties": {
                                  "0": {
                                    "type": "number"
                                  },
                                  "1": {
                                    "type": "number"
                                  }
                                },
                                "required": [
                                  "0",
                                  "1"
                                ],
                                "additionalProperties": true
                              },
                              {
                                "type": "array",
                                "minItems": 2,
                                "maxItems": 2,
                                "items": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "number"
                                  }
                                ]
                              }
                            ]
                          },
                          "linkIds": {
                            "anyOf": [
                              {
                                "type": "array",
                                "items": {
                                  "type": "number"
                                }
                              },
                              {
                                "type": "null"
                              }
                            ]
                          }
                        },
                        "required": [
                          "id",
                          "pos"
                        ],
                        "additionalProperties": true
                      }
                    }
                  },
                  "additionalProperties": true
                }
              ]
            },
            {
              "type": "null"
            }
          ]
        },
        "version": {
          "type": "number"
        },
        "models": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "name": {
                "type": "string"
              },
              "url": {
                "type": "string",
                "format": "uri"
              },
              "hash": {
                "type": "string"
              },
              "hash_type": {
                "type": "string"
              },
              "directory": {
                "type": "string"
              }
            },
            "required": [
              "name",
              "url",
              "directory"
            ],
            "additionalProperties": false
          }
        }
      },
      "required": [
        "last_node_id",
        "last_link_id",
        "nodes",
        "links",
        "version"
      ],
      "additionalProperties": true
    }
  },
  "$schema": "http://json-schema.org/draft-07/schema#"
}
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/specs/workflow_json_0.4.mdx)

[Previous](http://docs.comfy.org/specs/workflow_json)

[Node Definition JSONJSON schema for a ComfyUI Node.  
\
Next](http://docs.comfy.org/specs/nodedef_json)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [v0.4](http://docs.comfy.org#v0-4)

<!-- END Development/specs/workflow_json_0.4.md -->


<!-- BEGIN Development/tutorials/3d/hunyuan3D-2.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
  
  - [Hunyuan3D-2](http://docs.comfy.org/tutorials/3d/hunyuan3D-2)
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Hunyuan3D-2 Examples

# ComfyUI Hunyuan3D-2 Examples

This guide will demonstrate how to use Hunyuan3D-2 in ComfyUI to generate 3D assets.

# [​](http://docs.comfy.org#hunyuan3d-2-0-introduction) Hunyuan3D 2.0 Introduction

[Hunyuan3D 2.0](https://github.com/Tencent/Hunyuan3D-2) is an open-source 3D asset generation model released by Tencent, capable of generating high-fidelity 3D models with high-resolution texture maps through text or images.

Hunyuan3D 2.0 adopts a two-stage generation approach, first generating a geometry model without textures, then synthesizing high-resolution texture maps. This effectively separates the complexity of shape and texture generation. Below are the two core components of Hunyuan3D 2.0:

1. **Geometry Generation Model (Hunyuan3D-DiT)**: Based on a flow diffusion Transformer architecture, it generates untextured geometric models that precisely match input conditions.
2. **Texture Generation Model (Hunyuan3D-Paint)**: Combines geometric conditions and multi-view diffusion techniques to add high-resolution textures to models, supporting PBR materials.

**Key Advantages**

- **High-Precision Generation**: Sharp geometric structures, rich texture colors, support for PBR material generation, achieving near-realistic lighting effects.
- **Diverse Usage Methods**: Provides code calls, Blender plugins, Gradio applications, and online experience through the official website, suitable for different user needs.
- **Lightweight and Compatibility**: The Hunyuan3D-2mini model requires only 5GB VRAM, the standard version needs 6GB VRAM for shape generation, and the complete process (shape + texture) requires only 12GB VRAM.

Recently (March 18, 2025), Hunyuan3D 2.0 also introduced a multi-view shape generation model (Hunyuan3D-2mv), which supports generating more detailed geometric structures from inputs at different angles.

This example includes three workflows:

- Using Hunyuan3D-2mv with multiple view inputs to generate 3D models
- Using Hunyuan3D-2mv-turbo with multiple view inputs to generate 3D models
- Using Hunyuan3D-2 with a single view input to generate 3D models

ComfyUI now natively supports Hunyuan3D-2mv, but does not yet support texture and material generation. Please make sure you have updated to the latest version of [ComfyUI](https://github.com/comfyanonymous/ComfyUI) before starting.

The workflow example PNG images in this tutorial contain workflow JSON in their metadata:

- You can drag them directly into ComfyUI
- Or use the menu `Workflows` -&gt; `Open (ctrl+o)`

This will load the corresponding workflow and prompt you to download the required models. The generated `.glb` format models will be output to the `ComfyUI/output/mesh` folder.

## [​](http://docs.comfy.org#comfyui-hunyuan3d-2mv-workflow-example) ComfyUI Hunyuan3D-2mv Workflow Example

In the Hunyuan3D-2mv workflow, we’ll use multi-view images to generate a 3D model. Note that multiple view images are not mandatory in this workflow - you can use only the `front` view image to generate a 3D model.

### [​](http://docs.comfy.org#1-workflow) 1. Workflow

Please download the images below and drag into ComfyUI to load the workflow.

Download the images below we will use them as input images.

In this example, the input images have already been preprocessed to remove excess background. In actual use, you can use custom nodes like [ComfyUI\_essentials](https://github.com/cubiq/ComfyUI_essentials) to automatically remove excess background.

### [​](http://docs.comfy.org#2-manual-model-installation) 2. Manual Model Installation

Download the model below and save it to the corresponding ComfyUI folder

- hunyuan3d-dit-v2-mv: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2-mv.safetensors`

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── hunyuan3d-dit-v2-mv.safetensors  // renamed file
```

### [​](http://docs.comfy.org#3-steps-to-run-the-workflow) 3. Steps to Run the Workflow

1. Ensure that the Image Only Checkpoint Loader(img2vid model) has loaded our downloaded and renamed `hunyuan3d-dit-v2-mv.safetensors` model
2. Load the corresponding view images in each of the `Load Image` nodes
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

If you need to add more views, make sure to load other view images in the `Hunyuan3Dv2ConditioningMultiView` node, and ensure that you load the corresponding view images in the `Load Image` nodes.

## [​](http://docs.comfy.org#hunyuan3d-2mv-turbo-workflow) Hunyuan3D-2mv-turbo Workflow

In the Hunyuan3D-2mv-turbo workflow, we’ll use the Hunyuan3D-2mv-turbo model to generate 3D models. This model is a step distillation version of Hunyuan3D-2mv, allowing for faster 3D model generation. In this version of the workflow, we set `cfg` to 1.0 and add a `flux guidance` node to control the `distilled cfg` generation.

### [​](http://docs.comfy.org#1-workflow-2) 1. Workflow

Please download the images below and drag into ComfyUI to load the workflow.

Download the images below we will use them as input images.

### [​](http://docs.comfy.org#2-manual-model-installation-2) 2. Manual Model Installation

Download the model below and save it to the corresponding ComfyUI folder

- hunyuan3d-dit-v2-mv-turbo: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv-turbo/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2-mv-turbo.safetensors`

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── hunyuan3d-dit-v2-mv-turbo.safetensors  // renamed file
```

### [​](http://docs.comfy.org#3-steps-to-run-the-workflow-2) 3. Steps to Run the Workflow

1. Ensure that the `Image Only Checkpoint Loader(img2vid model)` node has loaded our renamed `hunyuan3d-dit-v2-mv-turbo.safetensors` model
2. Load the corresponding view images in each of the `Load Image` nodes
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## [​](http://docs.comfy.org#hunyuan3d-2-single-view-workflow) Hunyuan3D-2 Single View Workflow

In the Hunyuan3D-2 workflow, we’ll use the Hunyuan3D-2 model to generate 3D models. This model is not a multi-view model. In this workflow, we use the `Hunyuan3Dv2Conditioning` node instead of the `Hunyuan3Dv2ConditioningMultiView` node.

### [​](http://docs.comfy.org#1-workflow-3) 1. Workflow

Please download the image below and drag it into ComfyUI to load the workflow.

Download the image below we will use it as input image.

### [​](http://docs.comfy.org#2-manual-model-installation-3) 2. Manual Model Installation

Download the model below and save it to the corresponding ComfyUI folder

- hunyuan3d-dit-v2-0: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2/resolve/main/hunyuan3d-dit-v2-0/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2.safetensors`

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── hunyuan3d-dit-v2.safetensors  // renamed file
```

### [​](http://docs.comfy.org#3-steps-to-run-the-workflow-3) 3. Steps to Run the Workflow

1. Ensure that the `Image Only Checkpoint Loader(img2vid model)` node has loaded our renamed `hunyuan3d-dit-v2.safetensors` model
2. Load the image in the `Load Image` node
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## [​](http://docs.comfy.org#community-resources) Community Resources

Below are ComfyUI community resources related to Hunyuan3D-2

- [ComfyUI-Hunyuan3DWrapper](https://github.com/kijai/ComfyUI-Hunyuan3DWrapper)
- [Kijai/Hunyuan3D-2\_safetensors](https://huggingface.co/Kijai/Hunyuan3D-2_safetensors/tree/main)
- [ComfyUI-3D-Pack](https://github.com/MrForExample/ComfyUI-3D-Pack)

## [​](http://docs.comfy.org#hunyuan3d-2-0-open-source-model-series) Hunyuan3D 2.0 Open-Source Model Series

Currently, Hunyuan3D 2.0 has open-sourced multiple models covering the complete 3D generation process. You can visit [Hunyuan3D-2](https://github.com/Tencent/Hunyuan3D-2) for more information.

**Hunyuan3D-2mini Series**

ModelDescriptionDateParametersHuggingfaceHunyuan3D-DiT-v2-miniMini Image to Shape Model2025-03-180.6B[Visit](https://huggingface.co/tencent/Hunyuan3D-2mini/tree/main/hunyuan3d-dit-v2-mini)

**Hunyuan3D-2mv Series**

ModelDescriptionDateParametersHuggingfaceHunyuan3D-DiT-v2-mv-FastGuidance Distillation Version, can halve DIT inference time2025-03-181.1B[Visit](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv-fast)Hunyuan3D-DiT-v2-mvMulti-view Image to Shape Model, suitable for 3D creation requiring multiple angles to understand the scene2025-03-181.1B[Visit](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv)

**Hunyuan3D-2 Series**

ModelDescriptionDateParametersHuggingfaceHunyuan3D-DiT-v2-0-FastGuidance Distillation Model2025-02-031.1B[Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0-fast)Hunyuan3D-DiT-v2-0Image to Shape Model2025-01-211.1B[Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0)Hunyuan3D-Paint-v2-0Texture Generation Model2025-01-211.3B[Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-paint-v2-0)Hunyuan3D-Delight-v2-0Image Delight Model2025-01-211.3B[Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-delight-v2-0)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/3d/hunyuan3D-2.mdx)

[Previous](http://docs.comfy.org/tutorials/image/hidream/hidream-e1)

[LTX-Video  
\
Next](http://docs.comfy.org/tutorials/video/ltxv)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Hunyuan3D 2.0 Introduction](http://docs.comfy.org#hunyuan3d-2-0-introduction)
- [ComfyUI Hunyuan3D-2mv Workflow Example](http://docs.comfy.org#comfyui-hunyuan3d-2mv-workflow-example)
- [1. Workflow](http://docs.comfy.org#1-workflow)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow)
- [Hunyuan3D-2mv-turbo Workflow](http://docs.comfy.org#hunyuan3d-2mv-turbo-workflow)
- [1. Workflow](http://docs.comfy.org#1-workflow-2)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation-2)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow-2)
- [Hunyuan3D-2 Single View Workflow](http://docs.comfy.org#hunyuan3d-2-single-view-workflow)
- [1. Workflow](http://docs.comfy.org#1-workflow-3)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation-3)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow-3)
- [Community Resources](http://docs.comfy.org#community-resources)
- [Hunyuan3D 2.0 Open-Source Model Series](http://docs.comfy.org#hunyuan3d-2-0-open-source-model-series)

<!-- END Development/tutorials/3d/hunyuan3D-2.md -->


<!-- BEGIN Development/tutorials/api-nodes/black-forest-labs/flux-1-1-pro-ultra-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
    
    - [Flux 1.1 Pro Ultra Image](http://docs.comfy.org/tutorials/api-nodes/black-forest-labs/flux-1-1-pro-ultra-image)
  - Stability AI
  - Ideogram
  - Luma
  - OpenAI
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Flux 1.1 Pro Ultra Image API Node ComfyUI Official Workflow Examples

# Flux 1.1 Pro Ultra Image API Node ComfyUI Official Workflow Examples

This guide covers how to use the Flux 1.1 Pro Ultra Image API node in ComfyUI

FLUX 1.1 Pro Ultra is a high-performance AI image generation tool by BlackForestLabs, featuring ultra-high resolution and efficient generation capabilities. It supports up to 4MP resolution (4x the standard version) while keeping single image generation time under 10 seconds - 2.5x faster than similar high-resolution models.

The tool offers two core modes:

- **Ultra Mode**: Designed for high-resolution needs, perfect for advertising and e-commerce where detail magnification is important. It accurately reflects prompts while maintaining generation speed.
- **Raw Mode**: Focuses on natural realism, optimizing skin tones, lighting, and landscape details. Reduces the “AI look” and is ideal for photography and realistic style creation.

We now support the Flux 1.1 Pro Ultra Image node in ComfyUI. This guide will cover:

- Flux 1.1 Pro Text-to-Image
- Flux 1.1 Pro Image-to-Image (Remix)

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#flux-1-1-pro-ultra-image-node-documentation) Flux 1.1 Pro Ultra Image Node Documentation

Check the following documentation for detailed node parameter settings:

- [Flux 1.1 Pro Ultra Image](http://docs.comfy.org/built-in-nodes/api-node/image/bfl/flux-pro-ultra-image)

## [​](http://docs.comfy.org#flux-1-1-%5Bpro%5D-text-to-image-tutorial) Flux 1.1 \[pro] Text-to-Image Tutorial

### [​](http://docs.comfy.org#1-download-workflow-file) 1. Download Workflow File

Download and drag the following file into ComfyUI to load the workflow:

### [​](http://docs.comfy.org#2-complete-the-workflow-steps) 2. Complete the Workflow Steps

Follow the numbered steps to complete the basic workflow:

1. (Optional) Modify the prompt in the `Flux 1.1 [pro] Ultra Image` node
2. (Optional) Set `raw` parameter to `false` for more realistic output
3. Click `Run` or use shortcut `Ctrl(cmd) + Enter` to generate the image
4. After the API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory

## [​](http://docs.comfy.org#flux-1-1%5Bpro%5D-image-to-image-tutorial) Flux 1.1\[pro] Image-to-Image Tutorial

When adding an `image_prompt` to the node input, the output will blend features from the input image (Remix). The `image_prompt_strength` value affects the blend ratio: higher values make the output more similar to the input image.

### [​](http://docs.comfy.org#1-download-workflow-file-2) 1. Download Workflow File

Download and drag the following file into ComfyUI, or right-click the purple node in the Text-to-Image workflow and set `mode` to `always` to enable `image_prompt` input:

We’ll use this image as input:

### [​](http://docs.comfy.org#2-complete-the-workflow-steps-2) 2. Complete the Workflow Steps

Follow these numbered steps:

1. Click **Upload** on the `Load Image` node to upload your input image
2. (Optional) Adjust `image_prompt_strength` in `Flux 1.1 [pro] Ultra Image` to change the blend ratio
3. Click `Run` or use shortcut `Ctrl(cmd) + Enter` to generate the image
4. After the API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory

Here’s a comparison of outputs with different `image_prompt_strength` values:

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/black-forest-labs/flux-1-1-pro-ultra-image.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/pricing)

[Stable Image UltraThis article will introduce how to use the Stability AI Stable Image Ultra API node's text-to-image and image-to-image capabilities in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-image-ultra)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Flux 1.1 Pro Ultra Image Node Documentation](http://docs.comfy.org#flux-1-1-pro-ultra-image-node-documentation)
- [Flux 1.1 \[pro\] Text-to-Image Tutorial](http://docs.comfy.org#flux-1-1-%5Bpro%5D-text-to-image-tutorial)
- [1. Download Workflow File](http://docs.comfy.org#1-download-workflow-file)
- [2. Complete the Workflow Steps](http://docs.comfy.org#2-complete-the-workflow-steps)
- [Flux 1.1\[pro\] Image-to-Image Tutorial](http://docs.comfy.org#flux-1-1%5Bpro%5D-image-to-image-tutorial)
- [1. Download Workflow File](http://docs.comfy.org#1-download-workflow-file-2)
- [2. Complete the Workflow Steps](http://docs.comfy.org#2-complete-the-workflow-steps-2)

<!-- END Development/tutorials/api-nodes/black-forest-labs/flux-1-1-pro-ultra-image.md -->


<!-- BEGIN Development/tutorials/api-nodes/faq.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
  - Luma
  - OpenAI
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

FAQs about API Nodes

# FAQs about API Nodes

Some FAQs you may encounter when using API Nodes.

This article addresses common questions regarding the use of API nodes.

Why can't I find the API nodes?

Please update your ComfyUI to the latest version (the latest commit or the latest [desktop version](https://www.comfy.org/download)). We may add more API support in the future, and the corresponding nodes will be updated, so please keep your ComfyUI up to date.

Please note that you need to distinguish between the nightly version and the release version. In some cases, the latest `release` version may not be updated in time compared to the `nightly` version. Since we are still iterating quickly, please ensure you are using the latest version when you cannot find the corresponding node.

Why can't I use / log in to the API Nodes?

API access requires that your current request is based on a secure network environment. The current requirements for API access are as follows:

- The local network only allows access from `127.0.0.1` or `localhost`, which may mean that you cannot use the API Nodes in a ComfyUI service started with the `--listen` parameter in a LAN environment.
- Able to access our API service normally (a proxy service may be required in some regions).
- Your account does not have enough [credits](http://docs.comfy.org/interface/credits).

Why can't I use API node even after logging in, or why does it keep asking me to log in while using?

- Currently, only `127.0.0.1` or `localhost` access is supported.
- Ensure your account has enough credits.

Can API Nodes be used for free?

API Nodes require credits for API calls to closed-source models, so they do not support free usage.

How to purchase credits?

Please refer to the following documentation:

1. [Comfy Account](http://docs.comfy.org/interface/user): Find the `User` section in the settings menu to log in.
2. [Credits](http://docs.comfy.org/interface/credits): After logging in, the settings interface will show the credits menu. You can purchase credits in `Settings` → `Credits`. We use a prepaid system, so there will be no unexpected charges.
3. Complete the payment through Stripe.
4. Check if the credits have been updated. If not, try restarting or refreshing the page.

Are unused credits refundable?

Currently, we do not support refunds for credits. If you believe there is an error resulting in unused balance due to technical issues, please [contact support](mailto:support@comfy.org).

Can credits go negative?

Credits cannot go negative, so please ensure you have enough credits before making the corresponding API calls.

Where can I check usage and expenses?

Please visit the [Credits](http://docs.comfy.org/interface/credits) menu after logging in to check the corresponding credits.

Is it possible to use my own API Key?

Currently, the API Nodes are still in the testing phase and do not support this feature yet, but we have considered adding it.

Do credits expire?

No, your credits do not expire.

Can credits be transferred or shared?

No, your credits cannot be transferred to other users and are limited to the currently logged-in account, but we do not restrict the number of devices that can log in.

Can I use the same account on different devices?

We do not limit the number of devices that can log in; you can use your account anywhere you want.

How can I request for my account or information to be deleted??

Email a request to [support@comfy.org](mailto:support@comfy.org) and we will delete your information

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/faq.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/overview)

[PricingThis article lists the pricing of the current API Nodes.  
\
Next](http://docs.comfy.org/tutorials/api-nodes/pricing)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

<!-- END Development/tutorials/api-nodes/faq.md -->


<!-- BEGIN Development/tutorials/api-nodes/ideogram/ideogram-v3.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
    
    - [Ideogram 3.0](http://docs.comfy.org/tutorials/api-nodes/ideogram/ideogram-v3)
  - Luma
  - OpenAI
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Ideogram 3.0 API Node Official Examples

# ComfyUI Ideogram 3.0 API Node Official Examples

This guide covers how to use the Ideogram 3.0 API node in ComfyUI

Ideogram 3.0 is a powerful text-to-image model by Ideogram, known for its photorealistic quality, accurate text rendering, and consistent style control.

The [Ideogram V3](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3) node currently supports two modes:

- Text-to-Image mode
- Image Editing mode (when both image and mask inputs are provided)

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#ideogram-3-0-node-documentation) Ideogram 3.0 Node Documentation

Check the following documentation for detailed node parameter settings:

- [Ideogram V3](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3)

## [​](http://docs.comfy.org#ideogram-3-0-api-node-text-to-image-mode) Ideogram 3.0 API Node Text-to-Image Mode

When using [Ideogram V3](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3) without image and mask inputs, the node operates in Text-to-Image mode.

### [​](http://docs.comfy.org#1-download-workflow-file) 1. Download Workflow File

Download and drag the following file into ComfyUI to load the workflow:

### [​](http://docs.comfy.org#2-complete-the-workflow-steps) 2. Complete the Workflow Steps

Follow the numbered steps to complete the basic workflow:

1. Enter your image description in the `prompt` field of the `Ideogram V3` node
2. Click `Run` or use shortcut `Ctrl(cmd) + Enter` to generate the image
3. After the API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory

## [​](http://docs.comfy.org#ideogram-3-0-api-node-image-editing-mode) Ideogram 3.0 API Node Image Editing Mode

\[To be updated]

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/ideogram/ideogram-v3.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image)

[Luma Text to ImageThis guide explains how to use the Luma Text to Image API node in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Ideogram 3.0 Node Documentation](http://docs.comfy.org#ideogram-3-0-node-documentation)
- [Ideogram 3.0 API Node Text-to-Image Mode](http://docs.comfy.org#ideogram-3-0-api-node-text-to-image-mode)
- [1. Download Workflow File](http://docs.comfy.org#1-download-workflow-file)
- [2. Complete the Workflow Steps](http://docs.comfy.org#2-complete-the-workflow-steps)
- [Ideogram 3.0 API Node Image Editing Mode](http://docs.comfy.org#ideogram-3-0-api-node-image-editing-mode)

<!-- END Development/tutorials/api-nodes/ideogram/ideogram-v3.md -->


<!-- BEGIN Development/tutorials/api-nodes/luma/luma-image-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
  - Luma
    
    - [Luma Text to Image](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image)
    - [Luma Image to Image](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-image)
    - [Luma Text to Video](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video)
    - [Luma Image to Video](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video)
  - OpenAI
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Image to Image API Node ComfyUI Official Example

# Luma Image to Image API Node ComfyUI Official Example

This guide covers how to use the Luma Image to Image API node in ComfyUI

The [Luma Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image) node allows you to modify existing images based on text prompts using Luma AI technology, while preserving certain features and structures from the original image.

In this guide, we’ll show you how to set up an image-to-image workflow using this node.

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#luma-image-to-image-node-documentation) Luma Image to Image Node Documentation

Check the following documentation for detailed node parameter settings:

[**Luma Image to Image Node Documentation**  
\
Luma Image to Image API Node Documentation](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image)

## [​](http://docs.comfy.org#luma-image-to-image-api-node-workflow) Luma Image to Image API Node Workflow

This feature works well for changing objects and shapes. However, it may not be ideal for color changes. We recommend using lower weight values, around 0.0 to 0.1.

### [​](http://docs.comfy.org#1-download-workflow-file) 1. Download Workflow File

Download and drag the following image into ComfyUI to load the workflow (workflow information is included in the image metadata):

Download this image to use as input:

### [​](http://docs.comfy.org#2-complete-the-workflow-steps) 2. Complete the Workflow Steps

Follow these numbered steps:

1. Click **Upload** on the `Load Image` node to upload your input image
2. (Optional) Modify the workflow prompts
3. (Optional) Adjust `image_weight` to change input image influence (lower values stay closer to original)
4. Click `Run` or use shortcut `Ctrl(cmd) + Enter` to generate the image
5. After API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory

### [​](http://docs.comfy.org#3-results-with-different-image-weight-values) 3. Results with Different `image_weight` Values

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/luma/luma-image-to-image.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image)

[Luma Text to VideoLearn how to use the Luma Text to Video API node in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Luma Image to Image Node Documentation](http://docs.comfy.org#luma-image-to-image-node-documentation)
- [Luma Image to Image API Node Workflow](http://docs.comfy.org#luma-image-to-image-api-node-workflow)
- [1. Download Workflow File](http://docs.comfy.org#1-download-workflow-file)
- [2. Complete the Workflow Steps](http://docs.comfy.org#2-complete-the-workflow-steps)
- [3. Results with Different image\_weight Values](http://docs.comfy.org#3-results-with-different-image-weight-values)

<!-- END Development/tutorials/api-nodes/luma/luma-image-to-image.md -->


<!-- BEGIN Development/tutorials/api-nodes/luma/luma-image-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
  - Luma
    
    - [Luma Text to Image](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image)
    - [Luma Image to Image](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-image)
    - [Luma Text to Video](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video)
    - [Luma Image to Video](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video)
  - OpenAI
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Image to Video API Node ComfyUI Official Example

# Luma Image to Video API Node ComfyUI Official Example

Learn how to use the Luma Image to Video API node in ComfyUI

The [Luma Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video) node allows you to convert static images into smooth, dynamic videos using Luma AI’s advanced technology, bringing life and motion to your images.

In this guide, we’ll show you how to set up a workflow for image-to-video conversion using this node.

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#luma-image-to-video-node-documentation) Luma Image to Video Node Documentation

Check out the following documentation to learn more about the node’s parameters:

[**Luma Image to Video Node Docs**  
\
Luma Image to Video API node documentation](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video)

[**Luma Concepts Node Docs**  
\
Luma Concepts API node documentation](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)

## [​](http://docs.comfy.org#image-to-video-workflow-with-luma-api-node) Image to Video Workflow with Luma API Node

The Luma Image to Video node requires at least one image input (`first_image` or `last_image`) along with text prompts to determine the video’s motion effects. In this guide, we’ve created an example using `first_image` and `luma_concepts` to showcase Luma AI’s video generation capabilities.

### [​](http://docs.comfy.org#1-download-the-workflow) 1. Download the Workflow

The workflow information is included in the metadata of the video below. Download and drag it into ComfyUI to load the workflow.

Download the following image to use as input:

### [​](http://docs.comfy.org#2-follow-the-workflow-steps) 2. Follow the Workflow Steps

Follow these basic steps to run the workflow:

1. Upload your input image in the `first_image` node
2. (Optional) Write prompts in the Luma Image to Video node to describe how you want the image animated
3. (Optional) Modify the `Luma Concepts` node to control camera movement for professional cinematography
4. Click `Run` or use `Ctrl(cmd) + Enter` to generate the video
5. Once the API returns results, view the generated video in the `Save Video` node. The video will also be saved to the `ComfyUI/output/` directory

### [​](http://docs.comfy.org#3-additional-notes) 3. Additional Notes

- **Image Input Requirements**: At least one of `first_image` or `last_image` is required, with a maximum of 1 image per input
- **Luma Concepts**: Controls camera movement for professional video effects
- **Seed Parameter**: Only determines if the node should rerun, doesn’t affect generation results
- **Enable Input Nodes**: Right-click on purple “Bypass” mode nodes and set “mode” to “always” to enable inputs
- **Model Selection**: Different video generation models have unique characteristics, adjustable via the model parameter
- **Resolution and Duration**: Adjust output video resolution and length using resolution and duration parameters

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/luma/luma-image-to-video.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video)

[GPT-Image-1Learn how to use the OpenAI GPT-Image-1 API node to generate images in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/openai/gpt-image-1)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Luma Image to Video Node Documentation](http://docs.comfy.org#luma-image-to-video-node-documentation)
- [Image to Video Workflow with Luma API Node](http://docs.comfy.org#image-to-video-workflow-with-luma-api-node)
- [1. Download the Workflow](http://docs.comfy.org#1-download-the-workflow)
- [2. Follow the Workflow Steps](http://docs.comfy.org#2-follow-the-workflow-steps)
- [3. Additional Notes](http://docs.comfy.org#3-additional-notes)

<!-- END Development/tutorials/api-nodes/luma/luma-image-to-video.md -->


<!-- BEGIN Development/tutorials/api-nodes/luma/luma-text-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
  - Luma
    
    - [Luma Text to Image](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image)
    - [Luma Image to Image](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-image)
    - [Luma Text to Video](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video)
    - [Luma Image to Video](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video)
  - OpenAI
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Text to Image API Node ComfyUI Official Example

# Luma Text to Image API Node ComfyUI Official Example

This guide explains how to use the Luma Text to Image API node in ComfyUI

The [Luma Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image) node allows you to generate high-quality images from text prompts using Luma AI’s advanced technology, capable of creating photorealistic content and artistic style images.

In this guide, we’ll show you how to set up workflows using this node for text-to-image generation.

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#luma-text-to-image-node-documentation) Luma Text to Image Node Documentation

You can refer to the following documentation for detailed parameter settings:

[**Luma Text to Image Node Documentation**  
\
Luma Text to Image API Node Documentation](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image)

[**Luma Reference Node Documentation**  
\
Luma Reference API Node Documentation](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-reference)

## [​](http://docs.comfy.org#luma-text-to-image-api-node-workflow) Luma Text to Image API Node Workflow

When the `Luma Text to Image` node is used without any image inputs, it functions as a text-to-image workflow. In this guide, we’ve created examples using `style_image` and `image_luma_ref` to showcase Luma AI’s excellent image processing capabilities.

### [​](http://docs.comfy.org#1-download-workflow-files) 1. Download Workflow Files

The workflow information is included in the metadata of the image below. Download and drag it into ComfyUI to load the workflow.

Please download these images for input:

### [​](http://docs.comfy.org#2-follow-steps-to-run-the-workflow) 2. Follow Steps to Run the Workflow

Follow the numbered steps in the image to complete the basic workflow:

1. Upload the reference image in the `Load image` node
2. Upload the style reference image in the `Load image (renamed to styleref)` node
3. (Optional) Modify the prompts in the `Luma Text to Image` node
4. (Optional) Adjust the `style_image_weight` to control the style reference image’s influence
5. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to generate the image
6. After the API returns results, view the generated image in the `Save Image` node. Images are saved to the `ComfyUI/output/` directory

### [​](http://docs.comfy.org#3-additional-notes) 3. Additional Notes

- The [node](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image) allows up to 4 reference images and character references simultaneously.
- To enable multiple image inputs, right-click on the purple “Bypassed” nodes and set their `mode` to `always`

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/luma/luma-text-to-image.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/ideogram/ideogram-v3)

[Luma Image to ImageThis guide covers how to use the Luma Image to Image API node in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Luma Text to Image Node Documentation](http://docs.comfy.org#luma-text-to-image-node-documentation)
- [Luma Text to Image API Node Workflow](http://docs.comfy.org#luma-text-to-image-api-node-workflow)
- [1. Download Workflow Files](http://docs.comfy.org#1-download-workflow-files)
- [2. Follow Steps to Run the Workflow](http://docs.comfy.org#2-follow-steps-to-run-the-workflow)
- [3. Additional Notes](http://docs.comfy.org#3-additional-notes)

<!-- END Development/tutorials/api-nodes/luma/luma-text-to-image.md -->


<!-- BEGIN Development/tutorials/api-nodes/luma/luma-text-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
  - Luma
    
    - [Luma Text to Image](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image)
    - [Luma Image to Image](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-image)
    - [Luma Text to Video](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video)
    - [Luma Image to Video](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video)
  - OpenAI
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Text to Video API Node ComfyUI Official Guide

# Luma Text to Video API Node ComfyUI Official Guide

Learn how to use the Luma Text to Video API node in ComfyUI

The [Luma Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video) node allows you to create high-quality, smooth videos from text descriptions using Luma AI’s innovative video generation technology.

In this guide, we’ll show you how to set up a text-to-video workflow using this node.

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#luma-text-to-video-node-documentation) Luma Text to Video Node Documentation

Check out the following documentation to learn more about the node parameters:

[**Luma Text to Video Node Docs**  
\
Documentation for the Luma Text to Video API node](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video)

[**Luma Concepts Node Docs**  
\
Documentation for the Luma Concepts API node](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)

## [​](http://docs.comfy.org#text-to-video-workflow-with-luma-api-node) Text to Video Workflow with Luma API Node

The Luma Text to Video node requires text prompts to describe the video content. In this guide, we’ve created examples using `prompt` and `luma_concepts` to showcase Luma AI’s excellent video generation capabilities.

### [​](http://docs.comfy.org#1-download-the-workflow) 1. Download the Workflow

The workflow information is included in the metadata of the video below. Download and drag it into ComfyUI to load the workflow.

### [​](http://docs.comfy.org#2-follow-the-steps) 2. Follow the Steps

Follow these basic steps to run the workflow:

1. Write your prompt in the `Luma Text to Video` node to describe the video content you want
2. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to generate the video
3. After the API returns results, you can view the generated video in the `Save Video` node. The video will also be saved to the `ComfyUI/output/` directory

> (Optional) Modify the `Luma Concepts` node to control camera movements and add professional cinematography

### [​](http://docs.comfy.org#3-additional-notes) 3. Additional Notes

- **Writing Prompts**: Describe scenes, subjects, actions, and mood in detail for best results
- **Luma Concepts**: Mainly used for camera control to create professional video shots
- **Seed Parameter**: Only determines if the node should rerun, doesn’t affect generation results
- **Model Selection**: Different video models have different features, adjustable via the model parameter
- **Resolution and Duration**: Adjust output video resolution and length using these parameters
- **Ray 1.6 Model Note**: Duration and resolution parameters don’t work when using the Ray 1.6 model

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/luma/luma-text-to-video.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-image)

[Luma Image to VideoLearn how to use the Luma Image to Video API node in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Luma Text to Video Node Documentation](http://docs.comfy.org#luma-text-to-video-node-documentation)
- [Text to Video Workflow with Luma API Node](http://docs.comfy.org#text-to-video-workflow-with-luma-api-node)
- [1. Download the Workflow](http://docs.comfy.org#1-download-the-workflow)
- [2. Follow the Steps](http://docs.comfy.org#2-follow-the-steps)
- [3. Additional Notes](http://docs.comfy.org#3-additional-notes)

<!-- END Development/tutorials/api-nodes/luma/luma-text-to-video.md -->


<!-- BEGIN Development/tutorials/api-nodes/openai/dall-e-2.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
  - Luma
  - OpenAI
    
    - [GPT-Image-1](http://docs.comfy.org/tutorials/api-nodes/openai/gpt-image-1)
    - [DALL·E 2](http://docs.comfy.org/tutorials/api-nodes/openai/dall-e-2)
    - [DALL·E 3](http://docs.comfy.org/tutorials/api-nodes/openai/dall-e-3)
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

OpenAI DALL·E 2 Node

# OpenAI DALL·E 2 Node

Learn how to use the OpenAI DALL·E 2 API node to generate images in ComfyUI

OpenAI DALL·E 2 is part of the ComfyUI API Nodes series, allowing users to generate images through OpenAI’s **DALL·E 2** model.

This node supports:

- Text-to-image generation
- Image editing functionality (inpainting through masks)

## [​](http://docs.comfy.org#node-overview) Node Overview

The **OpenAI DALL·E 2** node generates images synchronously through OpenAI’s image generation API. It receives text prompts and returns images that match the description.

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#parameter-description) Parameter Description

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterDescription`prompt`Text prompt describing the image content you want to generate

### [​](http://docs.comfy.org#widget-parameters) Widget Parameters

ParameterDescriptionOptions/RangeDefault Value`seed`Seed value for image generation (currently not implemented in the backend)0 to 2^31-10`size`Output image dimensions”256x256”, “512x512”, “1024x1024""1024x1024”`n`Number of images to generate1 to 81

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterDescriptionOptions/RangeDefault Value`image`Optional reference image for image editingAny image inputNone`mask`Optional mask for local inpaintingMask inputNone

## [​](http://docs.comfy.org#usage-method) Usage Method

## [​](http://docs.comfy.org#workflow-examples) Workflow Examples

This API node currently supports two workflows:

- Text to Image
- Inpainting

Image to Image workflow is not supported

### [​](http://docs.comfy.org#text-to-image-example) Text to Image Example

The image below contains a simple text-to-image workflow. Please download the corresponding image and drag it into ComfyUI to load the workflow.

The corresponding example is very simple

You only need to load the `OpenAI DALL·E 2` node, input the description of the image you want to generate in the `prompt` node, connect a `Save Image` node, and then run the workflow.

### [​](http://docs.comfy.org#inpainting-workflow) Inpainting Workflow

DALL·E 2 supports image editing functionality, allowing you to use a mask to specify the area to be replaced. Below is a simple inpainting workflow example:

#### [​](http://docs.comfy.org#1-workflow-file-download) 1. Workflow File Download

Download the image below and drag it into ComfyUI to load the corresponding workflow.

We will use the image below as input:

#### [​](http://docs.comfy.org#2-workflow-file-usage-instructions) 2. Workflow File Usage Instructions

Since this workflow is relatively simple, if you want to manually implement the corresponding workflow yourself, you can follow the steps below:

1. Use the `Load Image` node to load the image
2. Right-click on the load image node and select `MaskEditor`
3. In the mask editor, use the brush to draw the area you want to redraw
4. Connect the loaded image to the `image` input of the **OpenAI DALL·E 2** node
5. Connect the mask to the `mask` input of the **OpenAI DALL·E 2** node
6. Edit the prompt in the `prompt` node
7. Run the workflow

**Notes**

- If you want to use the image editing functionality, you must provide both an image and a mask (both are required)
- The mask and image must be the same size
- When inputting large images, the node will automatically resize the image to an appropriate size
- The URLs returned by the API are valid for a short period, please save the results promptly
- Each generation consumes credits, charged according to image size and quantity

## [​](http://docs.comfy.org#faqs) FAQs

Why can't I find the API nodes?

Please update your ComfyUI to the latest version (the latest commit or the latest [desktop version](https://www.comfy.org/download)). We may add more API support in the future, and the corresponding nodes will be updated, so please keep your ComfyUI up to date.

Please note that you need to distinguish between the nightly version and the release version. In some cases, the latest `release` version may not be updated in time compared to the `nightly` version. Since we are still iterating quickly, please ensure you are using the latest version when you cannot find the corresponding node.

Why can't I use / log in to the API Nodes?

API access requires that your current request is based on a secure network environment. The current requirements for API access are as follows:

- The local network only allows access from `127.0.0.1` or `localhost`, which may mean that you cannot use the API Nodes in a ComfyUI service started with the `--listen` parameter in a LAN environment.
- Able to access our API service normally (a proxy service may be required in some regions).
- Your account does not have enough [credits](http://docs.comfy.org/interface/credits).

Why can't I use API node even after logging in, or why does it keep asking me to log in while using?

- Currently, only `127.0.0.1` or `localhost` access is supported.
- Ensure your account has enough credits.

Can API Nodes be used for free?

API Nodes require credits for API calls to closed-source models, so they do not support free usage.

How to purchase credits?

Please refer to the following documentation:

1. [Comfy Account](http://docs.comfy.org/interface/user): Find the `User` section in the settings menu to log in.
2. [Credits](http://docs.comfy.org/interface/credits): After logging in, the settings interface will show the credits menu. You can purchase credits in `Settings` → `Credits`. We use a prepaid system, so there will be no unexpected charges.
3. Complete the payment through Stripe.
4. Check if the credits have been updated. If not, try restarting or refreshing the page.

Are unused credits refundable?

Currently, we do not support refunds for credits. If you believe there is an error resulting in unused balance due to technical issues, please [contact support](mailto:support@comfy.org).

Can credits go negative?

Credits cannot go negative, so please ensure you have enough credits before making the corresponding API calls.

Where can I check usage and expenses?

Please visit the [Credits](http://docs.comfy.org/interface/credits) menu after logging in to check the corresponding credits.

Is it possible to use my own API Key?

Currently, the API Nodes are still in the testing phase and do not support this feature yet, but we have considered adding it.

Do credits expire?

No, your credits do not expire.

Can credits be transferred or shared?

No, your credits cannot be transferred to other users and are limited to the currently logged-in account, but we do not restrict the number of devices that can log in.

Can I use the same account on different devices?

We do not limit the number of devices that can log in; you can use your account anywhere you want.

How can I request for my account or information to be deleted??

Email a request to [support@comfy.org](mailto:support@comfy.org) and we will delete your information

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/openai/dall-e-2.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/openai/gpt-image-1)

[DALL·E 3Learn how to use the OpenAI DALL·E 3 API node to generate images in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/openai/dall-e-3)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Overview](http://docs.comfy.org#node-overview)
- [Parameter Description](http://docs.comfy.org#parameter-description)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Widget Parameters](http://docs.comfy.org#widget-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Usage Method](http://docs.comfy.org#usage-method)
- [Workflow Examples](http://docs.comfy.org#workflow-examples)
- [Text to Image Example](http://docs.comfy.org#text-to-image-example)
- [Inpainting Workflow](http://docs.comfy.org#inpainting-workflow)
- [1. Workflow File Download](http://docs.comfy.org#1-workflow-file-download)
- [2. Workflow File Usage Instructions](http://docs.comfy.org#2-workflow-file-usage-instructions)
- [FAQs](http://docs.comfy.org#faqs)

<!-- END Development/tutorials/api-nodes/openai/dall-e-2.md -->


<!-- BEGIN Development/tutorials/api-nodes/openai/dall-e-3.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
  - Luma
  - OpenAI
    
    - [GPT-Image-1](http://docs.comfy.org/tutorials/api-nodes/openai/gpt-image-1)
    - [DALL·E 2](http://docs.comfy.org/tutorials/api-nodes/openai/dall-e-2)
    - [DALL·E 3](http://docs.comfy.org/tutorials/api-nodes/openai/dall-e-3)
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

OpenAI DALL·E 3 Node

# OpenAI DALL·E 3 Node

Learn how to use the OpenAI DALL·E 3 API node to generate images in ComfyUI

OpenAI DALL·E 3 is part of the ComfyUI API Nodes series, allowing users to generate images through OpenAI’s **DALL·E 3** model. This node supports text-to-image generation functionality.

## [​](http://docs.comfy.org#node-overview) Node Overview

DALL·E 3 is OpenAI’s latest image generation model, capable of creating detailed and high-quality images based on text prompts. Through this node in ComfyUI, you can directly access DALL·E 3’s generation capabilities without leaving the ComfyUI interface.

The **OpenAI DALL·E 3** node generates images synchronously through OpenAI’s image generation API. It receives text prompts and returns images that match the description.

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#parameter-details) Parameter Details

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDescriptionpromptTextText prompt for generating images. Supports multi-line input, can describe in detail the image content you want to generate.

### [​](http://docs.comfy.org#widget-parameters) Widget Parameters

ParameterTypeOptionsDefault ValueDescriptionseedInteger0-21474836470Random seed used to control the generation resultqualityOptionstandard, hdstandardImage quality setting. The “hd” option generates higher quality images but may require more computational resourcesstyleOptionnatural, vividnaturalImage style. “Vivid” tends to generate hyperrealistic and dramatic images, while “natural” produces more natural, less exaggerated imagessizeOption1024x1024, 1024x1792, 1792x10241024x1024Size of the generated image. You can choose square or rectangular images in different orientations

## [​](http://docs.comfy.org#usage-examples) Usage Examples

You can download the image below and drag it into ComfyUI to load the corresponding workflow

Since the corresponding workflow is very simple, you can also directly add the **OpenAI DALL·E 3** node in ComfyUI, input the description of the image you want to generate, and then run the workflow

1. Add the **OpenAI DALL·E 3** node in ComfyUI
2. Enter the description of the image you want to generate in the prompt text box
3. Adjust optional parameters as needed (quality, style, size, etc.)
4. Run the workflow to generate the image

## [​](http://docs.comfy.org#faqs) FAQs

Why can't I find the API nodes?

Please update your ComfyUI to the latest version (the latest commit or the latest [desktop version](https://www.comfy.org/download)). We may add more API support in the future, and the corresponding nodes will be updated, so please keep your ComfyUI up to date.

Please note that you need to distinguish between the nightly version and the release version. In some cases, the latest `release` version may not be updated in time compared to the `nightly` version. Since we are still iterating quickly, please ensure you are using the latest version when you cannot find the corresponding node.

Why can't I use / log in to the API Nodes?

API access requires that your current request is based on a secure network environment. The current requirements for API access are as follows:

- The local network only allows access from `127.0.0.1` or `localhost`, which may mean that you cannot use the API Nodes in a ComfyUI service started with the `--listen` parameter in a LAN environment.
- Able to access our API service normally (a proxy service may be required in some regions).
- Your account does not have enough [credits](http://docs.comfy.org/interface/credits).

Why can't I use API node even after logging in, or why does it keep asking me to log in while using?

- Currently, only `127.0.0.1` or `localhost` access is supported.
- Ensure your account has enough credits.

Can API Nodes be used for free?

API Nodes require credits for API calls to closed-source models, so they do not support free usage.

How to purchase credits?

Please refer to the following documentation:

1. [Comfy Account](http://docs.comfy.org/interface/user): Find the `User` section in the settings menu to log in.
2. [Credits](http://docs.comfy.org/interface/credits): After logging in, the settings interface will show the credits menu. You can purchase credits in `Settings` → `Credits`. We use a prepaid system, so there will be no unexpected charges.
3. Complete the payment through Stripe.
4. Check if the credits have been updated. If not, try restarting or refreshing the page.

Are unused credits refundable?

Currently, we do not support refunds for credits. If you believe there is an error resulting in unused balance due to technical issues, please [contact support](mailto:support@comfy.org).

Can credits go negative?

Credits cannot go negative, so please ensure you have enough credits before making the corresponding API calls.

Where can I check usage and expenses?

Please visit the [Credits](http://docs.comfy.org/interface/credits) menu after logging in to check the corresponding credits.

Is it possible to use my own API Key?

Currently, the API Nodes are still in the testing phase and do not support this feature yet, but we have considered adding it.

Do credits expire?

No, your credits do not expire.

Can credits be transferred or shared?

No, your credits cannot be transferred to other users and are limited to the currently logged-in account, but we do not restrict the number of devices that can log in.

Can I use the same account on different devices?

We do not limit the number of devices that can log in; you can use your account anywhere you want.

How can I request for my account or information to be deleted??

Email a request to [support@comfy.org](mailto:support@comfy.org) and we will delete your information

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/openai/dall-e-3.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/openai/dall-e-2)

[Recraft Text to ImageLearn how to use the Recraft Text to Image API node in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Overview](http://docs.comfy.org#node-overview)
- [Parameter Details](http://docs.comfy.org#parameter-details)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Widget Parameters](http://docs.comfy.org#widget-parameters)
- [Usage Examples](http://docs.comfy.org#usage-examples)
- [FAQs](http://docs.comfy.org#faqs)

<!-- END Development/tutorials/api-nodes/openai/dall-e-3.md -->


<!-- BEGIN Development/tutorials/api-nodes/openai/gpt-image-1.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
  - Luma
  - OpenAI
    
    - [GPT-Image-1](http://docs.comfy.org/tutorials/api-nodes/openai/gpt-image-1)
    - [DALL·E 2](http://docs.comfy.org/tutorials/api-nodes/openai/dall-e-2)
    - [DALL·E 3](http://docs.comfy.org/tutorials/api-nodes/openai/dall-e-3)
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

OpenAI GPT-Image-1 Node

# OpenAI GPT-Image-1 Node

Learn how to use the OpenAI GPT-Image-1 API node to generate images in ComfyUI

OpenAI GPT-Image-1 is part of the ComfyUI API nodes series that allows users to generate images through OpenAI’s **GPT-Image-1** model. This is the same model used for image generation in ChatGPT 4o.

This node supports:

- Text-to-image generation
- Image editing functionality (inpainting through masks)

## [​](http://docs.comfy.org#node-overview) Node Overview

The **OpenAI GPT-Image-1** node synchronously generates images through OpenAI’s image generation API. It receives text prompts and returns images matching the description. GPT-Image-1 is OpenAI’s most advanced image generation model currently available, capable of creating highly detailed and realistic images.

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#parameter-description) Parameter Description

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDescription`prompt`TextText prompt describing the image content you want to generate

### [​](http://docs.comfy.org#widget-parameters) Widget Parameters

ParameterTypeOptionsDefaultDescription`seed`Integer0-21474836470Random seed used to control generation results`quality`Optionlow, medium, highlowImage quality setting, affects cost and generation time`background`Optionopaque, transparentopaqueWhether the returned image has a background`size`Optionauto, 1024x1024, 1024x1536, 1536x1024autoSize of the generated image`n`Integer1-81Number of images to generate

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeOptionsDefaultDescription`image`ImageAny image inputNoneOptional reference image for image editing`mask`MaskMask inputNoneOptional mask for inpainting (white areas will be replaced)

## [​](http://docs.comfy.org#usage-examples) Usage Examples

### [​](http://docs.comfy.org#text-to-image-example) Text-to-Image Example

The image below contains a simple text-to-image workflow. Please download the image and drag it into ComfyUI to load the corresponding workflow.

The corresponding workflow is very simple:

You only need to load the `OpenAI GPT-Image-1` node, input the description of the image you want to generate in the `prompt` node, connect a `Save Image` node, and then run the workflow.

### [​](http://docs.comfy.org#image-to-image-example) Image-to-Image Example

The image below contains a simple image-to-image workflow. Please download the image and drag it into ComfyUI to load the corresponding workflow.

We will use the image below as input:

In this workflow, we use the `OpenAI GPT-Image-1` node to generate images and the `Load Image` node to load the input image, then connect it to the `image` input of the `OpenAI GPT-Image-1` node.

### [​](http://docs.comfy.org#multiple-image-input-example) Multiple Image Input Example

Please download the image below and drag it into ComfyUI to load the corresponding workflow.

Use the hat image below as an additional input image.

The corresponding workflow is shown in the image below:

The `Batch Images` node is used to load multiple images into the `OpenAI GPT-Image-1` node.

### [​](http://docs.comfy.org#inpainting-workflow) Inpainting Workflow

GPT-Image-1 also supports image editing functionality, allowing you to specify areas to replace using a mask. Below is a simple inpainting workflow example:

Download the image below and drag it into ComfyUI to load the corresponding workflow. We will continue to use the input image from the image-to-image workflow section.

The corresponding workflow is shown in the image

Compared to the image-to-image workflow, we use the MaskEditor in the `Load Image` node through the right-click menu to draw a mask, then connect it to the `mask` input of the `OpenAI GPT-Image-1` node to complete the workflow.

**Notes**

- The mask and image must be the same size
- When inputting large images, the node will automatically resize the image to an appropriate size

## [​](http://docs.comfy.org#faqs) FAQs

Why can't I find the API nodes?

Please update your ComfyUI to the latest version (the latest commit or the latest [desktop version](https://www.comfy.org/download)). We may add more API support in the future, and the corresponding nodes will be updated, so please keep your ComfyUI up to date.

Please note that you need to distinguish between the nightly version and the release version. In some cases, the latest `release` version may not be updated in time compared to the `nightly` version. Since we are still iterating quickly, please ensure you are using the latest version when you cannot find the corresponding node.

Why can't I use / log in to the API Nodes?

API access requires that your current request is based on a secure network environment. The current requirements for API access are as follows:

- The local network only allows access from `127.0.0.1` or `localhost`, which may mean that you cannot use the API Nodes in a ComfyUI service started with the `--listen` parameter in a LAN environment.
- Able to access our API service normally (a proxy service may be required in some regions).
- Your account does not have enough [credits](http://docs.comfy.org/interface/credits).

Why can't I use API node even after logging in, or why does it keep asking me to log in while using?

- Currently, only `127.0.0.1` or `localhost` access is supported.
- Ensure your account has enough credits.

Can API Nodes be used for free?

API Nodes require credits for API calls to closed-source models, so they do not support free usage.

How to purchase credits?

Please refer to the following documentation:

1. [Comfy Account](http://docs.comfy.org/interface/user): Find the `User` section in the settings menu to log in.
2. [Credits](http://docs.comfy.org/interface/credits): After logging in, the settings interface will show the credits menu. You can purchase credits in `Settings` → `Credits`. We use a prepaid system, so there will be no unexpected charges.
3. Complete the payment through Stripe.
4. Check if the credits have been updated. If not, try restarting or refreshing the page.

Are unused credits refundable?

Currently, we do not support refunds for credits. If you believe there is an error resulting in unused balance due to technical issues, please [contact support](mailto:support@comfy.org).

Can credits go negative?

Credits cannot go negative, so please ensure you have enough credits before making the corresponding API calls.

Where can I check usage and expenses?

Please visit the [Credits](http://docs.comfy.org/interface/credits) menu after logging in to check the corresponding credits.

Is it possible to use my own API Key?

Currently, the API Nodes are still in the testing phase and do not support this feature yet, but we have considered adding it.

Do credits expire?

No, your credits do not expire.

Can credits be transferred or shared?

No, your credits cannot be transferred to other users and are limited to the currently logged-in account, but we do not restrict the number of devices that can log in.

Can I use the same account on different devices?

We do not limit the number of devices that can log in; you can use your account anywhere you want.

How can I request for my account or information to be deleted??

Email a request to [support@comfy.org](mailto:support@comfy.org) and we will delete your information

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/openai/gpt-image-1.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video)

[DALL·E 2Learn how to use the OpenAI DALL·E 2 API node to generate images in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/openai/dall-e-2)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Overview](http://docs.comfy.org#node-overview)
- [Parameter Description](http://docs.comfy.org#parameter-description)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Widget Parameters](http://docs.comfy.org#widget-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Usage Examples](http://docs.comfy.org#usage-examples)
- [Text-to-Image Example](http://docs.comfy.org#text-to-image-example)
- [Image-to-Image Example](http://docs.comfy.org#image-to-image-example)
- [Multiple Image Input Example](http://docs.comfy.org#multiple-image-input-example)
- [Inpainting Workflow](http://docs.comfy.org#inpainting-workflow)
- [FAQs](http://docs.comfy.org#faqs)

<!-- END Development/tutorials/api-nodes/openai/gpt-image-1.md -->


<!-- BEGIN Development/tutorials/api-nodes/overview.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
  - Luma
  - OpenAI
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

API Nodes

# API Nodes

In this article, we will introduce ComfyUI’s API Nodes and related information.

API Nodes are ComfyUI’s new way of calling closed-source models through API requests, providing ComfyUI users with access to external state-of-the-art AI models without complex API key setup.

## [​](http://docs.comfy.org#what-are-api-nodes%3F) What are API Nodes?

API Nodes are a set of special nodes that connect to external API services, allowing you to use closed-source or third-party hosted AI models directly in your ComfyUI workflows. These nodes are designed to seamlessly integrate the capabilities of external models while maintaining the open-source nature of ComfyUI’s core.

Currently supported models include:

- Black Forest Labs Flux 1.1\[pro] Ultra, Flux .1\[pro]
- Kling 2.0, 1.6, 1.5 &amp; Various Effects
- Luma Photon, Ray2, Ray1.6
- MiniMax Text-to-Video, Image-to-Video
- PixVerse V4 &amp; Effects
- Recraft V3, V2 &amp; Various Tools
- Stability AI Stable Image Ultra, Stable Diffusion 3.5 Large
- Google Veo2
- Ideogram V3, V2, V1
- OpenAI GPT4o image
- Pika 2.2

## [​](http://docs.comfy.org#prerequisites-for-using-api-nodes) Prerequisites for Using API Nodes

To use API Nodes, the following requirements must be met:

### [​](http://docs.comfy.org#1-comfyui-version-requirements) 1. ComfyUI Version Requirements

Please update your ComfyUI to the latest version, as we may add more API support in the future, and corresponding nodes will be updated, so please keep your ComfyUI up to date.

Please note the distinction between nightly and release versions. We recommend using the `nightly` version (which is the latest code commit), as the release version may not be updated in a timely manner. This refers to the development version and the stable version, and since we are still rapidly iterating, this document may not be updated promptly, so please pay attention to the version differences.

### [​](http://docs.comfy.org#2-network-environment-requirements) 2. Network Environment Requirements

API access requires that your current requests are based on a secure network environment. The current requirements for API access are as follows:

- Local networks only allow access from `127.0.0.1`. We do not support accessing via LAN IPs without `https` as it is insecure. This may mean that you cannot use API Nodes in a ComfyUI service launched with the `--listen` parameter in a LAN environment.
- You must be able to access our API services normally (in some regions, you may need to use a proxy service).

Accessing in an insecure context poses significant risks, which may result in the following consequences:

1. Authentication may be stolen, leading to the leakage of your account information.
2. Your account may be maliciously used, resulting in financial losses.

Even if we open this restriction in the future, we strongly advise against accessing API services through insecure network requests due to the high risks involved.

### [​](http://docs.comfy.org#3-account-and-credits-requirements) 3. Account and Credits Requirements

You need to be logged into your ComfyUI with a [Comfy account](http://docs.comfy.org/zh-CN/interface/user) and have a credit balance of [credits](http://docs.comfy.org/zh-CN/interface/credits) greater than 0.

Please refer to the corresponding documentation for account and credits to ensure this requirement:

- [Comfy account](http://docs.comfy.org/zh-CN/interface/user): Find the `User` section in the settings menu to log in.
- [Credits](http://docs.comfy.org/zh-CN/interface/credits): After logging in, the settings interface will show a credits menu where you can purchase credits. We use a prepaid system, so there will be no unexpected charges.

### [​](http://docs.comfy.org#4-using-the-corresponding-nodes) 4. Using the Corresponding Nodes

**Add to Workflow**: Add the API node to your workflow just like you would with other nodes. **Run**: Set the parameters and then run the workflow.

## [​](http://docs.comfy.org#log-in-with-api-key-on-non-whitelisted-websites) Log in with API Key on non-whitelisted websites

Currently, we have set up a whitelist to restrict the websites where you can log in to your ComfyUI account. If you need to log in to your ComfyUI account on some non-whitelisted websites, please refer to the account management section to learn how to log in using an API Key. In this case, the corresponding website does not need to be on our whitelist.

[**Account Management**  
\
Learn how to log in with ComfyUI API Key](http://docs.comfy.org/interface/user#logging-in-with-an-api-key)

## [​](http://docs.comfy.org#advantages-of-api-nodes) Advantages of API Nodes

API Nodes provide several important advantages for ComfyUI users:

- **Access to closed-source models**: Use state-of-the-art AI models without having to deploy them yourself
- **Seamless integration**: API nodes are fully compatible with other ComfyUI nodes and can be combined to create complex workflows
- **Simplified experience**: No need to manage API keys or handle complex API requests
- **Controlled costs**: The prepaid system ensures you have complete control over your spending with no unexpected charges

## [​](http://docs.comfy.org#pricing) Pricing

[**API Node Pricing**  
\
Please refer to the pricing page for the corresponding API pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)

## [​](http://docs.comfy.org#about-open-source-and-opt-in) About Open Source and Opt-in

It’s important to note that **API Nodes are completely optional**. ComfyUI will always remain fully open-source and free for local users. API nodes are designed as an “opt-in” feature, providing convenience for those who want access to external SOTA (state-of-the-art) models.

## [​](http://docs.comfy.org#use-cases) Use Cases

A powerful application of API Nodes is combining the output of external models with local nodes. For example:

- Using GPT-Image-1 to generate a base image, then transforming it into video with a local `wan` node
- Combining externally generated images with local upscaling or style transfer nodes
- Creating hybrid workflows that leverage the advantages of both closed-source and open-source models

This flexibility makes ComfyUI a truly universal generative AI interface, integrating various AI capabilities into a unified workflow, opening up more possibilities

## [​](http://docs.comfy.org#faqs) FAQs

Why can't I find the API nodes?

Please update your ComfyUI to the latest version (the latest commit or the latest [desktop version](https://www.comfy.org/download)). We may add more API support in the future, and the corresponding nodes will be updated, so please keep your ComfyUI up to date.

Please note that you need to distinguish between the nightly version and the release version. In some cases, the latest `release` version may not be updated in time compared to the `nightly` version. Since we are still iterating quickly, please ensure you are using the latest version when you cannot find the corresponding node.

Why can't I use / log in to the API Nodes?

API access requires that your current request is based on a secure network environment. The current requirements for API access are as follows:

- The local network only allows access from `127.0.0.1` or `localhost`, which may mean that you cannot use the API Nodes in a ComfyUI service started with the `--listen` parameter in a LAN environment.
- Able to access our API service normally (a proxy service may be required in some regions).
- Your account does not have enough [credits](http://docs.comfy.org/interface/credits).

Why can't I use API node even after logging in, or why does it keep asking me to log in while using?

- Currently, only `127.0.0.1` or `localhost` access is supported.
- Ensure your account has enough credits.

Can API Nodes be used for free?

API Nodes require credits for API calls to closed-source models, so they do not support free usage.

How to purchase credits?

Please refer to the following documentation:

1. [Comfy Account](http://docs.comfy.org/interface/user): Find the `User` section in the settings menu to log in.
2. [Credits](http://docs.comfy.org/interface/credits): After logging in, the settings interface will show the credits menu. You can purchase credits in `Settings` → `Credits`. We use a prepaid system, so there will be no unexpected charges.
3. Complete the payment through Stripe.
4. Check if the credits have been updated. If not, try restarting or refreshing the page.

Are unused credits refundable?

Currently, we do not support refunds for credits. If you believe there is an error resulting in unused balance due to technical issues, please [contact support](mailto:support@comfy.org).

Can credits go negative?

Credits cannot go negative, so please ensure you have enough credits before making the corresponding API calls.

Where can I check usage and expenses?

Please visit the [Credits](http://docs.comfy.org/interface/credits) menu after logging in to check the corresponding credits.

Is it possible to use my own API Key?

Currently, the API Nodes are still in the testing phase and do not support this feature yet, but we have considered adding it.

Do credits expire?

No, your credits do not expire.

Can credits be transferred or shared?

No, your credits cannot be transferred to other users and are limited to the currently logged-in account, but we do not restrict the number of devices that can log in.

Can I use the same account on different devices?

We do not limit the number of devices that can log in; you can use your account anywhere you want.

How can I request for my account or information to be deleted??

Email a request to [support@comfy.org](mailto:support@comfy.org) and we will delete your information

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/overview.mdx)

[Previous](http://docs.comfy.org/tutorials/audio/ace-step/ace-step-v1)

[FAQsSome FAQs you may encounter when using API Nodes.  
\
Next](http://docs.comfy.org/tutorials/api-nodes/faq)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [What are API Nodes?](http://docs.comfy.org#what-are-api-nodes%3F)
- [Prerequisites for Using API Nodes](http://docs.comfy.org#prerequisites-for-using-api-nodes)
- [1. ComfyUI Version Requirements](http://docs.comfy.org#1-comfyui-version-requirements)
- [2. Network Environment Requirements](http://docs.comfy.org#2-network-environment-requirements)
- [3. Account and Credits Requirements](http://docs.comfy.org#3-account-and-credits-requirements)
- [4. Using the Corresponding Nodes](http://docs.comfy.org#4-using-the-corresponding-nodes)
- [Log in with API Key on non-whitelisted websites](http://docs.comfy.org#log-in-with-api-key-on-non-whitelisted-websites)
- [Advantages of API Nodes](http://docs.comfy.org#advantages-of-api-nodes)
- [Pricing](http://docs.comfy.org#pricing)
- [About Open Source and Opt-in](http://docs.comfy.org#about-open-source-and-opt-in)
- [Use Cases](http://docs.comfy.org#use-cases)
- [FAQs](http://docs.comfy.org#faqs)

<!-- END Development/tutorials/api-nodes/overview.md -->


<!-- BEGIN Development/tutorials/api-nodes/pricing.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
  - Luma
  - OpenAI
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Pricing

# Pricing

This article lists the pricing of the current API Nodes.

The following table lists the pricing of the current API Nodes.

APISpecsPriceOpenAIdall-e-2, 1024×1024$0.02OpenAIdall-e-2, 256×256$0.016OpenAIdall-e-2, 512×512$0.018OpenAIdall-e-3, 1024×1024, hd$0.08OpenAIdall-e-3, 1024×1792, hd$0.12OpenAIdall-e-3, 1024×1024, standard$0.04OpenAIdall-e-3, 1024×1792, standard$0.08OpenAIgpt-image-1, input image tokens$10 / 1M tokensOpenAIgpt-image-1, input text tokens$5 / 1M tokensOpenAIgpt-image-1, output tokens$40 / 1M tokensBFLflux-dev$0.025BFLflux-pro-1.1$0.04BFLflux-pro-1.1-ultra$0.06BFLflux-pro-1.1-pro$0.05BFLflux tools (edit, fill, expand, canny)$0.05Veoveo-2.0-generate-001$0.5 / secondMiniMaxI2V-01-Director$0.43MiniMaxI2V-01-live$0.43MiniMaxI2V-01$0.43MiniMaxS2V-01 (not enabled yet)$0.65MiniMaxT2V-01-Director$0.43MiniMaxT2V-01$0.43IdeogramV2\_edit$0.08IdeogramV2\_TURBO\_edit$0.05IdeogramV1\_generate$0.06IdeogramV1\_TURBO\_generate$0.02IdeogramV2A\_generate$0.04IdeogramV2A\_TURBO\_generate$0.025IdeogramV2\_generate$0.08IdeogramV2\_TURBO\_generate$0.05IdeogramV2\_reframe$0.08IdeogramV2\_TURBO\_reframe$0.05IdeogramV1\_remix$0.02IdeogramV2A\_remix$0.04IdeogramV2A\_TURBO\_remix$0.025IdeogramV2\_remix$0.08IdeogramV2\_TURBO\_remix$0.05IdeogramV3\_edit\_BALANCED$0.06IdeogramV3\_edit\_QUALITY$0.09IdeogramV3\_edit\_TURBO$0.03IdeogramV3\_generate\_BALANCED$0.06IdeogramV3\_generate\_QUALITY$0.09IdeogramV3\_generate\_TURBO$0.03IdeogramV3\_reframe\_QUALITY$0.09IdeogramV3\_reframe\_BALANCED$0.06IdeogramV3\_reframe\_TURBO$0.03IdeogramV3\_remix\_BALANCED$0.06IdeogramV3\_remix\_QUALITY$0.09IdeogramV3\_remix\_TURBO$0.03IdeogramV3\_replace-background\_QUALITY$0.09IdeogramV3\_replace-background\_TURBO$0.03IdeogramV3\_replace-background\_BALANCED$0.06Runwaygen3a\_turbo$0.05 / sRunwaygen4\_turbo$0.05 / sRecraftrecraftv2\_digital\_illustration$0.022Recraftrecraftv2\_icon$0.044Recraftrecraftv2\_logo\_raster$0.022Recraftrecraftv2\_realistic\_image$0.022Recraftrecraftv2\_vector\_illustration$0.044Recraftrecraftv3\_digital\_illustration$0.04Recraftrecraftv3\_logo\_raster  $0.04Recraftrecraftv3\_realistic\_image$0.04Recraftrecraftv3\_vector\_illustration$0.08Recraftv1/images/removeBackground$0.01Recraftv1/images/imageToImage0.04−0.04 - 0.04−0.08Recraftv1/images/inpaint0.04−0.04 - 0.04−0.08Recraftv1/images/replaceBackground0.04−0.04 - 0.04−0.08Recraftv1/images/vectorize$0.01Recraftv1/images/creativeUpscale$0.25Recraftv1/images/crispUpscale$0.004Lumaray-1-6$0.0032 / 1M pixelsLumaray-2$0.0064 / 1M pixelsLumarayflash-2$0.0022 / 1M pixelsLumaText to Image$0.0019 / 1M pixelsLumaImage to Image$0.0073 / 1M pixelsKlingpro\_kling-v5$0.49Klingpro\_kling-v1-6$0.49Klingpro\_kling-v1$0.49Klingpro\_kling-v2-master$1.4Klingstd\_kling-v1-5$0.28Klingstd\_kling-v1-6$0.28Klingstd\_kling-v1$0.14Klingstd\_kling-v2-master$1.4Klingtext to image (kling-v1)$0.0035Klingimage to image (kling-v1)$0.0035Klingtext to image (kling-v1-5)$0.014Klingtext to image (kling-v2)$0.014Klingimage to image (kling-v1-5)$0.028KlingVirtual Try On (kolors v1, v1-5)0.07Klingvideo extension$0.28Klingvideo effects (hug, kiss, heart\_gesture)Priced the same as t2v based on mode, model, and duration.Klingvideo effects (fuzzyfuzzy/squish/expansion)$0.28Klingvideo effects (dizzydizzy/bloombloom)$0.49Klinglip sync (5s)$0.07Klinglip sync (10s)$0.14PixVerse8s, v4\_360p\_normal i2v/i2v/template$0.9PixVerse8s, v4\_540p\_normal$0.9PixVerse8s, v4\_720p\_normal$1.2PixVerse5s, v4\_360p\_fast$0.9PixVerse5s, v4\_540p\_fast$0.9PixVerse5s, v4\_720p\_fast$1.2PixVerse5s, v4\_1080p\_fast$1.2PixVerse5s, v4\_360p\_normal$0.45PixVerse5s, v4\_540p\_normal$0.45PixVerse5s, v4\_720p\_normal$0.6Stability AIv2beta/stable-image/generate/ultra$0.08Stability AIv2beta/stable-image/generate/core0.03Stability AIv2beta/stable-image/generate/sd3.50.035−0.035 - 0.035−0.065Stability AIv2beta/stable-image/upscale0.01−0.01 - 0.01−0.25Pika5s\_generate/v2.2/i2v\_1080p$0.45Pika5s\_generate/v2.2/i2v\_720p$0.2Pika5s\_generate/v2.2/pikaframes\_1080p$0.3Pika5s\_generate/v2.2/pikaframes\_720p$0.2Pika5s\_generate/v2.2/pikascenes\_1080p$0.5Pika5s\_generate/v2.2/pikascenes\_720p$0.3Pika5s\_generate/v2.2/t2v\_1080p$0.45Pika5s\_generate/v2.2/t2v\_720p$0.2Pika10s\_generate/v2.2/i2v\_1080p$1Pika10s\_generate/v2.2/i2v\_720p$0.6Pika10s\_generate/v2.2/pikaframes\_1080p$1Pika10s\_generate/v2.2/pikaframes\_720p$0.25Pika10s\_generate/v2.2/pikascenes\_1080p$1.5Pika10s\_generate/v2.2/pikascenes\_720p$0.4Pika10s\_generate/v2.2/t2v\_1080p$1Pika10s\_generate/v2.2/t2v\_720p$0.6Pika5s\_pikaffects\_720p$0.45Pika5s\_pikadditions\_720p$0.30Pika5s\_pikaswaps\_720p$0.30

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/pricing.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/faq)

[Flux 1.1 Pro Ultra ImageThis guide covers how to use the Flux 1.1 Pro Ultra Image API node in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/black-forest-labs/flux-1-1-pro-ultra-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

<!-- END Development/tutorials/api-nodes/pricing.md -->


<!-- BEGIN Development/tutorials/api-nodes/recraft/recraft-text-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
  - Ideogram
  - Luma
  - OpenAI
  - Recraft
    
    - [Recraft Text to Image](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Text to Image API Node ComfyUI Official Example

# Recraft Text to Image API Node ComfyUI Official Example

Learn how to use the Recraft Text to Image API node in ComfyUI

The [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image) node allows you to create high-quality images in various styles using Recraft AI’s image generation technology based on text descriptions.

In this guide, we’ll show you how to set up a text-to-image workflow using this node.

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#recraft-text-to-image-api-node-workflow) Recraft Text to Image API Node Workflow

### [​](http://docs.comfy.org#1-download-the-workflow-file) 1. Download the Workflow File

The workflow information is included in the metadata of the image below. Download and drag it into ComfyUI to load the workflow.

### [​](http://docs.comfy.org#2-follow-the-steps-to-run-the-workflow) 2. Follow the Steps to Run the Workflow

Follow these numbered steps to run the basic workflow:

1. (Optional) Change the `Recraft Color RGB` in the `Color` node to your desired color
2. (Optional) Modify the `Recraft Style` node to control the visual style, such as digital art, realistic photo, or logo design. This group includes other style nodes you can enable as needed
3. (Optional) Edit the `prompt` parameter in the `Recraft Text to Image` node. You can also change the `size` parameter
4. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to generate the image
5. After the API returns the result, you can view the generated image in the `Save Image` node. The image will also be saved to the `ComfyUI/output/` directory

> (Optional) We’ve included a **Convert to SVG** group in the workflow. Since the `Recraft Vectorize Image` node in this group consumes additional credits, enable it only when you need to convert the generated image to SVG format

### [​](http://docs.comfy.org#3-additional-notes) 3. Additional Notes

- **Recraft Style**: Offers various preset styles like realistic photos, digital art, and logo designs
- **Seed Parameter**: Only used to determine if the node should run again, the actual generation result is not affected by the seed value

## [​](http://docs.comfy.org#related-node-documentation) Related Node Documentation

Check the following documentation for detailed parameter settings of the nodes

[**Recraft Text to Image Node Documentation**  
\
Documentation for the Recraft Text to Image API node](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)

[**Recraft Style Node Documentation**  
\
Documentation for the Recraft Style - Realistic Image API node](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)

[**Recraft Controls Node Documentation**  
\
Documentation for the Recraft Controls API node](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/recraft/recraft-text-to-image.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/openai/dall-e-3)

[Contributing  
\
Next](http://docs.comfy.org/community/contributing)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Recraft Text to Image API Node Workflow](http://docs.comfy.org#recraft-text-to-image-api-node-workflow)
- [1. Download the Workflow File](http://docs.comfy.org#1-download-the-workflow-file)
- [2. Follow the Steps to Run the Workflow](http://docs.comfy.org#2-follow-the-steps-to-run-the-workflow)
- [3. Additional Notes](http://docs.comfy.org#3-additional-notes)
- [Related Node Documentation](http://docs.comfy.org#related-node-documentation)

<!-- END Development/tutorials/api-nodes/recraft/recraft-text-to-image.md -->


<!-- BEGIN Development/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
    
    - [Stable Image Ultra](http://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-image-ultra)
    - [Stable Diffusion 3.5 Image](http://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image)
  - Ideogram
  - Luma
  - OpenAI
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Stability AI Stable Diffusion 3.5 API Node ComfyUI Official Example

# Stability AI Stable Diffusion 3.5 API Node ComfyUI Official Example

This article will introduce how to use Stability AI Stable Diffusion 3.5 API node’s text-to-image and image-to-image capabilities in ComfyUI

The [Stability AI Stable Diffusion 3.5 Image](http://docs.comfy.org/zh-CN/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image) node allows you to use Stability AI’s Stable Diffusion 3.5 model to create high-quality, detail-rich image content through text prompts or reference images.

In this guide, we will show you how to set up workflows for both text-to-image and image-to-image generation using this node.

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#stability-ai-stable-diffusion-3-5-text-to-image-workflow) Stability AI Stable Diffusion 3.5 Text-to-Image Workflow

### [​](http://docs.comfy.org#1-workflow-file-download) 1. Workflow File Download

The image below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow.

### [​](http://docs.comfy.org#2-complete-the-workflow-step-by-step) 2. Complete the Workflow Step by Step

You can follow the numbered steps in the image to complete the basic text-to-image workflow:

1. (Optional) Modify the `prompt` parameter in the `Stability AI Stable Diffusion 3.5 Image` node to input your desired image description. More detailed prompts often result in better image quality.
2. (Optional) Select the `model` parameter to choose which SD 3.5 model version to use.
3. (Optional) Select the `style_preset` parameter to control the visual style of the image. Different presets produce images with different stylistic characteristics, such as “cinematic” or “anime”. Select “None” to not apply any specific style.
4. (Optional) Edit the `String(Multiline)` to modify negative prompts, specifying elements you don’t want to appear in the generated image.
5. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation.
6. After the API returns results, you can view the generated image in the `Save Image` node. The image will also be saved to the `ComfyUI/output/` directory.

### [​](http://docs.comfy.org#3-additional-notes) 3. Additional Notes

- **Prompt**: The prompt is one of the most important parameters in the generation process. Detailed, clear descriptions lead to better results. Can include elements like scene, subject, colors, lighting, and style.
- **CFG Scale**: Controls how closely the generator follows the prompt. Higher values make the image more closely match the prompt description, but too high may result in oversaturated or unnatural results.
- **Style Preset**: Offers various preset styles for quickly defining the overall style of the image.
- **Negative Prompt**: Used to specify elements you don’t want to appear in the generated image.
- **Seed Parameter**: Can be used to reproduce or fine-tune generation results, helpful for iteration during creation.
- Currently the `Load Image` node is in “Bypass” mode. To enable it, refer to the step guide and right-click the node to set “Mode” to “Always” to enable input, switching to image-to-image mode.
- `image_denoise` has no effect when there is no input image.

## [​](http://docs.comfy.org#stability-ai-stable-diffusion-3-5-image-to-image-workflow) Stability AI Stable Diffusion 3.5 Image-to-Image Workflow

### [​](http://docs.comfy.org#1-workflow-file-download-2) 1. Workflow File Download

The image below contains workflow information in its `metadata`. Please download and drag it into ComfyUI to load the corresponding workflow.

Download the image below to use as input !\[Stability AI Stable Diffusion 3.5 Image-to-Image Workflow Input Image](

### [​](http://docs.comfy.org#2-complete-the-workflow-step-by-step-2) 2. Complete the Workflow Step by Step

You can follow the numbered steps in the image to complete the image-to-image workflow:

1. Load a reference image through the `Load Image` node, which will serve as the basis for generation.
2. (Optional) Modify the `prompt` parameter in the `Stability AI Stable Diffusion 3.5 Image` node to describe elements you want to change or enhance in the reference image.
3. (Optional) Select the `style_preset` parameter to control the visual style of the image. Different presets produce images with different stylistic characteristics.
4. (Optional|Important) Adjust the `image_denoise` parameter (range 0.0-1.0) to control how much the original image is modified:
   
   - Values closer to 0.0 make the generated image more similar to the input reference image (at 0.0, it’s basically identical to the original)
   - Values closer to 1.0 make the generated image more like pure text-to-image generation (at 1.0, it’s as if no reference image was provided)
5. (Optional) Edit the `String(Multiline)` to modify negative prompts, specifying elements you don’t want to appear in the generated image.
6. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation.
7. After the API returns results, you can view the generated image in the `Save Image` node. The image will also be saved to the `ComfyUI/output/` directory.

### [​](http://docs.comfy.org#3-additional-notes-2) 3. Additional Notes

The image below shows a comparison of results with and without input image using the same parameter settings:

**Image Denoise**: This parameter determines how much of the original image’s features are preserved during generation. It’s the most crucial adjustment parameter in image-to-image mode. The image below shows the effects of different denoising strengths:

- **Reference Image Selection**: Choosing images with clear subjects and good composition usually yields better results.
- **Prompt Tips**: In image-to-image mode, prompts should focus more on elements you want to change or enhance, rather than describing everything already present in the image.
- **Mode Switching**: When an input image is provided, the node automatically switches from text-to-image mode to image-to-image mode, and aspect ratio parameters are ignored.

## [​](http://docs.comfy.org#related-node-details) Related Node Details

You can refer to the documentation below to understand detailed parameter settings for the corresponding node

[**Stability Stable Diffusion 3.5 Image Node Documentation**  
\
Stability Stable Diffusion 3.5 Image API Node Documentation](http://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-image-ultra)

[Ideogram 3.0This guide covers how to use the Ideogram 3.0 API node in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/ideogram/ideogram-v3)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Stability AI Stable Diffusion 3.5 Text-to-Image Workflow](http://docs.comfy.org#stability-ai-stable-diffusion-3-5-text-to-image-workflow)
- [1. Workflow File Download](http://docs.comfy.org#1-workflow-file-download)
- [2. Complete the Workflow Step by Step](http://docs.comfy.org#2-complete-the-workflow-step-by-step)
- [3. Additional Notes](http://docs.comfy.org#3-additional-notes)
- [Stability AI Stable Diffusion 3.5 Image-to-Image Workflow](http://docs.comfy.org#stability-ai-stable-diffusion-3-5-image-to-image-workflow)
- [1. Workflow File Download](http://docs.comfy.org#1-workflow-file-download-2)
- [2. Complete the Workflow Step by Step](http://docs.comfy.org#2-complete-the-workflow-step-by-step-2)
- [3. Additional Notes](http://docs.comfy.org#3-additional-notes-2)
- [Related Node Details](http://docs.comfy.org#related-node-details)

<!-- END Development/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image.md -->


<!-- BEGIN Development/tutorials/api-nodes/stability-ai/stable-image-ultra.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes
  
  - [Overview](http://docs.comfy.org/tutorials/api-nodes/overview)
  - [FAQs](http://docs.comfy.org/tutorials/api-nodes/faq)
  - [Pricing](http://docs.comfy.org/tutorials/api-nodes/pricing)
  - Black Forest Labs
  - Stability AI
    
    - [Stable Image Ultra](http://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-image-ultra)
    - [Stable Diffusion 3.5 Image](http://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image)
  - Ideogram
  - Luma
  - OpenAI
  - Recraft

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Stability AI Stable Image Ultra API Node ComfyUI Official Example

# Stability AI Stable Image Ultra API Node ComfyUI Official Example

This article will introduce how to use the Stability AI Stable Image Ultra API node’s text-to-image and image-to-image capabilities in ComfyUI

The [Stability Stable Image Ultra](http://docs.comfy.org/built-in-nodes/api-node/image/stability/stability-stable-image-ultra) node allows you to use Stability AI’s Stable Image Ultra model to create high-quality, detailed image content through text prompts or reference images.

In this guide, we will show you how to set up workflows for both text-to-image and image-to-image generation using this node.

To use the API nodes, you need to ensure that you are logged in properly and using a permitted network environment. Please refer to the [API Nodes Overview](http://docs.comfy.org/tutorials/api-nodes/overview) section of the documentation to understand the specific requirements for using the API nodes.

## [​](http://docs.comfy.org#stability-ai-stable-image-ultra-text-to-image-workflow) Stability AI Stable Image Ultra Text-to-Image Workflow

### [​](http://docs.comfy.org#1-workflow-file-download) 1. Workflow File Download

The workflow information is included in the metadata of the image below. Please download and drag it into ComfyUI to load the corresponding workflow.

### [​](http://docs.comfy.org#2-complete-the-workflow-execution-step-by-step) 2. Complete the Workflow Execution Step by Step

You can follow the numbered steps in the image to complete the basic text-to-image workflow:

1. (Optional) Modify the `prompt` parameter in the `Stability AI Stable Image Ultra` node to input your desired image description. More detailed prompts often lead to better image quality. You can use the `(word:weight)` format to control specific word weights, for example: `The sky was crisp (blue:0.3) and (green:0.8)` indicates the sky is blue and green, but green is more prominent.
2. (Optional) Select the `style_preset` parameter to control the visual style of the image. Different preset styles will produce images with different stylistic characteristics, such as “cinematic”, “anime”, etc. Selecting “None” will not apply any specific style.
3. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation.
4. After the API returns the result, you can view the generated image in the `Save Image` node, and the image will also be saved to the `ComfyUI/output/` directory.

### [​](http://docs.comfy.org#3-additional-notes) 3. Additional Notes

- **Prompt**: The prompt is one of the most important parameters in the generation process. Detailed, clear descriptions will lead to better results. It can include elements like scene, subject, colors, lighting, and style.
- **Style Preset**: Provides multiple preset styles such as cinematic, anime, digital art, etc., which can quickly define the overall style of the image.
- **Negative Prompt**: Used to specify elements you don’t want to appear in the generated image, helping avoid common issues like extra limbs or distorted faces.
- **Seed Parameter**: Can be used to reproduce or fine-tune generation results, helpful for iteration during the creative process.
- Currently, the `Load Image` node is in “Bypass” mode. To enable it, refer to the step guide and right-click on the corresponding node to set “Mode” to “Always” to enable input, which will switch to image-to-image mode.

## [​](http://docs.comfy.org#stability-ai-stable-image-ultra-image-to-image-workflow) Stability AI Stable Image Ultra Image-to-Image Workflow

### [​](http://docs.comfy.org#1-workflow-file-download-2) 1. Workflow File Download

The workflow information is included in the metadata of the image below. Please download and drag it into ComfyUI to load the corresponding workflow.

Download the image below which we will use as input

### [​](http://docs.comfy.org#2-complete-the-workflow-execution-step-by-step-2) 2. Complete the Workflow Execution Step by Step

You can follow the numbered steps in the image to complete the image-to-image workflow:

1. Load a reference image through the `Load Image` node, which will serve as the basis for generation.
2. (Optional) Modify the `prompt` parameter in the `Stability Stable Image Ultra` node to describe elements you want to change or enhance in the reference image.
3. (Optional) Adjust the `image_denoise` parameter (range 0.0-1.0) to control the degree of modification to the original image:
   
   - Values closer to 0.0 will make the generated image more similar to the input reference image
   - Values closer to 1.0 will make the generated image more like pure text-to-image generation
4. (Optional) You can also set `style_preset` and other parameters to further control the generation effect.
5. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation.
6. After the API returns the result, you can view the generated image in the `Save Image` node, and the image will also be saved to the `ComfyUI/output/` directory.

### [​](http://docs.comfy.org#3-additional-notes-2) 3. Additional Notes

**Image Denoise**: This parameter determines how much of the original image’s features are preserved during generation, and is the most crucial adjustment parameter in image-to-image mode. The image below shows the effects of different denoising strengths

- **Reference Image Selection**: Choosing images with clear subjects and good composition usually leads to better results.
- **Prompt Tips**: In image-to-image mode, prompts should focus more on what you want to change or enhance, rather than describing all elements already present in the image.

## [​](http://docs.comfy.org#related-node-documentation) Related Node Documentation

You can refer to the documentation below for detailed parameter settings and more information about the corresponding nodes

[**Stability Stable Image Ultra Node Documentation**  
\
Stability Stable Image Ultra API Node Documentation](http://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/api-nodes/stability-ai/stable-image-ultra.mdx)

[Previous](http://docs.comfy.org/tutorials/api-nodes/black-forest-labs/flux-1-1-pro-ultra-image)

[Stable Diffusion 3.5 ImageThis article will introduce how to use Stability AI Stable Diffusion 3.5 API node's text-to-image and image-to-image capabilities in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Stability AI Stable Image Ultra Text-to-Image Workflow](http://docs.comfy.org#stability-ai-stable-image-ultra-text-to-image-workflow)
- [1. Workflow File Download](http://docs.comfy.org#1-workflow-file-download)
- [2. Complete the Workflow Execution Step by Step](http://docs.comfy.org#2-complete-the-workflow-execution-step-by-step)
- [3. Additional Notes](http://docs.comfy.org#3-additional-notes)
- [Stability AI Stable Image Ultra Image-to-Image Workflow](http://docs.comfy.org#stability-ai-stable-image-ultra-image-to-image-workflow)
- [1. Workflow File Download](http://docs.comfy.org#1-workflow-file-download-2)
- [2. Complete the Workflow Execution Step by Step](http://docs.comfy.org#2-complete-the-workflow-execution-step-by-step-2)
- [3. Additional Notes](http://docs.comfy.org#3-additional-notes-2)
- [Related Node Documentation](http://docs.comfy.org#related-node-documentation)

<!-- END Development/tutorials/api-nodes/stability-ai/stable-image-ultra.md -->


<!-- BEGIN Development/tutorials/audio/ace-step/ace-step-v1.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
  
  - [ACE-Step Music Generation](http://docs.comfy.org/tutorials/audio/ace-step/ace-step-v1)
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI ACE-Step Native Example

# ComfyUI ACE-Step Native Example

This guide will help you create dynamic music using the ACE-Step model in ComfyUI

ACE-Step is an open-source foundational music generation model jointly developed by Chinese team StepFun and ACE Studio, aimed at providing music creators with efficient, flexible and high-quality music generation and editing tools.

The model is released under the [Apache-2.0](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-license) license and is free for commercial use.

As a powerful music generation foundation, ACE-Step provides rich extensibility. Through fine-tuning techniques like LoRA and ControlNet, developers can customize the model according to their actual needs. Whether it’s audio editing, vocal synthesis, accompaniment production, voice cloning or style transfer applications, ACE-Step provides stable and reliable technical support. This flexible architecture greatly simplifies the development process of music AI applications, allowing more creators to quickly apply AI technology to music creation.

Currently, ACE-Step has released related training code, including LoRA model training, and the corresponding ControlNet training code will be released in the future. You can visit their [Github](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-roadmap) to learn more details.

## [​](http://docs.comfy.org#ace-step-comfyui-text-to-audio-generation-workflow-example) ACE-Step ComfyUI Text-to-Audio Generation Workflow Example

### [​](http://docs.comfy.org#1-download-workflow-and-related-models) 1. Download Workflow and Related Models

Click the button below to download the corresponding workflow file. Drag it into ComfyUI to load the workflow information. The workflow includes model download information.

Click the button below to download the corresponding workflow file. Drag it into ComfyUI to load the workflow information. The workflow includes model download information.

[Download Json Format Workflow File](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/audio/ace-step/ace_step_1_t2m.json)

You can also manually download [ace\_step\_v1\_3.5b.safetensors](https://huggingface.co/Comfy-Org/ACE-Step_ComfyUI_repackaged/blob/main/all_in_one/ace_step_v1_3.5b.safetensors) and save it to the `ComfyUI/models/checkpoints` folder

### [​](http://docs.comfy.org#2-complete-the-workflow-step-by-step) 2. Complete the Workflow Step by Step

1. Ensure the `Load Checkpoints` node has loaded the `ace_step_v1_3.5b.safetensors` model
2. Input corresponding music styles etc. in the `tags` field of `TextEncodeAceStepAudio`
3. Input corresponding lyrics in the `lyrics` field of `TextEncodeAceStepAudio`
4. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the generation
5. After the generation is complete, you can view the generated audio in the `Save Audio` node. You can click to play and preview. The audio will also be saved to `ComfyUI/output/audio` (subdirectory determined by the `Save Audio` node).

## [​](http://docs.comfy.org#ace-step-comfyui-audio-to-audio-workflow) ACE-Step ComfyUI Audio-to-Audio Workflow

Similar to image-to-image workflows, you can input a piece of music and use the workflow below to resample and generate music. You can also adjust the difference from the original audio by controlling the `denoise` parameter in the `Ksampler`.

### [​](http://docs.comfy.org#1-download-workflow-file) 1. Download Workflow File

Click the button below to download the corresponding workflow file. Drag it into ComfyUI to load the workflow information.

[Download Json Format Workflow File](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/audio/ace-stepace_step_1_m2m_editing.json)

### [​](http://docs.comfy.org#2-complete-the-workflow-step-by-step-2) 2. Complete the Workflow Step by Step

1. Ensure the `Load Checkpoints` node has loaded the `ace_step_v1_3.5b.safetensors` model
2. Upload the music you want to edit in the `LoadAudio` node (you can use the results generated from the text-to-audio workflow in this article)
3. Input corresponding music styles etc. in the `tags` field of `TextEncodeAceStepAudio`
4. Input corresponding lyrics in the `lyrics` field of `TextEncodeAceStepAudio`, you can refer to the prompt guide part (still updating) or the lyrics examples on the ACE-Step project page
5. Modify the `denoise` parameter in the `Ksampler` node to adjust the amount of noise added during sampling, which controls the similarity to the original audio (smaller values result in greater similarity to the original audio; if set to `1.00`, it can be considered as if there is no audio input)
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the audio generation
7. After the generation is complete, you can view the generated audio in the `Save Audio` node. You can click to play and preview. The audio will also be saved to `ComfyUI/output/audio` (subdirectory determined by the `Save Audio` node).

You can also implement the lyrics modification and editing functionality from the ACE-Step project page, modifying the original lyrics to change the audio effect.

### [​](http://docs.comfy.org#3-lyrics-modification-and-editing-example) 3. Lyrics Modification and Editing Example

\[To be updated]

## [​](http://docs.comfy.org#ace-step-prompt-guide) ACE-Step Prompt Guide

ACE currently uses two types of prompts: `tags` and `lyrics`.

- `tags`: Mainly used to describe music styles, scenes, etc. Similar to prompts we use for other generations, they primarily describe the overall style and requirements of the audio, separated by English commas
- `lyrics`: Mainly used to describe lyrics, supporting lyric structure tags such as \[verse], \[chorus], and \[bridge] to distinguish different parts of the lyrics. You can also input instrument names for purely instrumental music

You can find rich examples of `tags` and `lyrics` on the [ACE-Step model homepage](https://ace-step.github.io/). You can refer to these examples to try corresponding prompts. This document’s prompt guide is organized based on the project to help you quickly try combinations to achieve your desired effect.

### [​](http://docs.comfy.org#tags-prompt) Tags (prompt)

#### [​](http://docs.comfy.org#mainstream-music-styles) Mainstream Music Styles

Use short tag combinations to generate specific music styles

- electronic
- rock
- pop
- funk
- soul
- cyberpunk
- Acid jazz
- electro
- em (electronic music)
- soft electric drums
- melodic

#### [​](http://docs.comfy.org#scene-types) Scene Types

Combine specific usage scenarios and atmospheres to generate music that matches the corresponding mood

- background music for parties
- radio broadcasts
- workout playlists

#### [​](http://docs.comfy.org#instrumental-elements) Instrumental Elements

- saxophone
- jazz
- piano, violin

#### [​](http://docs.comfy.org#vocal-types) Vocal Types

- female voice
- male voice
- clean vocals

#### [​](http://docs.comfy.org#professional-terms) Professional Terms

Use some professional terms commonly used in music to precisely control music effects

- 110 bpm (beats per minute is 110)
- fast tempo
- slow tempo
- loops
- fills
- acoustic guitar
- electric bass

### [​](http://docs.comfy.org#lyrics) Lyrics

#### [​](http://docs.comfy.org#lyric-structure-tags) Lyric Structure Tags

- \[outro]
- \[verse]
- \[chorus]
- \[bridge]

#### [​](http://docs.comfy.org#multilingual-support) Multilingual Support

- ACE-Step V1 supports multiple languages. When used, ACE-Step converts different languages into English letters and then generates music.
- In ComfyUI, we haven’t fully implemented the conversion of all languages to English letters. Currently, only [Japanese hiragana and katakana characters](https://github.com/comfyanonymous/ComfyUI/commit/5d3cc85e13833aeb6ef9242cdae243083e30c6fc) are implemented. So if you need to use multiple languages for music generation, you need to first convert the corresponding language to English letters, and then input the language code abbreviation at the beginning of the `lyrics`, such as Chinese `[zh]`, Korean `[ko]`, etc.

For example:

```plaintext
[zh]ni hao
[ko]an nyeong
```

Currently, ACE-Step supports 19 languages, but the following ten languages have better support:

- English
- Chinese: \[zh]
- Russian: \[ru]
- Spanish: \[es]
- Japanese: \[ja]
- German: \[de]
- French: \[fr]
- Portuguese: \[pt]
- Italian: \[it]
- Korean: \[ko]

The language tags above have not been fully tested at the time of writing this documentation. If any language tag is incorrect, please [submit an issue to our documentation repository](https://github.com/Comfy-Org/docs/issues) and we will make timely corrections.

## [​](http://docs.comfy.org#ace-step-related-resources) ACE-Step Related Resources

- [Project Page](https://ace-step.github.io/)
- [Hugging Face](https://huggingface.co/ACE-Step/ACE-Step-v1-3.5B)
- [GitHub](https://github.com/ace-step/ACE-Step)
- [Training Scripts](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-train)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/audio/ace-step/ace-step-v1.mdx)

[Previous](http://docs.comfy.org/tutorials/video/wan/wan-flf)

[OverviewIn this article, we will introduce ComfyUI's API Nodes and related information.  
\
Next](http://docs.comfy.org/tutorials/api-nodes/overview)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [ACE-Step ComfyUI Text-to-Audio Generation Workflow Example](http://docs.comfy.org#ace-step-comfyui-text-to-audio-generation-workflow-example)
- [1. Download Workflow and Related Models](http://docs.comfy.org#1-download-workflow-and-related-models)
- [2. Complete the Workflow Step by Step](http://docs.comfy.org#2-complete-the-workflow-step-by-step)
- [ACE-Step ComfyUI Audio-to-Audio Workflow](http://docs.comfy.org#ace-step-comfyui-audio-to-audio-workflow)
- [1. Download Workflow File](http://docs.comfy.org#1-download-workflow-file)
- [2. Complete the Workflow Step by Step](http://docs.comfy.org#2-complete-the-workflow-step-by-step-2)
- [3. Lyrics Modification and Editing Example](http://docs.comfy.org#3-lyrics-modification-and-editing-example)
- [ACE-Step Prompt Guide](http://docs.comfy.org#ace-step-prompt-guide)
- [Tags (prompt)](http://docs.comfy.org#tags-prompt)
- [Mainstream Music Styles](http://docs.comfy.org#mainstream-music-styles)
- [Scene Types](http://docs.comfy.org#scene-types)
- [Instrumental Elements](http://docs.comfy.org#instrumental-elements)
- [Vocal Types](http://docs.comfy.org#vocal-types)
- [Professional Terms](http://docs.comfy.org#professional-terms)
- [Lyrics](http://docs.comfy.org#lyrics)
- [Lyric Structure Tags](http://docs.comfy.org#lyric-structure-tags)
- [Multilingual Support](http://docs.comfy.org#multilingual-support)
- [ACE-Step Related Resources](http://docs.comfy.org#ace-step-related-resources)

<!-- END Development/tutorials/audio/ace-step/ace-step-v1.md -->


<!-- BEGIN Development/tutorials/basic/image-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
  
  - [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image)
  - [Image to Image](http://docs.comfy.org/tutorials/basic/image-to-image)
  - [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint)
  - [Outpaint](http://docs.comfy.org/tutorials/basic/outpaint)
  - [Upscale](http://docs.comfy.org/tutorials/basic/upscale)
  - [LoRA](http://docs.comfy.org/tutorials/basic/lora)
  - [Multiple LoRAs](http://docs.comfy.org/tutorials/basic/multiple-loras)
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Image to Image Workflow

# ComfyUI Image to Image Workflow

This guide will help you understand and complete an image to image workflow

## [​](http://docs.comfy.org#what-is-image-to-image) What is Image to Image

Image to Image is a workflow in ComfyUI that allows users to input an image and generate a new image based on it.

Image to Image can be used in scenarios such as:

- Converting original image styles, like transforming realistic photos into artistic styles
- Converting line art into realistic images
- Image restoration
- Colorizing old photos
- … and other scenarios

To explain it with an analogy: It’s like asking an artist to create a specific piece based on your reference image.

If you carefully compare this tutorial with the [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image) tutorial, you’ll notice that the Image to Image process is very similar to Text to Image, just with an additional input reference image as a condition. In Text to Image, we let the artist (image model) create freely based on our prompts, while in Image to Image, we let the artist create based on both our reference image and prompts.

## [​](http://docs.comfy.org#comfyui-image-to-image-workflow-example-guide) ComfyUI Image to Image Workflow Example Guide

### [​](http://docs.comfy.org#model-installation) Model Installation

Download the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) file and put it in your `ComfyUI/models/checkpoints` folder.

### [​](http://docs.comfy.org#image-to-image-workflow-and-input-image) Image to Image Workflow and Input Image

Download the image below and **drag it into ComfyUI** to load the workflow:

Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`.

Download the image below and we will use it as the input image:

### [​](http://docs.comfy.org#complete-the-workflow-step-by-step) Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

1. Ensure `Load Checkpoint` loads **v1-5-pruned-emaonly-fp16.safetensors**
2. Upload the input image to the `Load Image` node
3. Click `Queue` or press `Ctrl/Cmd + Enter` to generate

## [​](http://docs.comfy.org#key-points-of-image-to-image-workflow) Key Points of Image to Image Workflow

The key to the Image to Image workflow lies in the `denoise` parameter in the `KSampler` node, which should be **less than 1**

If you’ve adjusted the `denoise` parameter and generated images, you’ll notice:

- The smaller the `denoise` value, the smaller the difference between the generated image and the reference image
- The larger the `denoise` value, the larger the difference between the generated image and the reference image

This is because `denoise` determines the strength of noise added to the latent space image after converting the reference image. If `denoise` is 1, the latent space image will become completely random noise, making it the same as the latent space generated by the `empty latent image` node, losing all characteristics of the reference image.

For the corresponding principles, please refer to the principle explanation in the [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image) tutorial.

## [​](http://docs.comfy.org#try-it-yourself) Try It Yourself

1. Try modifying the `denoise` parameter in the **KSampler** node, gradually changing it from 1 to 0, and observe the changes in the generated images
2. Replace with your own prompts and reference images to generate your own image effects

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/basic/image-to-image.mdx)

[Previous](http://docs.comfy.org/tutorials/basic/text-to-image)

[InpaintThis guide will introduce you to the inpainting workflow in ComfyUI, walk you through an inpainting example, and cover topics like using the mask editor  
\
Next](http://docs.comfy.org/tutorials/basic/inpaint)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [What is Image to Image](http://docs.comfy.org#what-is-image-to-image)
- [ComfyUI Image to Image Workflow Example Guide](http://docs.comfy.org#comfyui-image-to-image-workflow-example-guide)
- [Model Installation](http://docs.comfy.org#model-installation)
- [Image to Image Workflow and Input Image](http://docs.comfy.org#image-to-image-workflow-and-input-image)
- [Complete the Workflow Step by Step](http://docs.comfy.org#complete-the-workflow-step-by-step)
- [Key Points of Image to Image Workflow](http://docs.comfy.org#key-points-of-image-to-image-workflow)
- [Try It Yourself](http://docs.comfy.org#try-it-yourself)

<!-- END Development/tutorials/basic/image-to-image.md -->


<!-- BEGIN Development/tutorials/basic/inpaint.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
  
  - [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image)
  - [Image to Image](http://docs.comfy.org/tutorials/basic/image-to-image)
  - [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint)
  - [Outpaint](http://docs.comfy.org/tutorials/basic/outpaint)
  - [Upscale](http://docs.comfy.org/tutorials/basic/upscale)
  - [LoRA](http://docs.comfy.org/tutorials/basic/lora)
  - [Multiple LoRAs](http://docs.comfy.org/tutorials/basic/multiple-loras)
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Inpainting Workflow

# ComfyUI Inpainting Workflow

This guide will introduce you to the inpainting workflow in ComfyUI, walk you through an inpainting example, and cover topics like using the mask editor

This article will introduce the concept of inpainting in AI image generation and guide you through creating an inpainting workflow in ComfyUI. We’ll cover:

- Using inpainting workflows to modify images
- Using the ComfyUI mask editor to draw masks
- `VAE Encoder (for Inpainting)` node

## [​](http://docs.comfy.org#about-inpainting) About Inpainting

In AI image generation, we often encounter situations where we’re satisfied with the overall image but there are elements we don’t want or that contain errors. Simply regenerating might produce a completely different image, so using inpainting to fix specific parts becomes very useful.

It’s like having an **artist (AI model)** paint a picture, but we’re still not satisfied with the specific details. We need to tell the artist **which areas to adjust (mask)**, and then let them **repaint (inpaint)** according to our requirements.

Common inpainting scenarios include:

- **Defect Repair:** Removing unwanted objects, fixing incorrect AI-generated body parts, etc.
- **Detail Optimization:** Precisely adjusting local elements (like modifying clothing textures, adjusting facial expressions)
- And other scenarios

## [​](http://docs.comfy.org#comfyui-inpainting-workflow-example) ComfyUI Inpainting Workflow Example

### [​](http://docs.comfy.org#model-and-resource-preparation) Model and Resource Preparation

#### [​](http://docs.comfy.org#1-model-installation) 1. Model Installation

Download the [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors) file and put it in your `ComfyUI/models/checkpoints` folder:

#### [​](http://docs.comfy.org#2-inpainting-asset) 2. Inpainting Asset

Please download the following image which we’ll use as input:

This image already contains an alpha channel (transparency mask), so you don’t need to manually draw a mask. This tutorial will also cover how to use the mask editor to draw masks.

#### [​](http://docs.comfy.org#3-inpainting-workflow) 3. Inpainting Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:

Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`.

### [​](http://docs.comfy.org#comfyui-inpainting-workflow-example-explanation) ComfyUI Inpainting Workflow Example Explanation

Follow the steps in the diagram below to ensure the workflow runs correctly.

1. Ensure `Load Checkpoint` loads `512-inpainting-ema.safetensors`
2. Upload the input image to the `Load Image` node
3. Click `Queue` or use `Ctrl + Enter` to generate

For comparison, here’s the result using the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) model:

You will find that the results generated by the [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors) model have better inpainting effects and more natural transitions. This is because this model is specifically designed for inpainting, which helps us better control the generation area, resulting in improved inpainting effects.

Do you remember the analogy we’ve been using? Different models are like artists with varying abilities, but each artist has their own limits. Choosing the right model can help you achieve better generation results.

You can try these approaches to achieve better results:

1. Modify positive and negative prompts with more specific descriptions
2. Try multiple runs using different seeds in the `KSampler` for different generation results
3. After learning about the mask editor in this tutorial, you can re-inpaint the generated results to achieve satisfactory outcomes.

Next, we’ll learn about using the **Mask Editor**. While our input image already includes an `alpha` transparency channel (the area we want to edit), so manual mask drawing isn’t necessary, you’ll often use the Mask Editor to create masks in practical applications.

### [​](http://docs.comfy.org#using-the-mask-editor) Using the Mask Editor

First right-click the `Save Image` node and select `Copy(Clipspace)`:

Then right-click the **Load Image** node and select `Paste(Clipspace)`:

Right-click the **Load Image** node again and select `Open in MaskEditor`:

1. Adjust brush parameters on the right panel
2. Use eraser to correct mistakes
3. Click `Save` when finished

The drawn content will be used as a Mask input to the VAE Encoder (for Inpainting) node for encoding

Then try adjusting your prompts and generating again until you achieve satisfactory results.

## [​](http://docs.comfy.org#vae-encoder-for-inpainting-node) VAE Encoder (for Inpainting) Node

Comparing this workflow with [Text-to-Image](http://docs.comfy.org/tutorials/basic/text-to-image) and [Image-to-Image](http://docs.comfy.org/tutorials/basic/image-to-image), you’ll notice the main differences are in the VAE section’s conditional inputs. In this workflow, we use the **VAE Encoder (for Inpainting)** node, specifically designed for inpainting to help us better control the generation area and achieve better results.

**Input Types**

Parameter NameFunction`pixels`Input image to be encoded into latent space.`vae`VAE model used to encode the image from pixel space to latent space.`mask`Image mask specifying which areas need modification.`grow_mask_by`Pixel value to expand the original mask outward, ensuring a transition area around the mask to avoid hard edges between inpainted and original areas.

**Output Types**

Parameter NameFunction`latent`Image encoded into latent space by the VAE.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/basic/inpaint.mdx)

[Previous](http://docs.comfy.org/tutorials/basic/image-to-image)

[OutpaintThis guide will introduce you to the outpainting workflow in ComfyUI and walk you through an outpainting example  
\
Next](http://docs.comfy.org/tutorials/basic/outpaint)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [About Inpainting](http://docs.comfy.org#about-inpainting)
- [ComfyUI Inpainting Workflow Example](http://docs.comfy.org#comfyui-inpainting-workflow-example)
- [Model and Resource Preparation](http://docs.comfy.org#model-and-resource-preparation)
- [1. Model Installation](http://docs.comfy.org#1-model-installation)
- [2. Inpainting Asset](http://docs.comfy.org#2-inpainting-asset)
- [3. Inpainting Workflow](http://docs.comfy.org#3-inpainting-workflow)
- [ComfyUI Inpainting Workflow Example Explanation](http://docs.comfy.org#comfyui-inpainting-workflow-example-explanation)
- [Using the Mask Editor](http://docs.comfy.org#using-the-mask-editor)
- [VAE Encoder (for Inpainting) Node](http://docs.comfy.org#vae-encoder-for-inpainting-node)

<!-- END Development/tutorials/basic/inpaint.md -->


<!-- BEGIN Development/tutorials/basic/lora.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
  
  - [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image)
  - [Image to Image](http://docs.comfy.org/tutorials/basic/image-to-image)
  - [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint)
  - [Outpaint](http://docs.comfy.org/tutorials/basic/outpaint)
  - [Upscale](http://docs.comfy.org/tutorials/basic/upscale)
  - [LoRA](http://docs.comfy.org/tutorials/basic/lora)
  - [Multiple LoRAs](http://docs.comfy.org/tutorials/basic/multiple-loras)
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI LoRA Example

# ComfyUI LoRA Example

This guide will help you understand and use a single LoRA model

**LoRA (Low-Rank Adaptation)** is an efficient technique for fine-tuning large generative models like Stable Diffusion. It introduces trainable low-rank matrices to the pre-trained model, adjusting only a portion of parameters rather than retraining the entire model, thus achieving optimization for specific tasks at a lower computational cost. Compared to base models like SD1.5, LoRA models are smaller and easier to train.

The image above compares generation with the same parameters using [dreamshaper\_8](https://civitai.com/models/4384?modelVersionId=128713) directly versus using the [blindbox\_V1Mix](https://civitai.com/models/25995/blindbox) LoRA model. As you can see, by using a LoRA model, we can generate images in different styles without adjusting the base model.

We will demonstrate how to use a LoRA model. All LoRA variants: Lycoris, loha, lokr, locon, etc… are used in the same way.

In this example, we will learn how to load and use a LoRA model in [ComfyUI](https://github.com/comfyanonymous/ComfyUI), covering the following topics:

1. Installing a LoRA model
2. Generating images using a LoRA model
3. A simple introduction to the `Load LoRA` node

## [​](http://docs.comfy.org#required-model-installation) Required Model Installation

Download the [dreamshaper\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model&format=SafeTensor&size=pruned&fp=fp16) file and put it in your `ComfyUI/models/checkpoints` folder.

Download the [blindbox\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model&format=SafeTensor&size=full&fp=fp16) file and put it in your `ComfyUI/models/loras` folder.

## [​](http://docs.comfy.org#lora-workflow-file) LoRA Workflow File

Download the image below and **drag it into ComfyUI** to load the workflow.

Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`.

## [​](http://docs.comfy.org#complete-the-workflow-step-by-step) Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

1. Ensure `Load Checkpoint` loads `dreamshaper_8.safetensors`
2. Ensure `Load LoRA` loads `blindbox_V1Mix.safetensors`
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to generate the image

## [​](http://docs.comfy.org#load-lora-node-introduction) Load LoRA Node Introduction

Models in the `ComfyUI\models\loras` folder will be detected by ComfyUI and can be loaded using this node.

### [​](http://docs.comfy.org#input-types) Input Types

Parameter NameFunction`model`Connect to the base model`clip`Connect to the CLIP model`lora_name`Select the LoRA model to load and use`strength_model`Affects how strongly the LoRA influences the model weights; higher values make the LoRA style stronger`strength_clip`Affects how strongly the LoRA influences the CLIP text embeddings

### [​](http://docs.comfy.org#output-types) Output Types

Parameter NameFunction`model`Outputs the model with LoRA adjustments applied`clip`Outputs the CLIP model with LoRA adjustments applied

This node supports chain connections, allowing multiple `Load LoRA` nodes to be linked in series to apply multiple LoRA models. For more details, please refer to [ComfyUI Multiple LoRAs Example](http://docs.comfy.org/tutorials/basic/multiple-loras)

## [​](http://docs.comfy.org#try-it-yourself) Try It Yourself

1. Try modifying the prompt or adjusting different parameters of the `Load LoRA` node, such as `strength_model`, to observe changes in the generated images and become familiar with the `Load LoRA` node.
2. Visit [CivitAI](https://civitai.com/models) to download other kinds of LoRA models and try using them.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/basic/lora.mdx)

[Previous](http://docs.comfy.org/tutorials/basic/upscale)

[Multiple LoRAsThis guide demonstrates how to apply multiple LoRA models simultaneously in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/basic/multiple-loras)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Required Model Installation](http://docs.comfy.org#required-model-installation)
- [LoRA Workflow File](http://docs.comfy.org#lora-workflow-file)
- [Complete the Workflow Step by Step](http://docs.comfy.org#complete-the-workflow-step-by-step)
- [Load LoRA Node Introduction](http://docs.comfy.org#load-lora-node-introduction)
- [Input Types](http://docs.comfy.org#input-types)
- [Output Types](http://docs.comfy.org#output-types)
- [Try It Yourself](http://docs.comfy.org#try-it-yourself)

<!-- END Development/tutorials/basic/lora.md -->


<!-- BEGIN Development/tutorials/basic/multiple-loras.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
  
  - [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image)
  - [Image to Image](http://docs.comfy.org/tutorials/basic/image-to-image)
  - [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint)
  - [Outpaint](http://docs.comfy.org/tutorials/basic/outpaint)
  - [Upscale](http://docs.comfy.org/tutorials/basic/upscale)
  - [LoRA](http://docs.comfy.org/tutorials/basic/lora)
  - [Multiple LoRAs](http://docs.comfy.org/tutorials/basic/multiple-loras)
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Multiple LoRAs Example

# ComfyUI Multiple LoRAs Example

This guide demonstrates how to apply multiple LoRA models simultaneously in ComfyUI

In our [ComfyUI LoRA Example](http://docs.comfy.org/tutorials/basic/lora), we introduced how to load and use a single LoRA model, mentioning the node’s chain connection capability.

This tutorial demonstrates chaining multiple `Load LoRA` nodes to apply two LoRA models simultaneously: [blindbox\_V1Mix](https://civitai.com/models/25995?modelVersionId=32988) and [MoXinV1](https://civitai.com/models/12597?modelVersionId=14856).

The comparison below shows individual effects of these LoRAs using identical parameters:

By chaining multiple LoRA models, we achieve a blended style in the final output:

## [​](http://docs.comfy.org#model-installation) Model Installation

Download the [dreamshaper\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model&format=SafeTensor&size=pruned&fp=fp16) file and put it in your `ComfyUI/models/checkpoints` folder.

Download the [blindbox\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model&format=SafeTensor&size=full&fp=fp16) file and put it in your `ComfyUI/models/loras` folder.

Download the [MoXinV1.safetensors](https://civitai.com/api/download/models/14856?type=Model&format=SafeTensor&size=full&fp=fp16) file and put it in your `ComfyUI/models/loras` folder.

## [​](http://docs.comfy.org#multi-lora-workflow) Multi-LoRA Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:

Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`.

## [​](http://docs.comfy.org#complete-the-workflow-step-by-step) Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

1. Ensure `Load Checkpoint` loads **dreamshaper\_8.safetensors**
2. Ensure first `Load LoRA` loads **blindbox\_V1Mix.safetensors**
3. Ensure second `Load LoRA` loads **MoXinV1.safetensors**
4. Click `Queue` or press `Ctrl/Cmd + Enter` to generate

## [​](http://docs.comfy.org#try-it-yourself) Try It Yourself

1. Adjust `strength_model` values in both `Load LoRA` nodes to control each LoRA’s influence
2. Explore [CivitAI](https://civitai.com/models) for additional LoRAs and create custom combinations

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/basic/multiple-loras.mdx)

[Previous](http://docs.comfy.org/tutorials/basic/lora)

[ControlNetThis guide will introduce you to the basic concepts of ControlNet and demonstrate how to generate corresponding images in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/controlnet/controlnet)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Model Installation](http://docs.comfy.org#model-installation)
- [Multi-LoRA Workflow](http://docs.comfy.org#multi-lora-workflow)
- [Complete the Workflow Step by Step](http://docs.comfy.org#complete-the-workflow-step-by-step)
- [Try It Yourself](http://docs.comfy.org#try-it-yourself)

<!-- END Development/tutorials/basic/multiple-loras.md -->


<!-- BEGIN Development/tutorials/basic/outpaint.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
  
  - [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image)
  - [Image to Image](http://docs.comfy.org/tutorials/basic/image-to-image)
  - [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint)
  - [Outpaint](http://docs.comfy.org/tutorials/basic/outpaint)
  - [Upscale](http://docs.comfy.org/tutorials/basic/upscale)
  - [LoRA](http://docs.comfy.org/tutorials/basic/lora)
  - [Multiple LoRAs](http://docs.comfy.org/tutorials/basic/multiple-loras)
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Outpainting Workflow Example

# ComfyUI Outpainting Workflow Example

This guide will introduce you to the outpainting workflow in ComfyUI and walk you through an outpainting example

This guide will introduce you to the concept of outpainting in AI image generation and how to create an outpainting workflow in ComfyUI. We will cover:

- Using outpainting workflow to extend an image
- Understanding and using outpainting-related nodes in ComfyUI
- Mastering the basic outpainting process

## [​](http://docs.comfy.org#about-outpainting) About Outpainting

In AI image generation, we often encounter situations where an existing image has good composition but the canvas area is too small, requiring us to extend the canvas to get a larger scene. This is where outpainting comes in.

Basically, it requires similar content to [Inpainting](http://docs.comfy.org/tutorials/basic/inpaint), but we use different nodes to **build the mask**.

Outpainting applications include:

- **Scene Extension:** Expand the scene range of the original image to show a more complete environment
- **Composition Adjustment:** Optimize the overall composition by extending the canvas
- **Content Addition:** Add more related scene elements to the original image

## [​](http://docs.comfy.org#comfyui-outpainting-workflow-example-explanation) ComfyUI Outpainting Workflow Example Explanation

### [​](http://docs.comfy.org#preparation) Preparation

#### [​](http://docs.comfy.org#1-model-installation) 1. Model Installation

Download the following model file and save it to `ComfyUI/models/checkpoints` directory:

- [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors)

#### [​](http://docs.comfy.org#2-input-image) 2. Input Image

Prepare an image you want to extend. In this example, we will use the following image:

#### [​](http://docs.comfy.org#3-outpainting-workflow) 3. Outpainting Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:

Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`.

### [​](http://docs.comfy.org#outpainting-workflow-usage-explanation) Outpainting Workflow Usage Explanation

The key steps of the outpainting workflow are as follows:

1. Load the locally installed model file in the `Load Checkpoint` node
2. Click the `Upload` button in the `Load Image` node to upload your image
3. Click the `Queue` button or use the shortcut `Ctrl + Enter` to execute the image generation

In this workflow, we mainly use the `Pad Image for outpainting` node to control the direction and range of image extension. This is actually an [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint.mdx) workflow, but we use different nodes to build the mask.

### [​](http://docs.comfy.org#pad-image-for-outpainting-node) Pad Image for outpainting Node

This node accepts an input image and outputs an extended image with a corresponding mask, where the mask is built based on the node parameters.

#### [​](http://docs.comfy.org#input-parameters) Input Parameters

Parameter NameFunction`image`Input image`left`Left padding amount`top`Top padding amount`right`Right padding amount`bottom`Bottom padding amount`feathering`Controls the smoothness of the transition between the original image and the added padding, higher values create smoother transitions

#### [​](http://docs.comfy.org#output-parameters) Output Parameters

Parameter NameFunction`image`Output `image` represents the padded image`mask`Output `mask` indicates the original image area and the added padding area

#### [​](http://docs.comfy.org#node-output-content) Node Output Content

After processing by the `Pad Image for outpainting` node, the output image and mask preview are as follows:

You can see the corresponding output results:

- The `Image` output is the extended image
- The `Mask` output is the mask marking the extension areas

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/basic/outpaint.mdx)

[Previous](http://docs.comfy.org/tutorials/basic/inpaint)

[UpscaleThis guide explains the concept of image upscaling in AI drawing and demonstrates how to implement an image upscaling workflow in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/basic/upscale)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [About Outpainting](http://docs.comfy.org#about-outpainting)
- [ComfyUI Outpainting Workflow Example Explanation](http://docs.comfy.org#comfyui-outpainting-workflow-example-explanation)
- [Preparation](http://docs.comfy.org#preparation)
- [1. Model Installation](http://docs.comfy.org#1-model-installation)
- [2. Input Image](http://docs.comfy.org#2-input-image)
- [3. Outpainting Workflow](http://docs.comfy.org#3-outpainting-workflow)
- [Outpainting Workflow Usage Explanation](http://docs.comfy.org#outpainting-workflow-usage-explanation)
- [Pad Image for outpainting Node](http://docs.comfy.org#pad-image-for-outpainting-node)
- [Input Parameters](http://docs.comfy.org#input-parameters)
- [Output Parameters](http://docs.comfy.org#output-parameters)
- [Node Output Content](http://docs.comfy.org#node-output-content)

<!-- END Development/tutorials/basic/outpaint.md -->


<!-- BEGIN Development/tutorials/basic/text-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
  
  - [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image)
  - [Image to Image](http://docs.comfy.org/tutorials/basic/image-to-image)
  - [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint)
  - [Outpaint](http://docs.comfy.org/tutorials/basic/outpaint)
  - [Upscale](http://docs.comfy.org/tutorials/basic/upscale)
  - [LoRA](http://docs.comfy.org/tutorials/basic/lora)
  - [Multiple LoRAs](http://docs.comfy.org/tutorials/basic/multiple-loras)
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Text to Image Workflow

# ComfyUI Text to Image Workflow

This guide will help you understand the concept of text-to-image in AI art generation and complete a text-to-image workflow in ComfyUI

This guide aims to introduce you to ComfyUI’s text-to-image workflow and help you understand the functionality and usage of various ComfyUI nodes.

In this document, we will:

- Complete a text-to-image workflow
- Gain a basic understanding of diffusion model principles
- Learn about the functions and roles of workflow nodes
- Get an initial understanding of the SD1.5 model

We’ll start by running a text-to-image workflow, followed by explanations of related concepts. Please choose the relevant sections based on your needs.

## [​](http://docs.comfy.org#about-text-to-image) About Text to Image

**Text to Image** is a fundamental process in AI art generation that creates images from text descriptions, with **diffusion models** at its core.

The text-to-image process requires the following elements:

- **Artist:** The image generation model
- **Canvas:** The latent space
- **Image Requirements (Prompts):** Including positive prompts (elements you want in the image) and negative prompts (elements you don’t want)

This text-to-image generation process can be simply understood as telling your requirements (positive and negative prompts) to an **artist (the image model)**, who then creates what you want based on these requirements.

## [​](http://docs.comfy.org#comfyui-text-to-image-workflow-example-guide) ComfyUI Text to Image Workflow Example Guide

### [​](http://docs.comfy.org#1-preparation) 1. Preparation

Ensure you have at least one SD1.5 model file in your `ComfyUI/models/checkpoints` folder, such as [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors)

If you haven’t installed it yet, please refer to the model installation section in [Getting Started with ComfyUI AI Art Generation](http://docs.comfy.org/get_started/first_generation).

### [​](http://docs.comfy.org#2-loading-the-text-to-image-workflow) 2. Loading the Text to Image Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:

Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`.

### [​](http://docs.comfy.org#3-loading-the-model-and-generating-your-first-image) 3. Loading the Model and Generating Your First Image

After installing the image model, follow the steps in the image below to load the model and generate your first image

Follow these steps according to the image numbers:

1. In the **Load Checkpoint** node, use the arrows or click the text area to ensure **v1-5-pruned-emaonly-fp16.safetensors** is selected, and the left/right arrows don’t show **null** text
2. Click the `Queue` button or use the shortcut `Ctrl + Enter` to execute image generation

After the process completes, you should see the resulting image in the **Save Image** node interface, which you can right-click to save locally

If you’re not satisfied with the result, try running the generation multiple times. Each time you run it, **KSampler** will use a different random seed based on the `seed` parameter, so each generation will produce different results

### [​](http://docs.comfy.org#4-start-experimenting) 4. Start Experimenting

Try modifying the text in the **CLIP Text Encoder**

The `Positive` connection to the KSampler node represents positive prompts, while the `Negative` connection represents negative prompts

Here are some basic prompting principles for the SD1.5 model:

- Use English whenever possible
- Separate prompts with English commas `,`
- Use phrases rather than long sentences
- Use specific descriptions
- Use expressions like `(golden hour:1.2)` to increase the weight of specific keywords, making them more likely to appear in the image. `1.2` is the weight, `golden hour` is the keyword
- Use keywords like `masterpiece, best quality, 4k` to improve generation quality

Here are several prompt examples you can try, or use your own prompts for generation:

**1. Anime Style**

Positive prompts:

```plaintext
anime style, 1girl with long pink hair, cherry blossom background, studio ghibli aesthetic, soft lighting, intricate details

masterpiece, best quality, 4k
```

Negative prompts:

```plaintext
low quality, blurry, deformed hands, extra fingers
```

**2. Realistic Style**

Positive prompts:

```plaintext
(ultra realistic portrait:1.3), (elegant woman in crimson silk dress:1.2), 
full body, soft cinematic lighting, (golden hour:1.2), 
(fujifilm XT4:1.1), shallow depth of field, 
(skin texture details:1.3), (film grain:1.1), 
gentle wind flow, warm color grading, (perfect facial symmetry:1.3)
```

Negative prompts:

```plaintext
(deformed, cartoon, anime, doll, plastic skin, overexposed, blurry, extra fingers)
```

**3. Specific Artist Style**

Positive prompts:

```plaintext
fantasy elf, detailed character, glowing magic, vibrant colors, long flowing hair, elegant armor, ethereal beauty, mystical forest, magical aura, high detail, soft lighting, fantasy portrait, Artgerm style
```

Negative prompts:

```plaintext
blurry, low detail, cartoonish, unrealistic anatomy, out of focus, cluttered, flat lighting
```

## [​](http://docs.comfy.org#text-to-image-working-principles) Text to Image Working Principles

The entire text-to-image process can be understood as a **reverse diffusion process**. The [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) we downloaded is a pre-trained model that can **generate target images from pure Gaussian noise**. We only need to input our prompts, and it can generate target images through denoising random noise.

We need to understand two concepts:

1. **Latent Space:** Latent Space is an abstract data representation method in diffusion models. Converting images from pixel space to latent space reduces storage space and makes it easier to train diffusion models and reduce denoising complexity. It’s like architects using blueprints (latent space) for design rather than designing directly on the building (pixel space), maintaining structural features while significantly reducing modification costs
2. **Pixel Space:** Pixel Space is the storage space for images, which is the final image we see, used to store pixel values.

If you want to learn more about diffusion models, you can read these papers:

- [Denoising Diffusion Probabilistic Models (DDPM)](https://arxiv.org/pdf/2006.11239)
- [Denoising Diffusion Implicit Models (DDIM)](https://arxiv.org/pdf/2010.02502)
- [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752)

## [​](http://docs.comfy.org#comfyui-text-to-image-workflow-node-explanation) ComfyUI Text to Image Workflow Node Explanation

### [​](http://docs.comfy.org#a-load-checkpoint-node) A. Load Checkpoint Node

This node is typically used to load the image generation model. A `checkpoint` usually contains three components: `MODEL (UNet)`, `CLIP`, and `VAE`

- `MODEL (UNet)`: The UNet model responsible for noise prediction and image generation during the diffusion process
- `CLIP`: The text encoder that converts our text prompts into vectors that the model can understand, as the model cannot directly understand text prompts
- `VAE`: The Variational AutoEncoder that converts images between pixel space and latent space, as diffusion models work in latent space while our images are in pixel space

### [​](http://docs.comfy.org#b-empty-latent-image-node) B. Empty Latent Image Node

Defines a latent space that outputs to the KSampler node. The Empty Latent Image node constructs a **pure noise latent space**

You can think of its function as defining the canvas size, which determines the dimensions of our final generated image

### [​](http://docs.comfy.org#c-clip-text-encoder-node) C. CLIP Text Encoder Node

Used to encode prompts, which are your requirements for the image

- The `Positive` condition input connected to the KSampler node represents positive prompts (elements you want in the image)
- The `Negative` condition input connected to the KSampler node represents negative prompts (elements you don’t want in the image)

The prompts are encoded into semantic vectors by the `CLIP` component from the `Load Checkpoint` node and output as conditions to the KSampler node

### [​](http://docs.comfy.org#d-ksampler-node) D. KSampler Node

The **KSampler** is the core of the entire workflow, where the entire noise denoising process occurs, ultimately outputting a latent space image

Here’s an explanation of the KSampler node parameters:

Parameter NameDescriptionFunction**model**Diffusion model used for denoisingDetermines the style and quality of generated images**positive**Positive prompt condition encodingGuides generation to include specified elements**negative**Negative prompt condition encodingSuppresses unwanted content**latent\_image**Latent space image to be denoisedServes as the input carrier for noise initialization**seed**Random seed for noise generationControls generation randomness**control\_after\_generate**Seed control mode after generationDetermines seed variation pattern in batch generation**steps**Number of denoising iterationsMore steps mean finer details but longer processing time**cfg**Classifier-free guidance scaleControls prompt constraint strength (too high leads to overfitting)**sampler\_name**Sampling algorithm nameDetermines the mathematical method for denoising path**scheduler**Scheduler typeControls noise decay rate and step size allocation**denoise**Denoising strength coefficientControls noise strength added to latent space, 0.0 preserves original input features, 1.0 is complete noise

In the KSampler node, the latent space uses `seed` as an initialization parameter to construct random noise, and semantic vectors `Positive` and `Negative` are input as conditions to the diffusion model

Then, based on the number of denoising steps specified by the `steps` parameter, denoising is performed. Each denoising step uses the denoising strength coefficient specified by the `denoise` parameter to denoise the latent space and generate a new latent space image

### [​](http://docs.comfy.org#e-vae-decode-node) E. VAE Decode Node

Converts the latent space image output from the **KSampler** into a pixel space image

### [​](http://docs.comfy.org#f-save-image-node) F. Save Image Node

Previews and saves the decoded image from latent space to the local `ComfyUI/output` folder

## [​](http://docs.comfy.org#introduction-to-sd1-5-model) Introduction to SD1.5 Model

**SD1.5 (Stable Diffusion 1.5)** is an AI image generation model developed by [Stability AI](https://stability.ai/). It’s the foundational version of the Stable Diffusion series, trained on **512×512** resolution images, making it particularly good at generating images at this resolution. With a size of about 4GB, it runs smoothly on **consumer-grade GPUs (e.g., 6GB VRAM)**. Currently, SD1.5 has a rich ecosystem, supporting various plugins (like ControlNet, LoRA) and optimization tools. As a milestone model in AI art generation, SD1.5 remains the best entry-level choice thanks to its open-source nature, lightweight architecture, and rich ecosystem. Although newer versions like SDXL/SD3 have been released, its value for consumer-grade hardware remains unmatched.

### [​](http://docs.comfy.org#basic-information) Basic Information

- **Release Date**: October 2022
- **Core Architecture**: Based on Latent Diffusion Model (LDM)
- **Training Data**: LAION-Aesthetics v2.5 dataset (approximately 590M training steps)
- **Open Source Features**: Fully open-source model/code/training data

### [​](http://docs.comfy.org#advantages-and-limitations) Advantages and Limitations

Model Advantages:

- Lightweight: Small size, only about 4GB, runs smoothly on consumer GPUs
- Low Entry Barrier: Supports a wide range of plugins and optimization tools
- Mature Ecosystem: Extensive plugin and tool support
- Fast Generation: Smooth operation on consumer GPUs

Model Limitations:

- Detail Handling: Hands/complex lighting prone to distortion
- Resolution Limits: Quality degrades for direct 1024x1024 generation
- Prompt Dependency: Requires precise English descriptions for control

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/basic/text-to-image.mdx)

[Previous](http://docs.comfy.org/interface/shortcuts)

[Image to ImageThis guide will help you understand and complete an image to image workflow  
\
Next](http://docs.comfy.org/tutorials/basic/image-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [About Text to Image](http://docs.comfy.org#about-text-to-image)
- [ComfyUI Text to Image Workflow Example Guide](http://docs.comfy.org#comfyui-text-to-image-workflow-example-guide)
- [1. Preparation](http://docs.comfy.org#1-preparation)
- [2. Loading the Text to Image Workflow](http://docs.comfy.org#2-loading-the-text-to-image-workflow)
- [3. Loading the Model and Generating Your First Image](http://docs.comfy.org#3-loading-the-model-and-generating-your-first-image)
- [4. Start Experimenting](http://docs.comfy.org#4-start-experimenting)
- [Text to Image Working Principles](http://docs.comfy.org#text-to-image-working-principles)
- [ComfyUI Text to Image Workflow Node Explanation](http://docs.comfy.org#comfyui-text-to-image-workflow-node-explanation)
- [A. Load Checkpoint Node](http://docs.comfy.org#a-load-checkpoint-node)
- [B. Empty Latent Image Node](http://docs.comfy.org#b-empty-latent-image-node)
- [C. CLIP Text Encoder Node](http://docs.comfy.org#c-clip-text-encoder-node)
- [D. KSampler Node](http://docs.comfy.org#d-ksampler-node)
- [E. VAE Decode Node](http://docs.comfy.org#e-vae-decode-node)
- [F. Save Image Node](http://docs.comfy.org#f-save-image-node)
- [Introduction to SD1.5 Model](http://docs.comfy.org#introduction-to-sd1-5-model)
- [Basic Information](http://docs.comfy.org#basic-information)
- [Advantages and Limitations](http://docs.comfy.org#advantages-and-limitations)

<!-- END Development/tutorials/basic/text-to-image.md -->


<!-- BEGIN Development/tutorials/basic/upscale.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
  
  - [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image)
  - [Image to Image](http://docs.comfy.org/tutorials/basic/image-to-image)
  - [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint)
  - [Outpaint](http://docs.comfy.org/tutorials/basic/outpaint)
  - [Upscale](http://docs.comfy.org/tutorials/basic/upscale)
  - [LoRA](http://docs.comfy.org/tutorials/basic/lora)
  - [Multiple LoRAs](http://docs.comfy.org/tutorials/basic/multiple-loras)
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Image Upscale Workflow

# ComfyUI Image Upscale Workflow

This guide explains the concept of image upscaling in AI drawing and demonstrates how to implement an image upscaling workflow in ComfyUI

## [​](http://docs.comfy.org#what-is-image-upscaling%3F) What is Image Upscaling?

Image Upscaling is the process of converting low-resolution images to high-resolution using algorithms. Unlike traditional interpolation methods, AI upscaling models (like ESRGAN) can intelligently reconstruct details while maintaining image quality.

For instance, the default SD1.5 model often struggles with large-size image generation. To achieve high-resolution results,we typically generate smaller images first and then use upscaling techniques.

This article covers one of many upscaling methods in ComfyUI. In this tutorial, we’ll guide you through:

1. Downloading and installing upscaling models
2. Performing basic image upscaling
3. Combining text-to-image workflows with upscaling

## [​](http://docs.comfy.org#upscaling-workflow) Upscaling Workflow

### [​](http://docs.comfy.org#model-installation) Model Installation

Required ESRGAN models download:

1

Visit OpenModelDB

Visit [OpenModelDB](https://openmodeldb.info/) to search and download upscaling models (e.g., RealESRGAN)

As shown:

1. Filter models by image type using the category selector
2. The model’s magnification factor is indicated in the top-right corner (e.g., 2x in the screenshot)

We’ll use the [4x-ESRGAN](https://openmodeldb.info/models/4x-ESRGAN) model for this tutorial. Click the `Download` button on the model detail page.

2

Save Model Files in Directory

Save the model file (.pth) in `ComfyUI/models/upscale_models` directory

### [​](http://docs.comfy.org#workflow-and-assets) Workflow and Assets

Download and drag the following image into ComfyUI to load the basic upscaling workflow:

Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`.

Use this image in smaller size as input:

### [​](http://docs.comfy.org#complete-the-workflow-step-by-step) Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

1. Ensure `Load Upscale Model` loads `4x-ESRGAN.pth`
2. Upload the input image to the `Load Image` node
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to generate the image

The core components are the `Load Upscale Model` and `Upscale Image (Using Model)` nodes, which receive an image input and upscale it using the selected model.

## [​](http://docs.comfy.org#text-to-image-combined-workflow) Text-to-Image Combined Workflow

After mastering basic upscaling, we can combine it with the [text-to-image](http://docs.comfy.org/tutorials/basic/text-to-image) workflow. For text-to-image basics, refer to the [text-to-image tutorial](http://docs.comfy.org/tutorials/basic/text-to-image).

Download and drag this image into ComfyUI to load the combined workflow:

This workflow connects the text-to-image output image directly to the upscaling nodes for final processing.

## [​](http://docs.comfy.org#additional-tips) Additional Tips

Model characteristics:

- **RealESRGAN**: General-purpose upscaling
- **BSRGAN**: Excels with text and sharp edges
- **SwinIR**: Preserves natural textures, ideal for landscapes

<!--THE END-->

1. **Chained Upscaling**: Combine multiple upscale nodes (e.g., 2x → 4x) for ultra-high magnification
2. **Hybrid Workflow**: Connect upscale nodes after generation for “generate+enhance” pipelines
3. **Comparative Testing**: Different models perform better on specific image types - test multiple options

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/basic/upscale.mdx)

[Previous](http://docs.comfy.org/tutorials/basic/outpaint)

[LoRAThis guide will help you understand and use a single LoRA model  
\
Next](http://docs.comfy.org/tutorials/basic/lora)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [What is Image Upscaling?](http://docs.comfy.org#what-is-image-upscaling%3F)
- [Upscaling Workflow](http://docs.comfy.org#upscaling-workflow)
- [Model Installation](http://docs.comfy.org#model-installation)
- [Workflow and Assets](http://docs.comfy.org#workflow-and-assets)
- [Complete the Workflow Step by Step](http://docs.comfy.org#complete-the-workflow-step-by-step)
- [Text-to-Image Combined Workflow](http://docs.comfy.org#text-to-image-combined-workflow)
- [Additional Tips](http://docs.comfy.org#additional-tips)

<!-- END Development/tutorials/basic/upscale.md -->


<!-- BEGIN Development/tutorials/controlnet/controlnet.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
  
  - [ControlNet](http://docs.comfy.org/tutorials/controlnet/controlnet)
  - [Pose ControlNet](http://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass)
  - [Depth ControlNet](http://docs.comfy.org/tutorials/controlnet/depth-controlnet)
  - [Depth T2I Adapter](http://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter)
  - [Mixing ControlNet](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets)
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI ControlNet Usage Example

# ComfyUI ControlNet Usage Example

This guide will introduce you to the basic concepts of ControlNet and demonstrate how to generate corresponding images in ComfyUI

Achieving precise control over image creation in AI image generation cannot be done with just one click. It typically requires numerous generation attempts to produce a satisfactory image. However, the emergence of **ControlNet** has effectively addressed this challenge.

ControlNet is a conditional control generation model based on diffusion models (such as Stable Diffusion), first proposed by [Lvmin Zhang](https://lllyasviel.github.io/) and Maneesh Agrawala et al. in 2023 in the paper [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543).

ControlNet models significantly enhance the controllability of image generation and the ability to reproduce details by introducing multimodal input conditions, such as edge detection maps, depth maps, and pose keypoints.

These conditioning constraints make image generation more controllable, allowing multiple ControlNet models to be used simultaneously during the drawing process for better results.

Before ControlNet, we could only rely on the model to generate images repeatedly until we were satisfied with the results, which involved a lot of randomness.

With the advent of ControlNet, we can control image generation by introducing additional conditions. For example, we can use a simple sketch to guide the image generation process, producing images that closely align with our sketch.

In this example, we will guide you through installing and using ControlNet models in [ComfyUI](https://github.com/comfyanonymous/ComfyUI), and complete a sketch-controlled image generation example.

The workflows for other types of ControlNet V1.1 models are similar to this example. You only need to select the appropriate model and upload the corresponding reference image based on your needs.

## [​](http://docs.comfy.org#controlnet-image-preprocessing-information) ControlNet Image Preprocessing Information

Different types of ControlNet models typically require different types of reference images:

> Image source: [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

Since the current **Comfy Core** nodes do not include all types of **preprocessors**, in the actual examples in this documentation, we will provide pre-processed images. However, in practical use, you may need to use custom nodes to preprocess images to meet the requirements of different ControlNet models. Below are some relevant custom nodes:

- [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
- [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

## [​](http://docs.comfy.org#comfyui-controlnet-workflow-example-explanation) ComfyUI ControlNet Workflow Example Explanation

### [​](http://docs.comfy.org#1-controlnet-workflow-assets) 1. ControlNet Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`. This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.

Please download the image below, which we will use as input:

### [​](http://docs.comfy.org#2-manual-model-installation) 2. Manual Model Installation

If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:

- [dreamCreationVirtual3DECommerce\_v10.safetensors](https://civitai.com/api/download/models/731340?type=Model&format=SafeTensor&size=full&fp=fp16)
- [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)
- [control\_v11p\_sd15\_scribble\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── dreamCreationVirtual3DECommerce_v10.safetensors
│   ├── vae/
│   │   └── vae-ft-mse-840000-ema-pruned.safetensors
│   └── controlnet/
│       └── control_v11p_sd15_scribble_fp16.safetensors
```

In this example, you could also use the VAE model embedded in dreamCreationVirtual3DECommerce\_v10.safetensors, but we’re following the model author’s recommendation to use a separate VAE model.

### [​](http://docs.comfy.org#3-step-by-step-workflow-execution) 3. Step-by-Step Workflow Execution

1. Ensure that `Load Checkpoint` can load **dreamCreationVirtual3DECommerce\_v10.safetensors**
2. Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**
3. Click `Upload` in the `Load Image` node to upload the input image provided earlier
4. Ensure that `Load ControlNet` can load **control\_v11p\_sd15\_scribble\_fp16.safetensors**
5. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## [​](http://docs.comfy.org#related-node-explanations) Related Node Explanations

### [​](http://docs.comfy.org#load-controlnet-node-explanation) Load ControlNet Node Explanation

Models located in `ComfyUI\models\controlnet` will be detected by ComfyUI and can be loaded through this node.

### [​](http://docs.comfy.org#apply-controlnet-node-explanation) Apply ControlNet Node Explanation

This node accepts the ControlNet model loaded by `load controlnet` and generates corresponding control conditions based on the input image.

**Input Types**

Parameter NameFunction`positive`Positive conditioning`negative`Negative conditioning`control_net`The ControlNet model to be applied`image`Preprocessed image used as reference for ControlNet application`vae`VAE model input`strength`Strength of ControlNet application; higher values increase ControlNet’s influence on the generated image`start_percent`Determines when to start applying ControlNet as a percentage; e.g., 0.2 means ControlNet guidance begins when 20% of diffusion is complete`end_percent`Determines when to stop applying ControlNet as a percentage; e.g., 0.8 means ControlNet guidance stops when 80% of diffusion is complete

**Output Types**

Parameter NameFunction`positive`Positive conditioning data processed by ControlNet`negative`Negative conditioning data processed by ControlNet

You can use chain connections to apply multiple ControlNet models, as shown in the image below. You can also refer to the [Mixing ControlNet Models](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets.mdx) guide to learn more about combining multiple ControlNet models.

You might see the `Apply ControlNet(Old)` node in some early workflows, which is an early version of the ControlNet node. It is currently deprecated and not visible by default in search and node lists. To enable it, go to **Settings** —&gt; **comfy** —&gt; **Node** and enable the `Show deprecated nodes in search` option. However, it’s recommended to use the new node.

## [​](http://docs.comfy.org#start-your-exploration) Start Your Exploration

1. Try creating similar sketches, or even draw your own, and use ControlNet models to generate images to experience the benefits of ControlNet.
2. Adjust the `Control Strength` parameter in the Apply ControlNet node to control the influence of the ControlNet model on the generated image.
3. Visit the [ControlNet-v1-1\_fp16\_safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/tree/main) repository to download other types of ControlNet models and try using them to generate images.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/controlnet/controlnet.mdx)

[Previous](http://docs.comfy.org/tutorials/basic/multiple-loras)

[Pose ControlNetThis guide will introduce you to the basic concepts of Pose ControlNet, and demonstrate how to generate large-sized images in ComfyUI using a two-pass generation approach  
\
Next](http://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [ControlNet Image Preprocessing Information](http://docs.comfy.org#controlnet-image-preprocessing-information)
- [ComfyUI ControlNet Workflow Example Explanation](http://docs.comfy.org#comfyui-controlnet-workflow-example-explanation)
- [1. ControlNet Workflow Assets](http://docs.comfy.org#1-controlnet-workflow-assets)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation)
- [3. Step-by-Step Workflow Execution](http://docs.comfy.org#3-step-by-step-workflow-execution)
- [Related Node Explanations](http://docs.comfy.org#related-node-explanations)
- [Load ControlNet Node Explanation](http://docs.comfy.org#load-controlnet-node-explanation)
- [Apply ControlNet Node Explanation](http://docs.comfy.org#apply-controlnet-node-explanation)
- [Start Your Exploration](http://docs.comfy.org#start-your-exploration)

<!-- END Development/tutorials/controlnet/controlnet.md -->


<!-- BEGIN Development/tutorials/controlnet/depth-controlnet.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
  
  - [ControlNet](http://docs.comfy.org/tutorials/controlnet/controlnet)
  - [Pose ControlNet](http://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass)
  - [Depth ControlNet](http://docs.comfy.org/tutorials/controlnet/depth-controlnet)
  - [Depth T2I Adapter](http://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter)
  - [Mixing ControlNet](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets)
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Depth ControlNet Usage Example

# ComfyUI Depth ControlNet Usage Example

This guide will introduce you to the basic concepts of Depth ControlNet and demonstrate how to generate corresponding images in ComfyUI

## [​](http://docs.comfy.org#introduction-to-depth-maps-and-depth-controlnet) Introduction to Depth Maps and Depth ControlNet

A depth map is a special type of image that uses grayscale values to represent the distance between objects in a scene and the observer or camera. In a depth map, the grayscale value is inversely proportional to distance: brighter areas (closer to white) indicate objects that are closer, while darker areas (closer to black) indicate objects that are farther away.

Depth ControlNet is a ControlNet model specifically trained to understand and utilize depth map information. It helps AI correctly interpret spatial relationships, ensuring that generated images conform to the spatial structure specified by the depth map, thereby enabling precise control over three-dimensional spatial layouts.

### [​](http://docs.comfy.org#application-scenarios-for-depth-maps-with-controlnet) Application Scenarios for Depth Maps with ControlNet

Depth maps have numerous applications in various scenarios:

1. **Portrait Scenes**: Control the spatial relationship between subjects and backgrounds, avoiding distortion in critical areas such as faces
2. **Landscape Scenes**: Control the hierarchical relationships between foreground, middle ground, and background
3. **Architectural Scenes**: Control the spatial structure and perspective relationships of buildings
4. **Product Showcase**: Control the separation and spatial positioning of products against their backgrounds

In this example, we will use a depth map to generate an architectural visualization scene.

## [​](http://docs.comfy.org#comfyui-controlnet-workflow-example-explanation) ComfyUI ControlNet Workflow Example Explanation

### [​](http://docs.comfy.org#1-controlnet-workflow-assets) 1. ControlNet Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`. This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.

Please download the image below, which we will use as input:

### [​](http://docs.comfy.org#2-model-installation) 2. Model Installation

If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:

- [architecturerealmix\_v11.safetensors](https://civitai.com/api/download/models/431755?type=Model&format=SafeTensor&size=full&fp=fp16)
- [control\_v11f1p\_sd15\_depth\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors?download=true)

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── architecturerealmix_v11.safetensors
│   └── controlnet/
│       └── control_v11f1p_sd15_depth_fp16.safetensors
```

### [​](http://docs.comfy.org#3-step-by-step-workflow-execution) 3. Step-by-Step Workflow Execution

1. Ensure that `Load Checkpoint` can load **architecturerealmix\_v11.safetensors**
2. Ensure that `Load ControlNet` can load **control\_v11f1p\_sd15\_depth\_fp16.safetensors**
3. Click `Upload` in the `Load Image` node to upload the depth image provided earlier
4. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## [​](http://docs.comfy.org#combining-depth-control-with-other-techniques) Combining Depth Control with Other Techniques

Based on different creative needs, you can combine Depth ControlNet with other types of ControlNet to achieve better results:

1. **Depth + Lineart**: Maintain spatial relationships while reinforcing outlines, suitable for architecture, products, and character design
2. **Depth + Pose**: Control character posture while maintaining correct spatial relationships, suitable for character scenes

For more information on using multiple ControlNet models together, please refer to the [Mixing ControlNet](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets.mdx) example.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/controlnet/depth-controlnet.mdx)

[Previous](http://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass)

[Depth T2I AdapterThis guide will introduce you to the basic concepts of Depth T2I Adapter and demonstrate how to generate corresponding images in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Introduction to Depth Maps and Depth ControlNet](http://docs.comfy.org#introduction-to-depth-maps-and-depth-controlnet)
- [Application Scenarios for Depth Maps with ControlNet](http://docs.comfy.org#application-scenarios-for-depth-maps-with-controlnet)
- [ComfyUI ControlNet Workflow Example Explanation](http://docs.comfy.org#comfyui-controlnet-workflow-example-explanation)
- [1. ControlNet Workflow Assets](http://docs.comfy.org#1-controlnet-workflow-assets)
- [2. Model Installation](http://docs.comfy.org#2-model-installation)
- [3. Step-by-Step Workflow Execution](http://docs.comfy.org#3-step-by-step-workflow-execution)
- [Combining Depth Control with Other Techniques](http://docs.comfy.org#combining-depth-control-with-other-techniques)

<!-- END Development/tutorials/controlnet/depth-controlnet.md -->


<!-- BEGIN Development/tutorials/controlnet/depth-t2i-adapter.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
  
  - [ControlNet](http://docs.comfy.org/tutorials/controlnet/controlnet)
  - [Pose ControlNet](http://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass)
  - [Depth ControlNet](http://docs.comfy.org/tutorials/controlnet/depth-controlnet)
  - [Depth T2I Adapter](http://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter)
  - [Mixing ControlNet](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets)
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Depth T2I Adapter Usage Example

# ComfyUI Depth T2I Adapter Usage Example

This guide will introduce you to the basic concepts of Depth T2I Adapter and demonstrate how to generate corresponding images in ComfyUI

## [​](http://docs.comfy.org#introduction-to-t2i-adapter) Introduction to T2I Adapter

[T2I-Adapter](https://huggingface.co/TencentARC/T2I-Adapter) is a lightweight adapter developed by [Tencent ARC Lab](https://github.com/TencentARC) designed to enhance the structural, color, and style control capabilities of text-to-image generation models (such as Stable Diffusion). It works by aligning external conditions (such as edge detection maps, depth maps, sketches, or color reference images) with the model’s internal features, achieving high-precision control without modifying the original model structure. With only about 77M parameters (approximately 300MB in size), its inference speed is about 3 times faster than [ControlNet](https://github.com/lllyasviel/ControlNet-v1-1-nightly), and it supports multiple condition combinations (such as sketch + color grid). Application scenarios include line art to image conversion, color style transfer, multi-element scene generation, and more.

### [​](http://docs.comfy.org#comparison-between-t2i-adapter-and-controlnet) Comparison Between T2I Adapter and ControlNet

Although their functions are similar, there are notable differences in implementation and application:

1. **Lightweight Design**: T2I Adapter has fewer parameters and a smaller memory footprint
2. **Inference Speed**: T2I Adapter is typically about 3 times faster than ControlNet
3. **Control Precision**: ControlNet offers more precise control in certain scenarios, while T2I Adapter is more suitable for lightweight control
4. **Multi-condition Combination**: T2I Adapter shows more significant resource advantages when combining multiple conditions

### [​](http://docs.comfy.org#main-types-of-t2i-adapter) Main Types of T2I Adapter

T2I Adapter provides various types to control different aspects:

- **Depth**: Controls the spatial structure and depth relationships in images
- **Line Art (Canny/Sketch)**: Controls image edges and lines
- **Keypose**: Controls character poses and actions
- **Segmentation (Seg)**: Controls scene layout through semantic segmentation
- **Color**: Controls the overall color scheme of images

In ComfyUI, using T2I Adapter is similar to [ControlNet](http://docs.comfy.org/tutorials/controlnet/controlnet.mdx) in terms of interface and workflow. In this example, we will demonstrate how to use a depth T2I Adapter to control an interior scene.

## [​](http://docs.comfy.org#value-of-depth-t2i-adapter-applications) Value of Depth T2I Adapter Applications

Depth maps have several important applications in image generation:

1. **Spatial Layout Control**: Accurately describes three-dimensional spatial structures, suitable for interior design and architectural visualization
2. **Object Positioning**: Controls the relative position and size of objects in a scene, suitable for product showcases and scene construction
3. **Perspective Relationships**: Maintains reasonable perspective and proportions, suitable for landscape and urban scene generation
4. **Light and Shadow Layout**: Natural light and shadow distribution based on depth information, enhancing realism

We will use interior design as an example to demonstrate how to use the depth T2I Adapter, but these techniques are applicable to other scenarios as well.

## [​](http://docs.comfy.org#comfyui-depth-t2i-adapter-workflow-example-explanation) ComfyUI Depth T2I Adapter Workflow Example Explanation

### [​](http://docs.comfy.org#1-depth-t2i-adapter-workflow-assets) 1. Depth T2I Adapter Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`. This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.

Please download the image below, which we will use as input:

### [​](http://docs.comfy.org#2-model-installation) 2. Model Installation

If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:

- [interiordesignsuperm\_v2.safetensors](https://civitai.com/api/download/models/93152?type=Model&format=SafeTensor&size=full&fp=fp16)
- [t2iadapter\_depth\_sd15v2.pth](https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_depth_sd15v2.pth?download=true)

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── interiordesignsuperm_v2.safetensors
│   └── controlnet/
│       └── t2iadapter_depth_sd15v2.pth
```

### [​](http://docs.comfy.org#3-step-by-step-workflow-execution) 3. Step-by-Step Workflow Execution

1. Ensure that `Load Checkpoint` can load **interiordesignsuperm\_v2.safetensors**
2. Ensure that `Load ControlNet` can load **t2iadapter\_depth\_sd15v2.pth**
3. Click `Upload` in the `Load Image` node to upload the input image provided earlier
4. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## [​](http://docs.comfy.org#general-tips-for-using-t2i-adapter) General Tips for Using T2I Adapter

### [​](http://docs.comfy.org#input-image-quality-optimization) Input Image Quality Optimization

Regardless of the application scenario, high-quality input images are key to successfully using T2I Adapter:

1. **Moderate Contrast**: Control images (such as depth maps, line art) should have clear contrast, but not excessively extreme
2. **Clear Boundaries**: Ensure that major structures and element boundaries are clearly distinguishable in the control image
3. **Noise Control**: Try to avoid excessive noise in control images, especially for depth maps and line art
4. **Reasonable Layout**: Control images should have a reasonable spatial layout and element distribution

## [​](http://docs.comfy.org#characteristics-of-t2i-adapter-usage) Characteristics of T2I Adapter Usage

One major advantage of T2I Adapter is its ability to easily combine multiple conditions for complex control effects:

1. **Depth + Edge**: Control spatial layout while maintaining clear structural edges, suitable for architecture and interior design
2. **Line Art + Color**: Control shapes while specifying color schemes, suitable for character design and illustrations
3. **Pose + Segmentation**: Control character actions while defining scene areas, suitable for complex narrative scenes

Mixing different T2I Adapters, or combining them with other control methods (such as ControlNet, regional prompts, etc.), can further expand creative possibilities. To achieve mixing, simply chain multiple `Apply ControlNet` nodes together in the same way as described in [Mixing ControlNet](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets.mdx).

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/controlnet/depth-t2i-adapter.mdx)

[Previous](http://docs.comfy.org/tutorials/controlnet/depth-controlnet)

[Mixing ControlNetIn this example, we will demonstrate how to mix multiple ControlNets and learn to use multiple ControlNet models to control image generation  
\
Next](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Introduction to T2I Adapter](http://docs.comfy.org#introduction-to-t2i-adapter)
- [Comparison Between T2I Adapter and ControlNet](http://docs.comfy.org#comparison-between-t2i-adapter-and-controlnet)
- [Main Types of T2I Adapter](http://docs.comfy.org#main-types-of-t2i-adapter)
- [Value of Depth T2I Adapter Applications](http://docs.comfy.org#value-of-depth-t2i-adapter-applications)
- [ComfyUI Depth T2I Adapter Workflow Example Explanation](http://docs.comfy.org#comfyui-depth-t2i-adapter-workflow-example-explanation)
- [1. Depth T2I Adapter Workflow Assets](http://docs.comfy.org#1-depth-t2i-adapter-workflow-assets)
- [2. Model Installation](http://docs.comfy.org#2-model-installation)
- [3. Step-by-Step Workflow Execution](http://docs.comfy.org#3-step-by-step-workflow-execution)
- [General Tips for Using T2I Adapter](http://docs.comfy.org#general-tips-for-using-t2i-adapter)
- [Input Image Quality Optimization](http://docs.comfy.org#input-image-quality-optimization)
- [Characteristics of T2I Adapter Usage](http://docs.comfy.org#characteristics-of-t2i-adapter-usage)

<!-- END Development/tutorials/controlnet/depth-t2i-adapter.md -->


<!-- BEGIN Development/tutorials/controlnet/mixing-controlnets.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
  
  - [ControlNet](http://docs.comfy.org/tutorials/controlnet/controlnet)
  - [Pose ControlNet](http://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass)
  - [Depth ControlNet](http://docs.comfy.org/tutorials/controlnet/depth-controlnet)
  - [Depth T2I Adapter](http://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter)
  - [Mixing ControlNet](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets)
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Mixing ControlNet Examples

# ComfyUI Mixing ControlNet Examples

In this example, we will demonstrate how to mix multiple ControlNets and learn to use multiple ControlNet models to control image generation

In AI image generation, a single control condition often fails to meet the requirements of complex scenes. Mixing multiple ControlNets allows you to control different regions or aspects of an image simultaneously, achieving more precise control over image generation.

In certain scenarios, mixing ControlNets can leverage the characteristics of different control conditions to achieve more refined conditional control:

1. **Scene Complexity**: Complex scenes require multiple control conditions working together
2. **Fine-grained Control**: By adjusting the strength parameter of each ControlNet, you can precisely control the degree of influence for each part
3. **Complementary Effects**: Different types of ControlNets can complement each other, compensating for the limitations of single controls
4. **Creative Expression**: Combining different controls can produce unique creative effects

### [​](http://docs.comfy.org#how-to-mix-controlnets) How to Mix ControlNets

When mixing multiple ControlNets, each ControlNet influences the image generation process according to its applied area. ComfyUI enables multiple ControlNet conditions to be applied sequentially in a layered manner through chain connections in the `Apply ControlNet` node:

## [​](http://docs.comfy.org#comfyui-controlnet-regional-division-mixing-example) ComfyUI ControlNet Regional Division Mixing Example

In this example, we will use a combination of **Pose ControlNet** and **Scribble ControlNet** to generate a scene containing multiple elements: a character on the left controlled by Pose ControlNet and a cat on a scooter on the right controlled by Scribble ControlNet.

### [​](http://docs.comfy.org#1-controlnet-mixing-workflow-assets) 1. ControlNet Mixing Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

This workflow image contains Metadata, and can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`. The system will automatically detect and prompt to download the required models.

Input pose image (controls the character pose on the left):

Input scribble image (controls the cat and scooter on the right):

### [​](http://docs.comfy.org#2-manual-model-installation) 2. Manual Model Installation

If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:

- [awpainting\_v14.safetensors](https://civitai.com/api/download/models/624939?type=Model&format=SafeTensor&size=full&fp=fp16)
- [control\_v11p\_sd15\_scribble\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)
- [control\_v11p\_sd15\_openpose\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)
- [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── awpainting_v14.safetensors
│   ├── controlnet/
│   │   └── control_v11p_sd15_scribble_fp16.safetensors
│   │   └── control_v11p_sd15_openpose_fp16.safetensors
│   ├── vae/
│   │   └── vae-ft-mse-840000-ema-pruned.safetensors
```

### [​](http://docs.comfy.org#3-step-by-step-workflow-execution) 3. Step-by-Step Workflow Execution

Follow these steps according to the numbered markers in the image:

1. Ensure that `Load Checkpoint` can load **awpainting\_v14.safetensors**
2. Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**

First ControlNet group using the Openpose model: 3. Ensure that `Load ControlNet Model` loads **control\_v11p\_sd15\_openpose\_fp16.safetensors** 4. Click `Upload` in the `Load Image` node to upload the pose image provided earlier

Second ControlNet group using the Scribble model: 5. Ensure that `Load ControlNet Model` loads **control\_v11p\_sd15\_scribble\_fp16.safetensors** 6. Click `Upload` in the `Load Image` node to upload the scribble image provided earlier 7. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## [​](http://docs.comfy.org#workflow-explanation) Workflow Explanation

#### [​](http://docs.comfy.org#strength-balance) Strength Balance

When controlling different regions of an image, balancing the strength parameters is particularly important:

- If the ControlNet strength for one region is significantly higher than another, it may cause that region’s control effect to overpower and suppress the other region
- It’s recommended to set similar strength values for ControlNets controlling different regions, for example, both set to 1.0

#### [​](http://docs.comfy.org#prompt-techniques) Prompt Techniques

For regional division mixing, the prompt needs to include descriptions of both regions:

```plaintext
"A woman in red dress, a cat riding a scooter, detailed background, high quality"
```

Such a prompt covers both the character and the cat on the scooter, ensuring the model pays attention to both control regions.

## [​](http://docs.comfy.org#multi-dimensional-control-applications-for-a-single-subject) Multi-dimensional Control Applications for a Single Subject

In addition to the regional division mixing shown in this example, another common mixing approach is to apply multi-dimensional control to the same subject. For example:

- **Pose + Depth**: Control character posture and spatial sense
- **Pose + Canny**: Control character posture and edge details
- **Pose + Reference**: Control character posture while referencing a specific style

In this type of application, reference images for multiple ControlNets should be aligned to the same subject, and their strengths should be adjusted to ensure proper balance.

By combining different types of ControlNets and specifying their control regions, you can achieve precise control over elements in your image.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/controlnet/mixing-controlnets.mdx)

[Previous](http://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter)

[Flux.1 Text-to-ImageThis guide provides a brief introduction to the Flux.1 model and guides you through using the Flux.1 model for text-to-image generation with examples including the full version and the FP8 Checkpoint version.  
\
Next](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [How to Mix ControlNets](http://docs.comfy.org#how-to-mix-controlnets)
- [ComfyUI ControlNet Regional Division Mixing Example](http://docs.comfy.org#comfyui-controlnet-regional-division-mixing-example)
- [1. ControlNet Mixing Workflow Assets](http://docs.comfy.org#1-controlnet-mixing-workflow-assets)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation)
- [3. Step-by-Step Workflow Execution](http://docs.comfy.org#3-step-by-step-workflow-execution)
- [Workflow Explanation](http://docs.comfy.org#workflow-explanation)
- [Strength Balance](http://docs.comfy.org#strength-balance)
- [Prompt Techniques](http://docs.comfy.org#prompt-techniques)
- [Multi-dimensional Control Applications for a Single Subject](http://docs.comfy.org#multi-dimensional-control-applications-for-a-single-subject)

<!-- END Development/tutorials/controlnet/mixing-controlnets.md -->


<!-- BEGIN Development/tutorials/controlnet/pose-controlnet-2-pass.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
  
  - [ControlNet](http://docs.comfy.org/tutorials/controlnet/controlnet)
  - [Pose ControlNet](http://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass)
  - [Depth ControlNet](http://docs.comfy.org/tutorials/controlnet/depth-controlnet)
  - [Depth T2I Adapter](http://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter)
  - [Mixing ControlNet](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets)
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Pose ControlNet Usage Example

# ComfyUI Pose ControlNet Usage Example

This guide will introduce you to the basic concepts of Pose ControlNet, and demonstrate how to generate large-sized images in ComfyUI using a two-pass generation approach

## [​](http://docs.comfy.org#introduction-to-openpose) Introduction to OpenPose

[OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) is an open-source real-time multi-person pose estimation system developed by Carnegie Mellon University (CMU), representing a significant breakthrough in the field of computer vision. The system can simultaneously detect multiple people in an image, capturing:

- **Body skeleton**: 18 keypoints, including head, shoulders, elbows, wrists, hips, knees, and ankles
- **Facial expressions**: 70 facial keypoints for capturing micro-expressions and facial contours
- **Hand details**: 21 hand keypoints for precisely expressing finger positions and gestures
- **Foot posture**: 6 foot keypoints, recording standing postures and movement details

In AI image generation, skeleton structure maps generated by OpenPose serve as conditional inputs for ControlNet, enabling precise control over the posture, actions, and expressions of generated characters. This allows us to generate realistic human figures with expected poses and actions, greatly improving the controllability and practical value of AI-generated content. Particularly for early Stable Diffusion 1.5 series models, skeletal maps generated by OpenPose can effectively prevent issues with distorted character actions, limbs, and expressions.

## [​](http://docs.comfy.org#comfyui-2-pass-pose-controlnet-usage-example) ComfyUI 2-Pass Pose ControlNet Usage Example

### [​](http://docs.comfy.org#1-pose-controlnet-workflow-assets) 1. Pose ControlNet Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`. This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.

Please download the image below, which we will use as input:

### [​](http://docs.comfy.org#2-manual-model-installation) 2. Manual Model Installation

If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:

- [control\_v11p\_sd15\_openpose\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)
- [majicmixRealistic\_v7.safetensors](https://civitai.com/api/download/models/176425?type=Model&format=SafeTensor&size=pruned&fp=fp16)
- [japaneseStyleRealistic\_v20.safetensors](https://civitai.com/api/download/models/85426?type=Model&format=SafeTensor&size=pruned&fp=fp16)
- [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── majicmixRealistic_v7.safetensors
│   │   └── japaneseStyleRealistic_v20.safetensors
│   ├── vae/
│   │   └── vae-ft-mse-840000-ema-pruned.safetensors
│   └── controlnet/
│       └── control_v11p_sd15_openpose_fp16.safetensors
```

### [​](http://docs.comfy.org#3-step-by-step-workflow-execution) 3. Step-by-Step Workflow Execution

Follow these steps according to the numbered markers in the image:

1. Ensure that `Load Checkpoint` can load **majicmixRealistic\_v7.safetensors**
2. Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**
3. Ensure that `Load ControlNet Model` can load **control\_v11p\_sd15\_openpose\_fp16.safetensors**
4. Click the select button in the `Load Image` node to upload the pose input image provided earlier, or use your own OpenPose skeleton map
5. Ensure that `Load Checkpoint` can load **japaneseStyleRealistic\_v20.safetensors**
6. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## [​](http://docs.comfy.org#explanation-of-the-pose-controlnet-2-pass-workflow) Explanation of the Pose ControlNet 2-Pass Workflow

This workflow uses a two-pass image generation approach, dividing the image creation process into two phases:

### [​](http://docs.comfy.org#first-phase%3A-basic-pose-image-generation) First Phase: Basic Pose Image Generation

In the first phase, the **majicmixRealistic\_v7** model is combined with Pose ControlNet to generate an initial character pose image:

1. First, load the majicmixRealistic\_v7 model via the `Load Checkpoint` node
2. Load the pose control model through the `Load ControlNet Model` node
3. The input pose image is fed into the `Apply ControlNet` node and combined with positive and negative prompt conditions
4. The first `KSampler` node (typically using 20-30 steps) generates a basic character pose image
5. The pixel-space image for the first phase is obtained through `VAE Decode`

This phase primarily focuses on correct character posture, pose, and basic structure, ensuring that the generated character conforms to the input skeletal pose.

### [​](http://docs.comfy.org#second-phase%3A-style-optimization-and-detail-enhancement) Second Phase: Style Optimization and Detail Enhancement

In the second phase, the output image from the first phase is used as a reference, with the **japaneseStyleRealistic\_v20** model performing stylization and detail enhancement:

1. The image generated in the first phase creates a larger resolution latent space through the `Upscale latent` node
2. The second `Load Checkpoint` loads the japaneseStyleRealistic\_v20 model, which focuses on details and style
3. The second `KSampler` node uses a lower `denoise` strength (typically 0.4-0.6) for refinement, preserving the basic structure from the first phase
4. Finally, a higher quality, larger resolution image is output through the second `VAE Decode` and `Save Image` nodes

This phase primarily focuses on style consistency, detail richness, and enhancing overall image quality.

## [​](http://docs.comfy.org#advantages-of-2-pass-image-generation) Advantages of 2-Pass Image Generation

Compared to single-pass generation, the two-pass image generation method offers the following advantages:

1. **Higher Resolution**: Two-pass processing can generate high-resolution images beyond the capabilities of single-pass generation
2. **Style Blending**: Can combine advantages of different models, such as using a realistic model in the first phase and a stylized model in the second phase
3. **Better Details**: The second phase can focus on optimizing details without having to worry about overall structure
4. **Precise Control**: Once pose control is completed in the first phase, the second phase can focus on refining style and details
5. **Reduced GPU Load**: Generating in two passes allows for high-quality large images with limited GPU resources

To learn more about techniques for mixing multiple ControlNets, please refer to the [Mixing ControlNet Models](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets.mdx) tutorial.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/controlnet/pose-controlnet-2-pass.mdx)

[Previous](http://docs.comfy.org/tutorials/controlnet/controlnet)

[Depth ControlNetThis guide will introduce you to the basic concepts of Depth ControlNet and demonstrate how to generate corresponding images in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/controlnet/depth-controlnet)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Introduction to OpenPose](http://docs.comfy.org#introduction-to-openpose)
- [ComfyUI 2-Pass Pose ControlNet Usage Example](http://docs.comfy.org#comfyui-2-pass-pose-controlnet-usage-example)
- [1. Pose ControlNet Workflow Assets](http://docs.comfy.org#1-pose-controlnet-workflow-assets)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation)
- [3. Step-by-Step Workflow Execution](http://docs.comfy.org#3-step-by-step-workflow-execution)
- [Explanation of the Pose ControlNet 2-Pass Workflow](http://docs.comfy.org#explanation-of-the-pose-controlnet-2-pass-workflow)
- [First Phase: Basic Pose Image Generation](http://docs.comfy.org#first-phase%3A-basic-pose-image-generation)
- [Second Phase: Style Optimization and Detail Enhancement](http://docs.comfy.org#second-phase%3A-style-optimization-and-detail-enhancement)
- [Advantages of 2-Pass Image Generation](http://docs.comfy.org#advantages-of-2-pass-image-generation)

<!-- END Development/tutorials/controlnet/pose-controlnet-2-pass.md -->


<!-- BEGIN Development/tutorials/flux/flux-1-controlnet.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
  
  - [Flux.1 Text-to-Image](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image)
  - [Flux.1 fill dev](http://docs.comfy.org/tutorials/flux/flux-1-fill-dev)
  - [Flux.1 ControlNet](http://docs.comfy.org/tutorials/flux/flux-1-controlnet)
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Flux.1 ControlNet Examples

# ComfyUI Flux.1 ControlNet Examples

This guide will demonstrate workflow examples using Flux.1 ControlNet.

## [​](http://docs.comfy.org#flux-1-controlnet-model-introduction) FLUX.1 ControlNet Model Introduction

FLUX.1 Canny and Depth are two powerful models from the [FLUX.1 Tools](https://blackforestlabs.ai/flux-1-tools/) launched by [Black Forest Labs](https://blackforestlabs.ai/). This toolkit is designed to add control and guidance capabilities to FLUX.1, enabling users to modify and recreate real or generated images.

**FLUX.1-Depth-dev** and **FLUX.1-Canny-dev** are both 12B parameter Rectified Flow Transformer models that can generate images based on text descriptions while maintaining the structural features of the input image. The Depth version maintains the spatial structure of the source image through depth map extraction techniques, while the Canny version uses edge detection techniques to preserve the structural features of the source image, allowing users to choose the appropriate control method based on different needs.

Both models have the following features:

- Top-tier output quality and detail representation
- Excellent prompt following ability while maintaining consistency with the original image
- Trained using guided distillation techniques for improved efficiency
- Open weights for the research community
- API interfaces (pro version) and open-source weights (dev version)

Additionally, Black Forest Labs also provides **FLUX.1-Depth-dev-lora** and **FLUX.1-Canny-dev-lora** adapter versions extracted from the complete models. These can be applied to the FLUX.1 \[dev] base model to provide similar functionality with smaller file size, especially suitable for resource-constrained environments.

We will use the full version of **FLUX.1-Canny-dev** and **FLUX.1-Depth-dev-lora** to complete the workflow examples.

All workflow images’s Metadata contains the corresponding model download information. You can load the workflows by:

- Dragging them directly into ComfyUI
- Or using the menu `Workflows` -&gt; `Open（ctrl+o）`

If you’re not using the Desktop Version or some models can’t be downloaded automatically, please refer to the manual installation sections to save the model files to the corresponding folder.

For image preprocessors, you can use the following custom nodes to complete image preprocessing. In this example, we will provide processed images as input.

- [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
- [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

## [​](http://docs.comfy.org#flux-1-canny-dev-complete-version-workflow) FLUX.1-Canny-dev Complete Version Workflow

### [​](http://docs.comfy.org#1-workflow-and-asset) 1. Workflow and Asset

Please download the workflow image below and drag it into ComfyUI to load the workflow

Please download the image below, which we will use as the input image

### [​](http://docs.comfy.org#2-manual-models-installation) 2. Manual Models Installation

If you have previously used the [complete version of Flux related workflows](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image), then you only need to download the **flux1-canny-dev.safetensors** model file. Since you need to first agree to the terms of [black-forest-labs/FLUX.1-Canny-dev](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev), please visit the [black-forest-labs/FLUX.1-Canny-dev](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev) page and make sure you have agreed to the corresponding terms as shown in the image below.

Complete model list:

- [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
- [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
- [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
- [flux1-canny-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev/resolve/main/flux1-canny-dev.safetensors?download=true) (Please ensure you have agreed to the corresponding repo’s terms)

File storage location:

```plaintext
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── t5xxl_fp16.safetensors
│   ├── vae/
│   │   └── ae.safetensors
│   └── diffusion_models/
│       └── flux1-canny-dev.safetensors
```

### [​](http://docs.comfy.org#3-step-by-step-workflow-execution) 3. Step-by-Step Workflow Execution

1. Make sure `ae.safetensors` is loaded in the `Load VAE` node
2. Make sure `flux1-canny-dev.safetensors` is loaded in the `Load Diffusion Model` node
3. Make sure the following models are loaded in the `DualCLIPLoader` node:
   
   - clip\_name1: t5xxl\_fp16.safetensors
   - clip\_name2: clip\_l.safetensors
4. Upload the provided input image in the `Load Image` node
5. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

### [​](http://docs.comfy.org#4-start-your-experimentation) 4. Start Your Experimentation

Try using the [FLUX.1-Depth-dev](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev) model to complete the Depth version of the workflow

You can use the image below as input

Or use the following custom nodes to complete image preprocessing:

- [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
- [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

## [​](http://docs.comfy.org#flux-1-depth-dev-lora-workflow) FLUX.1-Depth-dev-lora Workflow

The LoRA version workflow builds on the complete version by adding the LoRA model. Compared to the [complete version of the Flux workflow](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image), it adds nodes for loading and using the corresponding LoRA model.

### [​](http://docs.comfy.org#1-workflow-and-asset-2) 1. Workflow and Asset

Please download the workflow image below and drag it into ComfyUI to load the workflow

Please download the image below, which we will use as the input image

### [​](http://docs.comfy.org#2-manual-model-download) 2. Manual Model Download

If you have previously used the [complete version of Flux related workflows](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image), then you only need to download the **flux1-depth-dev-lora.safetensors** model file.

Complete model list:

- [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
- [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
- [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
- [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors?download=true)
- [flux1-depth-dev-lora.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev-lora/resolve/main/flux1-depth-dev-lora.safetensors?download=true)

File storage location:

```plaintext
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── t5xxl_fp16.safetensors
│   ├── vae/
│   │   └── ae.safetensors
│   ├── diffusion_models/
│   │   └── flux1-dev.safetensors
│   └── loras/
│       └── flux1-depth-dev-lora.safetensors
```

### [​](http://docs.comfy.org#3-step-by-step-workflow-execution-2) 3. Step-by-Step Workflow Execution

1. Make sure `flux1-dev.safetensors` is loaded in the `Load Diffusion Model` node
2. Make sure `flux1-depth-dev-lora.safetensors` is loaded in the `LoraLoaderModelOnly` node
3. Make sure the following models are loaded in the `DualCLIPLoader` node:
   
   - clip\_name1: t5xxl\_fp16.safetensors
   - clip\_name2: clip\_l.safetensors
4. Upload the provided input image in the `Load Image` node
5. Make sure `ae.safetensors` is loaded in the `Load VAE` node
6. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

### [​](http://docs.comfy.org#4-start-your-experimentation-2) 4. Start Your Experimentation

Try using the [FLUX.1-Canny-dev-lora](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev-lora) model to complete the Canny version of the workflow

Use [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet) or [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux) to complete image preprocessing

## [​](http://docs.comfy.org#community-versions-of-flux-controlnets) Community Versions of Flux Controlnets

XLab and InstantX + Shakker Labs have released Controlnets for Flux.

**InstantX:**

- [FLUX.1-dev-Controlnet-Canny](https://huggingface.co/InstantX/FLUX.1-dev-Controlnet-Canny/blob/main/diffusion_pytorch_model.safetensors)
- [FLUX.1-dev-ControlNet-Depth](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Depth/blob/main/diffusion_pytorch_model.safetensors)
- [FLUX.1-dev-ControlNet-Union-Pro](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro/blob/main/diffusion_pytorch_model.safetensors)

**XLab**: [flux-controlnet-collections](https://huggingface.co/XLabs-AI/flux-controlnet-collections)

Place these files in the `ComfyUI/models/controlnet` directory.

You can visit [Flux Controlnet Example](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/flux_controlnet_example.png) to get the corresponding workflow image, and use the image from [here](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/girl_in_field.png) as the input image.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/flux/flux-1-controlnet.mdx)

[Previous](http://docs.comfy.org/tutorials/flux/flux-1-fill-dev)

[HiDream-I1This guide will walk you through completing a ComfyUI native HiDream-I1 text-to-image workflow example  
\
Next](http://docs.comfy.org/tutorials/image/hidream/hidream-i1)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [FLUX.1 ControlNet Model Introduction](http://docs.comfy.org#flux-1-controlnet-model-introduction)
- [FLUX.1-Canny-dev Complete Version Workflow](http://docs.comfy.org#flux-1-canny-dev-complete-version-workflow)
- [1. Workflow and Asset](http://docs.comfy.org#1-workflow-and-asset)
- [2. Manual Models Installation](http://docs.comfy.org#2-manual-models-installation)
- [3. Step-by-Step Workflow Execution](http://docs.comfy.org#3-step-by-step-workflow-execution)
- [4. Start Your Experimentation](http://docs.comfy.org#4-start-your-experimentation)
- [FLUX.1-Depth-dev-lora Workflow](http://docs.comfy.org#flux-1-depth-dev-lora-workflow)
- [1. Workflow and Asset](http://docs.comfy.org#1-workflow-and-asset-2)
- [2. Manual Model Download](http://docs.comfy.org#2-manual-model-download)
- [3. Step-by-Step Workflow Execution](http://docs.comfy.org#3-step-by-step-workflow-execution-2)
- [4. Start Your Experimentation](http://docs.comfy.org#4-start-your-experimentation-2)
- [Community Versions of Flux Controlnets](http://docs.comfy.org#community-versions-of-flux-controlnets)

<!-- END Development/tutorials/flux/flux-1-controlnet.md -->


<!-- BEGIN Development/tutorials/flux/flux-1-fill-dev.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
  
  - [Flux.1 Text-to-Image](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image)
  - [Flux.1 fill dev](http://docs.comfy.org/tutorials/flux/flux-1-fill-dev)
  - [Flux.1 ControlNet](http://docs.comfy.org/tutorials/flux/flux-1-controlnet)
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Flux.1 fill dev Example

# ComfyUI Flux.1 fill dev Example

This guide demonstrates how to use Flux.1 fill dev to create Inpainting and Outpainting workflows.

## [​](http://docs.comfy.org#introduction-to-flux-1-fill-dev-model) Introduction to Flux.1 fill dev Model

Flux.1 fill dev is one of the core tools in the [FLUX.1 Tools suite](https://blackforestlabs.ai/flux-1-tools/) launched by [Black Forest Labs](https://blackforestlabs.ai/), specifically designed for image inpainting and outpainting.

Key features of Flux.1 fill dev:

- Powerful image inpainting and outpainting capabilities, with results second only to the commercial version FLUX.1 Fill \[pro].
- Excellent prompt understanding and following ability, precisely capturing user intent while maintaining high consistency with the original image.
- Advanced guided distillation training technology, making the model more efficient while maintaining high-quality output.
- Friendly licensing terms, with generated outputs usable for personal, scientific, and commercial purposes, please refer to the [FLUX.1 \[dev\] non-commercial license](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md) for details.

Open Source Repository: [FLUX.1 \[dev\]](https://huggingface.co/black-forest-labs/FLUX.1-dev)

This guide will demonstrate inpainting and outpainting workflows based on the Flux.1 fill dev model. If you’re not familiar with inpainting and outpainting workflows, you can refer to [ComfyUI Layout Inpainting Example](http://docs.comfy.org/tutorials/basic/inpaint) and [ComfyUI Image Extension Example](http://docs.comfy.org/tutorials/basic/outpaint) for some related explanations.

## [​](http://docs.comfy.org#flux-1-fill-dev-and-related-models-installation) Flux.1 Fill dev and related models installation

Before we begin, let’s complete the installation of the Flux.1 Fill dev model files. The inpainting and outpainting workflows will use exactly the same model files. If you’ve previously used the full version of the [Flux.1 Text-to-Image workflow](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image), then you only need to download the **flux1-fill-dev.safetensors** model file in this section.

However, since downloading the corresponding model requires agreeing to the corresponding usage agreement, please visit the [black-forest-labs/FLUX.1-Fill-dev](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev) page and make sure you have agreed to the corresponding agreement as shown in the image below.

Complete model list:

- [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
- [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
- [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
- [flux1-fill-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev/resolve/main/flux1-fill-dev.safetensors?download=true)

File storage location:

```plaintext
ComfyUI/
├── models/
│   ├── text_encoders/
│   │    ├── clip_l.safetensors
│   │    └── t5xxl_fp16.safetensors
│   ├── vae/
│   │    └── ae.safetensors
│   └── diffusion_models/
│        └── flux1-fill-dev.safetensors
```

## [​](http://docs.comfy.org#flux-1-fill-dev-inpainting-workflow) Flux.1 Fill dev inpainting workflow

### [​](http://docs.comfy.org#1-inpainting-workflow-and-asset) 1. Inpainting workflow and asset

Please download the image below and drag it into ComfyUI to load the corresponding workflow

Please download the image below, we will use it as the input image

The corresponding image already contains an alpha channel, so you don’t need to draw a mask separately. If you want to draw your own mask, please [click here](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint_input_original.png) to get the image without a mask, and refer to the MaskEditor usage section in the [ComfyUI Layout Inpainting Example](http://docs.comfy.org/tutorials/basic/inpaint#using-the-mask-editor) to learn how to draw a mask in the `Load Image` node.

### [​](http://docs.comfy.org#2-steps-to-run-the-workflow) 2. Steps to run the workflow

1. Ensure the `Load Diffusion Model` node has `flux1-fill-dev.safetensors` loaded.
2. Ensure the `DualCLIPLoader` node has the following models loaded:
   
   - clip\_name1: `t5xxl_fp16.safetensors`
   - clip\_name2: `clip_l.safetensors`
3. Ensure the `Load VAE` node has `ae.safetensors` loaded.
4. Upload the input image provided in the document to the `Load Image` node; if you’re using the version without a mask, remember to complete the mask drawing using the mask editor
5. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## [​](http://docs.comfy.org#flux-1-fill-dev-outpainting-workflow) Flux.1 Fill dev Outpainting Workflow

### [​](http://docs.comfy.org#1-outpainting-workflow-and-asset) 1. Outpainting workflow and asset

Please download the image below and drag it into ComfyUI to load the corresponding workflow

Please download the image below, we will use it as the input image

### [​](http://docs.comfy.org#2-steps-to-run-the-workflow-2) 2. Steps to run the workflow

1. Ensure the `Load Diffusion Model` node has `flux1-fill-dev.safetensors` loaded.
2. Ensure the `DualCLIPLoader` node has the following models loaded:
   
   - clip\_name1: `t5xxl_fp16.safetensors`
   - clip\_name2: `clip_l.safetensors`
3. Ensure the `Load VAE` node has `ae.safetensors` loaded.
4. Upload the input image provided in the document to the `Load Image` node
5. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/flux/flux-1-fill-dev.mdx)

[Previous](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image)

[Flux.1 ControlNetThis guide will demonstrate workflow examples using Flux.1 ControlNet.  
\
Next](http://docs.comfy.org/tutorials/flux/flux-1-controlnet)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Introduction to Flux.1 fill dev Model](http://docs.comfy.org#introduction-to-flux-1-fill-dev-model)
- [Flux.1 Fill dev and related models installation](http://docs.comfy.org#flux-1-fill-dev-and-related-models-installation)
- [Flux.1 Fill dev inpainting workflow](http://docs.comfy.org#flux-1-fill-dev-inpainting-workflow)
- [1. Inpainting workflow and asset](http://docs.comfy.org#1-inpainting-workflow-and-asset)
- [2. Steps to run the workflow](http://docs.comfy.org#2-steps-to-run-the-workflow)
- [Flux.1 Fill dev Outpainting Workflow](http://docs.comfy.org#flux-1-fill-dev-outpainting-workflow)
- [1. Outpainting workflow and asset](http://docs.comfy.org#1-outpainting-workflow-and-asset)
- [2. Steps to run the workflow](http://docs.comfy.org#2-steps-to-run-the-workflow-2)

<!-- END Development/tutorials/flux/flux-1-fill-dev.md -->


<!-- BEGIN Development/tutorials/flux/flux-1-text-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
  
  - [Flux.1 Text-to-Image](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image)
  - [Flux.1 fill dev](http://docs.comfy.org/tutorials/flux/flux-1-fill-dev)
  - [Flux.1 ControlNet](http://docs.comfy.org/tutorials/flux/flux-1-controlnet)
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Flux.1 Text-to-Image Workflow Example

# ComfyUI Flux.1 Text-to-Image Workflow Example

This guide provides a brief introduction to the Flux.1 model and guides you through using the Flux.1 model for text-to-image generation with examples including the full version and the FP8 Checkpoint version.

Flux is one of the largest open-source text-to-image generation models, with 12B parameters and an original file size of approximately 23GB. It was developed by [Black Forest Labs](https://blackforestlabs.ai/), a team founded by former Stable Diffusion team members. Flux is known for its excellent image quality and flexibility, capable of generating high-quality, diverse images.

Currently, the Flux.1 model has several main versions:

- **Flux.1 Pro:** The best performing model, closed-source, only available through API calls.
- [**Flux.1 \[dev\]：**](https://huggingface.co/black-forest-labs/FLUX.1-dev) Open-source but limited to non-commercial use, distilled from the Pro version, with performance close to the Pro version.
- [**Flux.1 \[schnell\]：**](https://huggingface.co/black-forest-labs/FLUX.1-schnell) Uses the Apache2.0 license, requires only 4 steps to generate images, suitable for low-spec hardware.

**Flux.1 Model Features**

- **Hybrid Architecture:** Combines the advantages of Transformer networks and diffusion models, effectively integrating text and image information, improving the alignment accuracy between generated images and prompts, with excellent fidelity to complex prompts.
- **Parameter Scale:** Flux has 12B parameters, capturing more complex pattern relationships and generating more realistic, diverse images.
- **Supports Multiple Styles:** Supports diverse styles, with excellent performance for various types of images.

In this example, we’ll introduce text-to-image examples using both Flux.1 Dev and Flux.1 Schnell versions, including the full version model and the simplified FP8 Checkpoint version.

- **Flux Full Version:** Best performance, but requires larger VRAM resources and installation of multiple model files.
- **Flux FP8 Checkpoint:** Requires only one fp8 version of the model, but quality is slightly reduced compared to the full version.

All workflow images’s Metadata contains the corresponding model download information. You can load the workflows by:

- Dragging them directly into ComfyUI
- Or using the menu `Workflows` -&gt; `Open（ctrl+o）`

If you’re not using the Desktop Version or some models can’t be downloaded automatically, please refer to the manual installation sections to save the model files to the corresponding folder. Make sure your ComfyUI is updated to the latest version before starting.

## [​](http://docs.comfy.org#flux-1-full-version-text-to-image-example) Flux.1 Full Version Text-to-Image Example

If you can’t download models from [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), make sure you’ve logged into Huggingface and agreed to the corresponding repository’s license agreement.

### [​](http://docs.comfy.org#flux-1-dev) Flux.1 Dev

#### [​](http://docs.comfy.org#1-workflow-file) 1. Workflow File

Please download the image below and drag it into ComfyUI to load the workflow.

#### [​](http://docs.comfy.org#2-manual-model-installation) 2. Manual Model Installation

- The `flux1-dev.safetensors` file requires agreeing to the [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev) agreement before downloading via browser.
- If your VRAM is low, you can try using [t5xxl\_fp8\_e4m3fn.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors?download=true) to replace the `t5xxl_fp16.safetensors` file.

Please download the following model files:

- [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
- [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true) Recommended when your VRAM is greater than 32GB.
- [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
- [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors)

Storage location:

```plaintext
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── t5xxl_fp16.safetensors
│   ├── vae/
│   │   └── ae.safetensors
│   └── diffusion_models/
│       └── flux1-dev.safetensors
```

#### [​](http://docs.comfy.org#3-steps-to-run-the-workflow) 3. Steps to Run the Workflow

Please refer to the image below to ensure all model files are loaded correctly

1. Ensure the `DualCLIPLoader` node has the following models loaded:
   
   - clip\_name1: t5xxl\_fp16.safetensors
   - clip\_name2: clip\_l.safetensors
2. Ensure the `Load Diffusion Model` node has `flux1-dev.safetensors` loaded
3. Make sure the `Load VAE` node has `ae.safetensors` loaded
4. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

Thanks to Flux’s excellent prompt following capability, we don’t need any negative prompts

### [​](http://docs.comfy.org#flux-1-schnell) Flux.1 Schnell

#### [​](http://docs.comfy.org#1-workflow-file-2) 1. Workflow File

Please download the image below and drag it into ComfyUI to load the workflow.

#### [​](http://docs.comfy.org#2-manual-models-installation) 2. Manual Models Installation

In this workflow, only two model files are different from the Flux1 Dev version workflow. For t5xxl, you can still use the fp16 version for better results.

- **t5xxl\_fp16.safetensors** -&gt; **t5xxl\_fp8.safetensors**
- **flux1-dev.safetensors** -&gt; **flux1-schnell.safetensors**

Complete model file list:

- [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
- [t5xxl\_fp8\_e4m3fn.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors?download=true)
- [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
- [flux1-schnell.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/flux1-schnell.safetensors)

File storage location:

```plaintext
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── t5xxl_fp8_e4m3fn.safetensors
│   ├── vae/
│   │   └── ae.safetensors
│   └── diffusion_models/
│       └── flux1-schnell.safetensors
```

#### [​](http://docs.comfy.org#3-steps-to-run-the-workflow-2) 3. Steps to Run the Workflow

1. Ensure the `DualCLIPLoader` node has the following models loaded:
   
   - clip\_name1: t5xxl\_fp8\_e4m3fn.safetensors
   - clip\_name2: clip\_l.safetensors
2. Ensure the `Load Diffusion Model` node has `flux1-schnell.safetensors` loaded
3. Ensure the `Load VAE` node has `ae.safetensors` loaded
4. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## [​](http://docs.comfy.org#flux-1-fp8-checkpoint-version-text-to-image-example) Flux.1 FP8 Checkpoint Version Text-to-Image Example

The fp8 version is a quantized version of the original Flux.1 fp16 version. To some extent, the quality of this version will be lower than that of the fp16 version, but it also requires less VRAM, and you only need to install one model file to try running it.

### [​](http://docs.comfy.org#flux-1-dev-2) Flux.1 Dev

Please download the image below and drag it into ComfyUI to load the workflow.

Please download [flux1-dev-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-dev/resolve/main/flux1-dev-fp8.safetensors?download=true) and save it to the `ComfyUI/models/checkpoints/` directory.

Ensure that the corresponding `Load Checkpoint` node loads `flux1-dev-fp8.safetensors`, and you can try to run the workflow.

### [​](http://docs.comfy.org#flux-1-schnell-2) Flux.1 Schnell

Please download the image below and drag it into ComfyUI to load the workflow.

Please download [flux1-schnell-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-schnell/resolve/main/flux1-schnell-fp8.safetensors?download=true) and save it to the `ComfyUI/models/checkpoints/` directory.

Ensure that the corresponding `Load Checkpoint` node loads `flux1-schnell-fp8.safetensors`, and you can try to run the workflow.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/flux/flux-1-text-to-image.mdx)

[Previous](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets)

[Flux.1 fill devThis guide demonstrates how to use Flux.1 fill dev to create Inpainting and Outpainting workflows.  
\
Next](http://docs.comfy.org/tutorials/flux/flux-1-fill-dev)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Flux.1 Full Version Text-to-Image Example](http://docs.comfy.org#flux-1-full-version-text-to-image-example)
- [Flux.1 Dev](http://docs.comfy.org#flux-1-dev)
- [1. Workflow File](http://docs.comfy.org#1-workflow-file)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow)
- [Flux.1 Schnell](http://docs.comfy.org#flux-1-schnell)
- [1. Workflow File](http://docs.comfy.org#1-workflow-file-2)
- [2. Manual Models Installation](http://docs.comfy.org#2-manual-models-installation)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow-2)
- [Flux.1 FP8 Checkpoint Version Text-to-Image Example](http://docs.comfy.org#flux-1-fp8-checkpoint-version-text-to-image-example)
- [Flux.1 Dev](http://docs.comfy.org#flux-1-dev-2)
- [Flux.1 Schnell](http://docs.comfy.org#flux-1-schnell-2)

<!-- END Development/tutorials/flux/flux-1-text-to-image.md -->


<!-- BEGIN Development/tutorials/image/hidream/hidream-e1.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
  
  - HiDream
    
    - [HiDream-I1](http://docs.comfy.org/tutorials/image/hidream/hidream-i1)
    - [HiDream-e1](http://docs.comfy.org/tutorials/image/hidream/hidream-e1)
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Native HiDream-e1 Workflow Example

# ComfyUI Native HiDream-e1 Workflow Example

This guide will help you understand and complete the ComfyUI native HiDream-I1 text-to-image workflow example

HiDream-E1 is an interactive image editing large model officially open-sourced by HiDream-ai on April 28, 2025, built based on HiDream-I1.

It allows you to edit images using natural language. The model is released under the [MIT License](https://github.com/HiDream-ai/HiDream-E1?tab=MIT-1-ov-file), supporting use in personal projects, scientific research, and commercial applications. In combination with the previously released [hidream-i1](http://docs.comfy.org/zh-CN/tutorials/advanced/hidream), it enables **creative capabilities from image generation to editing**.

**ComfyUI now natively supports HiDream E1**. In this guide, we will help you complete the workflow example of using HiDream E1 in ComfyUI.

For reference, this workflow takes about 500s for the first run and 370s for the second run with 28 sampling steps on Google Colab L4 with 22.5GB VRAM.

### [​](http://docs.comfy.org#hidream-e1-information) HiDream-E1 Information

**HiDream-E1 Model Download** Currently, HiDream provides a full version. Here is the model information:

NameInference StepsResolutionHuggingFace RepositoryHiDream-E1-Full28768x768[🤗 HiDream-E1-Full](https://huggingface.co/HiDream-ai/HiDream-E1-Full)

-[Github](https://github.com/HiDream-ai/HiDream-E1)

## [​](http://docs.comfy.org#comfyui-native-hidream-e1-workflow-example) ComfyUI Native HiDream-e1 Workflow Example

Please upgrade your ComfyUI to the latest version (latest commit) before starting to ensure your ComfyUI has the relevant support.

### [​](http://docs.comfy.org#1-download-hidream-e1-workflow-and-related-files) 1. Download HiDream-e1 Workflow and Related Files

#### [​](http://docs.comfy.org#1-1-download-workflow-file) 1.1 Download Workflow File

Please download the image below and drag it into ComfyUI. The workflow already contains model download information, and after loading, it will prompt you to download the corresponding models.

#### [​](http://docs.comfy.org#1-2-download-input-image) 1.2 Download Input Image

Please download the image below, which we will use as input

### [​](http://docs.comfy.org#2-manual-model-installation-for-hidream-e1-related-models) 2. Manual Model Installation for HiDream-e1 Related Models

All models mentioned in this guide can be found [here](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/tree/main/split_files). Please download the corresponding files and save them to the appropriate folders.

The following model files are shared models that we will use. Please click the corresponding links to download and save according to the model file storage location. We will guide you to download the corresponding **diffusion models** in the workflow.

**text\_encoders**:

- [clip\_l\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors)
- [clip\_g\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_g_hidream.safetensors)
- [t5xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors) This model has been used in many workflows, you may have already downloaded this file.
- [llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors)

**VAE**

- [ae.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/vae/ae.safetensors) This is Flux’s VAE model. If you have used Flux workflows before, you may have already downloaded this file.

**diffusion models**

- [hidream\_e1\_full\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_e1_full_bf16.safetensors)

Model file storage location

```plaintext
📂 ComfyUI/
├── 📂 models/
│   ├── 📂 text_encoders/
│   │   ├─── clip_l_hidream.safetensors
│   │   ├─── clip_g_hidream.safetensors
│   │   ├─── t5xxl_fp8_e4m3fn_scaled.safetensors
│   │   └─── llama_3.1_8b_instruct_fp8_scaled.safetensors
│   └── 📂 vae/
│   │   └── ae.safetensors
│   └── 📂 diffusion_models/
│       └── hidream_e1_full_bf16.safetensors   
```

### [​](http://docs.comfy.org#3-complete-the-hidream-e1-workflow-step-by-step) 3. Complete the HiDream-e1 Workflow Step by Step

Follow these steps to complete the workflow:

1. Make sure the `Load Diffusion Model` node has loaded the `hidream_e1_full_bf16.safetensors` model
2. Ensure that the four corresponding text encoders are correctly loaded in the `QuadrupleCLIPLoader`
   
   - clip\_l\_hidream.safetensors
   - clip\_g\_hidream.safetensors
   - t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   - llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node is using the `ae.safetensors` file
4. Load the input image we downloaded earlier in the `Load Image` node
5. (Important) Enter **the prompt for how you want to modify the image** in the `Empty Text Encoder(Positive)` node
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to generate the image

### [​](http://docs.comfy.org#additional-notes-on-comfyui-hidream-e1-workflow) Additional Notes on ComfyUI HiDream-e1 Workflow

- You may need to modify the prompt multiple times or generate multiple times to get better results
- This model has difficulty maintaining consistency when changing image styles, so try to make your prompts as complete as possible
- As the model supports a resolution of 768\*768, in actual testing with other dimensions, the image performance is poor or even significantly different at other dimensions

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/image/hidream/hidream-e1.mdx)

[Previous](http://docs.comfy.org/tutorials/image/hidream/hidream-i1)

[Hunyuan3D-2This guide will demonstrate how to use Hunyuan3D-2 in ComfyUI to generate 3D assets.  
\
Next](http://docs.comfy.org/tutorials/3d/hunyuan3D-2)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [HiDream-E1 Information](http://docs.comfy.org#hidream-e1-information)
- [ComfyUI Native HiDream-e1 Workflow Example](http://docs.comfy.org#comfyui-native-hidream-e1-workflow-example)
- [1. Download HiDream-e1 Workflow and Related Files](http://docs.comfy.org#1-download-hidream-e1-workflow-and-related-files)
- [1.1 Download Workflow File](http://docs.comfy.org#1-1-download-workflow-file)
- [1.2 Download Input Image](http://docs.comfy.org#1-2-download-input-image)
- [2. Manual Model Installation for HiDream-e1 Related Models](http://docs.comfy.org#2-manual-model-installation-for-hidream-e1-related-models)
- [3. Complete the HiDream-e1 Workflow Step by Step](http://docs.comfy.org#3-complete-the-hidream-e1-workflow-step-by-step)
- [Additional Notes on ComfyUI HiDream-e1 Workflow](http://docs.comfy.org#additional-notes-on-comfyui-hidream-e1-workflow)

<!-- END Development/tutorials/image/hidream/hidream-e1.md -->


<!-- BEGIN Development/tutorials/image/hidream/hidream-i1.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
  
  - HiDream
    
    - [HiDream-I1](http://docs.comfy.org/tutorials/image/hidream/hidream-i1)
    - [HiDream-e1](http://docs.comfy.org/tutorials/image/hidream/hidream-e1)
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Native HiDream-I1 Text-to-Image Workflow Example

# ComfyUI Native HiDream-I1 Text-to-Image Workflow Example

This guide will walk you through completing a ComfyUI native HiDream-I1 text-to-image workflow example

HiDream-I1 is a text-to-image model officially open-sourced by HiDream-ai on April 7, 2025. The model has 17B parameters and is released under the [MIT license](https://github.com/HiDream-ai/HiDream-I1/blob/main/LICENSE), supporting personal projects, scientific research, and commercial use. It currently performs excellently in multiple benchmark tests.

## [​](http://docs.comfy.org#model-features) Model Features

**Hybrid Architecture Design** A combination of Diffusion Transformer (DiT) and Mixture of Experts (MoE) architecture:

- Based on Diffusion Transformer (DiT), with dual-stream MMDiT modules processing multimodal information and single-stream DiT modules optimizing global consistency.
- Dynamic routing mechanism flexibly allocates computing resources, enhancing complex scene processing capabilities and delivering excellent performance in color restoration, edge processing, and other details.

**Multimodal Text Encoder Integration** Integrates four text encoders:

- OpenCLIP ViT-bigG, OpenAI CLIP ViT-L (visual semantic alignment)
- T5-XXL (long text parsing)
- Llama-3.1-8B-Instruct (instruction understanding) This combination achieves SOTA performance in complex semantic parsing of colors, quantities, spatial relationships, etc., with Chinese prompt support significantly outperforming similar open-source models.

**Original Model Versions**

HiDream-ai provides three versions of the HiDream-I1 model to meet different needs. Below are the links to the original model repositories:

Model NameDescriptionInference StepsRepository LinkHiDream-I1-FullFull version50[🤗 HiDream-I1-Full](https://huggingface.co/HiDream-ai/HiDream-I1-Full)HiDream-I1-DevDistilled dev28[🤗 HiDream-I1-Dev](https://huggingface.co/HiDream-ai/HiDream-I1-Dev)HiDream-I1-FastDistilled fast16[🤗 HiDream-I1-Fast](https://huggingface.co/HiDream-ai/HiDream-I1-Fast)

## [​](http://docs.comfy.org#about-this-workflow-example) About This Workflow Example

In this example, we will use the repackaged version from ComfyOrg. You can find all the model files we’ll use in this example in the [HiDream-I1\_ComfyUI](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/) repository.

Before starting, please update your ComfyUI version to ensure it’s at least after this [commit](https://github.com/comfyanonymous/ComfyUI/commit/9ad792f92706e2179c58b2e5348164acafa69288) to make sure your ComfyUI has native support for HiDream

## [​](http://docs.comfy.org#hidream-i1-workflow) HiDream-I1 Workflow

The model requirements for different ComfyUI native HiDream-I1 workflows are basically the same, with only the [diffusion models](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/tree/main/split_files/diffusion_models) files being different.

If you don’t know which version to choose, please refer to the following suggestions:

- **HiDream-I1-Full** can generate the highest quality images
- **HiDream-I1-Dev** balances high-quality image generation with speed
- **HiDream-I1-Fast** can generate images in just 16 steps, suitable for scenarios requiring real-time iteration

For the **dev** and **fast** versions, negative prompts are not needed, so please set the `cfg` parameter to `1.0` during sampling. We have noted the corresponding parameter settings in the relevant workflows.

The full versions of all three versions require a lot of VRAM - you may need more than 27GB of VRAM to run them smoothly. In the corresponding workflow tutorials, we will use the **fp8** version as a demonstration example to ensure that most users can run it smoothly. However, we will still provide download links for different versions of the model in the corresponding examples, and you can choose the appropriate file based on your VRAM situation.

### [​](http://docs.comfy.org#model-installation) Model Installation

The following model files are common files that we will use. Please click on the corresponding links to download and save them according to the model file save location. We will guide you to download the corresponding **diffusion models** in the corresponding workflows.

**text\_encoders**：

- [clip\_l\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors)
- [clip\_g\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_g_hidream.safetensors)
- [t5xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors) This model has been used in many workflows, you may have already downloaded this file.
- [llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors)

**VAE**

- [ae.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/vae/ae.safetensors) This is Flux’s VAE model, if you have used Flux’s workflow before, you may have already downloaded this file.

**diffusion models** We will guide you to download the corresponding model files in the corresponding workflows.

Model file save location

```plaintext
📂 ComfyUI/
├── 📂 models/
│   ├── 📂 text_encoders/
│   │   ├─── clip_l_hidream.safetensors
│   │   ├─── clip_g_hidream.safetensors
│   │   ├─── t5xxl_fp8_e4m3fn_scaled.safetensors
│   │   └─── llama_3.1_8b_instruct_fp8_scaled.safetensors
│   └── 📂 vae/
│   │   └── ae.safetensors
│   └── 📂 diffusion_models/
│       └── ...               # We will guide you to install in the corresponding version workflow       
```

### [​](http://docs.comfy.org#hidream-i1-full-version-workflow) HiDream-I1 Full Version Workflow

#### [​](http://docs.comfy.org#1-model-file-download) 1. Model File Download

Please select the appropriate version based on your hardware. Click the link and download the corresponding model file to save it to the `ComfyUI/models/diffusion_models/` folder.

- FP8 version: [hidream\_i1\_full\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp8.safetensors?download=true) requires more than 16GB of VRAM
- Full version: [hidream\_i1\_full\_f16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp16.safetensors?download=true) requires more than 27GB of VRAM

#### [​](http://docs.comfy.org#2-workflow-file-download) 2. Workflow File Download

Please download the image below and drag it into ComfyUI to load the corresponding workflow

#### [​](http://docs.comfy.org#3-complete-the-workflow-step-by-step) 3. Complete the Workflow Step by Step

Complete the workflow execution step by step

1. Make sure the `Load Diffusion Model` node is using the `hidream_i1_full_fp8.safetensors` file
2. Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly
   
   - clip\_l\_hidream.safetensors
   - clip\_g\_hidream.safetensors
   - t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   - llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node is using the `ae.safetensors` file
4. For the **full** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `3.0`
5. For the `Ksampler` node, you need to make the following settings
   
   - Set `steps` to `50`
   - Set `cfg` to `5.0`
   - (Optional) Set `sampler` to `lcm`
   - (Optional) Set `scheduler` to `normal`
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

### [​](http://docs.comfy.org#hidream-i1-dev-version-workflow) HiDream-I1 Dev Version Workflow

#### [​](http://docs.comfy.org#1-model-file-download-2) 1. Model File Download

Please select the appropriate version based on your hardware, click the link and download the corresponding model file to save to the `ComfyUI/models/diffusion_models/` folder.

- FP8 version: [hidream\_i1\_dev\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_fp8.safetensors?download=true) requires more than 16GB of VRAM
- Full version: [hidream\_i1\_dev\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_bf16.safetensors?download=true) requires more than 27GB of VRAM

#### [​](http://docs.comfy.org#2-workflow-file-download-2) 2. Workflow File Download

Please download the image below and drag it into ComfyUI to load the corresponding workflow

#### [​](http://docs.comfy.org#3-complete-the-workflow-step-by-step-2) 3. Complete the Workflow Step by Step

Complete the workflow execution step by step

1. Make sure the `Load Diffusion Model` node is using the `hidream_i1_dev_fp8.safetensors` file
2. Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly
   
   - clip\_l\_hidream.safetensors
   - clip\_g\_hidream.safetensors
   - t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   - llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node is using the `ae.safetensors` file
4. For the **dev** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `6.0`
5. For the `Ksampler` node, you need to make the following settings
   
   - Set `steps` to `28`
   - (Important) Set `cfg` to `1.0`
   - (Optional) Set `sampler` to `lcm`
   - (Optional) Set `scheduler` to `normal`
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

### [​](http://docs.comfy.org#hidream-i1-fast-version-workflow) HiDream-I1 Fast Version Workflow

#### [​](http://docs.comfy.org#1-model-file-download-3) 1. Model File Download

Please select the appropriate version based on your hardware, click the link and download the corresponding model file to save to the `ComfyUI/models/diffusion_models/` folder.

- FP8 version: [hidream\_i1\_fast\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_fp8.safetensors?download=true) requires more than 16GB of VRAM
- Full version: [hidream\_i1\_fast\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_fp8.safetensors?download=true) requires more than 27GB of VRAM

#### [​](http://docs.comfy.org#2-workflow-file-download-3) 2. Workflow File Download

Please download the image below and drag it into ComfyUI to load the corresponding workflow

#### [​](http://docs.comfy.org#3-complete-the-workflow-step-by-step-3) 3. Complete the Workflow Step by Step

Complete the workflow execution step by step

1. Make sure the `Load Diffusion Model` node is using the `hidream_i1_fast_fp8.safetensors` file
2. Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly
   
   - clip\_l\_hidream.safetensors
   - clip\_g\_hidream.safetensors
   - t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   - llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node is using the `ae.safetensors` file
4. For the **fast** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `3.0`
5. For the `Ksampler` node, you need to make the following settings
   
   - Set `steps` to `16`
   - (Important) Set `cfg` to `1.0`
   - (Optional) Set `sampler` to `lcm`
   - (Optional) Set `scheduler` to `normal`
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## [​](http://docs.comfy.org#other-related-resources) Other Related Resources

### [​](http://docs.comfy.org#gguf-version-models) GGUF Version Models

- [HiDream-I1-Full-gguf](https://huggingface.co/city96/HiDream-I1-Full-gguf)
- [HiDream-I1-Dev-gguf](https://huggingface.co/city96/HiDream-I1-Dev-gguf)

You need to use the “Unet Loader (GGUF)” node in City96’s [ComfyUI-GGUF](https://github.com/city96/ComfyUI-GGUF) to replace the “Load Diffusion Model” node.

### [​](http://docs.comfy.org#nf4-version-models) NF4 Version Models

- [HiDream-I1-nf4](https://github.com/hykilpikonna/HiDream-I1-nf4)
- Use the [ComfyUI-HiDream-Sampler](https://github.com/SanDiegoDude/ComfyUI-HiDream-Sampler) node to use the NF4 version model.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/image/hidream/hidream-i1.mdx)

[Previous](http://docs.comfy.org/tutorials/flux/flux-1-controlnet)

[HiDream-e1This guide will help you understand and complete the ComfyUI native HiDream-I1 text-to-image workflow example  
\
Next](http://docs.comfy.org/tutorials/image/hidream/hidream-e1)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Model Features](http://docs.comfy.org#model-features)
- [About This Workflow Example](http://docs.comfy.org#about-this-workflow-example)
- [HiDream-I1 Workflow](http://docs.comfy.org#hidream-i1-workflow)
- [Model Installation](http://docs.comfy.org#model-installation)
- [HiDream-I1 Full Version Workflow](http://docs.comfy.org#hidream-i1-full-version-workflow)
- [1. Model File Download](http://docs.comfy.org#1-model-file-download)
- [2. Workflow File Download](http://docs.comfy.org#2-workflow-file-download)
- [3. Complete the Workflow Step by Step](http://docs.comfy.org#3-complete-the-workflow-step-by-step)
- [HiDream-I1 Dev Version Workflow](http://docs.comfy.org#hidream-i1-dev-version-workflow)
- [1. Model File Download](http://docs.comfy.org#1-model-file-download-2)
- [2. Workflow File Download](http://docs.comfy.org#2-workflow-file-download-2)
- [3. Complete the Workflow Step by Step](http://docs.comfy.org#3-complete-the-workflow-step-by-step-2)
- [HiDream-I1 Fast Version Workflow](http://docs.comfy.org#hidream-i1-fast-version-workflow)
- [1. Model File Download](http://docs.comfy.org#1-model-file-download-3)
- [2. Workflow File Download](http://docs.comfy.org#2-workflow-file-download-3)
- [3. Complete the Workflow Step by Step](http://docs.comfy.org#3-complete-the-workflow-step-by-step-3)
- [Other Related Resources](http://docs.comfy.org#other-related-resources)
- [GGUF Version Models](http://docs.comfy.org#gguf-version-models)
- [NF4 Version Models](http://docs.comfy.org#nf4-version-models)

<!-- END Development/tutorials/image/hidream/hidream-i1.md -->


<!-- BEGIN Development/tutorials/video/hunyuan-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
  
  - [LTX-Video](http://docs.comfy.org/tutorials/video/ltxv)
  - [Hunyuan Video](http://docs.comfy.org/tutorials/video/hunyuan-video)
  - Wan Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Hunyuan Video Examples

# ComfyUI Hunyuan Video Examples

This guide shows how to use Hunyuan Text-to-Video and Image-to-Video workflows in ComfyUI

Hunyuan Video series is developed and open-sourced by [Tencent](https://huggingface.co/tencent), featuring a hybrid architecture that supports both [Text-to-Video](https://github.com/Tencent/HunyuanVideo) and [Image-to-Video](https://github.com/Tencent/HunyuanVideo-I2V) generation with a parameter scale of 13B.

Technical features:

- **Core Architecture:** Uses a DiT (Diffusion Transformer) architecture similar to Sora, effectively fusing text, image, and motion information to improve consistency, quality, and alignment between generated video frames. A unified full-attention mechanism enables multi-view camera transitions while ensuring subject consistency.
- **3D VAE:** The custom 3D VAE compresses videos into a compact latent space, making image-to-video generation more efficient.
- **Superior Image-Video-Text Alignment:** Utilizing MLLM text encoders that excel in both image and video generation, better following text instructions, capturing details, and performing complex reasoning.

You can learn more through the official repositories: [Hunyuan Video](https://github.com/Tencent/HunyuanVideo) and [Hunyuan Video-I2V](https://github.com/Tencent/HunyuanVideo-I2V).

This guide will walk you through setting up both **Text-to-Video** and **Image-to-Video** workflows in ComfyUI.

The workflow images in this tutorial contain metadata with model download information.

Simply drag them into ComfyUI or use the menu `Workflows` -&gt; `Open (ctrl+o)` to load the corresponding workflow, which will prompt you to download the required models.

Alternatively, this guide provides direct model links if automatic downloads fail or you are not using the Desktop version. All models are available [here](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/tree/main/split_files) for download.

## [​](http://docs.comfy.org#shared-models-for-all-workflows) Shared Models for All Workflows

The following models are used in both Text-to-Video and Image-to-Video workflows. Please download and save them to the specified directories:

- [clip\_l.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/clip_l.safetensors?download=true)
- [llava\_llama3\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/llava_llama3_fp8_scaled.safetensors?download=true)
- [hunyuan\_video\_vae\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/vae/hunyuan_video_vae_bf16.safetensors?download=true)

Storage location:

```plaintext
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── llava_llama3_fp8_scaled.safetensors
│   ├── vae/
│   │   └── hunyuan_video_vae_bf16.safetensors
```

## [​](http://docs.comfy.org#hunyuan-text-to-video-workflow) Hunyuan Text-to-Video Workflow

Hunyuan Text-to-Video was open-sourced in December 2024, supporting 5-second short video generation through natural language descriptions in both Chinese and English.

### [​](http://docs.comfy.org#1-workflow) 1. Workflow

Download the image below and drag it into ComfyUI to load the workflow:

### [​](http://docs.comfy.org#2-manual-models-installation) 2. Manual Models Installation

Download [hunyuan\_video\_t2v\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_t2v_720p_bf16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models` folder.

Ensure you have all these model files in the correct locations:

```plaintext
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors                       // Shared model
│   │   └── llava_llama3_fp8_scaled.safetensors      // Shared model
│   ├── vae/
│   │   └── hunyuan_video_vae_bf16.safetensors       // Shared model
│   └── diffusion_models/
│       └── hunyuan_video_t2v_720p_bf16.safetensors  // T2V model
```

### [​](http://docs.comfy.org#3-steps-to-run-the-workflow) 3. Steps to Run the Workflow

1. Ensure the `DualCLIPLoader` node has loaded these models:
   
   - clip\_name1: clip\_l.safetensors
   - clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. Ensure the `Load Diffusion Model` node has loaded `hunyuan_video_t2v_720p_bf16.safetensors`
3. Ensure the `Load VAE` node has loaded `hunyuan_video_vae_bf16.safetensors`
4. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

When the `length` parameter in the `EmptyHunyuanLatentVideo` node is set to 1, the model can generate a static image.

## [​](http://docs.comfy.org#hunyuan-image-to-video-workflow) Hunyuan Image-to-Video Workflow

Hunyuan Image-to-Video model was open-sourced on March 6, 2025, based on the HunyuanVideo framework. It transforms static images into smooth, high-quality videos and also provides LoRA training code to customize special video effects like hair growth, object transformation, etc.

Currently, the Hunyuan Image-to-Video model has two versions:

- v1 “concat”: Better motion fluidity but less adherence to the image guidance
- v2 “replace”: Updated the day after v1, with better image guidance but seemingly less dynamic compared to v1

v1 “concat”

v2 “replace”

### [​](http://docs.comfy.org#shared-model-for-v1-and-v2-versions) Shared Model for v1 and v2 Versions

Download the following file and save it to the `ComfyUI/models/clip_vision` directory:

- [llava\_llama3\_vision.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/clip_vision/llava_llama3_vision.safetensors?download=true)

### [​](http://docs.comfy.org#v1-%E2%80%9Cconcat%E2%80%9D-image-to-video-workflow) V1 “concat” Image-to-Video Workflow

#### [​](http://docs.comfy.org#1-workflow-and-asset) 1. Workflow and Asset

Download the workflow image below and drag it into ComfyUI to load the workflow:

Download the image below, which we’ll use as the starting frame for the image-to-video generation:

#### [​](http://docs.comfy.org#2-related-models-manual-installation) 2. Related models manual installation

- [hunyuan\_video\_image\_to\_video\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_image_to_video_720p_bf16.safetensors?download=true)

Ensure you have all these model files in the correct locations:

```plaintext
ComfyUI/
├── models/
│   ├── clip_vision/
│   │   └── llava_llama3_vision.safetensors                     // I2V shared model
│   ├── text_encoders/
│   │   ├── clip_l.safetensors                                  // Shared model
│   │   └── llava_llama3_fp8_scaled.safetensors                 // Shared model
│   ├── vae/
│   │   └── hunyuan_video_vae_bf16.safetensors                  // Shared model
│   └── diffusion_models/
│       └── hunyuan_video_image_to_video_720p_bf16.safetensors  // I2V v1 "concat" version model
```

#### [​](http://docs.comfy.org#3-steps-to-run-the-workflow-2) 3. Steps to Run the Workflow

1. Ensure that `DualCLIPLoader` has loaded these models:
   
   - clip\_name1: clip\_l.safetensors
   - clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. Ensure that `Load CLIP Vision` has loaded `llava_llama3_vision.safetensors`
3. Ensure that `Load Image Model` has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`
4. Ensure that `Load VAE` has loaded `vae_name: hunyuan_video_vae_bf16.safetensors`
5. Ensure that `Load Diffusion Model` has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`
6. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

### [​](http://docs.comfy.org#v2-%E2%80%9Creplace%E2%80%9D-image-to-video-workflow) v2 “replace” Image-to-Video Workflow

The v2 workflow is essentially the same as the v1 workflow. You just need to download the **replace** model and use it in the `Load Diffusion Model` node.

#### [​](http://docs.comfy.org#1-workflow-and-asset-2) 1. Workflow and Asset

Download the workflow image below and drag it into ComfyUI to load the workflow:

Download the image below, which we’ll use as the starting frame for the image-to-video generation:

#### [​](http://docs.comfy.org#2-related-models-manual-installation-2) 2. Related models manual installation

- [hunyuan\_video\_v2\_replace\_image\_to\_video\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors?download=true)

Ensure you have all these model files in the correct locations:

```plaintext
ComfyUI/
├── models/
│   ├── clip_vision/
│   │   └── llava_llama3_vision.safetensors                                // I2V shared model
│   ├── text_encoders/
│   │   ├── clip_l.safetensors                                             // Shared model
│   │   └── llava_llama3_fp8_scaled.safetensors                            // Shared model
│   ├── vae/
│   │   └── hunyuan_video_vae_bf16.safetensors                             // Shared model
│   └── diffusion_models/
│       └── hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors  // V2 "replace" version model
```

#### [​](http://docs.comfy.org#3-steps-to-run-the-workflow-3) 3. Steps to Run the Workflow

1. Ensure the `DualCLIPLoader` node has loaded these models:
   
   - clip\_name1: clip\_l.safetensors
   - clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. Ensure the `Load CLIP Vision` node has loaded `llava_llama3_vision.safetensors`
3. Ensure the `Load Image Model` node has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`
4. Ensure the `Load VAE` node has loaded `hunyuan_video_vae_bf16.safetensors`
5. Ensure the `Load Diffusion Model` node has loaded `hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors`
6. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## [​](http://docs.comfy.org#try-it-yourself) Try it yourself

Here are some images and prompts we provide. Based on that content or make an adjustment to create your own video.

```plaintext
Futuristic robot dancing ballet, dynamic motion, fast motion, fast shot, moving scene
```

* * *

```plaintext
Samurai waving sword and hitting the camera. camera angle movement, zoom in, fast scene, super fast, dynamic
```

* * *

```plaintext
flying car fastly moving and flying through the city
```

* * *

```plaintext
cyberpunk car race in night city, dynamic, super fast, fast shot
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/video/hunyuan-video.mdx)

[Previous](http://docs.comfy.org/tutorials/video/ltxv)

[Wan VideoThis guide demonstrates how to generate videos with first and last frames using Wan2.1 Video in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/video/wan/wan-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Shared Models for All Workflows](http://docs.comfy.org#shared-models-for-all-workflows)
- [Hunyuan Text-to-Video Workflow](http://docs.comfy.org#hunyuan-text-to-video-workflow)
- [1. Workflow](http://docs.comfy.org#1-workflow)
- [2. Manual Models Installation](http://docs.comfy.org#2-manual-models-installation)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow)
- [Hunyuan Image-to-Video Workflow](http://docs.comfy.org#hunyuan-image-to-video-workflow)
- [Shared Model for v1 and v2 Versions](http://docs.comfy.org#shared-model-for-v1-and-v2-versions)
- [V1 “concat” Image-to-Video Workflow](http://docs.comfy.org#v1-%E2%80%9Cconcat%E2%80%9D-image-to-video-workflow)
- [1. Workflow and Asset](http://docs.comfy.org#1-workflow-and-asset)
- [2. Related models manual installation](http://docs.comfy.org#2-related-models-manual-installation)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow-2)
- [v2 “replace” Image-to-Video Workflow](http://docs.comfy.org#v2-%E2%80%9Creplace%E2%80%9D-image-to-video-workflow)
- [1. Workflow and Asset](http://docs.comfy.org#1-workflow-and-asset-2)
- [2. Related models manual installation](http://docs.comfy.org#2-related-models-manual-installation-2)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow-3)
- [Try it yourself](http://docs.comfy.org#try-it-yourself)

<!-- END Development/tutorials/video/hunyuan-video.md -->


<!-- BEGIN Development/tutorials/video/ltxv.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
  
  - [LTX-Video](http://docs.comfy.org/tutorials/video/ltxv)
  - [Hunyuan Video](http://docs.comfy.org/tutorials/video/hunyuan-video)
  - Wan Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

LTX-Video

# LTX-Video

[LTX-Video](https://huggingface.co/Lightricks/LTX-Video) is a very efficient video model by lightricks. The important thing with this model is to give it long descriptive prompts.

## [​](http://docs.comfy.org#multi-frame-control) Multi Frame Control

Allows you to control the video with a series of images. You can download the input images: [starting frame](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house1.png) and [ending frame](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house2.png).

Drag the video directly into ComfyUI to run the workflow.

## [​](http://docs.comfy.org#image-to-video) Image to Video

Allows you to control the video with a first [frame image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/i2v/girl1.png).

Drag the video directly into ComfyUI to run the workflow.

## [​](http://docs.comfy.org#text-to-video) Text to Video

Drag the video directly into ComfyUI to run the workflow.

## [​](http://docs.comfy.org#requirements) Requirements

Download the following models and place them in the locations specified below:

- [ltx-video-2b-v0.9.5.safetensors](https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltx-video-2b-v0.9.5.safetensors?download=true)
- [t5xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/mochi_preview_repackaged/resolve/main/split_files/text_encoders/t5xxl_fp16.safetensors?download=true)

```plaintext
├── checkpoints/
│   └── ltx-video-2b-v0.9.5.safetensors
└── text_encoders/
    └── t5xxl_fp16.safetensors
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/video/ltxv.mdx)

[Previous](http://docs.comfy.org/tutorials/3d/hunyuan3D-2)

[Hunyuan VideoThis guide shows how to use Hunyuan Text-to-Video and Image-to-Video workflows in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/video/hunyuan-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Multi Frame Control](http://docs.comfy.org#multi-frame-control)
- [Image to Video](http://docs.comfy.org#image-to-video)
- [Text to Video](http://docs.comfy.org#text-to-video)
- [Requirements](http://docs.comfy.org#requirements)

<!-- END Development/tutorials/video/ltxv.md -->


<!-- BEGIN Development/tutorials/video/wan/fun-control.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
  
  - [LTX-Video](http://docs.comfy.org/tutorials/video/ltxv)
  - [Hunyuan Video](http://docs.comfy.org/tutorials/video/hunyuan-video)
  - Wan Video
    
    - [Wan Video](http://docs.comfy.org/tutorials/video/wan/wan-video)
    - [Wan2.1 Fun Control](http://docs.comfy.org/tutorials/video/wan/fun-control)
    - [Wan2.1 Fun InP](http://docs.comfy.org/tutorials/video/wan/fun-inp)
    - [First-Last Frame](http://docs.comfy.org/tutorials/video/wan/wan-flf)
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Wan2.1 Fun Control Video Examples

# ComfyUI Wan2.1 Fun Control Video Examples

This guide demonstrates how to use Wan2.1 Fun Control in ComfyUI to generate videos with control videos

## [​](http://docs.comfy.org#about-wan2-1-fun-control) About Wan2.1-Fun-Control

**Wan2.1-Fun-Control** is an open-source video generation and control project developed by Alibaba team. It introduces innovative Control Codes mechanisms combined with deep learning and multimodal conditional inputs to generate high-quality videos that conform to preset control conditions. The project focuses on precisely guiding generated video content through multimodal control conditions.

Currently, the Fun Control model supports various control conditions, including **Canny (line art), Depth, OpenPose (human posture), MLSD (geometric edges), and trajectory control.** The model also supports multi-resolution video prediction with options for 512, 768, and 1024 resolutions at 16 frames per second, generating videos up to 81 frames (approximately 5 seconds) in length.

Model versions:

- **1.3B** Lightweight: Suitable for local deployment and quick inference with **lower VRAM requirements**
- **14B** High-performance: Model size reaches 32GB+, offering better results but **requiring higher VRAM**

Here are the relevant code repositories:

- [Wan2.1-Fun-1.3B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Control)
- [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control)
- Code repository: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)

ComfyUI now **natively supports** the Wan2.1 Fun Control model. Before starting this tutorial, please update your ComfyUI to ensure you’re using a version after [this commit](https://github.com/comfyanonymous/ComfyUI/commit/3661c833bcc41b788a7c9f0e7bc48524f8ee5f82).

In this guide, we’ll provide two workflows:

1. A workflow using only native Comfy Core nodes
2. A workflow using custom nodes

Due to current limitations in native nodes for video support, the native-only workflow ensures users can complete the process without installing custom nodes. However, we’ve found that providing a good user experience for video generation is challenging without custom nodes, so we’re providing both workflow versions in this guide.

## [​](http://docs.comfy.org#model-installation) Model Installation

You only need to install these models once. The workflow images also contain model download information, so you can choose your preferred download method.

The following models can be found at [Wan\_2.1\_ComfyUI\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) and [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334)

Click the corresponding links to download. If you’ve used Wan-related workflows before, you only need to download the **Diffusion models**.

**Diffusion models** - choose 1.3B or 14B. The 14B version has a larger file size (32GB) and higher VRAM requirements:

- [wan2.1\_fun\_control\_1.3B\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_control_1.3B_bf16.safetensors?download=true)
- [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control/blob/main/diffusion_pytorch_model.safetensors?download=true): Rename to `Wan2.1-Fun-14B-Control.safetensors` after downloading

**Text encoders** - choose one of the following models (fp16 precision has a larger size and higher performance requirements):

- [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
- [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

- [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

- [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

File storage location:

```plaintext
📂 ComfyUI/
├── 📂 models/
│   ├── 📂 diffusion_models/
│   │   └── wan2.1_fun_control_1.3B_bf16.safetensors
│   ├── 📂 text_encoders/
│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors
│   └── 📂 vae/
│   │   └── wan_2.1_vae.safetensors
│   └── 📂 clip_vision/
│       └──  clip_vision_h.safetensors                 
```

## [​](http://docs.comfy.org#comfyui-native-workflow) ComfyUI Native Workflow

In this workflow, we use videos converted to **WebP format** since the `Load Image` node doesn’t currently support mp4 format. We also use **Canny Edge** to preprocess the original video. Because many users encounter installation failures and environment issues when installing custom nodes, this version of the workflow uses only native nodes to ensure a smoother experience.

Thanks to our powerful ComfyUI authors who provide feature-rich nodes. If you want to directly check the related version, see [Workflow Using Custom Nodes](http://docs.comfy.org/_sites/docs.comfy.org/tutorials/video/wan/fun-control#workflow-using-custom-nodes).

### [​](http://docs.comfy.org#1-workflow-file-download) 1. Workflow File Download

#### [​](http://docs.comfy.org#1-1-workflow-file) 1.1 Workflow File

Download the image below and drag it into ComfyUI to load the workflow:

#### [​](http://docs.comfy.org#1-2-input-images-and-videos-download) 1.2 Input Images and Videos Download

Please download the following image and video for input:

### [​](http://docs.comfy.org#2-complete-the-workflow-step-by-step) 2. Complete the Workflow Step by Step

1. Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_control_1.3B_bf16.safetensors`
2. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
4. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
5. Upload the starting frame to the `Load Image` node (renamed to `Start_image`)
6. Upload the control video to the second `Load Image` node. Note: This node currently doesn’t support mp4, only WebP videos
7. (Optional) Modify the prompt (both English and Chinese are supported)
8. (Optional) Adjust the video size in `WanFunControlToVideo`, avoiding overly large dimensions
9. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

### [​](http://docs.comfy.org#3-usage-notes) 3. Usage Notes

- Since we need to input the same number of frames as the control video into the `WanFunControlToVideo` node, if the specified frame count exceeds the actual control video frames, the excess frames may display scenes not conforming to control conditions. We’ll address this issue in the [Workflow Using Custom Nodes](http://docs.comfy.org/_sites/docs.comfy.org/tutorials/video/wan/fun-control#workflow-using-custom-nodes)
- Avoid setting overly large dimensions, as this can make the sampling process very time-consuming. Try generating smaller images first, then upscale
- Use your imagination to build upon this workflow by adding text-to-image or other types of workflows to achieve direct text-to-video generation or style transfer
- Use tools like [ComfyUI-comfyui\_controlnet\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) for richer control options

## [​](http://docs.comfy.org#workflow-using-custom-nodes) Workflow Using Custom Nodes

We’ll need to install the following two custom nodes:

- [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)
- [ComfyUI-comfyui\_controlnet\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

You can use [ComfyUI Manager](https://github.com/Comfy-Org/ComfyUI-Manager) to install missing nodes or follow the installation instructions for each custom node package.

### [​](http://docs.comfy.org#1-workflow-file-download-2) 1. Workflow File Download

#### [​](http://docs.comfy.org#1-1-workflow-file-2) 1.1 Workflow File

Download the image below and drag it into ComfyUI to load the workflow:

Due to the large size of video files, you can also click [here](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_use_custom_nodes.json) to download the workflow file in JSON format.

#### [​](http://docs.comfy.org#1-2-input-images-and-videos-download-2) 1.2 Input Images and Videos Download

Please download the following image and video for input:

### [​](http://docs.comfy.org#2-complete-the-workflow-step-by-step-2) 2. Complete the Workflow Step by Step

> The model part is essentially the same. If you’ve already experienced the native-only workflow, you can directly upload the corresponding images and run it.

01. Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_control_1.3B_bf16.safetensors`
02. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
03. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
04. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
05. Upload the starting frame to the `Load Image` node
06. Upload an mp4 format video to the `Load Video(Upload)` custom node. Note that the workflow has adjusted the default `frame_load_cap`
07. For the current image, the `DWPose Estimator` only uses the `detect_face` option
08. (Optional) Modify the prompt (both English and Chinese are supported)
09. (Optional) Adjust the video size in `WanFunControlToVideo`, avoiding overly large dimensions
10. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

### [​](http://docs.comfy.org#3-workflow-notes) 3. Workflow Notes

Thanks to the ComfyUI community authors for their custom node packages:

- This example uses `Load Video(Upload)` to support mp4 videos
- The `video_info` obtained from `Load Video(Upload)` allows us to maintain the same `fps` for the output video
- You can replace `DWPose Estimator` with other preprocessors from the `ComfyUI-comfyui_controlnet_aux` node package
- Prompts support multiple languages

## [​](http://docs.comfy.org#usage-tips) Usage Tips

- A useful tip is that you can combine multiple image preprocessing techniques and then use the `Image Blend` node to achieve the goal of applying multiple control methods simultaneously.
- You can use the `Video Combine` node from `ComfyUI-VideoHelperSuite` to save videos in mp4 format
- We use `SaveAnimatedWEBP` because we currently don’t support embedding workflow into **mp4** and some other custom nodes may not support embedding workflow too. To preserve the workflow in the video, we choose `SaveAnimatedWEBP` node.
- In the `WanFunControlToVideo` node, `control_video` is not mandatory, so sometimes you can skip using a control video, first generate a very small video size like 320x320, and then use them as control video input to achieve consistent results.
- [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)
- [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/video/wan/fun-control.mdx)

[Previous](http://docs.comfy.org/tutorials/video/wan/wan-video)

[Wan2.1 Fun InPThis guide demonstrates how to use Wan2.1 Fun InP in ComfyUI to generate videos with first and last frame control  
\
Next](http://docs.comfy.org/tutorials/video/wan/fun-inp)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [About Wan2.1-Fun-Control](http://docs.comfy.org#about-wan2-1-fun-control)
- [Model Installation](http://docs.comfy.org#model-installation)
- [ComfyUI Native Workflow](http://docs.comfy.org#comfyui-native-workflow)
- [1. Workflow File Download](http://docs.comfy.org#1-workflow-file-download)
- [1.1 Workflow File](http://docs.comfy.org#1-1-workflow-file)
- [1.2 Input Images and Videos Download](http://docs.comfy.org#1-2-input-images-and-videos-download)
- [2. Complete the Workflow Step by Step](http://docs.comfy.org#2-complete-the-workflow-step-by-step)
- [3. Usage Notes](http://docs.comfy.org#3-usage-notes)
- [Workflow Using Custom Nodes](http://docs.comfy.org#workflow-using-custom-nodes)
- [1. Workflow File Download](http://docs.comfy.org#1-workflow-file-download-2)
- [1.1 Workflow File](http://docs.comfy.org#1-1-workflow-file-2)
- [1.2 Input Images and Videos Download](http://docs.comfy.org#1-2-input-images-and-videos-download-2)
- [2. Complete the Workflow Step by Step](http://docs.comfy.org#2-complete-the-workflow-step-by-step-2)
- [3. Workflow Notes](http://docs.comfy.org#3-workflow-notes)
- [Usage Tips](http://docs.comfy.org#usage-tips)

<!-- END Development/tutorials/video/wan/fun-control.md -->


<!-- BEGIN Development/tutorials/video/wan/fun-inp.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
  
  - [LTX-Video](http://docs.comfy.org/tutorials/video/ltxv)
  - [Hunyuan Video](http://docs.comfy.org/tutorials/video/hunyuan-video)
  - Wan Video
    
    - [Wan Video](http://docs.comfy.org/tutorials/video/wan/wan-video)
    - [Wan2.1 Fun Control](http://docs.comfy.org/tutorials/video/wan/fun-control)
    - [Wan2.1 Fun InP](http://docs.comfy.org/tutorials/video/wan/fun-inp)
    - [First-Last Frame](http://docs.comfy.org/tutorials/video/wan/wan-flf)
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Wan2.1 Fun InP Video Examples

# ComfyUI Wan2.1 Fun InP Video Examples

This guide demonstrates how to use Wan2.1 Fun InP in ComfyUI to generate videos with first and last frame control

## [​](http://docs.comfy.org#about-wan2-1-fun-inp) About Wan2.1-Fun-InP

**Wan-Fun InP** is an open-source video generation model released by Alibaba, part of the Wan2.1-Fun series, focusing on generating videos from images with first and last frame control.

**Key features**:

- **First and last frame control**: Supports inputting both first and last frame images to generate transitional video between them, enhancing video coherence and creative freedom. Compared to earlier community versions, Alibaba’s official model produces more stable and significantly higher quality results.
- **Multi-resolution support**: Supports generating videos at 512×512, 768×768, 1024×1024 and other resolutions to accommodate different scenario requirements.

**Model versions**:

- **1.3B** Lightweight: Suitable for local deployment and quick inference with **lower VRAM requirements**
- **14B** High-performance: Model size reaches 32GB+, offering better results but requiring **higher VRAM**

Below are the relevant model weights and code repositories:

- [Wan2.1-Fun-1.3B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Input)
- [Wan2.1-Fun-14B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Input)
- Code repository: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)

Currently, ComfyUI natively supports the Wan2.1 Fun InP model. Before starting this tutorial, please update your ComfyUI to ensure your version is after [this commit](https://github.com/comfyanonymous/ComfyUI/commit/0a1f8869c9998bbfcfeb2e97aa96a6d3e0a2b5df).

## [​](http://docs.comfy.org#wan2-1-fun-inp-workflow) Wan2.1 Fun InP Workflow

Download the image below and drag it into ComfyUI to load the workflow:

### [​](http://docs.comfy.org#1-workflow-file-download) 1. Workflow File Download

### [​](http://docs.comfy.org#2-manual-model-installation) 2. Manual Model Installation

If automatic model downloading is ineffective, please download the models manually and save them to the corresponding folders.

The following models can be found at [Wan\_2.1\_ComfyUI\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) and [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334)

**Diffusion models** - choose 1.3B or 14B. The 14B version has a larger file size (32GB) and higher VRAM requirements:

- [wan2.1\_fun\_inp\_1.3B\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_inp_1.3B_bf16.safetensors?download=true)
- [Wan2.1-Fun-14B-InP](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-InP/resolve/main/diffusion_pytorch_model.safetensors?download=true): Rename to `Wan2.1-Fun-14B-InP.safetensors` after downloading

**Text encoders** - choose one of the following models (fp16 precision has a larger size and higher performance requirements):

- [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
- [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

- [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

- [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

File storage location:

```plaintext
📂 ComfyUI/
├── 📂 models/
│   ├── 📂 diffusion_models/
│   │   └── wan2.1_fun_inp_1.3B_bf16.safetensors
│   ├── 📂 text_encoders/
│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors
│   └── 📂 vae/
│   │   └── wan_2.1_vae.safetensors
│   └── 📂 clip_vision/
│       └──  clip_vision_h.safetensors                 
```

### [​](http://docs.comfy.org#3-complete-the-workflow-step-by-step) 3. Complete the Workflow Step by Step

1. Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_inp_1.3B_bf16.safetensors`
2. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
4. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
5. Upload the starting frame to the `Load Image` node (renamed to `Start_image`)
6. Upload the ending frame to the second `Load Image` node
7. (Optional) Modify the prompt (both English and Chinese are supported)
8. (Optional) Adjust the video size in `WanFunInpaintToVideo`, avoiding overly large dimensions
9. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

### [​](http://docs.comfy.org#4-workflow-notes) 4. Workflow Notes

Please make sure to use the correct model, as `wan2.1_fun_inp_1.3B_bf16.safetensors` and `wan2.1_fun_control_1.3B_bf16.safetensors` are stored in the same folder and have very similar names. Ensure you’re using the right model.

- When using Wan Fun InP, you may need to frequently modify prompts to ensure the accuracy of the corresponding scene transitions.

## [​](http://docs.comfy.org#other-wan2-1-fun-inp-or-video-related-custom-node-packages) Other Wan2.1 Fun InP or video-related custom node packages

- [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)
- [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)
- [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/video/wan/fun-inp.mdx)

[Previous](http://docs.comfy.org/tutorials/video/wan/fun-control)

[First-Last FrameThis guide explains how to complete Wan2.1 FLF2V video generation examples in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/video/wan/wan-flf)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [About Wan2.1-Fun-InP](http://docs.comfy.org#about-wan2-1-fun-inp)
- [Wan2.1 Fun InP Workflow](http://docs.comfy.org#wan2-1-fun-inp-workflow)
- [1. Workflow File Download](http://docs.comfy.org#1-workflow-file-download)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation)
- [3. Complete the Workflow Step by Step](http://docs.comfy.org#3-complete-the-workflow-step-by-step)
- [4. Workflow Notes](http://docs.comfy.org#4-workflow-notes)
- [Other Wan2.1 Fun InP or video-related custom node packages](http://docs.comfy.org#other-wan2-1-fun-inp-or-video-related-custom-node-packages)

<!-- END Development/tutorials/video/wan/fun-inp.md -->


<!-- BEGIN Development/tutorials/video/wan/wan-flf.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
  
  - [LTX-Video](http://docs.comfy.org/tutorials/video/ltxv)
  - [Hunyuan Video](http://docs.comfy.org/tutorials/video/hunyuan-video)
  - Wan Video
    
    - [Wan Video](http://docs.comfy.org/tutorials/video/wan/wan-video)
    - [Wan2.1 Fun Control](http://docs.comfy.org/tutorials/video/wan/fun-control)
    - [Wan2.1 Fun InP](http://docs.comfy.org/tutorials/video/wan/fun-inp)
    - [First-Last Frame](http://docs.comfy.org/tutorials/video/wan/wan-flf)
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Wan2.1 FLF2V Native Example

# ComfyUI Wan2.1 FLF2V Native Example

This guide explains how to complete Wan2.1 FLF2V video generation examples in ComfyUI

Wan FLF2V (First-Last Frame Video Generation) is an open-source video generation model developed by the Alibaba Tongyi Wanxiang team. Its open-source license is [Apache 2.0](https://github.com/Wan-Video/Wan2.1?tab=Apache-2.0-1-ov-file). Users only need to provide two images as the starting and ending frames, and the model automatically generates intermediate transition frames, outputting a logically coherent and naturally flowing 720p high-definition video.

**Core Technical Highlights**

1. **Precise First-Last Frame Control**: The matching rate of first and last frames reaches 98%, defining video boundaries through starting and ending scenes, intelligently filling intermediate dynamic changes to achieve scene transitions and object morphing effects.
2. **Stable and Smooth Video Generation**: Using CLIP semantic features and cross-attention mechanisms, the video jitter rate is reduced by 37% compared to similar models, ensuring natural and smooth transitions.
3. **Multi-functional Creative Capabilities**: Supports dynamic embedding of Chinese and English subtitles, generation of anime/realistic/fantasy and other styles, adapting to different creative needs.
4. **720p HD Output**: Directly generates 1280×720 resolution videos without post-processing, suitable for social media and commercial applications.
5. **Open-source Ecosystem Support**: Model weights, code, and training framework are fully open-sourced, supporting deployment on mainstream AI platforms.

**Technical Principles and Architecture**

1. **DiT Architecture**: Based on diffusion models and Diffusion Transformer architecture, combined with Full Attention mechanism to optimize spatiotemporal dependency modeling, ensuring video coherence.
2. **3D Causal Variational Encoder**: Wan-VAE technology compresses HD frames to 1/128 size while retaining subtle dynamic details, significantly reducing memory requirements.
3. **Three-stage Training Strategy**: Starting from 480P resolution pre-training, gradually upgrading to 720P, balancing generation quality and computational efficiency through phased optimization.

**Related Links**

- **GitHub Repository**: [GitHub](https://github.com/Wan-Video/Wan2.1)
- **Hugging Face Model Page**: [Hugging Face](https://huggingface.co/Wan-AI/Wan2.1-FLF2V-14B-720P)
- **ModelScope Community**: [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-FLF2V-14B-720P)

## [​](http://docs.comfy.org#wan2-1-flf2v-720p-comfyui-native-workflow-example) Wan2.1 FLF2V 720P ComfyUI Native Workflow Example

### [​](http://docs.comfy.org#1-download-workflow-files-and-related-input-files) 1. Download Workflow Files and Related Input Files

Since this model is trained on high-resolution images, using smaller sizes may not yield good results. In the example, we use a size of 720 * 1280, which may cause users with lower VRAM hard to run smoothly and will take a long time to generate. If needed, please modify the video generation size at the beginning.

Please download the WebP file below, and drag it into ComfyUI to load the corresponding workflow. The workflow has embedded the corresponding model download file information.

Please download the two images below, which we will use as the starting and ending frames of the video

### [​](http://docs.comfy.org#2-manual-model-installation) 2. Manual Model Installation

If corresponding

All models involved in this guide can be found [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files).

**diffusion\_models** Choose one version based on your hardware conditions

- FP16:[wan2.1\_flf2v\_720p\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_flf2v_720p_14B_fp16.safetensors?download=true)
- FP8:[wan2.1\_flf2v\_720p\_14B\_fp8\_e4m3fn.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/blob/main/split_files/diffusion_models/wan2.1_flf2v_720p_14B_fp8_e4m3fn.safetensors)

If you have previously tried Wan Video related workflows, you may already have the following files.

Choose one version from **Text encoders** for download,

- [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
- [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

- [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

- [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

File Storage Location

```plaintext
ComfyUI/
├── models/
│   ├── diffusion_models/
│   │   └─── wan2.1_flf2v_720p_14B_fp16.safetensors           # or FP8 version
│   ├── text_encoders/
│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors           # or your chosen version
│   ├── vae/
│   │   └──  wan_2.1_vae.safetensors
│   └── clip_vision/
│       └──  clip_vision_h.safetensors   
```

### [​](http://docs.comfy.org#3-complete-workflow-execution-step-by-step) 3. Complete Workflow Execution Step by Step

1. Ensure the `Load Diffusion Model` node has loaded `wan2.1_flf2v_720p_14B_fp16.safetensors` or `wan2.1_flf2v_720p_14B_fp8_e4m3fn.safetensors`
2. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
4. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
5. Upload the starting frame to the `Start_image` node
6. Upload the ending frame to the `End_image` node
7. (Optional) Modify the positive and negative prompts, both Chinese and English are supported
8. (**Important**) In `WanFirstLastFrameToVideo`, modify the corresponding video size. We default to using a size of 720 * 1280 to achieve better results for the generated video, but this may cause issues running smoothly on lower memory. You can initially try adjusting it to a size such as 480 * 854 to ensure smooth operation, and then adjust it back to 720 * 1280 when you need to generate larger-sized videos to ensure quality results.
9. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/video/wan/wan-flf.mdx)

[Previous](http://docs.comfy.org/tutorials/video/wan/fun-inp)

[ACE-Step Music GenerationThis guide will help you create dynamic music using the ACE-Step model in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/audio/ace-step/ace-step-v1)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Wan2.1 FLF2V 720P ComfyUI Native Workflow Example](http://docs.comfy.org#wan2-1-flf2v-720p-comfyui-native-workflow-example)
- [1. Download Workflow Files and Related Input Files](http://docs.comfy.org#1-download-workflow-files-and-related-input-files)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation)
- [3. Complete Workflow Execution Step by Step](http://docs.comfy.org#3-complete-workflow-execution-step-by-step)

<!-- END Development/tutorials/video/wan/wan-flf.md -->


<!-- BEGIN Development/tutorials/video/wan/wan-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
  
  - [LTX-Video](http://docs.comfy.org/tutorials/video/ltxv)
  - [Hunyuan Video](http://docs.comfy.org/tutorials/video/hunyuan-video)
  - Wan Video
    
    - [Wan Video](http://docs.comfy.org/tutorials/video/wan/wan-video)
    - [Wan2.1 Fun Control](http://docs.comfy.org/tutorials/video/wan/fun-control)
    - [Wan2.1 Fun InP](http://docs.comfy.org/tutorials/video/wan/fun-inp)
    - [First-Last Frame](http://docs.comfy.org/tutorials/video/wan/wan-flf)
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Wan2.1 Video Examples

# ComfyUI Wan2.1 Video Examples

This guide demonstrates how to generate videos with first and last frames using Wan2.1 Video in ComfyUI

Wan2.1 Video series is a video generation model open-sourced by Alibaba in February 2025 under the [Apache 2.0 license](https://github.com/Wan-Video/Wan2.1?tab=Apache-2.0-1-ov-file). It offers two versions:

- 14B (14 billion parameters)
- 1.3B (1.3 billion parameters) Covering multiple tasks including text-to-video (T2V) and image-to-video (I2V). The model not only outperforms existing open-source models in performance but more importantly, its lightweight version requires only 8GB of VRAM to run, significantly lowering the barrier to entry.

<!--THE END-->

- [Wan2.1 Code Repository](https://github.com/Wan-Video/Wan2.1)
- [Wan2.1 Model Repository](https://huggingface.co/Wan-AI)

## [​](http://docs.comfy.org#wan2-1-comfyui-native-workflow-examples) Wan2.1 ComfyUI Native Workflow Examples

Please update ComfyUI to the latest version before starting the examples to make sure you have native Wan Video support.

## [​](http://docs.comfy.org#model-installation) Model Installation

All models mentioned in this guide can be found [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files). Below are the common models you’ll need for the examples in this guide, which you can download in advance:

Choose one version from **Text encoders** to download:

- [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
- [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

- [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

- [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

File storage locations:

```plaintext
ComfyUI/
├── models/
│   ├── diffusion_models/
│   ├── ...                  # Let's download the models in the corresponding workflow
│   ├── text_encoders/
│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors
│   └── vae/
│   │   └──  wan_2.1_vae.safetensors
│   └── clip_vision/
│       └──  clip_vision_h.safetensors   
```

For diffusion models, we’ll use the fp16 precision models in this guide because we’ve found that they perform better than the bf16 versions. If you need other precision versions, please visit [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models) to download them.

## [​](http://docs.comfy.org#wan2-1-text-to-video-workflow) Wan2.1 Text-to-Video Workflow

Before starting the workflow, please download [wan2.1\_t2v\_1.3B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_t2v_1.3B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.

> If you need other t2v precision versions, please visit [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models) to download them.

### [​](http://docs.comfy.org#1-workflow-file-download) 1. Workflow File Download

Download the file below and drag it into ComfyUI to load the corresponding workflow:

### [​](http://docs.comfy.org#2-complete-the-workflow-step-by-step) 2. Complete the Workflow Step by Step

1. Make sure the `Load Diffusion Model` node has loaded the `wan2.1_t2v_1.3B_fp16.safetensors` model
2. Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model
3. Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model
4. (Optional) You can modify the video dimensions in the `EmptyHunyuanLatentVideo` node if needed
5. (Optional) If you need to modify the prompts (positive and negative), make changes in the `CLIP Text Encoder` node at number `5`
6. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation

## [​](http://docs.comfy.org#wan2-1-image-to-video-workflow) Wan2.1 Image-to-Video Workflow

**Since Wan Video separates the 480P and 720P models**, we’ll need to provide examples for both resolutions in this guide. In addition to using different models, they also have slight parameter differences.

### [​](http://docs.comfy.org#480p-version) 480P Version

#### [​](http://docs.comfy.org#1-workflow-and-input-image) 1. Workflow and Input Image

Download the image below and drag it into ComfyUI to load the corresponding workflow:

We’ll use the following image as input:

#### [​](http://docs.comfy.org#2-model-download) 2. Model Download

Please download [wan2.1\_i2v\_480p\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_480p_14B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.

#### [​](http://docs.comfy.org#3-complete-the-workflow-step-by-step) 3. Complete the Workflow Step by Step

1. Make sure the `Load Diffusion Model` node has loaded the `wan2.1_i2v_480p_14B_fp16.safetensors` model
2. Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model
3. Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model
4. Make sure the `Load CLIP Vision` node has loaded the `clip_vision_h.safetensors` model
5. Upload the provided input image in the `Load Image` node
6. (Optional) Enter the video description content you want to generate in the `CLIP Text Encoder` node
7. (Optional) You can modify the video dimensions in the `WanImageToVideo` node if needed
8. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation

### [​](http://docs.comfy.org#720p-version) 720P Version

#### [​](http://docs.comfy.org#1-workflow-and-input-image-2) 1. Workflow and Input Image

Download the image below and drag it into ComfyUI to load the corresponding workflow:

We’ll use the following image as input:

#### [​](http://docs.comfy.org#2-model-download-2) 2. Model Download

Please download [wan2.1\_i2v\_720p\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_720p_14B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.

#### [​](http://docs.comfy.org#3-complete-the-workflow-step-by-step-2) 3. Complete the Workflow Step by Step

1. Make sure the `Load Diffusion Model` node has loaded the `wan2.1_i2v_720p_14B_fp16.safetensors` model
2. Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model
3. Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model
4. Make sure the `Load CLIP Vision` node has loaded the `clip_vision_h.safetensors` model
5. Upload the provided input image in the `Load Image` node
6. (Optional) Enter the video description content you want to generate in the `CLIP Text Encoder` node
7. (Optional) You can modify the video dimensions in the `WanImageToVideo` node if needed
8. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/video/wan/wan-video.mdx)

[Previous](http://docs.comfy.org/tutorials/video/hunyuan-video)

[Wan2.1 Fun ControlThis guide demonstrates how to use Wan2.1 Fun Control in ComfyUI to generate videos with control videos  
\
Next](http://docs.comfy.org/tutorials/video/wan/fun-control)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Wan2.1 ComfyUI Native Workflow Examples](http://docs.comfy.org#wan2-1-comfyui-native-workflow-examples)
- [Model Installation](http://docs.comfy.org#model-installation)
- [Wan2.1 Text-to-Video Workflow](http://docs.comfy.org#wan2-1-text-to-video-workflow)
- [1. Workflow File Download](http://docs.comfy.org#1-workflow-file-download)
- [2. Complete the Workflow Step by Step](http://docs.comfy.org#2-complete-the-workflow-step-by-step)
- [Wan2.1 Image-to-Video Workflow](http://docs.comfy.org#wan2-1-image-to-video-workflow)
- [480P Version](http://docs.comfy.org#480p-version)
- [1. Workflow and Input Image](http://docs.comfy.org#1-workflow-and-input-image)
- [2. Model Download](http://docs.comfy.org#2-model-download)
- [3. Complete the Workflow Step by Step](http://docs.comfy.org#3-complete-the-workflow-step-by-step)
- [720P Version](http://docs.comfy.org#720p-version)
- [1. Workflow and Input Image](http://docs.comfy.org#1-workflow-and-input-image-2)
- [2. Model Download](http://docs.comfy.org#2-model-download-2)
- [3. Complete the Workflow Step by Step](http://docs.comfy.org#3-complete-the-workflow-step-by-step-2)

<!-- END Development/tutorials/video/wan/wan-video.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/bfl/flux-pro-ultra-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Introduction

# Introduction

Official documentation for ComfyUI. Contribute [here](https://github.com/Comfy-Org/docs).

## [​](http://docs.comfy.org#comfyui) [ComfyUI](https://github.com/comfyanonymous/ComfyUI)

The most powerful and modular stable diffusion GUI and backend. Written by [comfyanonymous](https://github.com/comfyanonymous) and other [contributors](https://github.com/comfyanonymous/ComfyUI/graphs/contributors).

- **ComfyUI** is a node-based interface and inference engine for generative AI
- Users can combine various AI models and operations through nodes to achieve highly customizable and controllable content generation
- ComfyUI is completely open source and can run on your local device

## [​](http://docs.comfy.org#getting-started-with-comfyui) Getting Started with ComfyUI

### [​](http://docs.comfy.org#comfyui-installation) ComfyUI Installation

ComfyUI currently offers multiple installation methods, supporting Windows, MacOS, and Linux systems:

ComfyUI Desktop (Recommended)

ComfyUI Desktop currently supports standalone installation for **Windows and MacOS (ARM)**, currently in Beta

- Code is open source on [Github](https://github.com/Comfy-Org/desktop)

You can choose the appropriate installation for your system and hardware below

- Windows
- MacOS(Apple Silicon)
- Linux

[**ComfyUI Desktop (Windows) Installation Guide**  
\
Suitable for **Windows** version with **Nvidia** GPU](http://docs.comfy.org/installation/desktop/windows)

[**ComfyUI Desktop (Windows) Installation Guide**  
\
Suitable for **Windows** version with **Nvidia** GPU](http://docs.comfy.org/installation/desktop/windows)

[**ComfyUI Desktop (MacOS) Installation Guide**  
\
Suitable for MacOS with **Apple Silicon**](http://docs.comfy.org/installation/desktop/macos)

ComfyUI Desktop **currently has no Linux prebuilds**, please visit the [Manual Installation](http://docs.comfy.org/installation/manual_install) section to install ComfyUI

ComfyUI Portable (Windows)

[**ComfyUI Portable (Windows) Installation Guide**  
\
Supports **Windows** ComfyUI version running on **Nvidia GPUs** or **CPU-only**, always use the latest commits and completely portable.](http://docs.comfy.org/installation/comfyui_portable_windows)

Manual Installation

[**ComfyUI Manual Installation Guide**  
\
Supports all system types and GPU types (Nvidia, AMD, Intel, Apple Silicon, Ascend NPU, Cambricon MLU)](http://docs.comfy.org/installation/manual_install)

## [​](http://docs.comfy.org#contributing-to-comfyui-ecosystem) Contributing to ComfyUI Ecosystem

If you’re planning to develop ComfyUI custom nodes (plugins), please read the following section.

[**Custom Node Development Guide**  
\
Learn how to build a custom node (plugin) for ComfyUI](http://docs.comfy.org/custom-nodes/overview)

## [​](http://docs.comfy.org#contributing-to-documentation) Contributing to Documentation

Fork the documentation [repo](https://github.com/comfyanonymous/ComfyUI) on Github and submit a PR to us

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/get_started/introduction.mdx)

[System RequirementsThis guide introduces some system requirements for ComfyUI, including hardware and software requirements  
\
Next](http://docs.comfy.org/installation/system_requirements)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [ComfyUI](http://docs.comfy.org#comfyui)
- [Getting Started with ComfyUI](http://docs.comfy.org#getting-started-with-comfyui)
- [ComfyUI Installation](http://docs.comfy.org#comfyui-installation)
- [Contributing to ComfyUI Ecosystem](http://docs.comfy.org#contributing-to-comfyui-ecosystem)
- [Contributing to Documentation](http://docs.comfy.org#contributing-to-documentation)

<!-- END Get_Started/built-in-nodes/api-node/image/bfl/flux-pro-ultra-image.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/ideogram/ideogram-v1.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
  - Ideogram
    
    - [Ideogram V2](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v2)
    - [Ideogram V3](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3)
    - [Ideogram V1](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v1)
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Ideogram V1 - ComfyUI Native Node Documentation

# Ideogram V1 - ComfyUI Native Node Documentation

Node for creating precise text rendering images using Ideogram API

The Ideogram V1 node allows you to generate images with high-quality text rendering capabilities using Ideogram’s text-to-image API.

## [​](http://docs.comfy.org#parameter-description) Parameter Description

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing the content to generateturbobooleanFalseWhether to use turbo mode (faster but possibly lower quality)aspect\_ratioselect”1:1”Image aspect ratiomagic\_prompt\_optionselect”AUTO”Determines whether to use MagicPrompt in generation, options: AUTO, ON, OFFseedinteger0Random seed value (0-2147483647)negative\_promptstring""Specifies elements you don’t want in the imagenum\_imagesinteger1Number of images to generate (1-8)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image result

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated on 2025-05-03)]

```python
class IdeogramV1(ComfyNodeABC):
    """
    Generates images synchronously using the Ideogram V1 model.

    Images links are available for a limited period of time; if you would like to keep the image, you must download it.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation",
                    },
                ),
                "turbo": (
                    IO.BOOLEAN,
                    {
                        "default": False,
                        "tooltip": "Whether to use turbo mode (faster generation, potentially lower quality)",
                    }
                ),
            },
            "optional": {
                "aspect_ratio": (
                    IO.COMBO,
                    {
                        "options": list(V1_V2_RATIO_MAP.keys()),
                        "default": "1:1",
                        "tooltip": "The aspect ratio for image generation.",
                    },
                ),
                "magic_prompt_option": (
                    IO.COMBO,
                    {
                        "options": ["AUTO", "ON", "OFF"],
                        "default": "AUTO",
                        "tooltip": "Determine if MagicPrompt should be used in generation",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2147483647,
                        "step": 1,
                        "control_after_generate": True,
                        "display": "number",
                    },
                ),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Description of what to exclude from the image",
                    },
                ),
                "num_images": (
                    IO.INT,
                    {"default": 1, "min": 1, "max": 8, "step": 1, "display": "number"},
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = (IO.IMAGE,)
    FUNCTION = "api_call"
    CATEGORY = "api node/image/ideogram/v1"
    DESCRIPTION = cleandoc(__doc__ or "")
    API_NODE = True

    def api_call(
        self,
        prompt,
        turbo=False,
        aspect_ratio="1:1",
        magic_prompt_option="AUTO",
        seed=0,
        negative_prompt="",
        num_images=1,
        auth_token=None,
    ):
        # Determine the model based on turbo setting
        aspect_ratio = V1_V2_RATIO_MAP.get(aspect_ratio, None)
        model = "V_1_TURBO" if turbo else "V_1"

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/ideogram/generate",
                method=HttpMethod.POST,
                request_model=IdeogramGenerateRequest,
                response_model=IdeogramGenerateResponse,
            ),
            request=IdeogramGenerateRequest(
                image_request=ImageRequest(
                    prompt=prompt,
                    model=model,
                    num_images=num_images,
                    seed=seed,
                    aspect_ratio=aspect_ratio if aspect_ratio != "ASPECT_1_1" else None,
                    magic_prompt_option=(
                        magic_prompt_option if magic_prompt_option != "AUTO" else None
                    ),
                    negative_prompt=negative_prompt if negative_prompt else None,
                )
            ),
            auth_token=auth_token,
        )

        response = operation.execute()

        if not response.data or len(response.data) == 0:
            raise Exception("No images were generated in the response")

        image_urls = [image_data.url for image_data in response.data if image_data.url]

        if not image_urls:
            raise Exception("No image URLs were generated in the response")

        return (download_and_process_images(image_urls),)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/ideogram/ideogram-v1.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3)

[Stability Stable Image UltraA node that generates high-quality images using Stability AI's ultra stable diffusion model  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameter Description](http://docs.comfy.org#parameter-description)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/ideogram/ideogram-v1.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/ideogram/ideogram-v2.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
  - Ideogram
    
    - [Ideogram V2](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v2)
    - [Ideogram V3](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3)
    - [Ideogram V1](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v1)
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Ideogram V2 - ComfyUI Built-in Node Documentation

# Ideogram V2 - ComfyUI Built-in Node Documentation

Node for creating high-quality images and text rendering using Ideogram V2 API

The Ideogram V2 node allows you to generate more refined images using Ideogram’s second-generation AI model, with significant improvements in text rendering, image quality, and overall aesthetics.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing the content to generateturbobooleanFalseWhether to use turbo mode (faster generation, possibly lower quality)

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDefaultDescriptionaspect\_ratiodropdown”1:1”Image aspect ratio, effective when resolution is set to “Auto”resolutiondropdown”Auto”Output image resolution, if not set to “Auto”, it will override the aspect\_ratio settingmagic\_prompt\_optiondropdown”AUTO”Determines whether to use MagicPrompt feature during generation, options are \[“AUTO”, “ON”, “OFF”]seedinteger0Random seed value, range 0-2147483647style\_typedropdown”NONE”Generation style type (V2 only), options are \[“AUTO”, “GENERAL”, “REALISTIC”, “DESIGN”, “RENDER\_3D”, “ANIME”]negative\_promptstring""Specifies elements you don’t want to appear in the imagenum\_imagesinteger1Number of images to generate, range 1-8

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image(s)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated on 2025-05-03)]

```python

class IdeogramV2(ComfyNodeABC):
    """
    Generates images synchronously using the Ideogram V2 model.

    Images links are available for a limited period of time; if you would like to keep the image, you must download it.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation",
                    },
                ),
                "turbo": (
                    IO.BOOLEAN,
                    {
                        "default": False,
                        "tooltip": "Whether to use turbo mode (faster generation, potentially lower quality)",
                    }
                ),
            },
            "optional": {
                "aspect_ratio": (
                    IO.COMBO,
                    {
                        "options": list(V1_V2_RATIO_MAP.keys()),
                        "default": "1:1",
                        "tooltip": "The aspect ratio for image generation. Ignored if resolution is not set to AUTO.",
                    },
                ),
                "resolution": (
                    IO.COMBO,
                    {
                        "options": list(V1_V1_RES_MAP.keys()),
                        "default": "Auto",
                        "tooltip": "The resolution for image generation. If not set to AUTO, this overrides the aspect_ratio setting.",
                    },
                ),
                "magic_prompt_option": (
                    IO.COMBO,
                    {
                        "options": ["AUTO", "ON", "OFF"],
                        "default": "AUTO",
                        "tooltip": "Determine if MagicPrompt should be used in generation",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2147483647,
                        "step": 1,
                        "control_after_generate": True,
                        "display": "number",
                    },
                ),
                "style_type": (
                    IO.COMBO,
                    {
                        "options": ["AUTO", "GENERAL", "REALISTIC", "DESIGN", "RENDER_3D", "ANIME"],
                        "default": "NONE",
                        "tooltip": "Style type for generation (V2 only)",
                    },
                ),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Description of what to exclude from the image",
                    },
                ),
                "num_images": (
                    IO.INT,
                    {"default": 1, "min": 1, "max": 8, "step": 1, "display": "number"},
                ),
                #"color_palette": (
                #    IO.STRING,
                #    {
                #        "multiline": False,
                #        "default": "",
                #        "tooltip": "Color palette preset name or hex colors with weights",
                #    },
                #),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = (IO.IMAGE,)
    FUNCTION = "api_call"
    CATEGORY = "api node/image/ideogram/v2"
    DESCRIPTION = cleandoc(__doc__ or "")
    API_NODE = True

    def api_call(
        self,
        prompt,
        turbo=False,
        aspect_ratio="1:1",
        resolution="Auto",
        magic_prompt_option="AUTO",
        seed=0,
        style_type="NONE",
        negative_prompt="",
        num_images=1,
        color_palette="",
        auth_token=None,
    ):
        aspect_ratio = V1_V2_RATIO_MAP.get(aspect_ratio, None)
        resolution = V1_V1_RES_MAP.get(resolution, None)
        # Determine the model based on turbo setting
        model = "V_2_TURBO" if turbo else "V_2"

        # Handle resolution vs aspect_ratio logic
        # If resolution is not AUTO, it overrides aspect_ratio
        final_resolution = None
        final_aspect_ratio = None

        if resolution != "AUTO":
            final_resolution = resolution
        else:
            final_aspect_ratio = aspect_ratio if aspect_ratio != "ASPECT_1_1" else None

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/ideogram/generate",
                method=HttpMethod.POST,
                request_model=IdeogramGenerateRequest,
                response_model=IdeogramGenerateResponse,
            ),
            request=IdeogramGenerateRequest(
                image_request=ImageRequest(
                    prompt=prompt,
                    model=model,
                    num_images=num_images,
                    seed=seed,
                    aspect_ratio=final_aspect_ratio,
                    resolution=final_resolution,
                    magic_prompt_option=(
                        magic_prompt_option if magic_prompt_option != "AUTO" else None
                    ),
                    style_type=style_type if style_type != "NONE" else None,
                    negative_prompt=negative_prompt if negative_prompt else None,
                    color_palette=color_palette if color_palette else None,
                )
            ),
            auth_token=auth_token,
        )

        response = operation.execute()

        if not response.data or len(response.data) == 0:
            raise Exception("No images were generated in the response")

        image_urls = [image_data.url for image_data in response.data if image_data.url]

        if not image_urls:
            raise Exception("No image URLs were generated in the response")

        return (download_and_process_images(image_urls),)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/ideogram/ideogram-v2.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)

[Ideogram V3Node for creating top-quality images and text rendering using Ideogram's latest V3 API  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/ideogram/ideogram-v2.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/ideogram/ideogram-v3.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
  - Ideogram
    
    - [Ideogram V2](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v2)
    - [Ideogram V3](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v3)
    - [Ideogram V1](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v1)
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Ideogram V3 - ComfyUI Native Node Documentation

# Ideogram V3 - ComfyUI Native Node Documentation

Node for creating top-quality images and text rendering using Ideogram’s latest V3 API

This node connects to the Ideogram V3 API to perform image generation tasks.

Currently, this node supports two image generation modes:

- **Text-to-Image Mode** - Generate new images from text prompts
- **Inpainting Mode** - Regenerate specific areas by providing an original image and mask

### [​](http://docs.comfy.org#text-to-image-mode) Text-to-Image Mode

This is the default mode, activated when no image or mask inputs are provided. Simply provide a prompt and the desired parameters:

1. Describe the image you want in the prompt field
2. Select an appropriate aspect ratio or resolution
3. Adjust other parameters like magic prompt, seed, and rendering quality
4. Run the node to generate the image

### [​](http://docs.comfy.org#inpainting-mode) Inpainting Mode

**Important Note**: This mode requires both image and mask inputs. If only one is provided, the node will throw an error.

1. Connect the original image to the `image` input port
2. Create a mask with the same dimensions as the original image, where white areas represent parts to be regenerated
3. Connect the mask to the `mask` input port
4. Describe what you want to generate in the masked area in the prompt
5. Run the node to perform local editing

## [​](http://docs.comfy.org#parameter-descriptions) Parameter Descriptions

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing the content to generateaspect\_ratiocombo”1:1”Image aspect ratio (text-to-image mode only)resolutioncombo”Auto”Image resolution, overrides aspect ratio when setmagic\_prompt\_optioncombo”AUTO”Magic prompt enhancement: AUTO, ON, or OFFseedint0Random seed value, 0 for random generationnum\_imagesint1Number of images to generate (1-8)rendering\_speedcombo”BALANCED”Rendering speed: BALANCED, TURBO, or QUALITY

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionimageimageInput image for inpainting mode (**must be provided with mask**)maskmaskMask for inpainting, white areas will be replaced (**must be provided with image**)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image

## [​](http://docs.comfy.org#how-it-works) How It Works

The Ideogram V3 node uses state-of-the-art AI models to process user input, capable of understanding complex design intentions and text layout requirements. It supports two main modes:

1. **Generation Mode**: Creates new images from text prompts
2. **Edit Mode**: Uses original image + mask combination, replacing only the areas specified by the mask

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class IdeogramV3(ComfyNodeABC):
    """
    Generates images synchronously using the Ideogram V3 model.

    Supports both regular image generation from text prompts and image editing with mask.
    Images links are available for a limited period of time; if you would like to keep the image, you must download it.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation or editing",
                    },
                ),
            },
            "optional": {
                "image": (
                    IO.IMAGE,
                    {
                        "default": None,
                        "tooltip": "Optional reference image for image editing.",
                    },
                ),
                "mask": (
                    IO.MASK,
                    {
                        "default": None,
                        "tooltip": "Optional mask for inpainting (white areas will be replaced)",
                    },
                ),
                "aspect_ratio": (
                    IO.COMBO,
                    {
                        "options": list(V3_RATIO_MAP.keys()),
                        "default": "1:1",
                        "tooltip": "The aspect ratio for image generation. Ignored if resolution is not set to Auto.",
                    },
                ),
                "resolution": (
                    IO.COMBO,
                    {
                        "options": V3_RESOLUTIONS,
                        "default": "Auto",
                        "tooltip": "The resolution for image generation. If not set to Auto, this overrides the aspect_ratio setting.",
                    },
                ),
                "magic_prompt_option": (
                    IO.COMBO,
                    {
                        "options": ["AUTO", "ON", "OFF"],
                        "default": "AUTO",
                        "tooltip": "Determine if MagicPrompt should be used in generation",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2147483647,
                        "step": 1,
                        "control_after_generate": True,
                        "display": "number",
                    },
                ),
                "num_images": (
                    IO.INT,
                    {"default": 1, "min": 1, "max": 8, "step": 1, "display": "number"},
                ),
                "rendering_speed": (
                    IO.COMBO,
                    {
                        "options": ["BALANCED", "TURBO", "QUALITY"],
                        "default": "BALANCED",
                        "tooltip": "Controls the trade-off between generation speed and quality",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = (IO.IMAGE,)
    FUNCTION = "api_call"
    CATEGORY = "api node/image/ideogram/v3"
    DESCRIPTION = cleandoc(__doc__ or "")
    API_NODE = True

    def api_call(
        self,
        prompt,
        image=None,
        mask=None,
        resolution="Auto",
        aspect_ratio="1:1",
        magic_prompt_option="AUTO",
        seed=0,
        num_images=1,
        rendering_speed="BALANCED",
        auth_token=None,
    ):
        # Check if both image and mask are provided for editing mode
        if image is not None and mask is not None:
            # Edit mode
            path = "/proxy/ideogram/ideogram-v3/edit"

            # Process image and mask
            input_tensor = image.squeeze().cpu()

            # Validate mask dimensions match image
            if mask.shape[1:] != image.shape[1:-1]:
                raise Exception("Mask and Image must be the same size")

            # Process image
            img_np = (input_tensor.numpy() * 255).astype(np.uint8)
            img = Image.fromarray(img_np)
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format="PNG")
            img_byte_arr.seek(0)
            img_binary = img_byte_arr
            img_binary.name = "image.png"

            # Process mask - white areas will be replaced
            mask_np = (mask.squeeze().cpu().numpy() * 255).astype(np.uint8)
            mask_img = Image.fromarray(mask_np)
            mask_byte_arr = io.BytesIO()
            mask_img.save(mask_byte_arr, format="PNG")
            mask_byte_arr.seek(0)
            mask_binary = mask_byte_arr
            mask_binary.name = "mask.png"

            # Create edit request
            edit_request = IdeogramV3EditRequest(
                prompt=prompt,
                rendering_speed=rendering_speed,
            )

            # Add optional parameters
            if magic_prompt_option != "AUTO":
                edit_request.magic_prompt = magic_prompt_option
            if seed != 0:
                edit_request.seed = seed
            if num_images > 1:
                edit_request.num_images = num_images

            # Execute the operation for edit mode
            operation = SynchronousOperation(
                endpoint=ApiEndpoint(
                    path=path,
                    method=HttpMethod.POST,
                    request_model=IdeogramV3EditRequest,
                    response_model=IdeogramGenerateResponse,
                ),
                request=edit_request,
                files={
                    "image": img_binary,
                    "mask": mask_binary,
                },
                content_type="multipart/form-data",
                auth_token=auth_token,
            )

        elif image is not None or mask is not None:
            # If only one of image or mask is provided, raise an error
            raise Exception("Ideogram V3 image editing requires both an image AND a mask")
        else:
            # Generation mode
            path = "/proxy/ideogram/ideogram-v3/generate"

            # Create generation request
            gen_request = IdeogramV3Request(
                prompt=prompt,
                rendering_speed=rendering_speed,
            )

            # Handle resolution vs aspect ratio
            if resolution != "Auto":
                gen_request.resolution = resolution
            elif aspect_ratio != "1:1":
                v3_aspect = V3_RATIO_MAP.get(aspect_ratio)
                if v3_aspect:
                    gen_request.aspect_ratio = v3_aspect

            # Add optional parameters
            if magic_prompt_option != "AUTO":
                gen_request.magic_prompt = magic_prompt_option
            if seed != 0:
                gen_request.seed = seed
            if num_images > 1:
                gen_request.num_images = num_images

            # Execute the operation for generation mode
            operation = SynchronousOperation(
                endpoint=ApiEndpoint(
                    path=path,
                    method=HttpMethod.POST,
                    request_model=IdeogramV3Request,
                    response_model=IdeogramGenerateResponse,
                ),
                request=gen_request,
                auth_token=auth_token,
            )

        # Execute the operation and process response
        response = operation.execute()

        if not response.data or len(response.data) == 0:
            raise Exception("No images were generated in the response")

        image_urls = [image_data.url for image_data in response.data if image_data.url]

        if not image_urls:
            raise Exception("No image URLs were generated in the response")

        return (download_and_process_images(image_urls),)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/ideogram/ideogram-v3.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v2)

[Ideogram V1Node for creating precise text rendering images using Ideogram API  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v1)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Text-to-Image Mode](http://docs.comfy.org#text-to-image-mode)
- [Inpainting Mode](http://docs.comfy.org#inpainting-mode)
- [Parameter Descriptions](http://docs.comfy.org#parameter-descriptions)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/ideogram/ideogram-v3.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/luma/luma-image-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
    
    - [Luma Reference](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-reference)
    - [Luma Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image)
    - [Luma Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image)
  - Recraft
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Image to Image - ComfyUI Built-in Node Documentation

# Luma Image to Image - ComfyUI Built-in Node Documentation

Node for modifying images using Luma AI

The Luma Image to Image node allows you to modify existing images using Luma AI technology based on text prompts, while preserving certain features and structure of the original image.

## [​](http://docs.comfy.org#node-function) Node Function

This node connects to Luma AI’s text-to-image API, enabling users to generate images through detailed text prompts. Luma AI is known for its excellent realism and detail, particularly excelling at generating photorealistic content and artistic style images.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing the content to generatemodelselect-Select which generation model to useaspect\_ratioselect16:9Set the aspect ratio of the output imageseedinteger0Seed value to determine if node should rerun, but actual results don’t depend on seedstyle\_image\_weightfloat1.0Weight of the style image, range 0.02-1.0, only effective when style\_image is provided

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

Without the following parameter inputs, the node functions in text-to-image mode

ParameterTypeDescriptionimage\_luma\_refLUMA\_REFLuma reference node connection, influences generation results through input images, can consider up to 4 imagesstyle\_imageimageStyle reference image, uses only 1 image, influences the style of generated images, adjusted through `style_image_weight`character\_imageimageAdds character features to the generated results, can be a batch of multiple images, up to 4 images

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageThe generated image

## [​](http://docs.comfy.org#usage-examples) Usage Examples

## [​](http://docs.comfy.org#how-it-works) How It Works

The Luma Image to Image node analyzes the input image and combines it with text prompts to guide the modification process. It uses Luma AI’s generation models to make creative changes to images based on prompts.

Node process:

1. First uploads the input image to ComfyAPI
2. Then sends the image URL with the prompt to Luma API
3. Waits for Luma AI to complete processing
4. Downloads and returns the generated image

The image\_weight parameter controls the degree of influence from the original image - values closer to 0 will preserve more of the original image features, while values closer to 1 allow for more substantial modifications.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated on 2025-05-05)]

```python

class LumaImageModifyNode(ComfyNodeABC):
    """
    Modifies images synchronously based on prompt and aspect ratio.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE,),
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation",
                    },
                ),
                "image_weight": (
                    IO.FLOAT,
                    {
                        "default": 1.0,
                        "min": 0.02,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Weight of the image; the closer to 0.0, the less the image will be modified.",
                    },
                ),
                "model": ([model.value for model in LumaImageModel],),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {},
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        model: str,
        image: torch.Tensor,
        image_weight: float,
        seed,
        auth_token=None,
        **kwargs,
    ):
        # first, upload image
        download_urls = upload_images_to_comfyapi(
            image, max_images=1, auth_token=auth_token
        )
        image_url = download_urls[0]
        # next, make Luma call with download url provided
        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/luma/generations/image",
                method=HttpMethod.POST,
                request_model=LumaImageGenerationRequest,
                response_model=LumaGeneration,
            ),
            request=LumaImageGenerationRequest(
                prompt=prompt,
                model=model,
                modify_image_ref=LumaModifyImageRef(
                    url=image_url, weight=round(image_weight, 2)
                ),
            ),
            auth_token=auth_token,
        )
        response_api: LumaGeneration = operation.execute()

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/luma/generations/{response_api.id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=LumaGeneration,
            ),
            completed_statuses=[LumaState.completed],
            failed_statuses=[LumaState.failed],
            status_extractor=lambda x: x.state,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        img_response = requests.get(response_poll.assets.image)
        img = process_image_response(img_response)
        return (img,)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/luma/luma-image-to-image.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image)

[Save SVGA utility node for saving SVG vector graphics to files  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Examples](http://docs.comfy.org#usage-examples)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/luma/luma-image-to-image.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/luma/luma-reference.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
    
    - [Luma Reference](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-reference)
    - [Luma Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image)
    - [Luma Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image)
  - Recraft
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Reference - ComfyUI Built-in Node Documentation

# Luma Reference - ComfyUI Built-in Node Documentation

Helper node providing reference images for Luma image generation

The Luma Reference node allows you to set reference images and weights to guide the creation process of Luma image generation nodes, making the generated images closer to specific features of the reference images.

## [​](http://docs.comfy.org#node-function) Node Function

This node works as a helper tool for Luma generation nodes, allowing users to provide reference images to influence generation results. It enables users to set the weight of reference images to control how much they affect the final result. Multiple Luma Reference nodes can be chained together, with a maximum of 4 working simultaneously according to API requirements.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageImage-Input image used as referenceweightFloat1.0Controls the strength of the reference image’s influence (0-1)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionluma\_refLUMA\_REFReference object containing image and weight

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Luma Text to Image Workflow Example**  
\
Luma Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image)

## [​](http://docs.comfy.org#how-it-works) How It Works

The Luma Reference node receives image input and allows setting a weight value. The node doesn’t directly generate or modify images but creates a reference object containing image data and weight information, which is then passed to Luma generation nodes.

During the generation process, Luma AI analyzes the features of the reference image and incorporates these features into the generation results based on the set weight. Higher weight values mean the generated image will be closer to the reference image’s features, while lower weight values indicate the reference image will only slightly influence the final result.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated on 2025-05-03)]

```python

class LumaReferenceNode(ComfyNodeABC):
    """
    Holds an image and weight for use with Luma Generate Image node.
    """

    RETURN_TYPES = (LumaIO.LUMA_REF,)
    RETURN_NAMES = ("luma_ref",)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "create_luma_reference"
    CATEGORY = "api node/image/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (
                    IO.IMAGE,
                    {
                        "tooltip": "Image to use as reference.",
                    },
                ),
                "weight": (
                    IO.FLOAT,
                    {
                        "default": 1.0,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Weight of image reference.",
                    },
                ),
            },
            "optional": {"luma_ref": (LumaIO.LUMA_REF,)},
        }

    def create_luma_reference(
        self, image: torch.Tensor, weight: float, luma_ref: LumaReferenceChain = None
    ):
        if luma_ref is not None:
            luma_ref = luma_ref.clone()
        else:
            luma_ref = LumaReferenceChain()
        luma_ref.add(LumaReference(image=image, weight=round(weight, 2)))
        return (luma_ref,)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/luma/luma-reference.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/bfl/flux-pro-ultra-image)

[Luma Text to ImageA node that converts text descriptions into high-quality images using Luma AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/luma/luma-reference.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/luma/luma-text-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
    
    - [Luma Reference](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-reference)
    - [Luma Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-text-to-image)
    - [Luma Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image)
  - Recraft
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Text to Image - ComfyUI Native Node Documentation

# Luma Text to Image - ComfyUI Native Node Documentation

A node that converts text descriptions into high-quality images using Luma AI

The Luma Text to Image node allows you to create highly realistic and artistic images from text descriptions using Luma AI’s advanced image generation capabilities.

## [​](http://docs.comfy.org#node-function) Node Function

This node connects to Luma AI’s text-to-image API, enabling users to generate images through detailed text prompts. Luma AI is known for its excellent realism and detail representation, particularly excelling at producing photorealistic content and artistic style images.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptString""Text prompt describing the content to generatemodelSelect-Choose which generation model to useaspect\_ratioSelect16:9Set the output image’s aspect ratioseedInteger0Seed value to determine if the node should re-run, but actual results are independent of the seedstyle\_image\_weightFloat1.0Style image weight, range 0.0-1.0, only applies when style\_image is provided, higher means stronger style reference

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionimage\_luma\_refLUMA\_REFLuma reference node connection to influence generation with input images; up to 4 imagesstyle\_imageImageStyle reference image; only 1 image will be usedcharacter\_imageImageCharacter reference images; can be a batch of multiple, up to 4 images

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEImageGenerated image result

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Luma Text to Image Usage Example**  
\
Detailed guide for Luma Text to Image workflow](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-image)

## [​](http://docs.comfy.org#how-it-works) How It Works

The Luma Text to Image node analyzes the text prompt provided by the user and creates corresponding images through Luma AI’s generation models. This process uses deep learning technology to understand text descriptions and convert them into visual representations. Users can fine-tune the generation process by adjusting various parameters, including resolution, guidance scale, and negative prompts.

Additionally, the node supports using reference images and concept guidance to further influence the generation results, allowing creators to more precisely achieve their creative vision.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated on 2025-05-03)]

```python

class LumaImageGenerationNode(ComfyNodeABC):
    """
    Generates images synchronously based on prompt and aspect ratio.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation",
                    },
                ),
                "model": ([model.value for model in LumaImageModel],),
                "aspect_ratio": (
                    [ratio.value for ratio in LumaAspectRatio],
                    {
                        "default": LumaAspectRatio.ratio_16_9,
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
                "style_image_weight": (
                    IO.FLOAT,
                    {
                        "default": 1.0,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Weight of style image. Ignored if no style_image provided.",
                    },
                ),
            },
            "optional": {
                "image_luma_ref": (
                    LumaIO.LUMA_REF,
                    {
                        "tooltip": "Luma Reference node connection to influence generation with input images; up to 4 images can be considered."
                    },
                ),
                "style_image": (
                    IO.IMAGE,
                    {"tooltip": "Style reference image; only 1 image will be used."},
                ),
                "character_image": (
                    IO.IMAGE,
                    {
                        "tooltip": "Character reference images; can be a batch of multiple, up to 4 images can be considered."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        model: str,
        aspect_ratio: str,
        seed,
        style_image_weight: float,
        image_luma_ref: LumaReferenceChain = None,
        style_image: torch.Tensor = None,
        character_image: torch.Tensor = None,
        auth_token=None,
        **kwargs,
    ):
        # handle image_luma_ref
        api_image_ref = None
        if image_luma_ref is not None:
            api_image_ref = self._convert_luma_refs(
                image_luma_ref, max_refs=4, auth_token=auth_token
            )
        # handle style_luma_ref
        api_style_ref = None
        if style_image is not None:
            api_style_ref = self._convert_style_image(
                style_image, weight=style_image_weight, auth_token=auth_token
            )
        # handle character_ref images
        character_ref = None
        if character_image is not None:
            download_urls = upload_images_to_comfyapi(
                character_image, max_images=4, auth_token=auth_token
            )
            character_ref = LumaCharacterRef(
                identity0=LumaImageIdentity(images=download_urls)
            )

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/luma/generations/image",
                method=HttpMethod.POST,
                request_model=LumaImageGenerationRequest,
                response_model=LumaGeneration,
            ),
            request=LumaImageGenerationRequest(
                prompt=prompt,
                model=model,
                aspect_ratio=aspect_ratio,
                image_ref=api_image_ref,
                style_ref=api_style_ref,
                character_ref=character_ref,
            ),
            auth_token=auth_token,
        )
        response_api: LumaGeneration = operation.execute()

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/luma/generations/{response_api.id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=LumaGeneration,
            ),
            completed_statuses=[LumaState.completed],
            failed_statuses=[LumaState.failed],
            status_extractor=lambda x: x.state,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        img_response = requests.get(response_poll.assets.image)
        img = process_image_response(img_response)
        return (img,)

    def _convert_luma_refs(
        self, luma_ref: LumaReferenceChain, max_refs: int, auth_token=None
    ):
        luma_urls = []
        ref_count = 0
        for ref in luma_ref.refs:
            download_urls = upload_images_to_comfyapi(
                ref.image, max_images=1, auth_token=auth_token
            )
            luma_urls.append(download_urls[0])
            ref_count += 1
            if ref_count >= max_refs:
                break
        return luma_ref.create_api_model(download_urls=luma_urls, max_refs=max_refs)

    def _convert_style_image(
        self, style_image: torch.Tensor, weight: float, auth_token=None
    ):
        chain = LumaReferenceChain(
            first_ref=LumaReference(image=style_image, weight=weight)
        )
        return self._convert_luma_refs(chain, max_refs=1, auth_token=auth_token)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/luma/luma-text-to-image.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-reference)

[Luma Image to ImageNode for modifying images using Luma AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/luma/luma-text-to-image.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/openai/openai-dalle2.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
  - Ideogram
  - Stability AI
  - OpenAI
    
    - [OpenAI GPT Image 1](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-gpt-image1)
    - [OpenAI DALL·E 2](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle2)
    - [OpenAI DALL·E 3](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle3)
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

OpenAI DALL·E 2 - ComfyUI Native Node Documentation

# OpenAI DALL·E 2 - ComfyUI Native Node Documentation

Node for generating images using OpenAI’s DALL·E 2 model

The OpenAI DALL·E 2 node allows you to use OpenAI’s DALL·E 2 API to generate creative images from text descriptions.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt for DALL·E to generate images, supports multi-line inputseedinteger0The result is not actually related to the seed, this parameter only determines whether to re-executesizeselect”1024x1024”Output image size, options: 256x256, 512x512, 1024x1024ninteger1Number of images to generate, range 1-8

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDefaultDescriptionimageimageNoneOptional reference image for image editingmaskmaskNoneOptional mask for inpainting (white areas will be replaced)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image(s)

## [​](http://docs.comfy.org#features) Features

- Basic function: Generate images from text prompts
- Image editing: When both image and mask parameters are provided, performs image editing (white masked areas will be replaced)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class OpenAIDalle2(ComfyNodeABC):
    """
    Generates images synchronously via OpenAI's DALL·E 2 endpoint.

    Uses the proxy at /proxy/openai/images/generations. Returned URLs are short‑lived,
    so download or cache results if you need to keep them.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Text prompt for DALL·E",
                    },
                ),
            },
            "optional": {
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2**31 - 1,
                        "step": 1,
                        "display": "number",
                        "control_after_generate": True,
                        "tooltip": "not implemented yet in backend",
                    },
                ),
                "size": (
                    IO.COMBO,
                    {
                        "options": ["256x256", "512x512", "1024x1024"],
                        "default": "1024x1024",
                        "tooltip": "Image size",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 8,
                        "step": 1,
                        "display": "number",
                        "tooltip": "How many images to generate",
                    },
                ),
                "image": (
                    IO.IMAGE,
                    {
                        "default": None,
                        "tooltip": "Optional reference image for image editing.",
                    },
                ),
                "mask": (
                    IO.MASK,
                    {
                        "default": None,
                        "tooltip": "Optional mask for inpainting (white areas will be replaced)",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = (IO.IMAGE,)
    FUNCTION = "api_call"
    CATEGORY = "api node/image/openai"
    DESCRIPTION = cleandoc(__doc__ or "")
    API_NODE = True

    def api_call(
        self,
        prompt,
        seed=0,
        image=None,
        mask=None,
        n=1,
        size="1024x1024",
        auth_token=None,
    ):
        model = "dall-e-2"
        path = "/proxy/openai/images/generations"
        content_type = "application/json"
        request_class = OpenAIImageGenerationRequest
        img_binary = None

        if image is not None and mask is not None:
            path = "/proxy/openai/images/edits"
            content_type = "multipart/form-data"
            request_class = OpenAIImageEditRequest

            input_tensor = image.squeeze().cpu()
            height, width, channels = input_tensor.shape
            rgba_tensor = torch.ones(height, width, 4, device="cpu")
            rgba_tensor[:, :, :channels] = input_tensor

            if mask.shape[1:] != image.shape[1:-1]:
                raise Exception("Mask and Image must be the same size")
            rgba_tensor[:, :, 3] = 1 - mask.squeeze().cpu()

            rgba_tensor = downscale_image_tensor(rgba_tensor.unsqueeze(0)).squeeze()

            image_np = (rgba_tensor.numpy() * 255).astype(np.uint8)
            img = Image.fromarray(image_np)
            img_byte_arr = io.BytesIO()
            img.save(img_byte_arr, format="PNG")
            img_byte_arr.seek(0)
            img_binary = img_byte_arr  # .getvalue()
            img_binary.name = "image.png"
        elif image is not None or mask is not None:
            raise Exception("Dall-E 2 image editing requires an image AND a mask")

        # Build the operation
        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=path,
                method=HttpMethod.POST,
                request_model=request_class,
                response_model=OpenAIImageGenerationResponse,
            ),
            request=request_class(
                model=model,
                prompt=prompt,
                n=n,
                size=size,
                seed=seed,
            ),
            files=(
                {
                    "image": img_binary,
                }
                if img_binary
                else None
            ),
            content_type=content_type,
            auth_token=auth_token,
        )

        response = operation.execute()

        img_tensor = validate_and_cast_response(response)
        return (img_tensor,)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/openai/openai-dalle2.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-gpt-image1)

[OpenAI DALL·E 3Node for generating high-quality images using OpenAI's DALL·E 3 model  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle3)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Features](http://docs.comfy.org#features)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/openai/openai-dalle2.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/openai/openai-dalle3.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
  - Ideogram
  - Stability AI
  - OpenAI
    
    - [OpenAI GPT Image 1](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-gpt-image1)
    - [OpenAI DALL·E 2](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle2)
    - [OpenAI DALL·E 3](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle3)
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

OpenAI DALL·E 3 - ComfyUI Native Node Documentation

# OpenAI DALL·E 3 - ComfyUI Native Node Documentation

Node for generating high-quality images using OpenAI’s DALL·E 3 model

This node connects to OpenAI’s DALL·E 3 API, allowing users to generate high-quality images through detailed text prompts. DALL·E 3 is OpenAI’s image generation model that offers significantly improved image quality, more accurate prompt understanding, and better detail rendering compared to previous versions.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#input-parameters) Input Parameters

ParameterTypeDefaultDescriptionpromptstring""Detailed text prompt describing what to generateseedinteger0Final result is not related to seed, this parameter only determines whether to re-executequalityselection”standard”Image quality, options: “standard” or “hd”styleselection”natural”Visual style, options: “natural” or “vivid”. “Vivid” makes the model tend to create more surreal and dramatic images, while “natural” generates more realistic, less surreal imagessizeselection”1024x1024”Output image size, options: “1024x1024”, “1024x1792”, or “1792x1024”

### [​](http://docs.comfy.org#output-parameters) Output Parameters

OutputTypeDescriptionIMAGEimageThe generated image result

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class OpenAIDalle3(ComfyNodeABC):
    """
    Generates images synchronously via OpenAI's DALL·E 3 endpoint.

    Uses the proxy at /proxy/openai/images/generations. Returned URLs are short‑lived,
    so download or cache results if you need to keep them.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Text prompt for DALL·E",
                    },
                ),
            },
            "optional": {
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2**31 - 1,
                        "step": 1,
                        "display": "number",
                        "control_after_generate": True,
                        "tooltip": "not implemented yet in backend",
                    },
                ),
                "quality": (
                    IO.COMBO,
                    {
                        "options": ["standard", "hd"],
                        "default": "standard",
                        "tooltip": "Image quality",
                    },
                ),
                "style": (
                    IO.COMBO,
                    {
                        "options": ["natural", "vivid"],
                        "default": "natural",
                        "tooltip": "Vivid causes the model to lean towards generating hyper-real and dramatic images. Natural causes the model to produce more natural, less hyper-real looking images.",
                    },
                ),
                "size": (
                    IO.COMBO,
                    {
                        "options": ["1024x1024", "1024x1792", "1792x1024"],
                        "default": "1024x1024",
                        "tooltip": "Image size",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = (IO.IMAGE,)
    FUNCTION = "api_call"
    CATEGORY = "api node/image/openai"
    DESCRIPTION = cleandoc(__doc__ or "")
    API_NODE = True

    def api_call(
        self,
        prompt,
        seed=0,
        style="natural",
        quality="standard",
        size="1024x1024",
        auth_token=None,
    ):
        model = "dall-e-3"

        # build the operation
        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/openai/images/generations",
                method=HttpMethod.POST,
                request_model=OpenAIImageGenerationRequest,
                response_model=OpenAIImageGenerationResponse,
            ),
            request=OpenAIImageGenerationRequest(
                model=model,
                prompt=prompt,
                quality=quality,
                size=size,
                style=style,
                seed=seed,
            ),
            auth_token=auth_token,
        )

        response = operation.execute()

        img_tensor = validate_and_cast_response(response)
        return (img_tensor,)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/openai/openai-dalle3.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle2)

[MiniMax Image to VideoA node that converts static images to dynamic videos using MiniMax AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-image-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Input Parameters](http://docs.comfy.org#input-parameters)
- [Output Parameters](http://docs.comfy.org#output-parameters)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/openai/openai-dalle3.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/openai/openai-gpt-image1.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
  - Ideogram
  - Stability AI
  - OpenAI
    
    - [OpenAI GPT Image 1](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-gpt-image1)
    - [OpenAI DALL·E 2](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle2)
    - [OpenAI DALL·E 3](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle3)
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

OpenAI GPT Image 1 - ComfyUI Native Node Documentation

# OpenAI GPT Image 1 - ComfyUI Native Node Documentation

Node for generating images using OpenAI’s GPT-4 Vision model

This node connects to OpenAI’s GPT Image 1 API, allowing users to generate images through detailed text prompts. Unlike traditional DALL·E models, GPT Image 1 leverages GPT-4’s language understanding capabilities to handle more complex prompts and generate images that better match user intent.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing what to generatequalityselection”low”Image quality level, options: “low”, “medium”, “high”sizeselection”auto”Output image size, options: “auto”, “1024x1024”, “1024x1536”, “1536x1024”

### [​](http://docs.comfy.org#image-editing-parameters) Image Editing Parameters

ParameterTypeDescriptionimageimageInput image for editing, supports multiple imagesmaskmaskOptional mask to specify areas to modify (single image only)

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionbackgroundselectionBackground options: “opaque” or “transparent”seedintegerRandom seed (not yet implemented in backend)nintegerNumber of images to generate, range 1-8

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image result

## [​](http://docs.comfy.org#how-it-works) How It Works

The OpenAI GPT Image 1 node combines GPT-4’s language understanding with image generation. It analyzes the text prompt to understand its meaning and intent, then generates matching images.

In image editing mode, the node can modify existing images. Using a mask allows precise control over which areas to change. Note that mask input only works with single image input.

Users can control the output by adjusting parameters like quality level, size, background handling, and number of generations.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class OpenAIGPTImage1(ComfyNodeABC):
    """
    Generates images synchronously via OpenAI's GPT Image 1 endpoint.

    Uses the proxy at /proxy/openai/images/generations. Returned URLs are short‑lived,
    so download or cache results if you need to keep them.
    """

    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls) -> InputTypeDict:
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Text prompt for GPT Image 1",
                    },
                ),
            },
            "optional": {
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2**31 - 1,
                        "step": 1,
                        "display": "number",
                        "control_after_generate": True,
                        "tooltip": "not implemented yet in backend",
                    },
                ),
                "quality": (
                    IO.COMBO,
                    {
                        "options": ["low", "medium", "high"],
                        "default": "low",
                        "tooltip": "Image quality, affects cost and generation time.",
                    },
                ),
                "background": (
                    IO.COMBO,
                    {
                        "options": ["opaque", "transparent"],
                        "default": "opaque",
                        "tooltip": "Return image with or without background",
                    },
                ),
                "size": (
                    IO.COMBO,
                    {
                        "options": ["auto", "1024x1024", "1024x1536", "1536x1024"],
                        "default": "auto",
                        "tooltip": "Image size",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 8,
                        "step": 1,
                        "display": "number",
                        "tooltip": "How many images to generate",
                    },
                ),
                "image": (
                    IO.IMAGE,
                    {
                        "default": None,
                        "tooltip": "Optional reference image for image editing.",
                    },
                ),
                "mask": (
                    IO.MASK,
                    {
                        "default": None,
                        "tooltip": "Optional mask for inpainting (white areas will be replaced)",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = (IO.IMAGE,)
    FUNCTION = "api_call"
    CATEGORY = "api node/image/openai"
    DESCRIPTION = cleandoc(__doc__ or "")
    API_NODE = True

    def api_call(
        self,
        prompt,
        seed=0,
        quality="low",
        background="opaque",
        image=None,
        mask=None,
        n=1,
        size="1024x1024",
        auth_token=None,
    ):
        model = "gpt-image-1"
        path = "/proxy/openai/images/generations"
        content_type="application/json"
        request_class = OpenAIImageGenerationRequest
        img_binaries = []
        mask_binary = None
        files = []

        if image is not None:
            path = "/proxy/openai/images/edits"
            request_class = OpenAIImageEditRequest
            content_type ="multipart/form-data"

            batch_size = image.shape[0]

            for i in range(batch_size):
                single_image = image[i : i + 1]
                scaled_image = downscale_image_tensor(single_image).squeeze()

                image_np = (scaled_image.numpy() * 255).astype(np.uint8)
                img = Image.fromarray(image_np)
                img_byte_arr = io.BytesIO()
                img.save(img_byte_arr, format="PNG")
                img_byte_arr.seek(0)
                img_binary = img_byte_arr
                img_binary.name = f"image_{i}.png"

                img_binaries.append(img_binary)
                if batch_size == 1:
                    files.append(("image", img_binary))
                else:
                    files.append(("image[]", img_binary))

        if mask is not None:
            if image.shape[0] != 1:
                raise Exception("Cannot use a mask with multiple image")
            if image is None:
                raise Exception("Cannot use a mask without an input image")
            if mask.shape[1:] != image.shape[1:-1]:
                raise Exception("Mask and Image must be the same size")
            batch, height, width = mask.shape
            rgba_mask = torch.zeros(height, width, 4, device="cpu")
            rgba_mask[:, :, 3] = 1 - mask.squeeze().cpu()

            scaled_mask = downscale_image_tensor(rgba_mask.unsqueeze(0)).squeeze()

            mask_np = (scaled_mask.numpy() * 255).astype(np.uint8)
            mask_img = Image.fromarray(mask_np)
            mask_img_byte_arr = io.BytesIO()
            mask_img.save(mask_img_byte_arr, format="PNG")
            mask_img_byte_arr.seek(0)
            mask_binary = mask_img_byte_arr
            mask_binary.name = "mask.png"
            files.append(("mask", mask_binary))

        # Build the operation
        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=path,
                method=HttpMethod.POST,
                request_model=request_class,
                response_model=OpenAIImageGenerationResponse,
            ),
            request=request_class(
                model=model,
                prompt=prompt,
                quality=quality,
                background=background,
                n=n,
                seed=seed,
                size=size,
            ),
            files=files if files else None,
            content_type=content_type,
            auth_token=auth_token,
        )

        response = operation.execute()

        img_tensor = validate_and_cast_response(response)
        return (img_tensor,)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/openai/openai-gpt-image1.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image)

[OpenAI DALL·E 2Node for generating images using OpenAI's DALL·E 2 model  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle2)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Image Editing Parameters](http://docs.comfy.org#image-editing-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/openai/openai-gpt-image1.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/recraft/recraft-color-rgb.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Color RGB - ComfyUI Native Node Documentation

# Recraft Color RGB - ComfyUI Native Node Documentation

Helper node for defining color controls in Recraft image generation

The Recraft Color RGB node lets you define precise RGB color values to control colors in Recraft image generation.

## [​](http://docs.comfy.org#node-function) Node Function

This node creates a color configuration object that connects to the Recraft Controls node to specify colors used in generated images.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionrinteger0Red channel (0-255)ginteger0Green channel (0-255)binteger0Blue channel (0-255)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionrecraft\_colorRecraft ColorColor config object to connect to Recraft Controls

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Recraft Text to Image Workflow Example**  
\
Recraft Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftColorRGBNode:
    """
    Create Recraft Color by choosing specific RGB values.
    """

    RETURN_TYPES = (RecraftIO.COLOR,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    RETURN_NAMES = ("recraft_color",)
    FUNCTION = "create_color"
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "r": (IO.INT, {
                    "default": 0,
                    "min": 0,
                    "max": 255,
                    "tooltip": "Red value of color."
                }),
                "g": (IO.INT, {
                    "default": 0,
                    "min": 0,
                    "max": 255,
                    "tooltip": "Green value of color."
                }),
                "b": (IO.INT, {
                    "default": 0,
                    "min": 0,
                    "max": 255,
                    "tooltip": "Blue value of color."
                }),
            },
            "optional": {
                "recraft_color": (RecraftIO.COLOR,),
            }
        }

    def create_color(self, r: int, g: int, b: int, recraft_color: RecraftColorChain=None):
        recraft_color = recraft_color.clone() if recraft_color else RecraftColorChain()
        recraft_color.add(RecraftColor(r, g, b))
        return (recraft_color, )

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-color-rgb.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)

[Recraft Text to ImageA Recraft API node that generates high-quality images from text descriptions  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/recraft/recraft-color-rgb.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/recraft/recraft-controls.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Controls - ComfyUI Native Node Documentation

# Recraft Controls - ComfyUI Native Node Documentation

Node providing advanced control parameters for Recraft image generation

The Recraft Controls node lets you define control parameters (like colors and background colors) to guide Recraft’s image generation process. This node combines multiple control inputs into a unified control object.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptioncolorsRecraft ColorColor controls for image generationbackground\_colorRecraft ColorBackground color control

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionrecraft\_controlsRecraft ControlsControl config object for Recraft generation nodes

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Recraft Text to Image Workflow Example**  
\
Recraft Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

## [​](http://docs.comfy.org#how-it-works) How It Works

Node process:

1. Collects input control parameters (colors and background\_color)
2. Combines these parameters into a structured control object
3. Outputs this control object for connecting to Recraft generation nodes

When connected to Recraft generation nodes, these control parameters influence the AI generation process. The AI considers multiple factors beyond just the text prompt’s semantic content. If color inputs are configured, the AI will try to use these colors appropriately in the generated image.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftControlsNode:
    """
    Create Recraft Controls for customizing Recraft generation.
    """

    RETURN_TYPES = (RecraftIO.CONTROLS,)
    RETURN_NAMES = ("recraft_controls",)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "create_controls"
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
            },
            "optional": {
                "colors": (RecraftIO.COLOR,),
                "background_color": (RecraftIO.COLOR,),
            }
        }

    def create_controls(self, colors: RecraftColorChain=None, background_color: RecraftColorChain=None):
        return (RecraftControls(colors=colors, background_color=background_color), )

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-controls.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)

[Recraft Replace BackgroundA Recraft API node that automatically detects foreground subjects and replaces backgrounds  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/recraft/recraft-controls.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/recraft/recraft-creative-upscale.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Creative Upscale - ComfyUI Native Node Documentation

# Recraft Creative Upscale - ComfyUI Native Node Documentation

A Recraft API node that uses AI to creatively enhance image details and resolution

The Recraft Creative Upscale node uses Recraft’s API to increase image resolution while creatively enhancing and enriching image details.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageimage-Input image to be creatively upscaled

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageHigh-resolution image after creative upscaling

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftCreativeUpscaleNode(RecraftCrispUpscaleNode):
    """
    Upscale image synchronously.
    Enhances a given raster image using ‘creative upscale’ tool, boosting resolution with a focus on refining small details and faces.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    RECRAFT_PATH = "/proxy/recraft/images/creativeUpscale"
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-creative-upscale.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)

[Recraft Image to ImageA Recraft API node that generates new images based on text prompts and reference images  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/recraft/recraft-creative-upscale.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Crisp Upscale - ComfyUI Native Node Documentation

# Recraft Crisp Upscale - ComfyUI Native Node Documentation

A Recraft API node that enhances image clarity and resolution using AI

The Recraft Crisp Upscale node uses Recraft’s API to improve image resolution and clarity.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageimage-Input image to be upscaled

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageUpscaled and enhanced output image

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftCrispUpscaleNode:
    """
    Upscale image synchronously.
    Enhances a given raster image using ‘crisp upscale’ tool, increasing image resolution, making the image sharper and cleaner.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    RECRAFT_PATH = "/proxy/recraft/images/crispUpscale"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
            },
            "optional": {
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        auth_token=None,
        **kwargs,
    ):
        images = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                path=self.RECRAFT_PATH,
                auth_token=auth_token,
            )
            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))
            pbar.update(1)

        images_tensor = torch.cat(images, dim=0)
        return (images_tensor,)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)

[Recraft Color RGBHelper node for defining color controls in Recraft image generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/recraft/recraft-image-inpainting.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Image Inpainting - ComfyUI Native Node Documentation

# Recraft Image Inpainting - ComfyUI Native Node Documentation

Selectively modify image regions using Recraft API

The Recraft Image Inpainting node lets you modify specific areas of an image while keeping the rest unchanged. By providing an image, mask and text prompt, you can generate new content to fill the selected areas.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageimage-Input image to modifymaskmask-Black and white mask defining areas to changepromptstring""Text describing what to generate in masked areaninteger1Number of results to generate (1-6)seedinteger0Random seed value

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionrecraft\_styleRecraft StyleStyle settings for generated contentnegative\_promptstringElements to avoid in generated contentrecraft\_controlsRecraft ControlsAdditional controls like colors

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageModified image result

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftImageInpaintingNode:
    """
    Modify image based on prompt and mask.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
                "mask": (IO.MASK, ),
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation.",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 6,
                        "tooltip": "The number of images to generate.",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "recraft_style": (RecraftIO.STYLEV3,),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        mask: torch.Tensor,
        prompt: str,
        n: int,
        seed,
        auth_token=None,
        recraft_style: RecraftStyle = None,
        negative_prompt: str = None,
        **kwargs,
    ):
        default_style = RecraftStyle(RecraftStyleV3.realistic_image)
        if recraft_style is None:
            recraft_style = default_style

        if not negative_prompt:
            negative_prompt = None

        request = RecraftImageGenerationRequest(
            prompt=prompt,
            negative_prompt=negative_prompt,
            model=RecraftModel.recraftv3,
            n=n,
            style=recraft_style.style,
            substyle=recraft_style.substyle,
            style_id=recraft_style.style_id,
            random_seed=seed,
        )

        # prepare mask tensor
        _, H, W, _ = image.shape
        mask = mask.unsqueeze(-1)
        mask = mask.movedim(-1,1)
        mask = common_upscale(mask, width=W, height=H, upscale_method="nearest-exact", crop="disabled")
        mask = mask.movedim(1,-1)
        mask = (mask > 0.5).float()

        images = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                mask=mask[i:i+1],
                path="/proxy/recraft/images/inpaint",
                request=request,
                auth_token=auth_token,
            )
            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))
            pbar.update(1)

        images_tensor = torch.cat(images, dim=0)
        return (images_tensor, )
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-image-inpainting.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)

[Recraft Vectorize ImageA Recraft API node that converts raster images to vector SVG format  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/recraft/recraft-image-inpainting.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/recraft/recraft-image-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Image to Image - ComfyUI Native Node Documentation

# Recraft Image to Image - ComfyUI Native Node Documentation

A Recraft API node that generates new images based on text prompts and reference images

The Recraft Image to Image node uses Recraft’s API to generate new images based on a reference image and text prompts.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageimage-Reference image inputpromptstring""Text description for the generated imageninteger1Number of images to generate (1-6)seedinteger0Random seed value

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionrecraft\_styleRecraft StyleStyle settings for generated imagesnegative\_promptstringElements to avoid in generated imagesrecraft\_controlsRecraft ControlsAdditional controls like colors

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image result

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class RecraftImageToImageNode:
    """
    Modify image based on prompt and strength.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation.",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 6,
                        "tooltip": "The number of images to generate.",
                    },
                ),
                "strength": (
                    IO.FLOAT,
                    {
                        "default": 0.5,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Defines the difference with the original image, should lie in [0, 1], where 0 means almost identical, and 1 means miserable similarity."
                    }
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "recraft_style": (RecraftIO.STYLEV3,),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
                "recraft_controls": (
                    RecraftIO.CONTROLS,
                    {
                        "tooltip": "Optional additional controls over the generation via the Recraft Controls node."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        prompt: str,
        n: int,
        strength: float,
        seed,
        auth_token=None,
        recraft_style: RecraftStyle = None,
        negative_prompt: str = None,
        recraft_controls: RecraftControls = None,
        **kwargs,
    ):
        default_style = RecraftStyle(RecraftStyleV3.realistic_image)
        if recraft_style is None:
            recraft_style = default_style

        controls_api = None
        if recraft_controls:
            controls_api = recraft_controls.create_api_model()

        if not negative_prompt:
            negative_prompt = None

        request = RecraftImageGenerationRequest(
            prompt=prompt,
            negative_prompt=negative_prompt,
            model=RecraftModel.recraftv3,
            n=n,
            strength=round(strength, 2),
            style=recraft_style.style,
            substyle=recraft_style.substyle,
            style_id=recraft_style.style_id,
            controls=controls_api,
            random_seed=seed,
        )

        images = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                path="/proxy/recraft/images/imageToImage",
                request=request,
                auth_token=auth_token,
            )
            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))
            pbar.update(1)

        images_tensor = torch.cat(images, dim=0)
        return (images_tensor, )
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-image-to-image.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)

[Recraft Crisp UpscaleA Recraft API node that enhances image clarity and resolution using AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/recraft/recraft-image-to-image.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/recraft/recraft-remove-background.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Remove Background - ComfyUI Native Node Documentation

# Recraft Remove Background - ComfyUI Native Node Documentation

A Recraft API node that automatically removes image backgrounds and creates transparent alpha channels

The Recraft Remove Background node uses Recraft’s API to intelligently detect and remove image backgrounds, creating images with transparent backgrounds and corresponding alpha masks.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageimage-Input image to remove background from

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageImage with background removed (with alpha channel)MASKmaskMask of the main subject (white areas are preserved)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftRemoveBackgroundNode:
    """
    Remove background from image, and return processed image and mask.
    """

    RETURN_TYPES = (IO.IMAGE, IO.MASK)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
            },
            "optional": {
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        auth_token=None,
        **kwargs,
    ):
        images = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                path="/proxy/recraft/images/removeBackground",
                auth_token=auth_token,
            )
            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))
            pbar.update(1)

        images_tensor = torch.cat(images, dim=0)
        # use alpha channel as masks, in B,H,W format
        masks_tensor = images_tensor[:,:,:,-1:].squeeze(-1)
        return (images_tensor, masks_tensor)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-remove-background.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)

[Recraft Style - Logo RasterHelper node for setting logo raster style in Recraft image generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/recraft/recraft-remove-background.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/recraft/recraft-replace-background.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Replace Background - ComfyUI Native Node Documentation

# Recraft Replace Background - ComfyUI Native Node Documentation

A Recraft API node that automatically detects foreground subjects and replaces backgrounds

The Recraft Replace Background node uses Recraft’s API to intelligently detect subjects in images and generate new backgrounds based on text prompts.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageimage-Input image with subject to preservepromptstring""Text prompt for background generationninteger1Number of images to generate (1-6)seedinteger0Random seed value for node re-runs

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionrecraft\_styleRecraft StyleStyle settings for background generationnegative\_promptstringText describing elements to avoid

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageFinal image with replaced background

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class RecraftReplaceBackgroundNode:
    """
    Replace background on image, based on provided prompt.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation.",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 6,
                        "tooltip": "The number of images to generate.",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "recraft_style": (RecraftIO.STYLEV3,),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        prompt: str,
        n: int,
        seed,
        auth_token=None,
        recraft_style: RecraftStyle = None,
        negative_prompt: str = None,
        **kwargs,
    ):
        default_style = RecraftStyle(RecraftStyleV3.realistic_image)
        if recraft_style is None:
            recraft_style = default_style

        if not negative_prompt:
            negative_prompt = None

        request = RecraftImageGenerationRequest(
            prompt=prompt,
            negative_prompt=negative_prompt,
            model=RecraftModel.recraftv3,
            n=n,
            style=recraft_style.style,
            substyle=recraft_style.substyle,
            style_id=recraft_style.style_id,
        )

        images = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                path="/proxy/recraft/images/replaceBackground",
                request=request,
                auth_token=auth_token,
            )
            images.append(torch.cat([bytesio_to_image_tensor(x) for x in sub_bytes], dim=0))
            pbar.update(1)

        images_tensor = torch.cat(images, dim=0)
        return (images_tensor, )

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-replace-background.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)

[Ideogram V2Node for creating high-quality images and text rendering using Ideogram V2 API  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v2)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/recraft/recraft-replace-background.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Style - Digital Illustration - ComfyUI Native Node Documentation

# Recraft Style - Digital Illustration - ComfyUI Native Node Documentation

A helper node for setting digital illustration style in Recraft image generation

This node creates a style configuration object that guides Recraft’s image generation process towards a digital illustration look.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionsubstyleselectNoneSpecific substyle of digital illustration

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionrecraft\_styleRecraft StyleStyle config object to connect to Recraft generation nodes

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Recraft Text to Image Workflow Example**  
\
Recraft Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftStyleV3DigitalIllustrationNode(RecraftStyleV3RealisticImageNode):
    """
    Select digital_illustration style and optional substyle.
    """

    RECRAFT_STYLE = RecraftStyleV3.digital_illustration

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)

[Recraft Remove BackgroundA Recraft API node that automatically removes image backgrounds and creates transparent alpha channels  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Style - Logo Raster - ComfyUI Built-in Node Documentation

# Recraft Style - Logo Raster - ComfyUI Built-in Node Documentation

Helper node for setting logo raster style in Recraft image generation

This node creates a style configuration object that guides Recraft’s image generation process toward professional logo design effects. By selecting different substyles, you can define the design style, complexity and use cases of the generated logo.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionsubstyleSelection-Specific substyle for logo raster (Required)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionrecraft\_styleRecraft StyleStyle configuration object, connects to Recraft generation node

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Recraft Text to Image Workflow Example**  
\
Recraft Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python
class RecraftStyleV3LogoRasterNode(RecraftStyleV3RealisticImageNode):
    """
    Select vector_illustration style and optional substyle.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "substyle": (get_v3_substyles(s.RECRAFT_STYLE, include_none=False),),
            }
        }

    RECRAFT_STYLE = RecraftStyleV3.logo_raster
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)

[Recraft ControlsNode providing advanced control parameters for Recraft image generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Style - Realistic Image - ComfyUI Native Node Documentation

# Recraft Style - Realistic Image - ComfyUI Native Node Documentation

A helper node for setting realistic photo style in Recraft image generation

The Recraft Style - Realistic Image node helps set up realistic photo styles for Recraft image generation, with various substyle options to control the visual characteristics of generated images.

## [​](http://docs.comfy.org#node-function) Node Function

This node creates a style configuration object that guides Recraft’s image generation process towards realistic photo effects.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionsubstyleselectNoneSpecific substyle of realistic photo (Required)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionrecraft\_styleRecraft StyleStyle config object to connect to Recraft generation nodes

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Recraft Text to Image Workflow Example**  
\
Recraft Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class RecraftStyleV3RealisticImageNode:
    """
    Select realistic_image style and optional substyle.
    """

    RETURN_TYPES = (RecraftIO.STYLEV3,)
    RETURN_NAMES = ("recraft_style",)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "create_style"
    CATEGORY = "api node/image/Recraft"

    RECRAFT_STYLE = RecraftStyleV3.realistic_image

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "substyle": (get_v3_substyles(s.RECRAFT_STYLE),),
            }
        }

    def create_style(self, substyle: str):
        if substyle == "None":
            substyle = None
        return (RecraftStyle(self.RECRAFT_STYLE, substyle),)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)

[Recraft Text to VectorA Recraft API node that generates scalable vector graphics from text descriptions  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/recraft/recraft-text-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Text to Image - ComfyUI Built-in Node Documentation

# Recraft Text to Image - ComfyUI Built-in Node Documentation

A Recraft API node that generates high-quality images from text descriptions

The Recraft Text to Image node lets you generate high-quality images from text prompts by directly connecting to Recraft AI’s image generation API to create images in various styles.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text description for the imagesizeselect1024x1024Output image sizenint1Number of images (1-6)seedint0Random seed value

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionrecraft\_styleRecraft StyleImage style setting, default is “realistic photo”negative\_promptstringElements to exclude from generationrecraft\_controlsRecraft ControlsAdditional control parameters (colors, etc.)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image(s)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python
class RecraftTextToImageNode:
    """
    Generates images synchronously based on prompt and resolution.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation.",
                    },
                ),
                "size": (
                    [res.value for res in RecraftImageSize],
                    {
                        "default": RecraftImageSize.res_1024x1024,
                        "tooltip": "The size of the generated image.",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 6,
                        "tooltip": "The number of images to generate.",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "recraft_style": (RecraftIO.STYLEV3,),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
                "recraft_controls": (
                    RecraftIO.CONTROLS,
                    {
                        "tooltip": "Optional additional controls over the generation via the Recraft Controls node."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        size: str,
        n: int,
        seed,
        recraft_style: RecraftStyle = None,
        negative_prompt: str = None,
        recraft_controls: RecraftControls = None,
        auth_token=None,
        **kwargs,
    ):
        default_style = RecraftStyle(RecraftStyleV3.realistic_image)
        if recraft_style is None:
            recraft_style = default_style

        controls_api = None
        if recraft_controls:
            controls_api = recraft_controls.create_api_model()

        if not negative_prompt:
            negative_prompt = None

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/recraft/image_generation",
                method=HttpMethod.POST,
                request_model=RecraftImageGenerationRequest,
                response_model=RecraftImageGenerationResponse,
            ),
            request=RecraftImageGenerationRequest(
                prompt=prompt,
                negative_prompt=negative_prompt,
                model=RecraftModel.recraftv3,
                size=size,
                n=n,
                style=recraft_style.style,
                substyle=recraft_style.substyle,
                style_id=recraft_style.style_id,
                controls=controls_api,
            ),
            auth_token=auth_token,
        )
        response: RecraftImageGenerationResponse = operation.execute()
        images = []
        for data in response.data:
            image = bytesio_to_image_tensor(
                download_url_to_bytesio(data.url, timeout=1024)
            )
            if len(image.shape) < 4:
                image = image.unsqueeze(0)
            images.append(image)
        output_image = torch.cat(images, dim=0)

        return (output_image,)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-text-to-image.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)

[Recraft Image InpaintingSelectively modify image regions using Recraft API  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/recraft/recraft-text-to-image.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/recraft/recraft-text-to-vector.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Text to Vector - ComfyUI Native Node Documentation

# Recraft Text to Vector - ComfyUI Native Node Documentation

A Recraft API node that generates scalable vector graphics from text descriptions

The Recraft Text to Vector node lets you create high-quality vector graphics (SVG format) from text descriptions using Recraft’s API. It’s perfect for making logos, icons, and infinitely scalable illustrations.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text description of the vector graphic to generatesubstyleselect-Vector style subtypesizeselect1024x1024Canvas size for the vector outputninteger1Number of results to generate (1-6)seedinteger0Random seed value

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionnegative\_promptstringElements to exclude from generationrecraft\_controlsRecraft ControlsAdditional control parameters (colors, etc.)

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionSVGvectorGenerated SVG vector graphic, connect to SaveSVG node to save

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class RecraftTextToVectorNode:
    """
    Generates SVG synchronously based on prompt and resolution.
    """

    RETURN_TYPES = (RecraftIO.SVG,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the image generation.",
                    },
                ),
                "substyle": (get_v3_substyles(RecraftStyleV3.vector_illustration),),
                "size": (
                    [res.value for res in RecraftImageSize],
                    {
                        "default": RecraftImageSize.res_1024x1024,
                        "tooltip": "The size of the generated image.",
                    },
                ),
                "n": (
                    IO.INT,
                    {
                        "default": 1,
                        "min": 1,
                        "max": 6,
                        "tooltip": "The number of images to generate.",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
                "recraft_controls": (
                    RecraftIO.CONTROLS,
                    {
                        "tooltip": "Optional additional controls over the generation via the Recraft Controls node."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        substyle: str,
        size: str,
        n: int,
        seed,
        negative_prompt: str = None,
        recraft_controls: RecraftControls = None,
        auth_token=None,
        **kwargs,
    ):
        # create RecraftStyle so strings will be formatted properly (i.e. "None" will become None)
        recraft_style = RecraftStyle(RecraftStyleV3.vector_illustration, substyle=substyle)

        controls_api = None
        if recraft_controls:
            controls_api = recraft_controls.create_api_model()

        if not negative_prompt:
            negative_prompt = None

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/recraft/image_generation",
                method=HttpMethod.POST,
                request_model=RecraftImageGenerationRequest,
                response_model=RecraftImageGenerationResponse,
            ),
            request=RecraftImageGenerationRequest(
                prompt=prompt,
                negative_prompt=negative_prompt,
                model=RecraftModel.recraftv3,
                size=size,
                n=n,
                style=recraft_style.style,
                substyle=recraft_style.substyle,
                controls=controls_api,
            ),
            auth_token=auth_token,
        )
        response: RecraftImageGenerationResponse = operation.execute()
        svg_data = []
        for data in response.data:
            svg_data.append(download_url_to_bytesio(data.url, timeout=1024))

        return (SVG(svg_data),)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-text-to-vector.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)

[Recraft Creative UpscaleA Recraft API node that uses AI to creatively enhance image details and resolution  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/recraft/recraft-text-to-vector.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/recraft/recraft-vectorize-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Recraft Vectorize Image - ComfyUI Built-in Node Documentation

# Recraft Vectorize Image - ComfyUI Built-in Node Documentation

A Recraft API node that converts raster images to vector SVG format

The Recraft Vectorize Image node uses Recraft’s API to convert raster images (like photos, PNGs or JPEGs) into vector SVG format.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionimageImage-Input image to be converted to vector

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionSVGVectorConverted SVG vector graphic, needs to be connected to SaveSVG node to save

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Recraft Text to Image Workflow Example**  
\
Recraft Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class RecraftVectorizeImageNode:
    """
    Generates SVG synchronously from an input image.
    """

    RETURN_TYPES = (RecraftIO.SVG,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Recraft"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (IO.IMAGE, ),
            },
            "optional": {
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        image: torch.Tensor,
        auth_token=None,
        **kwargs,
    ):
        svgs = []
        total = image.shape[0]
        pbar = ProgressBar(total)
        for i in range(total):
            sub_bytes = handle_recraft_file_request(
                image=image[i],
                path="/proxy/recraft/images/vectorize",
                auth_token=auth_token,
            )
            svgs.append(SVG(sub_bytes))
            pbar.update(1)

        return (SVG.combine_all(svgs), )

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/recraft-vectorize-image.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)

[Recraft Style - Digital IllustrationA helper node for setting digital illustration style in Recraft image generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/recraft/recraft-vectorize-image.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/recraft/save-svg.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
    
    - [Save SVG](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/save-svg)
    - [Recraft Style - Realistic Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)
    - [Recraft Text to Vector](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-vector)
    - [Recraft Creative Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-creative-upscale)
    - [Recraft Image to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-to-image)
    - [Recraft Crisp Upscale](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-crisp-upscale)
    - [Recraft Color RGB](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-color-rgb)
    - [Recraft Text to Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-text-to-image)
    - [Recraft Image Inpainting](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-image-inpainting)
    - [Recraft Vectorize Image](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-vectorize-image)
    - [Recraft Style - Digital Illustration](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-digital-illustration)
    - [Recraft Remove Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-remove-background)
    - [Recraft Style - Logo Raster](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-logo-raster)
    - [Recraft Controls](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-controls)
    - [Recraft Replace Background](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-replace-background)
  - Ideogram
  - Stability AI
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Save SVG - ComfyUI Built-in Node Documentation

# Save SVG - ComfyUI Built-in Node Documentation

A utility node for saving SVG vector graphics to files

The Save SVG node allows you to save SVG data from Recraft vector generation nodes as usable files in the filesystem. This is an essential component for handling and exporting vector graphics.

## [​](http://docs.comfy.org#node-function) Node Function

This node receives SVG vector data and saves it as standard SVG files in the filesystem. It supports automatic file naming and save path specification, allowing vector graphics to be opened and edited in other software.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionsvgSVG-SVG vector data to savefilename\_prefixstring”recraft”Prefix for the filenameoutput\_dirstring-Output directory, defaults to ComfyUI output folder at `ComfyUI/output/svg/`indexinteger-1Save index, -1 means save all generated SVGs

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionSVGSVGPasses through the input SVG data

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Recraft Text to Image Workflow Example**  
\
Recraft Text to Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python
class SaveSVGNode:
    """
    Save SVG files on disk.
    """

    def __init__(self):
        self.output_dir = folder_paths.get_output_directory()
        self.type = "output"
        self.prefix_append = ""

    RETURN_TYPES = ()
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "save_svg"
    CATEGORY = "api node/image/Recraft"
    OUTPUT_NODE = True

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "svg": (RecraftIO.SVG,),
                "filename_prefix": ("STRING", {"default": "svg/ComfyUI", "tooltip": "The prefix for the file to save. This may include formatting information such as %date:yyyy-MM-dd% or %Empty Latent Image.width% to include values from nodes."})
            },
            "hidden": {
                "prompt": "PROMPT",
                "extra_pnginfo": "EXTRA_PNGINFO"
            }
        }

    def save_svg(self, svg: SVG, filename_prefix="svg/ComfyUI", prompt=None, extra_pnginfo=None):
        filename_prefix += self.prefix_append
        full_output_folder, filename, counter, subfolder, filename_prefix = folder_paths.get_save_image_path(filename_prefix, self.output_dir)
        results = list()

        # Prepare metadata JSON
        metadata_dict = {}
        if prompt is not None:
            metadata_dict["prompt"] = prompt
        if extra_pnginfo is not None:
            metadata_dict.update(extra_pnginfo)

        # Convert metadata to JSON string
        metadata_json = json.dumps(metadata_dict, indent=2) if metadata_dict else None

        for batch_number, svg_bytes in enumerate(svg.data):
            filename_with_batch_num = filename.replace("%batch_num%", str(batch_number))
            file = f"{filename_with_batch_num}_{counter:05}_.svg"

            # Read SVG content
            svg_bytes.seek(0)
            svg_content = svg_bytes.read().decode('utf-8')

            # Inject metadata if available
            if metadata_json:
                # Create metadata element with CDATA section
                metadata_element = f"""  <metadata>
    <![CDATA[
{metadata_json}
    ]]>
  </metadata>
"""
                # Insert metadata after opening svg tag using regex
                import re
                svg_content = re.sub(r'(<svg[^>]*>)', r'\1\n' + metadata_element, svg_content)

            # Write the modified SVG to file
            with open(os.path.join(full_output_folder, file), 'wb') as svg_file:
                svg_file.write(svg_content.encode('utf-8'))

            results.append({
                "filename": file,
                "subfolder": subfolder,
                "type": self.type
            })
            counter += 1
        return { "ui": { "images": results } }

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/recraft/save-svg.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/luma/luma-image-to-image)

[Recraft Style - Realistic ImageA helper node for setting realistic photo style in Recraft image generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/recraft/recraft-style-realistic-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/recraft/save-svg.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
  - Ideogram
  - Stability AI
    
    - [Stability Stable Image Ultra](http://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra)
    - [Stability AI SD 3.5 Image](http://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image)
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Stability AI Stable Diffusion 3.5 - ComfyUI Native Node Documentation

# Stability AI Stable Diffusion 3.5 - ComfyUI Native Node Documentation

A node that generates high-quality images using Stability AI Stable Diffusion 3.5 model

The Stability AI Stable Diffusion 3.5 Image node uses Stability AI’s Stable Diffusion 3.5 API to generate high-quality images. It supports both text-to-image and image-to-image generation, capable of creating detailed visual content from text prompts.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionpromptstring""What you want to see in the output image. Strong, descriptive prompts that clearly define elements, colors and themes will yield better resultsmodelselect-Choose which Stability SD 3.5 model to useaspect\_ratioselect”1:1”Width to height ratio of generated imagestyle\_presetselect”None”Optional preset style for the desired imagecfg\_scalefloat4.0How strictly the diffusion process adheres to the prompt text (higher values keep your image closer to your prompt). Range: 1.0 - 10.0, Step: 0.1seedinteger0Random seed for noise generation (0-4294967294)

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDefaultDescriptionimageimage-Input image. When provided, the node switches to image-to-image modenegative\_promptstring""Keywords of what you don’t want to see in the output image. This is an advanced featureimage\_denoisefloat0.5Denoising strength for input image. 0.0 yields image identical to input, 1.0 is as if no image was provided at all. Range: 0.0 - 1.0, Step: 0.01. Only effective when image is provided

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Stability AI Stable Diffusion 3.5 Image Workflow Example**  
\
Stability AI Stable Diffusion 3.5 Image Workflow Example](http://docs.comfy.org/tutorials/api-nodes/stability-ai/stable-diffusion-3-5-image)

## [​](http://docs.comfy.org#notes) Notes

- When an input image is provided, the node switches from text-to-image mode to image-to-image mode
- In image-to-image mode, aspect ratio parameters are ignored
- Mode selection automatically switches based on whether an image is provided:
  
  - No image provided: text-to-image mode
  - Image provided: image-to-image mode
- If style\_preset is set to “None”, no preset style will be applied

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-07)]

```python
class StabilityStableImageSD_3_5Node:
    """
    Generates images synchronously based on prompt and resolution.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/Stability AI"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "What you wish to see in the output image. A strong, descriptive prompt that clearly defines elements, colors, and subjects will lead to better results."
                    },
                ),
                "model": ([x.value for x in Stability_SD3_5_Model],),
                "aspect_ratio": ([x.value for x in StabilityAspectRatio],
                    {
                        "default": StabilityAspectRatio.ratio_1_1,
                        "tooltip": "Aspect ratio of generated image.",
                    },
                ),
                "style_preset": (get_stability_style_presets(),
                    {
                        "tooltip": "Optional desired style of generated image.",
                    },
                ),
                "cfg_scale": (
                    IO.FLOAT,
                    {
                        "default": 4.0,
                        "min": 1.0,
                        "max": 10.0,
                        "step": 0.1,
                        "tooltip": "How strictly the diffusion process adheres to the prompt text (higher values keep your image closer to your prompt)",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 4294967294,
                        "control_after_generate": True,
                        "tooltip": "The random seed used for creating the noise.",
                    },
                ),
            },
            "optional": {
                "image": (IO.IMAGE,),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "Keywords of what you do not wish to see in the output image. This is an advanced feature."
                    },
                ),
                "image_denoise": (
                    IO.FLOAT,
                    {
                        "default": 0.5,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Denoise of input image; 0.0 yields image identical to input, 1.0 is as if no image was provided at all.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(self, model: str, prompt: str, aspect_ratio: str, style_preset: str, seed: int, cfg_scale: float,
                 negative_prompt: str=None, image: torch.Tensor = None, image_denoise: float=None,
                 auth_token=None):
        validate_string(prompt, strip_whitespace=False)
        # prepare image binary if image present
        image_binary = None
        mode = Stability_SD3_5_GenerationMode.text_to_image
        if image is not None:
            image_binary = tensor_to_bytesio(image, total_pixels=1504*1504).read()
            mode = Stability_SD3_5_GenerationMode.image_to_image
            aspect_ratio = None
        else:
            image_denoise = None

        if not negative_prompt:
            negative_prompt = None
        if style_preset == "None":
            style_preset = None

        files = {
            "image": image_binary
        }

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/stability/v2beta/stable-image/generate/sd3",
                method=HttpMethod.POST,
                request_model=StabilityStable3_5Request,
                response_model=StabilityStableUltraResponse,
            ),
            request=StabilityStable3_5Request(
                prompt=prompt,
                negative_prompt=negative_prompt,
                aspect_ratio=aspect_ratio,
                seed=seed,
                strength=image_denoise,
                style_preset=style_preset,
                cfg_scale=cfg_scale,
                model=model,
                mode=mode,
            ),
            files=files,
            content_type="multipart/form-data",
            auth_token=auth_token,
        )
        response_api = operation.execute()

        if response_api.finish_reason != "SUCCESS":
            raise Exception(f"Stable Diffusion 3.5 Image generation failed: {response_api.finish_reason}.")

        image_data = base64.b64decode(response_api.image)
        returned_image = bytesio_to_image_tensor(BytesIO(image_data))

        return (returned_image,)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra)

[OpenAI GPT Image 1Node for generating images using OpenAI's GPT-4 Vision model  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-gpt-image1)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Notes](http://docs.comfy.org#notes)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
  
  - BFL
  - Luma
  - Recraft
  - Ideogram
  - Stability AI
    
    - [Stability Stable Image Ultra](http://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra)
    - [Stability AI SD 3.5 Image](http://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image)
  - OpenAI
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Stability Stable Image Ultra - ComfyUI Native Node Documentation

# Stability Stable Image Ultra - ComfyUI Native Node Documentation

A node that generates high-quality images using Stability AI’s ultra stable diffusion model

The Stability Stable Image Ultra node uses Stability AI’s Stable Diffusion Ultra API to generate high-quality images. It supports both text-to-image and image-to-image generation, creating detailed and artistic visuals from text prompts.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionpromptstring""Text description of what you want to generate. Better results come from clear, descriptive prompts that specify elements, colors and themes. You can control word importance using `(word:weight)` format, where weight is 0-1. For example: `The sky was (blue:0.3) and (green:0.8)` makes the sky more green than blue.aspect\_ratioselect”1:1”Width to height ratio of output imagestyle\_presetselect”None”Optional preset style for generated imageseedinteger0Random seed for noise generation (0-4294967294)

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDefaultDescriptionimageimage-Input image for image-to-image generationnegative\_promptstring""Describes what you don’t want in the output image. This is an advanced featureimage\_denoisefloat0.5Denoising strength for input image (0.0-1.0). 0.0 keeps input image unchanged, 1.0 is like having no input image

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionIMAGEimageGenerated image

## [​](http://docs.comfy.org#notes) Notes

- image\_denoise has no effect when no input image is provided
- No preset style is applied when style\_preset is “None”
- For image-to-image, input images are converted to the proper format before API submission

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class StabilityStableImageUltraNode:
    """
    Generates images synchronously based on prompt and resolution.
    """

    RETURN_TYPES = (IO.IMAGE,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/image/stability"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "What you wish to see in the output image. A strong, descriptive prompt that clearly defines" +
                                    "What you wish to see in the output image. A strong, descriptive prompt that clearly defines" +
                                    "elements, colors, and subjects will lead to better results. " +
                                    "To control the weight of a given word use the format `(word:weight)`," +
                                    "where `word` is the word you'd like to control the weight of and `weight`" +
                                    "is a value between 0 and 1. For example: `The sky was a crisp (blue:0.3) and (green:0.8)`" +
                                    "would convey a sky that was blue and green, but more green than blue."
                    },
                ),
                "aspect_ratio": ([x.value for x in StabilityAspectRatio],
                    {
                        "default": StabilityAspectRatio.ratio_1_1,
                        "tooltip": "Aspect ratio of generated image.",
                    },
                ),
                "style_preset": (get_stability_style_presets(),
                    {
                        "tooltip": "Optional desired style of generated image.",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 4294967294,
                        "control_after_generate": True,
                        "tooltip": "The random seed used for creating the noise.",
                    },
                ),
            },
            "optional": {
                "image": (IO.IMAGE,),
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "A blurb of text describing what you do not wish to see in the output image. This is an advanced feature."
                    },
                ),
                "image_denoise": (
                    IO.FLOAT,
                    {
                        "default": 0.5,
                        "min": 0.0,
                        "max": 1.0,
                        "step": 0.01,
                        "tooltip": "Denoise of input image; 0.0 yields image identical to input, 1.0 is as if no image was provided at all.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(self, prompt: str, aspect_ratio: str, style_preset: str, seed: int,
                 negative_prompt: str=None, image: torch.Tensor = None, image_denoise: float=None,
                 auth_token=None):
        # prepare image binary if image present
        image_binary = None
        if image is not None:
            image_binary = tensor_to_bytesio(image, 1504 * 1504).read()
        else:
            image_denoise = None

        if not negative_prompt:
            negative_prompt = None
        if style_preset == "None":
            style_preset = None

        files = {
            "image": image_binary
        }

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/stability/v2beta/stable-image/generate/ultra",
                method=HttpMethod.POST,
                request_model=StabilityStableUltraRequest,
                response_model=StabilityStableUltraResponse,
            ),
            request=StabilityStableUltraRequest(
                prompt=prompt,
                negative_prompt=negative_prompt,
                aspect_ratio=aspect_ratio,
                seed=seed,
                strength=image_denoise,
                style_preset=style_preset,
            ),
            files=files,
            content_type="multipart/form-data",
            auth_token=auth_token,
        )
        response_api = operation.execute()

        if response_api.finish_reason != "SUCCESS":
            raise Exception(f"Stable Image Ultra generation failed: {response_api.finish_reason}.")

        image_data = base64.b64decode(response_api.image)
        returned_image = bytesio_to_image_tensor(BytesIO(image_data))

        return (returned_image,)


```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/ideogram/ideogram-v1)

[Stability AI SD 3.5 ImageA node that generates high-quality images using Stability AI Stable Diffusion 3.5 model  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-diffusion-3-5-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Notes](http://docs.comfy.org#notes)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/image/stability-ai/stability-ai-stable-image-ultra.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/video/google/google-veo2-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
    
    - [Google Veo2 Video](http://docs.comfy.org/built-in-nodes/api-node/video/google/google-veo2-video)
  - Kling
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Google Veo2 Video - ComfyUI Native Node Documentation

# Google Veo2 Video - ComfyUI Native Node Documentation

A node that generates videos from text descriptions using Google’s Veo2 technology

The Google Veo2 Video node generates high-quality videos from text descriptions using Google’s Veo2 API technology, converting text prompts into dynamic video content.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text description of the video content to generateaspect\_ratioselect”16:9”Output video aspect ratio, “16:9” or “9:16”negative\_promptstring""Text describing what to avoid in the videoduration\_secondsinteger5Video duration, 5-8 secondsenhance\_promptbooleanTrueWhether to use AI to enhance the promptperson\_generationselect”ALLOW”Allow or block person generation, “ALLOW” or “BLOCK”seedinteger0Random seed, 0 means randomly generated

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDefaultDescriptionimageimageNoneOptional reference image to guide video creation

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOvideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class VeoVideoGenerationNode(ComfyNodeABC):
    """
    Generates videos from text prompts using Google's Veo API.

    This node can create videos from text descriptions and optional image inputs,
    with control over parameters like aspect ratio, duration, and more.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Text description of the video",
                    },
                ),
                "aspect_ratio": (
                    IO.COMBO,
                    {
                        "options": ["16:9", "9:16"],
                        "default": "16:9",
                        "tooltip": "Aspect ratio of the output video",
                    },
                ),
            },
            "optional": {
                "negative_prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Negative text prompt to guide what to avoid in the video",
                    },
                ),
                "duration_seconds": (
                    IO.INT,
                    {
                        "default": 5,
                        "min": 5,
                        "max": 8,
                        "step": 1,
                        "display": "number",
                        "tooltip": "Duration of the output video in seconds",
                    },
                ),
                "enhance_prompt": (
                    IO.BOOLEAN,
                    {
                        "default": True,
                        "tooltip": "Whether to enhance the prompt with AI assistance",
                    }
                ),
                "person_generation": (
                    IO.COMBO,
                    {
                        "options": ["ALLOW", "BLOCK"],
                        "default": "ALLOW",
                        "tooltip": "Whether to allow generating people in the video",
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFF,
                        "step": 1,
                        "display": "number",
                        "control_after_generate": True,
                        "tooltip": "Seed for video generation (0 for random)",
                    },
                ),
                "image": (IO.IMAGE, {
                    "default": None,
                    "tooltip": "Optional reference image to guide video generation",
                }),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    RETURN_TYPES = (IO.VIDEO,)
    FUNCTION = "generate_video"
    CATEGORY = "api node/video/Veo"
    DESCRIPTION = "Generates videos from text prompts using Google's Veo API"
    API_NODE = True

    def generate_video(
        self,
        prompt,
        aspect_ratio="16:9",
        negative_prompt="",
        duration_seconds=5,
        enhance_prompt=True,
        person_generation="ALLOW",
        seed=0,
        image=None,
        auth_token=None,
    ):
        # Prepare the instances for the request
        instances = []

        instance = {
            "prompt": prompt
        }

        # Add image if provided
        if image is not None:
            image_base64 = convert_image_to_base64(image)
            if image_base64:
                instance["image"] = {
                    "bytesBase64Encoded": image_base64,
                    "mimeType": "image/png"
                }

        instances.append(instance)

        # Create parameters dictionary
        parameters = {
            "aspectRatio": aspect_ratio,
            "personGeneration": person_generation,
            "durationSeconds": duration_seconds,
            "enhancePrompt": enhance_prompt,
        }

        # Add optional parameters if provided
        if negative_prompt:
            parameters["negativePrompt"] = negative_prompt
        if seed > 0:
            parameters["seed"] = seed

        # Initial request to start video generation
        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/veo/generate",
                method=HttpMethod.POST,
                request_model=Veo2GenVidRequest,
                response_model=Veo2GenVidResponse
            ),
            request=Veo2GenVidRequest(
                instances=instances,
                parameters=parameters
            ),
            auth_token=auth_token
        )

        initial_response = initial_operation.execute()
        operation_name = initial_response.name

        logging.info(f"Veo generation started with operation name: {operation_name}")

        # Define status extractor function
        def status_extractor(response):
            # Only return "completed" if the operation is done, regardless of success or failure
            # We'll check for errors after polling completes
            return "completed" if response.done else "pending"

        # Define progress extractor function
        def progress_extractor(response):
            # Could be enhanced if the API provides progress information
            return None

        # Define the polling operation
        poll_operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path="/proxy/veo/poll",
                method=HttpMethod.POST,
                request_model=Veo2GenVidPollRequest,
                response_model=Veo2GenVidPollResponse
            ),
            completed_statuses=["completed"],
            failed_statuses=[],  # No failed statuses, we'll handle errors after polling
            status_extractor=status_extractor,
            progress_extractor=progress_extractor,
            request=Veo2GenVidPollRequest(
                operationName=operation_name
            ),
            auth_token=auth_token,
            poll_interval=5.0
        )

        # Execute the polling operation
        poll_response = poll_operation.execute()

        # Now check for errors in the final response
        # Check for error in poll response
        if hasattr(poll_response, 'error') and poll_response.error:
            error_message = f"Veo API error: {poll_response.error.message} (code: {poll_response.error.code})"
            logging.error(error_message)
            raise Exception(error_message)

        # Check for RAI filtered content
        if (hasattr(poll_response.response, 'raiMediaFilteredCount') and
            poll_response.response.raiMediaFilteredCount > 0):

            # Extract reason message if available
            if (hasattr(poll_response.response, 'raiMediaFilteredReasons') and
                poll_response.response.raiMediaFilteredReasons):
                reason = poll_response.response.raiMediaFilteredReasons[0]
                error_message = f"Content filtered by Google's Responsible AI practices: {reason} ({poll_response.response.raiMediaFilteredCount} videos filtered.)"
            else:
                error_message = f"Content filtered by Google's Responsible AI practices ({poll_response.response.raiMediaFilteredCount} videos filtered.)"

            logging.error(error_message)
            raise Exception(error_message)

        # Extract video data
        video_data = None
        if poll_response.response and hasattr(poll_response.response, 'videos') and poll_response.response.videos and len(poll_response.response.videos) > 0:
            video = poll_response.response.videos[0]

            # Check if video is provided as base64 or URL
            if hasattr(video, 'bytesBase64Encoded') and video.bytesBase64Encoded:
                # Decode base64 string to bytes
                video_data = base64.b64decode(video.bytesBase64Encoded)
            elif hasattr(video, 'gcsUri') and video.gcsUri:
                # Download from URL
                video_url = video.gcsUri
                video_response = requests.get(video_url)
                video_data = video_response.content
            else:
                raise Exception("Video returned but no data or URL was provided")
        else:
            raise Exception("Video generation completed but no video was returned")

        if not video_data:
            raise Exception("No video data was returned")

        logging.info("Video generation completed successfully")

        # Convert video data to BytesIO object
        video_io = io.BytesIO(video_data)

        # Return VideoFromFile object
        return (VideoFromFile(video_io),)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/google/google-veo2-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-text-to-video)

[Kling Camera ControlsA node that provides camera control parameters for Kling video generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/video/google/google-veo2-video.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
    
    - [Kling Camera Controls](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)
    - [Kling Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)
    - [Kling Image to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)
    - [Kling Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)
    - [Kling Start-End Frame to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)
    - [Kling Text to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Kling Image to Video (Camera Control) - ComfyUI Built-in Node

# Kling Image to Video (Camera Control) - ComfyUI Built-in Node

Image to video conversion node with camera control features

The Kling Image to Video (Camera Control) node converts static images into videos with professional camera movements. It supports camera controls like zoom, rotation, pan, tilt and first-person view while maintaining focus on the original image content.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionstart\_frameImage-Input image to convert to videopromptString""Text prompt describing video action and contentnegative\_promptString""Elements to avoid in the videocfg\_scaleFloat7.0Controls how closely to follow the promptaspect\_ratioSelect16:9Output video aspect ratio

### [​](http://docs.comfy.org#camera-control-parameters) Camera Control Parameters

ParameterTypeDescriptioncamera\_controlCameraControlCamera control config from Kling Camera Controls node

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class KlingCameraControlI2VNode(KlingImage2VideoNode):
    """
    Kling Image to Video Camera Control Node. This node is a image to video node, but it supports controlling the camera.
    Duration, mode, and model_name request fields are hard-coded because camera control is only supported in pro mode with the kling-v1-5 model at 5s duration as of 2025-05-02.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "start_frame": model_field_to_node_input(
                    IO.IMAGE, KlingImage2VideoRequest, "image"
                ),
                "prompt": model_field_to_node_input(
                    IO.STRING, KlingImage2VideoRequest, "prompt", multiline=True
                ),
                "negative_prompt": model_field_to_node_input(
                    IO.STRING,
                    KlingImage2VideoRequest,
                    "negative_prompt",
                    multiline=True,
                ),
                "cfg_scale": model_field_to_node_input(
                    IO.FLOAT, KlingImage2VideoRequest, "cfg_scale"
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "aspect_ratio",
                    enum_type=AspectRatio,
                ),
                "camera_control": (
                    "CAMERA_CONTROL",
                    {
                        "tooltip": "Can be created using the Kling Camera Controls node. Controls the camera movement and motion during the video generation.",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    DESCRIPTION = "Transform still images into cinematic videos with professional camera movements that simulate real-world cinematography. Control virtual camera actions including zoom, rotation, pan, tilt, and first-person view, while maintaining focus on your original image."

    def api_call(
        self,
        start_frame: torch.Tensor,
        prompt: str,
        negative_prompt: str,
        cfg_scale: float,
        aspect_ratio: str,
        camera_control: CameraControl,
        auth_token: Optional[str] = None,
    ):
        return super().api_call(
            model_name="kling-v1-5",
            start_frame=start_frame,
            cfg_scale=cfg_scale,
            mode="pro",
            aspect_ratio=aspect_ratio,
            duration="5",
            prompt=prompt,
            negative_prompt=negative_prompt,
            camera_control=camera_control,
            auth_token=auth_token,
        )



```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)

[Kling Image to VideoA node that converts static images to dynamic videos using Kling's AI technology  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Camera Control Parameters](http://docs.comfy.org#camera-control-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
    
    - [Kling Camera Controls](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)
    - [Kling Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)
    - [Kling Image to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)
    - [Kling Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)
    - [Kling Start-End Frame to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)
    - [Kling Text to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Kling Text to Video (Camera Control) - ComfyUI Built-in Node

# Kling Text to Video (Camera Control) - ComfyUI Built-in Node

A text to video generation node with camera control features

The Kling Text to Video (Camera Control) node converts text into videos with professional camera movements. It extends the standard Kling Text to Video node by adding camera control capabilities.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptString""Text prompt describing video contentnegative\_promptString""Elements to avoid in the videocfg\_scaleFloat7.0Controls how closely to follow the promptaspect\_ratioSelect”16:9”Output video aspect ratiocamera\_controlCAMERA\_CONTROL-Camera settings from Kling Camera Controls node

### [​](http://docs.comfy.org#fixed-parameters) Fixed Parameters

Note: The following parameters are fixed and cannot be changed:

- Model: kling-v1-5
- Mode: pro
- Duration: 5 seconds

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class KlingCameraControlT2VNode(KlingTextToVideoNode):
    """
    Kling Text to Video Camera Control Node. This node is a text to video node, but it supports controlling the camera.
    Duration, mode, and model_name request fields are hard-coded because camera control is only supported in pro mode with the kling-v1-5 model at 5s duration as of 2025-05-02.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": model_field_to_node_input(
                    IO.STRING, KlingText2VideoRequest, "prompt", multiline=True
                ),
                "negative_prompt": model_field_to_node_input(
                    IO.STRING,
                    KlingText2VideoRequest,
                    "negative_prompt",
                    multiline=True,
                ),
                "cfg_scale": model_field_to_node_input(
                    IO.FLOAT, KlingText2VideoRequest, "cfg_scale"
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.COMBO,
                    KlingText2VideoRequest,
                    "aspect_ratio",
                    enum_type=AspectRatio,
                ),
                "camera_control": (
                    "CAMERA_CONTROL",
                    {
                        "tooltip": "Can be created using the Kling Camera Controls node. Controls the camera movement and motion during the video generation.",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    DESCRIPTION = "Transform text into cinematic videos with professional camera movements that simulate real-world cinematography. Control virtual camera actions including zoom, rotation, pan, tilt, and first-person view, while maintaining focus on your original text."

    def api_call(
        self,
        prompt: str,
        negative_prompt: str,
        cfg_scale: float,
        aspect_ratio: str,
        camera_control: Optional[CameraControl] = None,
        auth_token: Optional[str] = None,
    ):
        return super().api_call(
            model_name="kling-v1-5",
            cfg_scale=cfg_scale,
            mode="pro",
            aspect_ratio=aspect_ratio,
            duration="5",
            prompt=prompt,
            negative_prompt=negative_prompt,
            camera_control=camera_control,
            auth_token=auth_token,
        )



```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)

[Luma Text to VideoA node that converts text descriptions to videos using Luma AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Fixed Parameters](http://docs.comfy.org#fixed-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
    
    - [Kling Camera Controls](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)
    - [Kling Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)
    - [Kling Image to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)
    - [Kling Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)
    - [Kling Start-End Frame to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)
    - [Kling Text to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Kling Camera Controls - ComfyUI Built-in Node Documentation

# Kling Camera Controls - ComfyUI Built-in Node Documentation

A node that provides camera control parameters for Kling video generation

The Kling Camera Controls node defines virtual camera behavior parameters to control camera movement and view changes during Kling video generation.

## [​](http://docs.comfy.org#parameters) Parameters

ParameterTypeDefaultDescriptioncamera\_control\_typeSelect”simple”Preset camera motion types. simple: Custom camera movement; down\_back: Camera moves down and back; forward\_up: Camera moves forward and up; right\_turn\_forward: Rotate right and move forward; left\_turn\_forward: Rotate left and move forwardhorizontal\_movementFloat0Controls camera movement on horizontal axis (x-axis). Negative values move left, positive values move rightvertical\_movementFloat0Controls camera movement on vertical axis (y-axis). Negative values move down, positive values move uppanFloat0.5Controls camera rotation in vertical plane (x-axis). Negative values rotate down, positive values rotate uptiltFloat0Controls camera rotation in horizontal plane (y-axis). Negative values rotate left, positive values rotate rightrollFloat0Controls camera roll amount (z-axis). Negative values rotate counterclockwise, positive values rotate clockwisezoomFloat0Controls camera focal length. Negative values narrow field of view, positive values widen it

**Note**: At least one non-zero camera control parameter is required for the effect to work.

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptioncamera\_controlCAMERA\_CONTROLConfiguration object with camera settings

**Note**: Not all model and mode combinations support camera control. Please check the Kling API documentation for details.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class KlingCameraControls(KlingNodeBase):
    """Kling Camera Controls Node"""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "camera_control_type": (
                    IO.COMBO,
                    {
                        "options": [
                            camera_control_type.value
                            for camera_control_type in CameraType
                        ],
                        "default": "simple",
                        "tooltip": "Predefined camera movements type. simple: Customizable camera movement. down_back: Camera descends and moves backward. forward_up: Camera moves forward and tilts up. right_turn_forward: Rotate right and move forward. left_turn_forward: Rotate left and move forward.",
                    },
                ),
                "horizontal_movement": get_camera_control_input_config(
                    "Controls camera's movement along horizontal axis (x-axis). Negative indicates left, positive indicates right"
                ),
                "vertical_movement": get_camera_control_input_config(
                    "Controls camera's movement along vertical axis (y-axis). Negative indicates downward, positive indicates upward."
                ),
                "pan": get_camera_control_input_config(
                    "Controls camera's rotation in vertical plane (x-axis). Negative indicates downward rotation, positive indicates upward rotation.",
                    default=0.5,
                ),
                "tilt": get_camera_control_input_config(
                    "Controls camera's rotation in horizontal plane (y-axis). Negative indicates left rotation, positive indicates right rotation.",
                ),
                "roll": get_camera_control_input_config(
                    "Controls camera's rolling amount (z-axis). Negative indicates counterclockwise, positive indicates clockwise.",
                ),
                "zoom": get_camera_control_input_config(
                    "Controls change in camera's focal length. Negative indicates narrower field of view, positive indicates wider field of view.",
                ),
            }
        }

    DESCRIPTION = "Kling Camera Controls Node. Not all model and mode combinations support camera control. Please refer to the Kling API documentation for more information."
    RETURN_TYPES = ("CAMERA_CONTROL",)
    RETURN_NAMES = ("camera_control",)
    FUNCTION = "main"

    @classmethod
    def VALIDATE_INPUTS(
        cls,
        horizontal_movement: float,
        vertical_movement: float,
        pan: float,
        tilt: float,
        roll: float,
        zoom: float,
    ) -> bool | str:
        if not is_valid_camera_control_configs(
            [
                horizontal_movement,
                vertical_movement,
                pan,
                tilt,
                roll,
                zoom,
            ]
        ):
            return "Invalid camera control configs: at least one of the values must be non-zero"
        return True

    def main(
        self,
        camera_control_type: str,
        horizontal_movement: float,
        vertical_movement: float,
        pan: float,
        tilt: float,
        roll: float,
        zoom: float,
    ) -> tuple[CameraControl]:
        return (
            CameraControl(
                type=CameraType(camera_control_type),
                config=CameraConfig(
                    horizontal=horizontal_movement,
                    vertical=vertical_movement,
                    pan=pan,
                    roll=roll,
                    tilt=tilt,
                    zoom=zoom,
                ),
            ),
        )
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/google/google-veo2-video)

[Kling Text to VideoA node that converts text descriptions into videos using Kling's AI technology  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
    
    - [Kling Camera Controls](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)
    - [Kling Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)
    - [Kling Image to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)
    - [Kling Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)
    - [Kling Start-End Frame to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)
    - [Kling Text to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Kling Image to Video - ComfyUI Built-in Node

# Kling Image to Video - ComfyUI Built-in Node

A node that converts static images to dynamic videos using Kling’s AI technology

The Kling Image to Video node converts static images into dynamic video content using Kling’s image-to-video API.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

All parameters below are required:

ParameterTypeDefaultDescriptionstart\_frameImage-Input source imagepromptString""Text prompt describing video action and contentnegative\_promptString""Elements to avoid in the videocfg\_scaleFloat7.0Controls how closely to follow the promptmodel\_nameSelect”kling-v1-5”Model type to useaspect\_ratioSelect”16:9”Output video aspect ratiodurationSelect”5s”Generated video durationmodeSelect”pro”Video generation mode

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated videovideo\_idStringUnique video identifierdurationStringActual video duration

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class KlingImage2VideoNode(KlingNodeBase):
    """Kling Image to Video Node"""

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "start_frame": model_field_to_node_input(
                    IO.IMAGE, KlingImage2VideoRequest, "image"
                ),
                "prompt": model_field_to_node_input(
                    IO.STRING, KlingImage2VideoRequest, "prompt", multiline=True
                ),
                "negative_prompt": model_field_to_node_input(
                    IO.STRING,
                    KlingImage2VideoRequest,
                    "negative_prompt",
                    multiline=True,
                ),
                "model_name": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "model_name",
                    enum_type=KlingVideoGenModelName,
                ),
                "cfg_scale": model_field_to_node_input(
                    IO.FLOAT, KlingImage2VideoRequest, "cfg_scale"
                ),
                "mode": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "mode",
                    enum_type=KlingVideoGenMode,
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "aspect_ratio",
                    enum_type=KlingVideoGenAspectRatio,
                ),
                "duration": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "duration",
                    enum_type=KlingVideoGenDuration,
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = ("VIDEO", "STRING", "STRING")
    RETURN_NAMES = ("VIDEO", "video_id", "duration")
    DESCRIPTION = "Kling Image to Video Node"

    def get_response(self, task_id: str, auth_token: str) -> KlingImage2VideoResponse:
        return poll_until_finished(
            auth_token,
            ApiEndpoint(
                path=f"{PATH_IMAGE_TO_VIDEO}/{task_id}",
                method=HttpMethod.GET,
                request_model=KlingImage2VideoRequest,
                response_model=KlingImage2VideoResponse,
            ),
        )

    def api_call(
        self,
        start_frame: torch.Tensor,
        prompt: str,
        negative_prompt: str,
        model_name: str,
        cfg_scale: float,
        mode: str,
        aspect_ratio: str,
        duration: str,
        camera_control: Optional[KlingCameraControl] = None,
        end_frame: Optional[torch.Tensor] = None,
        auth_token: Optional[str] = None,
    ) -> tuple[VideoFromFile]:
        validate_prompts(prompt, negative_prompt, MAX_PROMPT_LENGTH_I2V)
        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=PATH_IMAGE_TO_VIDEO,
                method=HttpMethod.POST,
                request_model=KlingImage2VideoRequest,
                response_model=KlingImage2VideoResponse,
            ),
            request=KlingImage2VideoRequest(
                model_name=KlingVideoGenModelName(model_name),
                image=tensor_to_base64_string(start_frame),
                image_tail=(
                    tensor_to_base64_string(end_frame)
                    if end_frame is not None
                    else None
                ),
                prompt=prompt,
                negative_prompt=negative_prompt if negative_prompt else None,
                cfg_scale=cfg_scale,
                mode=KlingVideoGenMode(mode),
                aspect_ratio=KlingVideoGenAspectRatio(aspect_ratio),
                duration=KlingVideoGenDuration(duration),
                camera_control=camera_control,
            ),
            auth_token=auth_token,
        )

        task_creation_response = initial_operation.execute()
        validate_task_creation_response(task_creation_response)
        task_id = task_creation_response.data.task_id

        final_response = self.get_response(task_id, auth_token)
        validate_video_result_response(final_response)

        video = get_video_from_response(final_response)
        return video_result_to_node_output(video)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)

[Kling Start-End Frame to VideoA node that creates smooth video transitions between start and end frames using Kling's AI technology  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
    
    - [Kling Camera Controls](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)
    - [Kling Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)
    - [Kling Image to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)
    - [Kling Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)
    - [Kling Start-End Frame to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)
    - [Kling Text to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Kling Start-End Frame to Video - ComfyUI Built-in Node

# Kling Start-End Frame to Video - ComfyUI Built-in Node

A node that creates smooth video transitions between start and end frames using Kling’s AI technology

The Kling Start-End Frame to Video node lets you create smooth video transitions between two images. It automatically generates all the intermediate frames to produce a fluid transformation.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDescriptionstart\_frameImageStarting image for the videoend\_frameImageEnding image for the videopromptStringText describing video content and transitionnegative\_promptStringElements to avoid in the videocfg\_scaleFloatControls how closely to follow the promptaspect\_ratioSelectOutput video aspect ratiomodeSelectVideo generation settings (mode/duration/model)

### [​](http://docs.comfy.org#mode-options) Mode Options

Available mode combinations:

- standard mode / 5s duration / kling-v1
- standard mode / 5s duration / kling-v1-5
- pro mode / 5s duration / kling-v1
- pro mode / 5s duration / kling-v1-5
- pro mode / 5s duration / kling-v1-6
- pro mode / 10s duration / kling-v1-5
- pro mode / 10s duration / kling-v1-6

Default: “pro mode / 5s duration / kling-v1”

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#how-it-works) How It Works

The node analyzes the start and end frames to create a smooth transition sequence between them. It sends the images and parameters to Kling’s API server, which generates all necessary intermediate frames for a fluid transformation.

The transition style and content can be guided using prompts, while negative prompts help avoid unwanted elements.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python


class KlingStartEndFrameNode(KlingImage2VideoNode):
    """
    Kling First Last Frame Node. This node allows creation of a video from a first and last frame. It calls the normal image to video endpoint, but only allows the subset of input options that support the `image_tail` request field.
    """

    @staticmethod
    def get_mode_string_mapping() -> dict[str, tuple[str, str, str]]:
        """
        Returns a mapping of mode strings to their corresponding (mode, duration, model_name) tuples.
        Only includes config combos that support the `image_tail` request field.
        """
        return {
            "standard mode / 5s duration / kling-v1": ("std", "5", "kling-v1"),
            "standard mode / 5s duration / kling-v1-5": ("std", "5", "kling-v1-5"),
            "pro mode / 5s duration / kling-v1": ("pro", "5", "kling-v1"),
            "pro mode / 5s duration / kling-v1-5": ("pro", "5", "kling-v1-5"),
            "pro mode / 5s duration / kling-v1-6": ("pro", "5", "kling-v1-6"),
            "pro mode / 10s duration / kling-v1-5": ("pro", "10", "kling-v1-5"),
            "pro mode / 10s duration / kling-v1-6": ("pro", "10", "kling-v1-6"),
        }

    @classmethod
    def INPUT_TYPES(s):
        modes = list(KlingStartEndFrameNode.get_mode_string_mapping().keys())
        return {
            "required": {
                "start_frame": model_field_to_node_input(
                    IO.IMAGE, KlingImage2VideoRequest, "image"
                ),
                "end_frame": model_field_to_node_input(
                    IO.IMAGE, KlingImage2VideoRequest, "image_tail"
                ),
                "prompt": model_field_to_node_input(
                    IO.STRING, KlingImage2VideoRequest, "prompt", multiline=True
                ),
                "negative_prompt": model_field_to_node_input(
                    IO.STRING,
                    KlingImage2VideoRequest,
                    "negative_prompt",
                    multiline=True,
                ),
                "cfg_scale": model_field_to_node_input(
                    IO.FLOAT, KlingImage2VideoRequest, "cfg_scale"
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.COMBO,
                    KlingImage2VideoRequest,
                    "aspect_ratio",
                    enum_type=AspectRatio,
                ),
                "mode": (
                    modes,
                    {
                        "default": modes[2],
                        "tooltip": "The configuration to use for the video generation following the format: mode / duration / model_name.",
                    },
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    DESCRIPTION = "Generate a video sequence that transitions between your provided start and end images. The node creates all frames in between, producing a smooth transformation from the first frame to the last."

    def parse_inputs_from_mode(self, mode: str) -> tuple[str, str, str]:
        """Parses the mode input into a tuple of (model_name, duration, mode)."""
        return KlingStartEndFrameNode.get_mode_string_mapping()[mode]

    def api_call(
        self,
        start_frame: torch.Tensor,
        end_frame: torch.Tensor,
        prompt: str,
        negative_prompt: str,
        cfg_scale: float,
        aspect_ratio: str,
        mode: str,
        auth_token: Optional[str] = None,
    ):
        mode, duration, model_name = self.parse_inputs_from_mode(mode)
        return super().api_call(
            prompt=prompt,
            negative_prompt=negative_prompt,
            model_name=model_name,
            start_frame=start_frame,
            cfg_scale=cfg_scale,
            mode=mode,
            aspect_ratio=aspect_ratio,
            duration=duration,
            end_frame=end_frame,
            auth_token=auth_token,
        )


```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)

[Kling Text to Video (Camera Control)A text to video generation node with camera control features  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Mode Options](http://docs.comfy.org#mode-options)
- [Output](http://docs.comfy.org#output)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
    
    - [Kling Camera Controls](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)
    - [Kling Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video)
    - [Kling Image to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)
    - [Kling Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-image-to-video)
    - [Kling Start-End Frame to Video](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-start-end-frame-to-video)
    - [Kling Text to Video (Camera Control)](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Kling Text to Video - ComfyUI Built-in Node

# Kling Text to Video - ComfyUI Built-in Node

A node that converts text descriptions into videos using Kling’s AI technology

The Kling Text to Video node connects to Kling’s API service to generate videos from text descriptions. Users simply provide descriptive text to create corresponding video content.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionpromptString""Text prompt describing desired video contentnegative\_promptString""Elements to avoid in the videocfg\_scaleFloat7.0Controls how closely to follow the promptmodel\_nameSelect”kling-v2-master”Video generation model to useaspect\_ratioSelectAspectRatio enumOutput video aspect ratiodurationSelectDuration enumLength of generated videomodeSelectMode enumVideo generation mode

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated videoKling IDStringTask identifierDuration (sec)StringVideo length in seconds

## [​](http://docs.comfy.org#how-it-works) How It Works

The node sends text prompts to Kling’s API server, which processes and returns the generated video. The process includes initial request and status polling. When complete, the node downloads and outputs the video.

Users can control the generation by adjusting parameters like negative prompts, configuration scale, and video properties. The system validates prompt length to ensure API compliance.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class KlingTextToVideoNode(KlingNodeBase):
    """Kling Text to Video Node"""

    @staticmethod
    def poll_for_task_status(task_id: str, auth_token: str) -> KlingText2VideoResponse:
        """Polls the Kling API endpoint until the task reaches a terminal state."""
        polling_operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"{PATH_TEXT_TO_VIDEO}/{task_id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=KlingText2VideoResponse,
            ),
            completed_statuses=[
                TaskStatus.succeed.value,
            ],
            failed_statuses=[TaskStatus.failed.value],
            status_extractor=lambda response: (
                response.data.task_status.value
                if response.data and response.data.task_status
                else None
            ),
            auth_token=auth_token,
        )
        return polling_operation.execute()

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": model_field_to_node_input(
                    IO.STRING, KlingText2VideoRequest, "prompt", multiline=True
                ),
                "negative_prompt": model_field_to_node_input(
                    IO.STRING, KlingText2VideoRequest, "negative_prompt", multiline=True
                ),
                "model_name": model_field_to_node_input(
                    IO.COMBO,
                    KlingText2VideoRequest,
                    "model_name",
                    enum_type=ModelName,
                    default="kling-v2-master",
                ),
                "cfg_scale": model_field_to_node_input(
                    IO.FLOAT, KlingText2VideoRequest, "cfg_scale"
                ),
                "mode": model_field_to_node_input(
                    IO.COMBO, KlingText2VideoRequest, "mode", enum_type=Mode
                ),
                "duration": model_field_to_node_input(
                    IO.COMBO, KlingText2VideoRequest, "duration", enum_type=Duration
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.COMBO,
                    KlingText2VideoRequest,
                    "aspect_ratio",
                    enum_type=AspectRatio,
                ),
            },
            "hidden": {"auth_token": "AUTH_TOKEN_COMFY_ORG"},
        }

    RETURN_TYPES = ("VIDEO", "STRING", "STRING")
    RETURN_NAMES = ("VIDEO", "Kling ID", "Duration (sec)")
    DESCRIPTION = "Kling Text to Video Node"

    def api_call(
        self,
        prompt: str,
        negative_prompt: str,
        model_name: str,
        cfg_scale: float,
        mode: str,
        duration: int,
        aspect_ratio: str,
        camera_control: Optional[CameraControl] = None,
        auth_token: Optional[str] = None,
    ) -> tuple[VideoFromFile, str, str]:
        validate_prompts(prompt, negative_prompt, MAX_PROMPT_LENGTH_T2V)
        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=PATH_TEXT_TO_VIDEO,
                method=HttpMethod.POST,
                request_model=KlingText2VideoRequest,
                response_model=KlingText2VideoResponse,
            ),
            request=KlingText2VideoRequest(
                prompt=prompt if prompt else None,
                negative_prompt=negative_prompt if negative_prompt else None,
                duration=Duration(duration),
                mode=Mode(mode),
                model_name=ModelName(model_name),
                cfg_scale=cfg_scale,
                aspect_ratio=AspectRatio(aspect_ratio),
                camera_control=camera_control,
            ),
            auth_token=auth_token,
        )

        initial_response = initial_operation.execute()
        if not is_valid_initial_response(initial_response):
            error_msg = f"Kling initial request failed. Code: {initial_response.code}, Message: {initial_response.message}, Data: {initial_response.data}"
            logging.error(error_msg)
            raise KlingApiError(error_msg)

        task_id = initial_response.data.task_id
        final_response = self.poll_for_task_status(task_id, auth_token)
        if not is_valid_video_response(final_response):
            error_msg = (
                f"Kling task {task_id} succeeded but no video data found in response."
            )
            logging.error(error_msg)
            raise KlingApiError(error_msg)

        video = final_response.data.task_result.videos[0]
        logging.debug("Kling task %s succeeded. Video URL: %s", task_id, video.url)
        return (
            download_url_to_video_output(video.url),
            str(video.id),
            str(video.duration),
        )

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-controls)

[Kling Image to Video (Camera Control)Image to video conversion node with camera control features  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-i2v)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Output](http://docs.comfy.org#output)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/video/kwai_vgi/kling-text-to-video.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/video/luma/luma-concepts.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
    
    - [Luma Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video)
    - [Luma Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video)
    - [Luma Concepts](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Concepts - ComfyUI Native Node Documentation

# Luma Concepts - ComfyUI Native Node Documentation

A helper node that provides concept guidance for Luma image generation

The Luma Concepts node allows you to apply predefined camera concepts to the Luma generation process, providing precise control over camera angles and perspectives without complex prompt descriptions.

## [​](http://docs.comfy.org#node-function) Node Function

This node serves as a helper tool for Luma generation nodes, enabling users to select and apply predefined camera concepts. These concepts include different shooting angles (like overhead or low angle), camera distances (like close-up or long shot), and movement styles (like push-in or follow). It simplifies the creative workflow by providing an intuitive way to control camera effects in the generated output.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDescriptionconcept1selectFirst camera concept choice, includes various presets and “none”concept2selectSecond camera concept choice, includes various presets and “none”concept3selectThird camera concept choice, includes various presets and “none”concept4selectFourth camera concept choice, includes various presets and “none”

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionluma\_conceptsLUMA\_CONCEPTSOptional Camera Concepts to merge with selected concepts

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionluma\_conceptsLUMA\_CONCEPTCombined object containing all selected concepts

## [​](http://docs.comfy.org#usage-examples) Usage Examples

[**Luma Text to Video Workflow Example**  
\
Luma Text to Video Workflow Example](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video)

[**Luma Image to Video Workflow Example**  
\
Luma Image to Video Workflow Example](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video)

## [​](http://docs.comfy.org#how-it-works) How It Works

The Luma Concepts node offers a variety of predefined camera concepts including:

- Camera distances (close-up, medium shot, long shot)
- View angles (eye level, overhead, low angle)
- Movement types (push-in, follow, orbit)
- Special effects (handheld, stabilized, floating)

Users can select up to 4 concepts to use together. The node creates an object containing the selected camera concepts, which is then passed to Luma generation nodes. During generation, Luma AI uses these camera concepts to influence the viewpoint and composition of the output, ensuring the results reflect the chosen photographic effects.

By combining multiple camera concepts, users can create complex camera guidance without writing detailed prompt descriptions. This is particularly useful when specific camera angles or compositions are needed.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class LumaConceptsNode(ComfyNodeABC):
    """
    Holds one or more Camera Concepts for use with Luma Text to Video and Luma Image to Video nodes.
    """

    RETURN_TYPES = (LumaIO.LUMA_CONCEPTS,)
    RETURN_NAMES = ("luma_concepts",)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "create_concepts"
    CATEGORY = "api node/image/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "concept1": (get_luma_concepts(include_none=True),),
                "concept2": (get_luma_concepts(include_none=True),),
                "concept3": (get_luma_concepts(include_none=True),),
                "concept4": (get_luma_concepts(include_none=True),),
            },
            "optional": {
                "luma_concepts": (
                    LumaIO.LUMA_CONCEPTS,
                    {
                        "tooltip": "Optional Camera Concepts to add to the ones chosen here."
                    },
                ),
            },
        }

    def create_concepts(
        self,
        concept1: str,
        concept2: str,
        concept3: str,
        concept4: str,
        luma_concepts: LumaConceptChain = None,
    ):
        chain = LumaConceptChain(str_list=[concept1, concept2, concept3, concept4])
        if luma_concepts is not None:
            chain = luma_concepts.clone_and_merge(chain)
        return (chain,)


```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/luma/luma-concepts.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video)

[Pika 2.2 Text to VideoA node that converts text descriptions into videos using Pika AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-text-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Examples](http://docs.comfy.org#usage-examples)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/video/luma/luma-concepts.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/video/luma/luma-image-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
    
    - [Luma Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video)
    - [Luma Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video)
    - [Luma Concepts](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Image to Video - ComfyUI Native API Node Documentation

# Luma Image to Video - ComfyUI Native API Node Documentation

A node that converts static images to dynamic videos using Luma AI

The Luma Image to Video node uses Luma AI’s technology to transform static images into smooth, dynamic videos, bringing your images to life.

## [​](http://docs.comfy.org#node-function) Node Function

This node connects to Luma AI’s image-to-video API, allowing users to create dynamic videos from input images. It understands the image content and generates natural, coherent motion while maintaining the original visual style. Combined with text prompts, users can precisely control the video’s dynamic effects.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing video motion and contentmodelselect-Video generation model to useresolutionselect”540p”Output video resolutiondurationselect-Video length optionsloopbooleanFalseWhether to loop the videoseedinteger0Seed value for node rerun, results are nondeterministic

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionfirst\_imageimageFirst frame of video (required if no last\_image)last\_imageimageLast frame of video (required if no first\_image)luma\_conceptsLUMA\_CONCEPTSConcepts for controlling camera motion and shot style

### [​](http://docs.comfy.org#requirements) Requirements

- Either **first\_image** or **last\_image** must be provided
- Each image input (first\_image and last\_image) accepts only 1 image

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOvideoGenerated video

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Luma Image to Video Workflow Example**  
\
Luma Image to Video Workflow Tutorial](http://docs.comfy.org/tutorials/api-nodes/luma/luma-image-to-video)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class LumaImageToVideoGenerationNode(ComfyNodeABC):
    """
    Generates videos synchronously based on prompt, input images, and output_size.
    """

    RETURN_TYPES = (IO.VIDEO,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/video/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the video generation",
                    },
                ),
                "model": ([model.value for model in LumaVideoModel],),
                # "aspect_ratio": ([ratio.value for ratio in LumaAspectRatio], {
                #     "default": LumaAspectRatio.ratio_16_9,
                # }),
                "resolution": (
                    [resolution.value for resolution in LumaVideoOutputResolution],
                    {
                        "default": LumaVideoOutputResolution.res_540p,
                    },
                ),
                "duration": ([dur.value for dur in LumaVideoModelOutputDuration],),
                "loop": (
                    IO.BOOLEAN,
                    {
                        "default": False,
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "first_image": (
                    IO.IMAGE,
                    {"tooltip": "First frame of generated video."},
                ),
                "last_image": (IO.IMAGE, {"tooltip": "Last frame of generated video."}),
                "luma_concepts": (
                    LumaIO.LUMA_CONCEPTS,
                    {
                        "tooltip": "Optional Camera Concepts to dictate camera motion via the Luma Concepts node."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        model: str,
        resolution: str,
        duration: str,
        loop: bool,
        seed,
        first_image: torch.Tensor = None,
        last_image: torch.Tensor = None,
        luma_concepts: LumaConceptChain = None,
        auth_token=None,
        **kwargs,
    ):
        if first_image is None and last_image is None:
            raise Exception(
                "At least one of first_image and last_image requires an input."
            )
        keyframes = self._convert_to_keyframes(first_image, last_image, auth_token)
        duration = duration if model != LumaVideoModel.ray_1_6 else None
        resolution = resolution if model != LumaVideoModel.ray_1_6 else None

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/luma/generations",
                method=HttpMethod.POST,
                request_model=LumaGenerationRequest,
                response_model=LumaGeneration,
            ),
            request=LumaGenerationRequest(
                prompt=prompt,
                model=model,
                aspect_ratio=LumaAspectRatio.ratio_16_9,  # ignored, but still needed by the API for some reason
                resolution=resolution,
                duration=duration,
                loop=loop,
                keyframes=keyframes,
                concepts=luma_concepts.create_api_model() if luma_concepts else None,
            ),
            auth_token=auth_token,
        )
        response_api: LumaGeneration = operation.execute()

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/luma/generations/{response_api.id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=LumaGeneration,
            ),
            completed_statuses=[LumaState.completed],
            failed_statuses=[LumaState.failed],
            status_extractor=lambda x: x.state,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        vid_response = requests.get(response_poll.assets.video)
        return (VideoFromFile(BytesIO(vid_response.content)),)

    def _convert_to_keyframes(
        self,
        first_image: torch.Tensor = None,
        last_image: torch.Tensor = None,
        auth_token=None,
    ):
        if first_image is None and last_image is None:
            return None
        frame0 = None
        frame1 = None
        if first_image is not None:
            download_urls = upload_images_to_comfyapi(
                first_image, max_images=1, auth_token=auth_token
            )
            frame0 = LumaImageReference(type="image", url=download_urls[0])
        if last_image is not None:
            download_urls = upload_images_to_comfyapi(
                last_image, max_images=1, auth_token=auth_token
            )
            frame1 = LumaImageReference(type="image", url=download_urls[0])
        return LumaKeyframes(frame0=frame0, frame1=frame1)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/luma/luma-image-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video)

[Luma ConceptsA helper node that provides concept guidance for Luma image generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Requirements](http://docs.comfy.org#requirements)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/video/luma/luma-image-to-video.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/video/luma/luma-text-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
    
    - [Luma Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-text-to-video)
    - [Luma Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video)
    - [Luma Concepts](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Luma Text to Video - ComfyUI Native Node Documentation

# Luma Text to Video - ComfyUI Native Node Documentation

A node that converts text descriptions to videos using Luma AI

The Luma Text to Video node lets you create high-quality, smooth videos from text descriptions using Luma AI’s video generation technology.

## [​](http://docs.comfy.org#node-function) Node Function

This node connects to Luma AI’s text-to-video API, allowing users to generate dynamic video content from detailed text prompts.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#basic-parameters) Basic Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing the video content to generatemodelselect-Video generation model to useaspect\_ratioselect”ratio\_16\_9”Video aspect ratioresolutionselect”res\_540p”Video resolutiondurationselect-Video length optionsloopbooleanFalseWhether to loop the videoseedinteger0Seed value for node rerun, results are nondeterministic

When using Ray 1.6 model, duration and resolution parameters will not take effect.

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionluma\_conceptsLUMA\_CONCEPTSCamera concepts to control motion via Luma Concepts node

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOvideoGenerated video

## [​](http://docs.comfy.org#usage-example) Usage Example

[**Luma Text to Video Workflow Example**  
\
Luma Text to Video Workflow Example](http://docs.comfy.org/tutorials/api-nodes/luma/luma-text-to-video)

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class LumaTextToVideoGenerationNode(ComfyNodeABC):
    """
    Generates videos synchronously based on prompt and output_size.
    """

    RETURN_TYPES = (IO.VIDEO,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/video/Luma"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the video generation",
                    },
                ),
                "model": ([model.value for model in LumaVideoModel],),
                "aspect_ratio": (
                    [ratio.value for ratio in LumaAspectRatio],
                    {
                        "default": LumaAspectRatio.ratio_16_9,
                    },
                ),
                "resolution": (
                    [resolution.value for resolution in LumaVideoOutputResolution],
                    {
                        "default": LumaVideoOutputResolution.res_540p,
                    },
                ),
                "duration": ([dur.value for dur in LumaVideoModelOutputDuration],),
                "loop": (
                    IO.BOOLEAN,
                    {
                        "default": False,
                    },
                ),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "Seed to determine if node should re-run; actual results are nondeterministic regardless of seed.",
                    },
                ),
            },
            "optional": {
                "luma_concepts": (
                    LumaIO.LUMA_CONCEPTS,
                    {
                        "tooltip": "Optional Camera Concepts to dictate camera motion via the Luma Concepts node."
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        model: str,
        aspect_ratio: str,
        resolution: str,
        duration: str,
        loop: bool,
        seed,
        luma_concepts: LumaConceptChain = None,
        auth_token=None,
        **kwargs,
    ):
        duration = duration if model != LumaVideoModel.ray_1_6 else None
        resolution = resolution if model != LumaVideoModel.ray_1_6 else None

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/luma/generations",
                method=HttpMethod.POST,
                request_model=LumaGenerationRequest,
                response_model=LumaGeneration,
            ),
            request=LumaGenerationRequest(
                prompt=prompt,
                model=model,
                resolution=resolution,
                aspect_ratio=aspect_ratio,
                duration=duration,
                loop=loop,
                concepts=luma_concepts.create_api_model() if luma_concepts else None,
            ),
            auth_token=auth_token,
        )
        response_api: LumaGeneration = operation.execute()

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/luma/generations/{response_api.id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=LumaGeneration,
            ),
            completed_statuses=[LumaState.completed],
            failed_statuses=[LumaState.failed],
            status_extractor=lambda x: x.state,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        vid_response = requests.get(response_poll.assets.video)
        return (VideoFromFile(BytesIO(vid_response.content)),)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/luma/luma-text-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/kwai_vgi/kling-camera-control-t2v)

[Luma Image to VideoA node that converts static images to dynamic videos using Luma AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-image-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Function](http://docs.comfy.org#node-function)
- [Parameters](http://docs.comfy.org#parameters)
- [Basic Parameters](http://docs.comfy.org#basic-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Usage Example](http://docs.comfy.org#usage-example)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/video/luma/luma-text-to-video.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/video/minimax/minimax-image-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
    
    - [MiniMax Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-image-to-video)
    - [MiniMax Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-text-to-video)
  - Google
  - Kling
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

MiniMax Image to Video - ComfyUI Native Node Documentation

# MiniMax Image to Video - ComfyUI Native Node Documentation

A node that converts static images to dynamic videos using MiniMax AI

The MiniMax Image to Video node uses MiniMax’s API to generate videos from input images and text prompts.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionimageimage-Input image used as the first frame of videoprompt\_textstring""Text prompt to guide video generationmodelselect”I2V-01”Available models: “I2V-01-Director”, “I2V-01”, “I2V-01-live”

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionseedintegerRandom seed for noise generation

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOvideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node source code (Updated on 2025-05-03)]

```python

class MinimaxImageToVideoNode(MinimaxTextToVideoNode):
    """
    Generates videos synchronously based on an image and prompt, and optional parameters using Minimax's API.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": (
                    IO.IMAGE,
                    {
                        "tooltip": "Image to use as first frame of video generation"
                    },
                ),
                "prompt_text": (
                    "STRING",
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Text prompt to guide the video generation",
                    },
                ),
                "model": (
                    [
                        "I2V-01-Director",
                        "I2V-01",
                        "I2V-01-live",
                    ],
                    {
                        "default": "I2V-01",
                        "tooltip": "Model to use for video generation",
                    },
                ),
            },
            "optional": {
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "The random seed used for creating the noise.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    RETURN_TYPES = ("VIDEO",)
    DESCRIPTION = "Generates videos from an image and prompts using Minimax's API"
    FUNCTION = "generate_video"
    CATEGORY = "api node/video/Minimax"
    API_NODE = True
    OUTPUT_NODE = True
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/minimax/minimax-image-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/image/openai/openai-dalle3)

[MiniMax Text to VideoA node that converts text descriptions into videos using MiniMax AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-text-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/video/minimax/minimax-image-to-video.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/video/minimax/minimax-text-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
    
    - [MiniMax Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-image-to-video)
    - [MiniMax Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-text-to-video)
  - Google
  - Kling
  - Luma
  - Pika
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

MiniMax Text to Video - ComfyUI Native Node Documentation

# MiniMax Text to Video - ComfyUI Native Node Documentation

A node that converts text descriptions into videos using MiniMax AI

The MiniMax Text to Video node connects to MiniMax’s API to generate high-quality, smooth videos from text prompts. It supports different video generation models to create short video clips in various styles.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionprompt\_textString""Text prompt that guides the video generationmodelSelect”T2V-01”Video model to use, options are “T2V-01” and “T2V-01-Director”seedInteger0Random seed for noise generation, defaults to 0

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-03)]

```python

class MinimaxTextToVideoNode:
    """
    Generates videos synchronously based on a prompt, and optional parameters using Minimax's API.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt_text": (
                    "STRING",
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Text prompt to guide the video generation",
                    },
                ),
                "model": (
                    [
                        "T2V-01",
                        "T2V-01-Director",
                    ],
                    {
                        "default": "T2V-01",
                        "tooltip": "Model to use for video generation",
                    },
                ),
            },
            "optional": {
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 0xFFFFFFFFFFFFFFFF,
                        "control_after_generate": True,
                        "tooltip": "The random seed used for creating the noise.",
                    },
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    RETURN_TYPES = ("VIDEO",)
    DESCRIPTION = "Generates videos from prompts using Minimax's API"
    FUNCTION = "generate_video"
    CATEGORY = "api node/video/Minimax"
    API_NODE = True
    OUTPUT_NODE = True

    def generate_video(
        self,
        prompt_text,
        seed=0,
        model="T2V-01",
        image: torch.Tensor=None, # used for ImageToVideo
        subject: torch.Tensor=None, # used for SubjectToVideo
        auth_token=None,
    ):
        '''
        Function used between Minimax nodes - supports T2V, I2V, and S2V, based on provided arguments.
        '''
        # upload image, if passed in
        image_url = None
        if image is not None:
            image_url = upload_images_to_comfyapi(image, max_images=1, auth_token=auth_token)[0]

        # TODO: figure out how to deal with subject properly, API returns invalid params when using S2V-01 model
        subject_reference = None
        if subject is not None:
            subject_url = upload_images_to_comfyapi(subject, max_images=1, auth_token=auth_token)[0]
            subject_reference = [SubjectReferenceItem(image=subject_url)]


        video_generate_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/minimax/video_generation",
                method=HttpMethod.POST,
                request_model=MinimaxVideoGenerationRequest,
                response_model=MinimaxVideoGenerationResponse,
            ),
            request=MinimaxVideoGenerationRequest(
                model=Model(model),
                prompt=prompt_text,
                callback_url=None,
                first_frame_image=image_url,
                subject_reference=subject_reference,
                prompt_optimizer=None,
            ),
            auth_token=auth_token,
        )
        response = video_generate_operation.execute()

        task_id = response.task_id
        if not task_id:
            raise Exception(f"Minimax generation failed: {response.base_resp}")

        video_generate_operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path="/proxy/minimax/query/video_generation",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=MinimaxTaskResultResponse,
                query_params={"task_id": task_id},
            ),
            completed_statuses=["Success"],
            failed_statuses=["Fail"],
            status_extractor=lambda x: x.status.value,
            auth_token=auth_token,
        )
        task_result = video_generate_operation.execute()

        file_id = task_result.file_id
        if file_id is None:
            raise Exception("Request was not successful. Missing file ID.")
        file_retrieve_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/minimax/files/retrieve",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=MinimaxFileRetrieveResponse,
                query_params={"file_id": int(file_id)},
            ),
            request=EmptyRequest(),
            auth_token=auth_token,
        )
        file_result = file_retrieve_operation.execute()

        file_url = file_result.file.download_url
        if file_url is None:
            raise Exception(
                f"No video was found in the response. Full response: {file_result.model_dump()}"
            )
        logging.info(f"Generated video URL: {file_url}")

        video_io = download_url_to_bytesio(file_url)
        if video_io is None:
            error_msg = f"Failed to download video from {file_url}"
            logging.error(error_msg)
            raise Exception(error_msg)
        return (VideoFromFile(video_io),)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/minimax/minimax-text-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/minimax/minimax-image-to-video)

[Google Veo2 VideoA node that generates videos from text descriptions using Google's Veo2 technology  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/google/google-veo2-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/video/minimax/minimax-text-to-video.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/video/pika/pika-image-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
  - Pika
    
    - [Pika 2.2 Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-text-to-video)
    - [Pika 2.2 Scenes](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-scenes)
    - [Pika 2.2 Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-image-to-video)
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Pika 2.2 Image to Video - ComfyUI Native Node Documentation

# Pika 2.2 Image to Video - ComfyUI Native Node Documentation

A node that converts static images to dynamic videos using Pika AI

The Pika 2.2 Image to Video node connects to Pika’s latest 2.2 API to transform static images into dynamic videos. It preserves the visual features of the original image while adding natural motion based on text prompts.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionimageImage-Input image to convert to videoprompt\_textString""Text prompt describing video motion and contentnegative\_promptString""Elements to avoid in the videoseedInteger0Random seed for generationresolutionSelect”1080p”Output video resolutiondurationSelect”5s”Length of generated video

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#how-it-works) How It Works

The node sends the input image and parameters (prompts, resolution, duration, etc.) to Pika’s API server as multipart form data. The API processes this and returns the generated video. Users can control the output by adjusting the prompts, negative prompts, random seed and other parameters.

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-05)]

```python

class PikaImageToVideoV2_2(PikaNodeBase):
    """Pika 2.2 Image to Video Node."""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image": (
                    IO.IMAGE,
                    {"tooltip": "The image to convert to video"},
                ),
                **cls.get_base_inputs_types(PikaBodyGenerate22I2vGenerate22I2vPost),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    DESCRIPTION = "Sends an image and prompt to the Pika API v2.2 to generate a video."
    RETURN_TYPES = ("VIDEO",)

    def api_call(
        self,
        image: torch.Tensor,
        prompt_text: str,
        negative_prompt: str,
        seed: int,
        resolution: str,
        duration: int,
        auth_token: Optional[str] = None,
    ) -> tuple[VideoFromFile]:
        """API call for Pika 2.2 Image to Video."""
        # Convert image to BytesIO
        image_bytes_io = tensor_to_bytesio(image)
        image_bytes_io.seek(0)  # Reset stream position

        # Prepare file data for multipart upload
        pika_files = {"image": ("image.png", image_bytes_io, "image/png")}

        # Prepare non-file data using the Pydantic model
        pika_request_data = PikaBodyGenerate22I2vGenerate22I2vPost(
            promptText=prompt_text,
            negativePrompt=negative_prompt,
            seed=seed,
            resolution=resolution,
            duration=duration,
        )

        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=PATH_IMAGE_TO_VIDEO,
                method=HttpMethod.POST,
                request_model=PikaBodyGenerate22I2vGenerate22I2vPost,
                response_model=PikaGenerateResponse,
            ),
            request=pika_request_data,
            files=pika_files,
            content_type="multipart/form-data",
            auth_token=auth_token,
        )

        return self.execute_task(initial_operation, auth_token)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/pika/pika-image-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-scenes)

[PixVerse TemplateA helper node that provides preset templates for PixVerse video generation  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-template)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Output](http://docs.comfy.org#output)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/video/pika/pika-image-to-video.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/video/pika/pika-scenes.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
  - Pika
    
    - [Pika 2.2 Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-text-to-video)
    - [Pika 2.2 Scenes](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-scenes)
    - [Pika 2.2 Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-image-to-video)
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Pika 2.2 Scenes - ComfyUI Built-in Node Documentation

# Pika 2.2 Scenes - ComfyUI Built-in Node Documentation

A node that creates coherent scene videos from multiple images using Pika AI

The Pika 2.2 Scenes node allows you to upload multiple images and generate a high-quality video incorporating these elements. It uses Pika’s 2.2 API to create smooth scene transitions between the images.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionprompt\_textstring""Text prompt describing video content and scenesnegative\_promptstring""Elements to exclude from the videoseedinteger0Random seed for generationingredients\_modeselect”creative”Image combination moderesolutionselectAPI defaultOutput video resolutiondurationselectAPI defaultOutput video lengthaspect\_ratiofloat1.7777777777777777 (16:9)Video aspect ratio, range 0.4-2.5

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDescriptionimage\_ingredient\_1imageFirst scene imageimage\_ingredient\_2imageSecond scene imageimage\_ingredient\_3imageThird scene imageimage\_ingredient\_4imageFourth scene imageimage\_ingredient\_5imageFifth scene image

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOvideoGenerated video

## [​](http://docs.comfy.org#how-it-works) How It Works

The Pika 2.2 Scenes node analyzes all input images and creates a video containing these image elements. The node sends the images and parameters to Pika’s API server, which processes them and returns the generated video.

Users can guide the video style and content through prompts, and exclude unwanted elements using negative prompts. The node supports up to 5 input images as ingredients and generates the final video based on the specified combination mode, resolution, duration, and aspect ratio.

## [​](http://docs.comfy.org#source-code) Source Code

```python

class PikaScenesV2_2(PikaNodeBase):
    """Pika 2.2 Scenes Node."""

    @classmethod
    def INPUT_TYPES(cls):
        image_ingredient_input = (
            IO.IMAGE,
            {"tooltip": "Image that will be used as ingredient to create a video."},
        )
        return {
            "required": {
                **cls.get_base_inputs_types(
                    PikaBodyGenerate22C2vGenerate22PikascenesPost,
                ),
                "ingredients_mode": model_field_to_node_input(
                    IO.COMBO,
                    PikaBodyGenerate22C2vGenerate22PikascenesPost,
                    "ingredientsMode",
                    enum_type=IngredientsMode,
                    default="creative",
                ),
                "aspect_ratio": model_field_to_node_input(
                    IO.FLOAT,
                    PikaBodyGenerate22C2vGenerate22PikascenesPost,
                    "aspectRatio",
                    step=0.001,
                    min=0.4,
                    max=2.5,
                    default=1.7777777777777777,
                ),
            },
            "optional": {
                "image_ingredient_1": image_ingredient_input,
                "image_ingredient_2": image_ingredient_input,
                "image_ingredient_3": image_ingredient_input,
                "image_ingredient_4": image_ingredient_input,
                "image_ingredient_5": image_ingredient_input,
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    DESCRIPTION = "Combine your images to create a video with the objects in them. Upload multiple images as ingredients and generate a high-quality video that incorporates all of them."
    RETURN_TYPES = ("VIDEO",)

    def api_call(
        self,
        prompt_text: str,
        negative_prompt: str,
        seed: int,
        resolution: str,
        duration: int,
        ingredients_mode: str,
        aspect_ratio: float,
        image_ingredient_1: Optional[torch.Tensor] = None,
        image_ingredient_2: Optional[torch.Tensor] = None,
        image_ingredient_3: Optional[torch.Tensor] = None,
        image_ingredient_4: Optional[torch.Tensor] = None,
        image_ingredient_5: Optional[torch.Tensor] = None,
        auth_token: Optional[str] = None,
    ) -> tuple[VideoFromFile]:
        """API call for Pika Scenes 2.2."""
        all_image_bytes_io = []
        for image in [
            image_ingredient_1,
            image_ingredient_2,
            image_ingredient_3,
            image_ingredient_4,
            image_ingredient_5,
        ]:
            if image is not None:
                image_bytes_io = tensor_to_bytesio(image)
                image_bytes_io.seek(0)
                all_image_bytes_io.append(image_bytes_io)

        # Prepare files data for multipart upload
        pika_files = [
            ("images", (f"image_{i}.png", image_bytes_io, "image/png"))
            for i, image_bytes_io in enumerate(all_image_bytes_io)
        ]

        # Prepare non-file data using the Pydantic model
        pika_request_data = PikaBodyGenerate22C2vGenerate22PikascenesPost(
            ingredientsMode=ingredients_mode,
            promptText=prompt_text,
            negativePrompt=negative_prompt,
            seed=seed,
            resolution=resolution,
            duration=duration,
            aspectRatio=aspect_ratio,
        )

        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=PATH_PIKASCENES,
                method=HttpMethod.POST,
                request_model=PikaBodyGenerate22C2vGenerate22PikascenesPost,
                response_model=PikaGenerateResponse,
            ),
            request=pika_request_data,
            files=pika_files,
            content_type="multipart/form-data",
            auth_token=auth_token,
        )

        return self.execute_task(initial_operation, auth_token)


```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/pika/pika-scenes.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-text-to-video)

[Pika 2.2 Image to VideoA node that converts static images to dynamic videos using Pika AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-image-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [How It Works](http://docs.comfy.org#how-it-works)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/video/pika/pika-scenes.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/video/pika/pika-text-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
  - Pika
    
    - [Pika 2.2 Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-text-to-video)
    - [Pika 2.2 Scenes](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-scenes)
    - [Pika 2.2 Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-image-to-video)
  - PixVerse

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Pika 2.2 Text to Video - ComfyUI Native Node Documentation

# Pika 2.2 Text to Video - ComfyUI Native Node Documentation

A node that converts text descriptions into videos using Pika AI

The Pika 2.2 Text to Video node uses Pika’s 2.2 API to create videos from text descriptions. It connects to Pika’s text-to-video API, allowing users to generate videos using text prompts with various control parameters.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionprompt\_textString""Text prompt describing the video contentnegative\_promptString""Elements to exclude from the videoseedInteger0Random seed for generationresolutionSelect”1080p”Output video resolutiondurationSelect”5s”Length of generated videoaspect\_ratioFloat1.7777777777777777Video aspect ratio, range 0.4-2.5, step 0.001

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-05)]

```python

class PikaTextToVideoNodeV2_2(PikaNodeBase):
    """Pika 2.2 Text to Video Node."""

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                **cls.get_base_inputs_types(PikaBodyGenerate22T2vGenerate22T2vPost),
                "aspect_ratio": model_field_to_node_input(
                    IO.FLOAT,
                    PikaBodyGenerate22T2vGenerate22T2vPost,
                    "aspectRatio",
                    step=0.001,
                    min=0.4,
                    max=2.5,
                    default=1.7777777777777777,
                ),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    RETURN_TYPES = ("VIDEO",)
    DESCRIPTION = "Sends a text prompt to the Pika API v2.2 to generate a video."

    def api_call(
        self,
        prompt_text: str,
        negative_prompt: str,
        seed: int,
        resolution: str,
        duration: int,
        aspect_ratio: float,
        auth_token: Optional[str] = None,
    ) -> tuple[VideoFromFile]:
        """API call for Pika 2.2 Text to Video."""
        initial_operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path=PATH_TEXT_TO_VIDEO,
                method=HttpMethod.POST,
                request_model=PikaBodyGenerate22T2vGenerate22T2vPost,
                response_model=PikaGenerateResponse,
            ),
            request=PikaBodyGenerate22T2vGenerate22T2vPost(
                promptText=prompt_text,
                negativePrompt=negative_prompt,
                seed=seed,
                resolution=resolution,
                duration=duration,
                aspectRatio=aspect_ratio,
            ),
            auth_token=auth_token,
            content_type="application/x-www-form-urlencoded",
        )

        return self.execute_task(initial_operation, auth_token)


```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/pika/pika-text-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/luma/luma-concepts)

[Pika 2.2 ScenesA node that creates coherent scene videos from multiple images using Pika AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-scenes)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/video/pika/pika-text-to-video.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
  - Pika
  - PixVerse
    
    - [PixVerse Template](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-template)
    - [PixVerse Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video)
    - [PixVerse Transition Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-transition-video)
    - [PixVerse Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

PixVerse Image to Video - ComfyUI Native Node Documentation

# PixVerse Image to Video - ComfyUI Native Node Documentation

A node that converts static images to dynamic videos using PixVerse AI

The PixVerse Image to Video node uses PixVerse’s API to transform static images into dynamic videos. It preserves the visual features of the original image while adding natural motion based on text prompts.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionimageImage-Input image to convert to videopromptString""Text prompt describing video motion/contentnegative\_promptString""Elements to avoid in the videoseedInteger-1Random seed (-1 for random)qualitySelect”high”Output video quality levelaspect\_ratioSelect”r16\_9”Output video aspect ratiodurationSelect”seconds\_4”Length of generated videomotion\_modeSelect”standard”Video motion style

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDefaultDescriptionpixverse\_templatePIXVERSE\_TEMPLATENoneOptional PixVerse template

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-05)]

```python
class PixverseImageToVideoNode(ComfyNodeABC):
    """
    Pixverse Image to Video

    Generates videos from an image and prompts.
    """

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "image": ("IMAGE",),
                "prompt": ("STRING", {"multiline": True, "default": ""}),
                "negative_prompt": ("STRING", {"multiline": True, "default": ""}),
                "seed": ("INT", {"default": -1, "min": -1, "max": 0xffffffffffffffff}),
                "quality": (list(PixverseQuality.__members__.keys()), {"default": "high"}),
                "aspect_ratio": (list(PixverseAspectRatio.__members__.keys()), {"default": "r16_9"}),
                "duration": (list(PixverseDuration.__members__.keys()), {"default": "seconds_4"}),
                "motion_mode": (list(PixverseMotionMode.__members__.keys()), {"default": "standard"}),
            },
            "optional": {
                "pixverse_template": ("PIXVERSE_TEMPLATE",),
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    RETURN_TYPES = ("VIDEO",)
    DESCRIPTION = "Generates videos from an image and prompts using Pixverse's API"
    FUNCTION = "generate_video"
    CATEGORY = "api node/video/Pixverse"
    API_NODE = True
    OUTPUT_NODE = True
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video.mdx)

[Previous  
\
PixVerse Transition VideoCreate smooth transition videos between start and end frames using PixVerse AI](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-transition-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/video/pixverse/pixverse-template.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
  - Pika
  - PixVerse
    
    - [PixVerse Template](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-template)
    - [PixVerse Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video)
    - [PixVerse Transition Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-transition-video)
    - [PixVerse Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

PixVerse Template - ComfyUI Native Node Documentation

# PixVerse Template - ComfyUI Native Node Documentation

A helper node that provides preset templates for PixVerse video generation

The PixVerse Template node lets you choose from predefined video generation templates to control the style and effects of PixVerse video generation nodes. This helper node connects to PixVerse video generation nodes, allowing users to quickly apply preset video styles without manually adjusting complex parameter combinations.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDescriptiontemplateSelectChoose a template from available video presets

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionpixverse\_templatePixverseIO.TEMPLATEConfiguration object containing the selected template ID

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-05)]

```python

class PixverseTemplateNode:
    """
    Select template for Pixverse Video generation.
    """

    RETURN_TYPES = (PixverseIO.TEMPLATE,)
    RETURN_NAMES = ("pixverse_template",)
    FUNCTION = "create_template"
    CATEGORY = "api node/video/Pixverse"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "template": (list(pixverse_templates.keys()), ),
            }
        }

    def create_template(self, template: str):
        template_id = pixverse_templates.get(template, None)
        if template_id is None:
            raise Exception(f"Template '{template}' is not recognized.")
        # just return the integer
        return (template_id,)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/pixverse/pixverse-template.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/pika/pika-image-to-video)

[PixVerse Text to VideoA node that converts text descriptions into videos using PixVerse AI technology  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/video/pixverse/pixverse-template.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
  - Pika
  - PixVerse
    
    - [PixVerse Template](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-template)
    - [PixVerse Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video)
    - [PixVerse Transition Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-transition-video)
    - [PixVerse Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

PixVerse Text to Video - ComfyUI Built-in Node Documentation

# PixVerse Text to Video - ComfyUI Built-in Node Documentation

A node that converts text descriptions into videos using PixVerse AI technology

The PixVerse Text to Video node connects to PixVerse’s text-to-video API, allowing users to generate high-quality videos from text descriptions. Users can customize their creations by adjusting various parameters like video quality, duration, and motion mode.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionpromptstring""Text prompt describing the video contentaspect\_ratioselect-Output video aspect ratioqualityselectPixverseQuality.res\_540pVideo quality levelduration\_secondsselect-Video durationmotion\_modeselect-Video motion modeseedinteger0Random seed for consistent generation results

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDefaultDescriptionnegative\_promptstring""Elements to exclude from the videopixverse\_templatePIXVERSE\_TEMPLATENoneOptional template for style settings

### [​](http://docs.comfy.org#limitations) Limitations

- 1080p quality only supports normal motion mode with 5-second duration
- Non 5-second durations only support normal motion mode

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOvideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

\[Node Source Code (Updated 2025-05-05)]

```python

class PixverseTextToVideoNode(ComfyNodeABC):
    """
    Generates videos synchronously based on prompt and output_size.
    """

    RETURN_TYPES = (IO.VIDEO,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/video/Pixverse"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the video generation",
                    },
                ),
                "aspect_ratio": (
                    [ratio.value for ratio in PixverseAspectRatio],
                ),
                "quality": (
                    [resolution.value for resolution in PixverseQuality],
                    {
                        "default": PixverseQuality.res_540p,
                    },
                ),
                "duration_seconds": ([dur.value for dur in PixverseDuration],),
                "motion_mode": ([mode.value for mode in PixverseMotionMode],),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2147483647,
                        "control_after_generate": True,
                        "tooltip": "Seed for video generation.",
                    },
                ),
            },
            "optional": {
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
                "pixverse_template": (
                    PixverseIO.TEMPLATE,
                    {
                        "tooltip": "An optional template to influence style of generation, created by the Pixverse Template node."
                    }
                )
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        prompt: str,
        aspect_ratio: str,
        quality: str,
        duration_seconds: int,
        motion_mode: str,
        seed,
        negative_prompt: str=None,
        pixverse_template: int=None,
        auth_token=None,
        **kwargs,
    ):
        # 1080p is limited to 5 seconds duration
        # only normal motion_mode supported for 1080p or for non-5 second duration
        if quality == PixverseQuality.res_1080p:
            motion_mode = PixverseMotionMode.normal
            duration_seconds = PixverseDuration.dur_5
        elif duration_seconds != PixverseDuration.dur_5:
            motion_mode = PixverseMotionMode.normal

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/pixverse/video/text/generate",
                method=HttpMethod.POST,
                request_model=PixverseTextVideoRequest,
                response_model=PixverseVideoResponse,
            ),
            request=PixverseTextVideoRequest(
                prompt=prompt,
                aspect_ratio=aspect_ratio,
                quality=quality,
                duration=duration_seconds,
                motion_mode=motion_mode,
                negative_prompt=negative_prompt if negative_prompt else None,
                template_id=pixverse_template,
                seed=seed,
            ),
            auth_token=auth_token,
        )
        response_api = operation.execute()

        if response_api.Resp is None:
            raise Exception(f"Pixverse request failed: '{response_api.ErrMsg}'")

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/pixverse/video/result/{response_api.Resp.video_id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=PixverseGenerationStatusResponse,
            ),
            completed_statuses=[PixverseStatus.successful],
            failed_statuses=[PixverseStatus.contents_moderation, PixverseStatus.failed, PixverseStatus.deleted],
            status_extractor=lambda x: x.Resp.status,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        vid_response = requests.get(response_poll.Resp.url)
        return (VideoFromFile(BytesIO(vid_response.content)),)

```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-template)

[PixVerse Transition VideoCreate smooth transition videos between start and end frames using PixVerse AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-transition-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Limitations](http://docs.comfy.org#limitations)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video.md -->


<!-- BEGIN Get_Started/built-in-nodes/api-node/video/pixverse/pixverse-transition-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video
  
  - MiniMax
  - Google
  - Kling
  - Luma
  - Pika
  - PixVerse
    
    - [PixVerse Template](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-template)
    - [PixVerse Text to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video)
    - [PixVerse Transition Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-transition-video)
    - [PixVerse Image to Video](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

PixVerse Transition Video - ComfyUI Native Node Documentation

# PixVerse Transition Video - ComfyUI Native Node Documentation

Create smooth transition videos between start and end frames using PixVerse AI

The Pixverse Transition Video node connects to PixVerse’s API to generate smooth video transitions between two images. It automatically creates all intermediate frames to produce fluid transformations, perfect for morphing effects, scene transitions, and object evolution.

## [​](http://docs.comfy.org#parameters) Parameters

### [​](http://docs.comfy.org#required-parameters) Required Parameters

ParameterTypeDefaultDescriptionfirst\_frameImage-Starting frame imagelast\_frameImage-Ending frame imagepromptString""Text prompt describing video and transitionqualitySelect”PixverseQuality.res\_540p”Output video qualityduration\_secondsSelect-Length of generated videomotion\_modeSelect-Video motion styleseedInteger0Random seed (range: 0-2147483647)

### [​](http://docs.comfy.org#optional-parameters) Optional Parameters

ParameterTypeDefaultDescriptionnegative\_promptString""Elements to avoid in videopixverse\_templatePIXVERSE\_TEMPLATENoneOptional style preset

### [​](http://docs.comfy.org#parameter-constraints) Parameter Constraints

- When quality is set to 1080p, motion\_mode is forced to normal and duration\_seconds to 5 seconds
- When duration\_seconds is not 5 seconds, motion\_mode is forced to normal

### [​](http://docs.comfy.org#output) Output

OutputTypeDescriptionVIDEOVideoGenerated video

## [​](http://docs.comfy.org#source-code) Source Code

```python

class PixverseTransitionVideoNode(ComfyNodeABC):
    """
    Generates videos synchronously based on prompt and output_size.
    """

    RETURN_TYPES = (IO.VIDEO,)
    DESCRIPTION = cleandoc(__doc__ or "")  # Handle potential None value
    FUNCTION = "api_call"
    API_NODE = True
    CATEGORY = "api node/video/Pixverse"

    @classmethod
    def INPUT_TYPES(s):
        return {
            "required": {
                "first_frame": (
                    IO.IMAGE,
                ),
                "last_frame": (
                    IO.IMAGE,
                ),
                "prompt": (
                    IO.STRING,
                    {
                        "multiline": True,
                        "default": "",
                        "tooltip": "Prompt for the video generation",
                    },
                ),
                "quality": (
                    [resolution.value for resolution in PixverseQuality],
                    {
                        "default": PixverseQuality.res_540p,
                    },
                ),
                "duration_seconds": ([dur.value for dur in PixverseDuration],),
                "motion_mode": ([mode.value for mode in PixverseMotionMode],),
                "seed": (
                    IO.INT,
                    {
                        "default": 0,
                        "min": 0,
                        "max": 2147483647,
                        "control_after_generate": True,
                        "tooltip": "Seed for video generation.",
                    },
                ),
            },
            "optional": {
                "negative_prompt": (
                    IO.STRING,
                    {
                        "default": "",
                        "forceInput": True,
                        "tooltip": "An optional text description of undesired elements on an image.",
                    },
                ),
                "pixverse_template": (
                    PixverseIO.TEMPLATE,
                    {
                        "tooltip": "An optional template to influence style of generation, created by the Pixverse Template node."
                    }
                )
            },
            "hidden": {
                "auth_token": "AUTH_TOKEN_COMFY_ORG",
            },
        }

    def api_call(
        self,
        first_frame: torch.Tensor,
        last_frame: torch.Tensor,
        prompt: str,
        quality: str,
        duration_seconds: int,
        motion_mode: str,
        seed,
        negative_prompt: str=None,
        pixverse_template: int=None,
        auth_token=None,
        **kwargs,
    ):
        first_frame_id = upload_image_to_pixverse(first_frame, auth_token=auth_token)
        last_frame_id = upload_image_to_pixverse(last_frame, auth_token=auth_token)

        # 1080p is limited to 5 seconds duration
        # only normal motion_mode supported for 1080p or for non-5 second duration
        if quality == PixverseQuality.res_1080p:
            motion_mode = PixverseMotionMode.normal
            duration_seconds = PixverseDuration.dur_5
        elif duration_seconds != PixverseDuration.dur_5:
            motion_mode = PixverseMotionMode.normal

        operation = SynchronousOperation(
            endpoint=ApiEndpoint(
                path="/proxy/pixverse/video/transition/generate",
                method=HttpMethod.POST,
                request_model=PixverseTransitionVideoRequest,
                response_model=PixverseVideoResponse,
            ),
            request=PixverseTransitionVideoRequest(
                first_frame_img=first_frame_id,
                last_frame_img=last_frame_id,
                prompt=prompt,
                quality=quality,
                duration=duration_seconds,
                motion_mode=motion_mode,
                negative_prompt=negative_prompt if negative_prompt else None,
                template_id=pixverse_template,
                seed=seed,
            ),
            auth_token=auth_token,
        )
        response_api = operation.execute()

        if response_api.Resp is None:
            raise Exception(f"Pixverse request failed: '{response_api.ErrMsg}'")

        operation = PollingOperation(
            poll_endpoint=ApiEndpoint(
                path=f"/proxy/pixverse/video/result/{response_api.Resp.video_id}",
                method=HttpMethod.GET,
                request_model=EmptyRequest,
                response_model=PixverseGenerationStatusResponse,
            ),
            completed_statuses=[PixverseStatus.successful],
            failed_statuses=[PixverseStatus.contents_moderation, PixverseStatus.failed, PixverseStatus.deleted],
            status_extractor=lambda x: x.Resp.status,
            auth_token=auth_token,
        )
        response_poll = operation.execute()

        vid_response = requests.get(response_poll.Resp.url)
        return (VideoFromFile(BytesIO(vid_response.content)),)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/api-node/video/pixverse/pixverse-transition-video.mdx)

[Previous](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-text-to-video)

[PixVerse Image to VideoA node that converts static images to dynamic videos using PixVerse AI  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/video/pixverse/pixverse-image-to-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Parameters](http://docs.comfy.org#parameters)
- [Required Parameters](http://docs.comfy.org#required-parameters)
- [Optional Parameters](http://docs.comfy.org#optional-parameters)
- [Parameter Constraints](http://docs.comfy.org#parameter-constraints)
- [Output](http://docs.comfy.org#output)
- [Source Code](http://docs.comfy.org#source-code)

<!-- END Get_Started/built-in-nodes/api-node/video/pixverse/pixverse-transition-video.md -->


<!-- BEGIN Get_Started/built-in-nodes/overview.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Built-in Nodes

- [Overview](http://docs.comfy.org/built-in-nodes/overview)

##### API Node

- Image
- Video

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Built-in Nodes

# ComfyUI Built-in Nodes

Introduction to ComfyUI Built-in Nodes

Built-in nodes are ComfyUI’s default nodes. They are core functionalities of ComfyUI that you can use without installing any third-party custom node packages.

As we have just started updating this section, the content is not yet complete. We will gradually add more content in the future.

If you find any errors in the content, you can submit an issue or PR in this [repo](https://github.com/Comfy-Org/docs) to help us improve.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/built-in-nodes/overview.mdx)

[Flux pro ultra image  
\
Next](http://docs.comfy.org/built-in-nodes/api-node/image/bfl/flux-pro-ultra-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

<!-- END Get_Started/built-in-nodes/overview.md -->


<!-- BEGIN Get_Started/comfy-cli/getting-started.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Getting Started

# Getting Started

### [​](http://docs.comfy.org#overview) Overview

`comfy-cli` is a [command line tool](https://github.com/Comfy-Org/comfy-cli) that makes it easier to install and manage Comfy.

### [​](http://docs.comfy.org#install-cli) Install CLI

pip

homebrew

```bash
pip install comfy-cli
```

To get shell completion hints:

```bash
comfy --install-completion
```

### [​](http://docs.comfy.org#install-comfyui) Install ComfyUI

Create a virtual environment with any Python version greater than 3.9.

conda

venv

```bash
conda create -n comfy-env python=3.11
conda activate comfy-env
```

Install ComfyUI

```bash
comfy install
```

You still need to install CUDA, or ROCm depending on your GPU.

### [​](http://docs.comfy.org#run-comfyui) Run ComfyUI

```bash
comfy launch
```

### [​](http://docs.comfy.org#manage-custom-nodes) Manage Custom Nodes

```bash
comfy node install <NODE_NAME>
```

We use `cm-cli` for installing custom nodes. See the [docs](https://github.com/ltdrdata/ComfyUI-Manager/blob/main/docs/en/cm-cli.md) for more information.

### [​](http://docs.comfy.org#manage-models) Manage Models

Downloading models with `comfy-cli` is easy. Just run:

```bash
comfy model download <url> models/checkpoints
```

### [​](http://docs.comfy.org#contributing) Contributing

We encourage contributions to comfy-cli! If you have suggestions, ideas, or bug reports, please open an issue on our [GitHub repository](https://github.com/Comfy-Org/comfy-cli/issues). If you want to contribute code, fork the repository and submit a pull request.

Refer to the [Dev Guide](https://github.com/Comfy-Org/comfy-cli/blob/main/DEV_README.md) for further details.

### [​](http://docs.comfy.org#analytics) Analytics

We track usage of the CLI to improve the user experience. You can disable this by running:

```bash
comfy tracking disable
```

To re-enable tracking, run:

```bash
comfy tracking enable
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/comfy-cli/getting-started.mdx)

[Previous](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

[Reference  
\
Next](http://docs.comfy.org/comfy-cli/reference)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Overview](http://docs.comfy.org#overview)
- [Install CLI](http://docs.comfy.org#install-cli)
- [Install ComfyUI](http://docs.comfy.org#install-comfyui)
- [Run ComfyUI](http://docs.comfy.org#run-comfyui)
- [Manage Custom Nodes](http://docs.comfy.org#manage-custom-nodes)
- [Manage Models](http://docs.comfy.org#manage-models)
- [Contributing](http://docs.comfy.org#contributing)
- [Analytics](http://docs.comfy.org#analytics)

<!-- END Get_Started/comfy-cli/getting-started.md -->


<!-- BEGIN Get_Started/comfy-cli/reference.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Reference

# Reference

# [​](http://docs.comfy.org#cli) CLI

## [​](http://docs.comfy.org#nodes) Nodes

**Usage**:

```plaintext
$ comfy node [OPTIONS] COMMAND [ARGS]...
```

**Options**:

- `--install-completion`: Install completion for the current shell.
- `--show-completion`: Show completion for the current shell, to copy it or customize the installation.
- `--help`: Show this message and exit.

**Commands**:

- `deps-in-workflow`
- `disable`
- `enable`
- `fix`
- `install`
- `install-deps`
- `reinstall`
- `restore-dependencies`
- `restore-snapshot`
- `save-snapshot`: Save a snapshot of the current ComfyUI…
- `show`
- `simple-show`
- `uninstall`
- `update`

### [​](http://docs.comfy.org#deps-in-workflow) `deps-in-workflow`

**Usage**:

```plaintext
$ deps-in-workflow [OPTIONS]
```

**Options**:

- `--workflow TEXT`: Workflow file (.json/.png) \[required]
- `--output TEXT`: Workflow file (.json/.png) \[required]
- `--channel TEXT`: Specify the operation mode
- `--mode TEXT`: \[remote|local|cache]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#disable) `disable`

**Usage**:

```plaintext
$ disable [OPTIONS] ARGS...
```

**Arguments**:

- `ARGS...`: disable custom nodes \[required]

**Options**:

- `--channel TEXT`: Specify the operation mode
- `--mode TEXT`: \[remote|local|cache]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#enable) `enable`

**Usage**:

```plaintext
$ enable [OPTIONS] ARGS...
```

**Arguments**:

- `ARGS...`: enable custom nodes \[required]

**Options**:

- `--channel TEXT`: Specify the operation mode
- `--mode TEXT`: \[remote|local|cache]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#fix) `fix`

**Usage**:

```plaintext
$ fix [OPTIONS] ARGS...
```

**Arguments**:

- `ARGS...`: fix dependencies for specified custom nodes \[required]

**Options**:

- `--channel TEXT`: Specify the operation mode
- `--mode TEXT`: \[remote|local|cache]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#install) `install`

**Usage**:

```plaintext
$ install [OPTIONS] ARGS...
```

**Arguments**:

- `ARGS...`: install custom nodes \[required]

**Options**:

- `--channel TEXT`: Specify the operation mode
- `--mode TEXT`: \[remote|local|cache]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#install-deps) `install-deps`

**Usage**:

```plaintext
$ install-deps [OPTIONS]
```

**Options**:

- `--deps TEXT`: Dependency spec file (.json)
- `--workflow TEXT`: Workflow file (.json/.png)
- `--channel TEXT`: Specify the operation mode
- `--mode TEXT`: \[remote|local|cache]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#reinstall) `reinstall`

**Usage**:

```plaintext
$ reinstall [OPTIONS] ARGS...
```

**Arguments**:

- `ARGS...`: reinstall custom nodes \[required]

**Options**:

- `--channel TEXT`: Specify the operation mode
- `--mode TEXT`: \[remote|local|cache]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#restore-dependencies) `restore-dependencies`

**Usage**:

```plaintext
$ restore-dependencies [OPTIONS]
```

**Options**:

- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#restore-snapshot) `restore-snapshot`

**Usage**:

```plaintext
$ restore-snapshot [OPTIONS] PATH
```

**Arguments**:

- `PATH`: \[required]

**Options**:

- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#save-snapshot) `save-snapshot`

Save a snapshot of the current ComfyUI environment

**Usage**:

```plaintext
$ save-snapshot [OPTIONS]
```

**Options**:

- `--output TEXT`: Specify the output file path. (.json/.yaml)
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#show) `show`

**Usage**:

```plaintext
$ show [OPTIONS] ARGS...
```

**Arguments**:

- `ARGS...`: \[installed|enabled|not-installed|disabled|all|snapshot|snapshot-list] \[required]

**Options**:

- `--channel TEXT`: Specify the operation mode
- `--mode TEXT`: \[remote|local|cache]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#simple-show) `simple-show`

**Usage**:

```plaintext
$ simple-show [OPTIONS] ARGS...
```

**Arguments**:

- `ARGS...`: \[installed|enabled|not-installed|disabled|all|snapshot|snapshot-list] \[required]

**Options**:

- `--channel TEXT`: Specify the operation mode
- `--mode TEXT`: \[remote|local|cache]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#uninstall) `uninstall`

**Usage**:

```plaintext
$ uninstall [OPTIONS] ARGS...
```

**Arguments**:

- `ARGS...`: uninstall custom nodes \[required]

**Options**:

- `--channel TEXT`: Specify the operation mode
- `--mode TEXT`: \[remote|local|cache]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#update) `update`

**Usage**:

```plaintext
$ update [OPTIONS] ARGS...
```

**Arguments**:

- `ARGS...`: update custom nodes \[required]

**Options**:

- `--channel TEXT`: Specify the operation mode
- `--mode TEXT`: \[remote|local|cache]
- `--help`: Show this message and exit.

## [​](http://docs.comfy.org#models) Models

**Usage**:

```plaintext
$ comfy model [OPTIONS] COMMAND [ARGS]...
```

**Options**:

- `--install-completion`: Install completion for the current shell.
- `--show-completion`: Show completion for the current shell, to copy it or customize the installation.
- `--help`: Show this message and exit.

**Commands**:

- `download`: Download a model to a specified relative…
- `list`: Display a list of all models currently…
- `remove`: Remove one or more downloaded models,…

### [​](http://docs.comfy.org#download) `download`

Download a model to a specified relative path if it is not already downloaded.

**Usage**:

```plaintext
$ download [OPTIONS]
```

**Options**:

- `--url TEXT`: The URL from which to download the model \[required]
- `--relative-path TEXT`: The relative path from the current workspace to install the model. \[default: models/checkpoints]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#list) `list`

Display a list of all models currently downloaded in a table format.

**Usage**:

```plaintext
$ list [OPTIONS]
```

**Options**:

- `--relative-path TEXT`: The relative path from the current workspace where the models are stored. \[default: models/checkpoints]
- `--help`: Show this message and exit.

### [​](http://docs.comfy.org#remove) `remove`

Remove one or more downloaded models, either by specifying them directly or through an interactive selection.

**Usage**:

```plaintext
$ remove [OPTIONS]
```

**Options**:

- `--relative-path TEXT`: The relative path from the current workspace where the models are stored. \[default: models/checkpoints]
- `--model-names TEXT`: List of model filenames to delete, separated by spaces
- `--help`: Show this message and exit.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/comfy-cli/reference.mdx)

[Previous](http://docs.comfy.org/comfy-cli/getting-started)

[Overview  
\
Next](http://docs.comfy.org/custom-nodes/overview)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [CLI](http://docs.comfy.org#cli)
- [Nodes](http://docs.comfy.org#nodes)
- [deps-in-workflow](http://docs.comfy.org#deps-in-workflow)
- [disable](http://docs.comfy.org#disable)
- [enable](http://docs.comfy.org#enable)
- [fix](http://docs.comfy.org#fix)
- [install](http://docs.comfy.org#install)
- [install-deps](http://docs.comfy.org#install-deps)
- [reinstall](http://docs.comfy.org#reinstall)
- [restore-dependencies](http://docs.comfy.org#restore-dependencies)
- [restore-snapshot](http://docs.comfy.org#restore-snapshot)
- [save-snapshot](http://docs.comfy.org#save-snapshot)
- [show](http://docs.comfy.org#show)
- [simple-show](http://docs.comfy.org#simple-show)
- [uninstall](http://docs.comfy.org#uninstall)
- [update](http://docs.comfy.org#update)
- [Models](http://docs.comfy.org#models)
- [download](http://docs.comfy.org#download)
- [list](http://docs.comfy.org#list)
- [remove](http://docs.comfy.org#remove)

<!-- END Get_Started/comfy-cli/reference.md -->


<!-- BEGIN Get_Started/community/contributing.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Contributing

# Contributing

### [​](http://docs.comfy.org#create-a-pr) Create a PR

Fork the [repo](https://github.com/comfyanonymous/ComfyUI), and create a PR.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/community/contributing.mdx)

[Previous  
\
Recraft Text to ImageLearn how to use the Recraft Text to Image API node in ComfyUI](http://docs.comfy.org/tutorials/api-nodes/recraft/recraft-text-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Create a PR](http://docs.comfy.org#create-a-pr)

<!-- END Get_Started/community/contributing.md -->


<!-- BEGIN Get_Started/custom-nodes/backend/datatypes.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
  
  - [Properties](http://docs.comfy.org/custom-nodes/backend/server_overview)
  - [Lifecycle](http://docs.comfy.org/custom-nodes/backend/lifecycle)
  - [Publishing to the Manager](http://docs.comfy.org/custom-nodes/backend/manager)
  - [Datatypes](http://docs.comfy.org/custom-nodes/backend/datatypes)
  - [Images, Latents, and Masks](http://docs.comfy.org/custom-nodes/backend/images_and_masks)
  - [Hidden and Flexible inputs](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)
  - [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)
  - [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion)
  - [Data lists](http://docs.comfy.org/custom-nodes/backend/lists)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/backend/snippets)
  - [Working with torch.Tensor](http://docs.comfy.org/custom-nodes/backend/tensors)
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Datatypes

# Datatypes

These are the most important built in datatypes. You can also [define your own](http://docs.comfy.org/more_on_inputs#custom-datatypes).

Datatypes are used on the client side to prevent a workflow from passing the wrong form of data into a node - a bit like strong typing. The JavaScript client side code will generally not allow a node output to be connected to an input of a different datatype, although a few exceptions are noted below.

## [​](http://docs.comfy.org#comfy-datatypes) Comfy datatypes

### [​](http://docs.comfy.org#combo) COMBO

- No additional parameters in `INPUT_TYPES`
- Python datatype: defined as `list[str]`, output value is `str`

Represents a dropdown menu widget. Unlike other datatypes, `COMBO` it is not specified in `INPUT_TYPES` by a `str`, but by a `list[str]` corresponding to the options in the dropdown list, with the first option selected by default.

`COMBO` inputs are often dynamically generated at run time. For instance, in the built-in `CheckpointLoaderSimple` node, you find

```plaintext
"ckpt_name": (folder_paths.get_filename_list("checkpoints"), )
```

or they might just be a fixed list of options,

```plaintext
"play_sound": (["no","yes"], {}),
```

### [​](http://docs.comfy.org#primitive-and-reroute) Primitive and reroute

Primitive and reroute nodes only exist on the client side. They do not have an intrinsic datatype, but when connected they take on the datatype of the input or output to which they have been connected (which is why they can’t connect to a `*` input…)

## [​](http://docs.comfy.org#python-datatypes) Python datatypes

### [​](http://docs.comfy.org#int) INT

- Additional parameters in `INPUT_TYPES`:
  
  - `default` is required
  - `min` and `max` are optional
- Python datatype `int`

### [​](http://docs.comfy.org#float) FLOAT

- Additional parameters in `INPUT_TYPES`:
  
  - `default` is required
  - `min`, `max`, `step` are optional
- Python datatype `float`

### [​](http://docs.comfy.org#string) STRING

- Additional parameters in `INPUT_TYPES`:
  
  - `default` is required
- Python datatype `str`

### [​](http://docs.comfy.org#boolean) BOOLEAN

- Additional parameters in `INPUT_TYPES`:
  
  - `default` is required
- Python datatype `bool`

## [​](http://docs.comfy.org#tensor-datatypes) Tensor datatypes

### [​](http://docs.comfy.org#image) IMAGE

- No additional parameters in `INPUT_TYPES`
- Python datatype `torch.Tensor` with *shape* \[B,H,W,C]

A batch of `B` images, height `H`, width `W`, with `C` channels (generally `C=3` for `RGB`).

### [​](http://docs.comfy.org#latent) LATENT

- No additional parameters in `INPUT_TYPES`
- Python datatype `dict`, containing a `torch.Tensor` with *shape* \[B,C,H,W]

The `dict` passed contains the key `samples`, which is a `torch.Tensor` with *shape* \[B,C,H,W] representing a batch of `B` latents, with `C` channels (generally `C=4` for existing stable diffusion models), height `H`, width `W`.

The height and width are 1/8 of the corresponding image size (which is the value you set in the Empty Latent Image node).

Other entries in the dictionary contain things like latent masks.

### [​](http://docs.comfy.org#mask) MASK

- No additional parameters in `INPUT_TYPES`
- Python datatype `torch.Tensor` with *shape* \[H,W] or \[B,C,H,W]

### [​](http://docs.comfy.org#audio) AUDIO

- No additional parameters in `INPUT_TYPES`
- Python datatype `dict`, containing a `torch.Tensor` with *shape* \[B, C, T] and a sample rate.

The `dict` passed contains the key `waveform`, which is a `torch.Tensor` with *shape* \[B, C, T] representing a batch of `B` audio samples, with `C` channels (`C=2` for stereo and `C=1` for mono), and `T` time steps (i.e., the number of audio samples).

The `dict` contains another key `sample_rate`, which indicates the sampling rate of the audio.

## [​](http://docs.comfy.org#custom-sampling-datatypes) Custom Sampling datatypes

### [​](http://docs.comfy.org#noise) Noise

The `NOISE` datatype represents a *source* of noise (not the actual noise itself). It can be represented by any Python object that provides a method to generate noise, with the signature `generate_noise(self, input_latent:Tensor) -> Tensor`, and a property, `seed:Optional[int]`.

The `seed` is passed into `sample` guider in the `SamplerCustomAdvanced`, but does not appear to be used in any of the standard guiders. It is Optional, so you can generally set it to None.

When noise is to be added, the latent is passed into this method, which should return a `Tensor` of the same shape containing the noise.

See the [noise mixing example](http://docs.comfy.org/snippets#creating-noise-variations)

### [​](http://docs.comfy.org#sampler) Sampler

The `SAMPLER` datatype represents a sampler, which is represented as a Python object providing a `sample` method. Stable diffusion sampling is beyond the scope of this guide; see `comfy/samplers.py` if you want to dig into this part of the code.

### [​](http://docs.comfy.org#sigmas) Sigmas

The `SIGMAS` datatypes represents the values of sigma before and after each step in the sampling process, as produced by a scheduler. This is represented as a one-dimensional tensor, of length `steps+1`, where each element represents the noise expected to be present before the corresponding step, with the final value representing the noise present after the final step.

A `normal` scheduler, with 20 steps and denoise of 1, for an SDXL model, produces:

```plaintext
tensor([14.6146, 10.7468,  8.0815,  6.2049,  4.8557,  
         3.8654,  3.1238,  2.5572,  2.1157,  1.7648,  
         1.4806,  1.2458,  1.0481,  0.8784,  0.7297,  
         0.5964,  0.4736,  0.3555,  0.2322,  0.0292,  0.0000])
```

The starting value of sigma depends on the model, which is why a scheduler node requires a `MODEL` input to produce a SIGMAS output

### [​](http://docs.comfy.org#guider) Guider

A `GUIDER` is a generalisation of the denoising process, as ‘guided’ by a prompt or any other form of conditioning. In Comfy the guider is represented by a `callable` Python object providing a `__call__(*args, **kwargs)` method which is called by the sample.

The `__call__` method takes (in `args[0]`) a batch of noisy latents (tensor `[B,C,H,W]`), and returns a prediction of the noise (a tensor of the same shape).

## [​](http://docs.comfy.org#model-datatypes) Model datatypes

There are a number of more technical datatypes for stable diffusion models. The most significant ones are `MODEL`, `CLIP`, `VAE` and `CONDITIONING`. Working with these is (for the time being) beyond the scope of this guide!

## [​](http://docs.comfy.org#additional-parameters) Additional Parameters

Below is a list of officially supported keys that can be used in the ‘extra options’ portion of an input definition.

You can use additional keys for your own custom widgets, but should *not* reuse any of the keys below for other purposes.

KeyDescription`default`The default value of the widget`min`The minimum value of a number (`FLOAT` or `INT`)`max`The maximum value of a number (`FLOAT` or `INT`)`step`The amount to increment or decrement a widget`label_on`The label to use in the UI when the bool is `True` (`BOOL`)`label_off`The label to use in the UI when the bool is `False` (`BOOL`)`defaultInput`Defaults to an input socket rather than a supported widget`forceInput``defaultInput` and also don’t allow converting to a widget`multiline`Use a multiline text box (`STRING`)`placeholder`Placeholder text to display in the UI when empty (`STRING`)`dynamicPrompts`Causes the front-end to evaluate dynamic prompts`lazy`Declares that this input uses [Lazy Evaluation](http://docs.comfy.org/lazy_evaluation)`rawLink`When a link exists, rather than receiving the evaluated value, you will receive the link (i.e. `["nodeId", <outputIndex>]`). Primarily useful when your node uses [Node Expansion](http://docs.comfy.org/expansion).

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/backend/datatypes.mdx)

[Previous](http://docs.comfy.org/custom-nodes/backend/manager)

[Images, Latents, and Masks  
\
Next](http://docs.comfy.org/custom-nodes/backend/images_and_masks)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Comfy datatypes](http://docs.comfy.org#comfy-datatypes)
- [COMBO](http://docs.comfy.org#combo)
- [Primitive and reroute](http://docs.comfy.org#primitive-and-reroute)
- [Python datatypes](http://docs.comfy.org#python-datatypes)
- [INT](http://docs.comfy.org#int)
- [FLOAT](http://docs.comfy.org#float)
- [STRING](http://docs.comfy.org#string)
- [BOOLEAN](http://docs.comfy.org#boolean)
- [Tensor datatypes](http://docs.comfy.org#tensor-datatypes)
- [IMAGE](http://docs.comfy.org#image)
- [LATENT](http://docs.comfy.org#latent)
- [MASK](http://docs.comfy.org#mask)
- [AUDIO](http://docs.comfy.org#audio)
- [Custom Sampling datatypes](http://docs.comfy.org#custom-sampling-datatypes)
- [Noise](http://docs.comfy.org#noise)
- [Sampler](http://docs.comfy.org#sampler)
- [Sigmas](http://docs.comfy.org#sigmas)
- [Guider](http://docs.comfy.org#guider)
- [Model datatypes](http://docs.comfy.org#model-datatypes)
- [Additional Parameters](http://docs.comfy.org#additional-parameters)

<!-- END Get_Started/custom-nodes/backend/datatypes.md -->


<!-- BEGIN Get_Started/custom-nodes/backend/expansion.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
  
  - [Properties](http://docs.comfy.org/custom-nodes/backend/server_overview)
  - [Lifecycle](http://docs.comfy.org/custom-nodes/backend/lifecycle)
  - [Publishing to the Manager](http://docs.comfy.org/custom-nodes/backend/manager)
  - [Datatypes](http://docs.comfy.org/custom-nodes/backend/datatypes)
  - [Images, Latents, and Masks](http://docs.comfy.org/custom-nodes/backend/images_and_masks)
  - [Hidden and Flexible inputs](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)
  - [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)
  - [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion)
  - [Data lists](http://docs.comfy.org/custom-nodes/backend/lists)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/backend/snippets)
  - [Working with torch.Tensor](http://docs.comfy.org/custom-nodes/backend/tensors)
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Node Expansion

# Node Expansion

## [​](http://docs.comfy.org#node-expansion) Node Expansion

Normally, when a node is executed, that execution function immediately returns the output results of that node. “Node Expansion” is a relatively advanced technique that allows nodes to return a new subgraph of nodes that should take its place in the graph. This technique is what allows custom nodes to implement loops.

### [​](http://docs.comfy.org#a-simple-example) A Simple Example

First, here’s a simple example of what node expansion looks like:

We highly recommend using the `GraphBuilder` class when creating subgraphs. It isn’t mandatory, but it prevents you from making many easy mistakes.

```python
def load_and_merge_checkpoints(self, checkpoint_path1, checkpoint_path2, ratio):
    from comfy_execution.graph_utils import GraphBuilder # Usually at the top of the file
    graph = GraphBuilder()
    checkpoint_node1 = graph.node("CheckpointLoaderSimple", checkpoint_path=checkpoint_path1)
    checkpoint_node2 = graph.node("CheckpointLoaderSimple", checkpoint_path=checkpoint_path2)
    merge_model_node = graph.node("ModelMergeSimple", model1=checkpoint_node1.out(0), model2=checkpoint_node2.out(0), ratio=ratio)
    merge_clip_node = graph.node("ClipMergeSimple", clip1=checkpoint_node1.out(1), clip2=checkpoint_node2.out(1), ratio=ratio)
    return {
        # Returning (MODEL, CLIP, VAE) outputs
        "result": (merge_model_node.out(0), merge_clip_node.out(0), checkpoint_node1.out(2)),
        "expand": graph.finalize(),
    }
```

While this same node could previously be implemented by manually calling into ComfyUI internals, using expansion means that each subnode will be cached separately (so if you change `model2`, you don’t have to reload `model1`).

### [​](http://docs.comfy.org#requirements) Requirements

In order to perform node expansion, a node must return a dictionary with the following keys:

1. `result`: A tuple of the outputs of the node. This may be a mix of finalized values (like you would return from a normal node) and node outputs.
2. `expand`: The finalized graph to perform expansion on. See below if you are not using the `GraphBuilder`.

#### [​](http://docs.comfy.org#additional-requirements-if-not-using-graphbuilder) Additional Requirements if not using GraphBuilder

The format expected from the `expand` key is the same as the ComfyUI API format. The following requirements are handled by the `GraphBuilder`, but must be handled manually if you choose to forego it:

1. Node IDs must be unique across the entire graph. (This includes between multiple executions of the same node due to the use of lists.)
2. Node IDs must be deterministic and consistent between multiple executions of the graph (including partial executions due to caching).

Even if you don’t want to use the `GraphBuilder` for actually building the graph (e.g. because you’re loading the raw json of the graph from a file), you can use the `GraphBuilder.alloc_prefix()` function to generate a prefix and `comfy.graph_utils.add_graph_prefix` to fix existing graphs to meet these requirements.

### [​](http://docs.comfy.org#efficient-subgraph-caching) Efficient Subgraph Caching

While you can pass non-literal inputs to nodes within the subgraph (like torch tensors), this can inhibit caching *within* the subgraph. When possible, you should pass links to subgraph objects rather than the node itself. (You can declare an input as a `rawLink` within the input’s [Additional Parameters](http://docs.comfy.org/datatypes#additional-parameters) to do this easily.)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/backend/expansion.mdx)

[Previous](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)

[Data lists  
\
Next](http://docs.comfy.org/custom-nodes/backend/lists)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node Expansion](http://docs.comfy.org#node-expansion)
- [A Simple Example](http://docs.comfy.org#a-simple-example)
- [Requirements](http://docs.comfy.org#requirements)
- [Additional Requirements if not using GraphBuilder](http://docs.comfy.org#additional-requirements-if-not-using-graphbuilder)
- [Efficient Subgraph Caching](http://docs.comfy.org#efficient-subgraph-caching)

<!-- END Get_Started/custom-nodes/backend/expansion.md -->


<!-- BEGIN Get_Started/custom-nodes/backend/images_and_masks.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
  
  - [Properties](http://docs.comfy.org/custom-nodes/backend/server_overview)
  - [Lifecycle](http://docs.comfy.org/custom-nodes/backend/lifecycle)
  - [Publishing to the Manager](http://docs.comfy.org/custom-nodes/backend/manager)
  - [Datatypes](http://docs.comfy.org/custom-nodes/backend/datatypes)
  - [Images, Latents, and Masks](http://docs.comfy.org/custom-nodes/backend/images_and_masks)
  - [Hidden and Flexible inputs](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)
  - [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)
  - [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion)
  - [Data lists](http://docs.comfy.org/custom-nodes/backend/lists)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/backend/snippets)
  - [Working with torch.Tensor](http://docs.comfy.org/custom-nodes/backend/tensors)
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Images, Latents, and Masks

# Images, Latents, and Masks

When working with these datatypes, you will need to know about the `torch.Tensor` class. Complete documentation is [here](https://pytorch.org/docs/stable/tensors.html), or an introduction to the key concepts required for Comfy [here](http://docs.comfy.org/tensors).

If your node has a single output which is a tensor, remember to return `(image,)` not `(image)`

Most of the concepts below are illustrated in the [example code snippets](http://docs.comfy.org/snippets).

## [​](http://docs.comfy.org#images) Images

An IMAGE is a `torch.Tensor` with shape `[B,H,W,C]`, `C=3`. If you are going to save or load images, you will need to convert to and from `PIL.Image` format - see the code snippets below! Note that some `pytorch` operations offer (or expect) `[B,C,H,W]`, known as ‘channel first’, for reasons of computational efficiency. Just be careful.

### [​](http://docs.comfy.org#working-with-pil-image) Working with PIL.Image

If you want to load and save images, you’ll want to use PIL:

```python
from PIL import Image, ImageOps
```

## [​](http://docs.comfy.org#masks) Masks

A MASK is a `torch.Tensor` with shape `[B,H,W]`. In many contexts, masks have binary values (0 or 1), which are used to indicate which pixels should undergo specific operations. In some cases values between 0 and 1 are used indicate an extent of masking, (for instance, to alter transparency, adjust filters, or composite layers).

### [​](http://docs.comfy.org#masks-from-the-load-image-node) Masks from the Load Image Node

The `LoadImage` node uses an image’s alpha channel (the “A” in “RGBA”) to create MASKs. The values from the alpha channel are normalized to the range \[0,1] (torch.float32) and then inverted. The `LoadImage` node always produces a MASK output when loading an image. Many images (like JPEGs) don’t have an alpha channel. In these cases, `LoadImage` creates a default mask with the shape `[1, 64, 64]`.

### [​](http://docs.comfy.org#understanding-mask-shapes) Understanding Mask Shapes

In libraries like `numpy`, `PIL`, and many others, single-channel images (like masks) are typically represented as 2D arrays, shape `[H,W]`. This means the `C` (channel) dimension is implicit, and thus unlike IMAGE types, batches of MASKs have only three dimensions: `[B, H, W]`. It is not uncommon to encounter a mask which has had the `B` dimension implicitly squeezed, giving a tensor `[H,W]`.

To use a MASK, you will often have to match shapes by unsqueezing to produce a shape `[B,H,W,C]` with `C=1` To unsqueezing the `C` dimension, so you should `unsqueeze(-1)`, to unsqueeze `B`, you `unsqueeze(0)`. If your node receives a MASK as input, you would be wise to always check `len(mask.shape)`.

## [​](http://docs.comfy.org#latents) Latents

A LATENT is a `dict`; the latent sample is referenced by the key `samples` and has shape `[B,C,H,W]`, with `C=4`.

LATENT is channel first, IMAGE is channel last

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/backend/images_and_masks.mdx)

[Previous](http://docs.comfy.org/custom-nodes/backend/datatypes)

[Hidden and Flexible inputs  
\
Next](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Images](http://docs.comfy.org#images)
- [Working with PIL.Image](http://docs.comfy.org#working-with-pil-image)
- [Masks](http://docs.comfy.org#masks)
- [Masks from the Load Image Node](http://docs.comfy.org#masks-from-the-load-image-node)
- [Understanding Mask Shapes](http://docs.comfy.org#understanding-mask-shapes)
- [Latents](http://docs.comfy.org#latents)

<!-- END Get_Started/custom-nodes/backend/images_and_masks.md -->


<!-- BEGIN Get_Started/custom-nodes/backend/lazy_evaluation.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
  
  - [Properties](http://docs.comfy.org/custom-nodes/backend/server_overview)
  - [Lifecycle](http://docs.comfy.org/custom-nodes/backend/lifecycle)
  - [Publishing to the Manager](http://docs.comfy.org/custom-nodes/backend/manager)
  - [Datatypes](http://docs.comfy.org/custom-nodes/backend/datatypes)
  - [Images, Latents, and Masks](http://docs.comfy.org/custom-nodes/backend/images_and_masks)
  - [Hidden and Flexible inputs](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)
  - [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)
  - [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion)
  - [Data lists](http://docs.comfy.org/custom-nodes/backend/lists)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/backend/snippets)
  - [Working with torch.Tensor](http://docs.comfy.org/custom-nodes/backend/tensors)
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Lazy Evaluation

# Lazy Evaluation

## [​](http://docs.comfy.org#lazy-evaluation) Lazy Evaluation

By default, all `required` and `optional` inputs are evaluated before a node can be run. Sometimes, however, an input won’t necessarily be used and evaluating it would result in unnecessary processing. Here are some examples of nodes where lazy evaluation may be beneficial:

1. A `ModelMergeSimple` node where the ratio is either `0.0` (in which case the first model doesn’t need to be loaded) or `1.0` (in which case the second model doesn’t need to be loaded).
2. Interpolation between two images where the ratio (or mask) is either entirely `0.0` or entirely `1.0`.
3. A Switch node where one input determines which of the other inputs will be passed through.

There is very little cost in making an input lazy. If it’s something you can do, you generally should.

### [​](http://docs.comfy.org#creating-lazy-inputs) Creating Lazy Inputs

There are two steps to making an input a “lazy” input. They are:

1. Mark the input as lazy in the dictionary returned by `INPUT_TYPES`
2. Define a method named `check_lazy_status` (note: *not* a class method) that will be called prior to evaluation to determine if any more inputs are necessary.

To demonstrate these, we’ll make a “MixImages” node that interpolates between two images according to a mask. If the entire mask is `0.0`, we don’t need to evaluate any part of the tree leading up to the second image. If the entire mask is `1.0`, we can skip evaluating the first image.

#### [​](http://docs.comfy.org#defining-input-types) Defining `INPUT_TYPES`

Declaring that an input is lazy is as simple as adding a `lazy: True` key-value pair to the input’s options dictionary.

```python
@classmethod
def INPUT_TYPES(cls):
    return {
        "required": {
            "image1": ("IMAGE",{"lazy": True}),
            "image2": ("IMAGE",{"lazy": True}),
            "mask": ("MASK",),
        },
    }
```

In this example, `image1` and `image2` are both marked as lazy inputs, but `mask` will always be evaluated.

#### [​](http://docs.comfy.org#defining-check-lazy-status) Defining `check_lazy_status`

A `check_lazy_status` method is called if there are one or more lazy inputs that are not yet available. This method receives the same arguments as the standard execution function. All available inputs are passed in with their final values while unavailable lazy inputs have a value of `None`.

The responsibility of the `check_lazy_status` function is to return a list of the names of any lazy inputs that are needed to proceed. If all lazy inputs are available, the function should return an empty list.

Note that `check_lazy_status` may be called multiple times. (For example, you might find after evaluating one lazy input that you need to evaluate another.)

Note that because the function uses actual input values, it is *not* a class method.

```python
def check_lazy_status(self, mask, image1, image2):
    mask_min = mask.min()
    mask_max = mask.max()
    needed = []
    if image1 is None and (mask_min != 1.0 or mask_max != 1.0):
        needed.append("image1")
    if image2 is None and (mask_min != 0.0 or mask_max != 0.0):
        needed.append("image2")
    return needed
```

### [​](http://docs.comfy.org#full-example) Full Example

```python
class LazyMixImages:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "image1": ("IMAGE",{"lazy": True}),
                "image2": ("IMAGE",{"lazy": True}),
                "mask": ("MASK",),
            },
        }

    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "mix"

    CATEGORY = "Examples"

    def check_lazy_status(self, mask, image1, image2):
        mask_min = mask.min()
        mask_max = mask.max()
        needed = []
        if image1 is None and (mask_min != 1.0 or mask_max != 1.0):
            needed.append("image1")
        if image2 is None and (mask_min != 0.0 or mask_max != 0.0):
            needed.append("image2")
        return needed

    # Not trying to handle different batch sizes here just to keep the demo simple
    def mix(self, mask, image1, image2):
        mask_min = mask.min()
        mask_max = mask.max()
        if mask_min == 0.0 and mask_max == 0.0:
            return (image1,)
        elif mask_min == 1.0 and mask_max == 1.0:
            return (image2,)

        result = image1 * (1. - mask) + image2 * mask,
        return (result[0],)
```

## [​](http://docs.comfy.org#execution-blocking) Execution Blocking

While Lazy Evaluation is the recommended way to “disable” part of a graph, there are times when you want to disable an `OUTPUT` node that doesn’t implement lazy evaluation itself. If it’s an output node that you developed yourself, you should just add lazy evaluation as follows:

1. Add a required (if this is a new node) or optional (if you care about backward compatibility) input for `enabled` that defaults to `True`
2. Make all other inputs `lazy` inputs
3. Only evaluate the other inputs if `enabled` is `True`

If it’s not a node you control, you can make use of a `comfy_execution.graph.ExecutionBlocker`. This special object can be returned as an output from any socket. Any nodes which receive an `ExecutionBlocker` as input will skip execution and return that `ExecutionBlocker` for any outputs.

**There is intentionally no way to stop an ExecutionBlocker from propagating forward.** If you think you want this, you should really be using Lazy Evaluation.

### [​](http://docs.comfy.org#usage) Usage

There are two ways to construct and use an `ExecutionBlocker`

1. Pass `None` into the constructor to silently block execution. This is useful for cases where blocking execution is part of a successful run — like disabling an output.

```python
def silent_passthrough(self, passthrough, blocked):
    if blocked:
        return (ExecutionBlocker(None),)
    else:
        return (passthrough,)
```

2. Pass a string into the constructor to display an error message when a node is blocked due to receiving the object. This can be useful if you want to display a meaningful error message if someone uses a meaningless output — for example, the `VAE` output when loading a model that doesn’t contain VAEs.

```python
def load_checkpoint(self, ckpt_name):
    ckpt_path = folder_paths.get_full_path("checkpoints", ckpt_name)
    model, clip, vae = load_checkpoint(ckpt_path)
    if vae is None:
        # This error is more useful than a "'NoneType' has no attribute" error
        # in a later node
        vae = ExecutionBlocker(f"No VAE contained in the loaded model {ckpt_name}")
    return (model, clip, vae)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/backend/lazy_evaluation.mdx)

[Previous](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)

[Node Expansion  
\
Next](http://docs.comfy.org/custom-nodes/backend/expansion)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Lazy Evaluation](http://docs.comfy.org#lazy-evaluation)
- [Creating Lazy Inputs](http://docs.comfy.org#creating-lazy-inputs)
- [Defining INPUT\_TYPES](http://docs.comfy.org#defining-input-types)
- [Defining check\_lazy\_status](http://docs.comfy.org#defining-check-lazy-status)
- [Full Example](http://docs.comfy.org#full-example)
- [Execution Blocking](http://docs.comfy.org#execution-blocking)
- [Usage](http://docs.comfy.org#usage)

<!-- END Get_Started/custom-nodes/backend/lazy_evaluation.md -->


<!-- BEGIN Get_Started/custom-nodes/backend/lifecycle.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
  
  - [Properties](http://docs.comfy.org/custom-nodes/backend/server_overview)
  - [Lifecycle](http://docs.comfy.org/custom-nodes/backend/lifecycle)
  - [Publishing to the Manager](http://docs.comfy.org/custom-nodes/backend/manager)
  - [Datatypes](http://docs.comfy.org/custom-nodes/backend/datatypes)
  - [Images, Latents, and Masks](http://docs.comfy.org/custom-nodes/backend/images_and_masks)
  - [Hidden and Flexible inputs](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)
  - [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)
  - [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion)
  - [Data lists](http://docs.comfy.org/custom-nodes/backend/lists)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/backend/snippets)
  - [Working with torch.Tensor](http://docs.comfy.org/custom-nodes/backend/tensors)
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Lifecycle

# Lifecycle

## [​](http://docs.comfy.org#how-comfy-loads-custom-nodes) How Comfy loads custom nodes

When Comfy starts, it scans the directory `custom_nodes` for Python modules, and attempts to load them. If the module exports `NODE_CLASS_MAPPINGS`, it will be treated as a custom node.

A python module is a directory containing an `__init__.py` file. The module exports whatever is listed in the `__all__` attribute defined in `__init__.py`.

### [​](http://docs.comfy.org#init-py) **init**.py

`__init__.py` is executed when Comfy attempts to import the module. For a module to be recognized as containing custom node definitions, it needs to export `NODE_CLASS_MAPPINGS`. If it does (and if nothing goes wrong in the import), the nodes defined in the module will be available in Comfy. If there is an error in your code, Comfy will continue, but will report the module as having failed to load. So check the Python console!

A very simple `__init__.py` file would look like this:

```python
from .python_file import MyCustomNode
NODE_CLASS_MAPPINGS = { "My Custom Node" : MyCustomNode }
__all__ = ["NODE_CLASS_MAPPINGS"]
```

#### [​](http://docs.comfy.org#node-class-mappings) NODE\_CLASS\_MAPPINGS

`NODE_CLASS_MAPPINGS` must be a `dict` mapping custom node names (unique across the Comfy install) to the corresponding node class.

#### [​](http://docs.comfy.org#node-display-name-mappings) NODE\_DISPLAY\_NAME\_MAPPINGS

`__init__.py` may also export `NODE_DISPLAY_NAME_MAPPINGS`, which maps the same unique name to a display name for the node. If `NODE_DISPLAY_NAME_MAPPINGS` is not provided, Comfy will use the unique name as the display name.

#### [​](http://docs.comfy.org#web-directory) WEB\_DIRECTORY

If you are deploying client side code, you will also need to export the path, relative to the module, in which the JavaScript files are to be found. It is conventional to place these in a subdirectory of your custom node named `js`.

*Only* `.js` files will be served; you can’t deploy `.css` or other types in this way

In previous versions of Comfy, `__init__.py` was required to copy the JavaScript files into the main Comfy web subdirectory. You will still see code that does this. Don’t.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/backend/lifecycle.mdx)

[Previous](http://docs.comfy.org/custom-nodes/backend/server_overview)

[Publishing to the Manager  
\
Next](http://docs.comfy.org/custom-nodes/backend/manager)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [How Comfy loads custom nodes](http://docs.comfy.org#how-comfy-loads-custom-nodes)
- [init.py](http://docs.comfy.org#init-py)
- [NODE\_CLASS\_MAPPINGS](http://docs.comfy.org#node-class-mappings)
- [NODE\_DISPLAY\_NAME\_MAPPINGS](http://docs.comfy.org#node-display-name-mappings)
- [WEB\_DIRECTORY](http://docs.comfy.org#web-directory)

<!-- END Get_Started/custom-nodes/backend/lifecycle.md -->


<!-- BEGIN Get_Started/custom-nodes/backend/lists.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
  
  - [Properties](http://docs.comfy.org/custom-nodes/backend/server_overview)
  - [Lifecycle](http://docs.comfy.org/custom-nodes/backend/lifecycle)
  - [Publishing to the Manager](http://docs.comfy.org/custom-nodes/backend/manager)
  - [Datatypes](http://docs.comfy.org/custom-nodes/backend/datatypes)
  - [Images, Latents, and Masks](http://docs.comfy.org/custom-nodes/backend/images_and_masks)
  - [Hidden and Flexible inputs](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)
  - [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)
  - [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion)
  - [Data lists](http://docs.comfy.org/custom-nodes/backend/lists)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/backend/snippets)
  - [Working with torch.Tensor](http://docs.comfy.org/custom-nodes/backend/tensors)
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Data lists

# Data lists

## [​](http://docs.comfy.org#length-one-processing) Length one processing

Internally, the Comfy server represents data flowing from one node to the next as a Python `list`, normally length 1, of the relevant datatype. In normal operation, when a node returns an output, each element in the output `tuple` is separately wrapped in a list (length 1); then when the next node is called, the data is unwrapped and passed to the main function.

You generally don’t need to worry about this, since Comfy does the wrapping and unwrapping.

This isn’t about batches. A batch (of, for instance, latents, or images) is a *single entry* in the list (see [tensor datatypes](http://docs.comfy.org/images_and_masks))

## [​](http://docs.comfy.org#list-processing) List processing

In some circumstance, multiple data instances are processed in a single workflow, in which case the internal data will be a list containing the data instances. An example of this might be processing a series of images one at a time to avoid running out of VRAM, or handling images of different sizes.

By default, Comfy will process the values in the list sequentially:

- if the inputs are `list`s of different lengths, the shorter ones are padded by repeating the last value
- the main method is called once for each value in the input lists
- the outputs are `list`s, each of which is the same length as the longest input

The relevant code can be found in the method `map_node_over_list` in `execution.py`.

However, as Comfy wraps node outputs into a `list` of length one, if the `tuple` returned by a custom node contains a `list`, that `list` will be wrapped, and treated as a single piece of data. In order to tell Comfy that the list being returned should not be wrapped, but treated as a series of data for sequential processing, the node should provide a class attribute `OUTPUT_IS_LIST`, which is a `tuple[bool]`, of the same length as `RETURN_TYPES`, specifying which outputs which should be so treated.

A node can also override the default input behaviour and receive the whole list in a single call. This is done by setting a class attribute `INPUT_IS_LIST` to `True`.

Here’s a (lightly annotated) example from the built in nodes - `ImageRebatch` takes one or more batches of images (received as a list, because `INPUT_IS_LIST - True`) and rebatches them into batches of the requested size.

`INPUT_IS_LIST` is node level - all inputs get the same treatment. So the value of the `batch_size` widget is given by `batch_size[0]`.

```python

class ImageRebatch:
    @classmethod
    def INPUT_TYPES(s):
        return {"required": { "images": ("IMAGE",),
                              "batch_size": ("INT", {"default": 1, "min": 1, "max": 4096}) }}
    RETURN_TYPES = ("IMAGE",)
    INPUT_IS_LIST = True
    OUTPUT_IS_LIST = (True, )
    FUNCTION = "rebatch"
    CATEGORY = "image/batch"

    def rebatch(self, images, batch_size):
        batch_size = batch_size[0]    # everything comes as a list, so batch_size is list[int]

        output_list = []
        all_images = []
        for img in images:                    # each img is a batch of images
            for i in range(img.shape[0]):     # each i is a single image
                all_images.append(img[i:i+1])

        for i in range(0, len(all_images), batch_size): # take batch_size chunks and turn each into a new batch
            output_list.append(torch.cat(all_images[i:i+batch_size], dim=0))  # will die horribly if the image batches had different width or height!

        return (output_list,)
```

#### [​](http://docs.comfy.org#input-is-list) INPUT\_IS\_LIST

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/backend/lists.mdx)

[Previous](http://docs.comfy.org/custom-nodes/backend/expansion)

[Annotated Examples  
\
Next](http://docs.comfy.org/custom-nodes/backend/snippets)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Length one processing](http://docs.comfy.org#length-one-processing)
- [List processing](http://docs.comfy.org#list-processing)
- [INPUT\_IS\_LIST](http://docs.comfy.org#input-is-list)

<!-- END Get_Started/custom-nodes/backend/lists.md -->


<!-- BEGIN Get_Started/custom-nodes/backend/manager.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
  
  - [Properties](http://docs.comfy.org/custom-nodes/backend/server_overview)
  - [Lifecycle](http://docs.comfy.org/custom-nodes/backend/lifecycle)
  - [Publishing to the Manager](http://docs.comfy.org/custom-nodes/backend/manager)
  - [Datatypes](http://docs.comfy.org/custom-nodes/backend/datatypes)
  - [Images, Latents, and Masks](http://docs.comfy.org/custom-nodes/backend/images_and_masks)
  - [Hidden and Flexible inputs](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)
  - [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)
  - [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion)
  - [Data lists](http://docs.comfy.org/custom-nodes/backend/lists)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/backend/snippets)
  - [Working with torch.Tensor](http://docs.comfy.org/custom-nodes/backend/tensors)
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Publishing to the Manager

# Publishing to the Manager

### [​](http://docs.comfy.org#using-comfyui-manager) Using ComfyUI Manager

To make your custom node available through **ComfyUI Manager** you need to save it as a git repository (generally at `github.com`) and then submit a Pull Request on the **ComfyUI Manager** git, in which you have edited `custom-node-list.json` to add your node. [More details](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#how-to-register-your-custom-node-into-comfyui-manager).

When a user installs the node, **ComfyUI Manager** will:

1

Git Clone

git clone the repository,

2

Install Python Dependencies

install the pip dependencies listed in the custom node repository under `requirements.txt` (if present),

```plaintext
pip install -r requirements.txt
```

As is always the case with `pip`, it is possible that your node requirements will be in conflict with other custom nodes. Don’t make your `requirements.txt` any more restrictive than they need to be.

3

Run Install Script

execute `install.py`, if it is present in the custom node repository.

`install.py` is executed from the root path of the custom node

### [​](http://docs.comfy.org#comfyui-manager-files) ComfyUI Manager files

As indicated above, there are a number of files and scripts that **ComfyUI Manager** will use to manage the lifecycle of a custom node. These are all optional.

- `requirements.txt` - Python dependencies as mentioned above
- `install.py`, `uninstall.py` - executed when the custom node is installed or uninstalled
  
  Users can just delete the directory, so you can’t rely on `uninstall.py` being run
- `disable.py`, `enable.py` - executed when a custom node is disabled or re-enabled
  
  `enable.py` is only run when a disabled node is re-enabled - it should just reverse anything done in `disable.py`
  
  Disabled custom node subdirectory have `.disabled` appended to their names, and Comfy ignores these modules
- `node_list.json` - only required if the custom nodes pattern of NODE\_CLASS\_MAPPINGS is not conventional.

See the [ComfyUI Manager guide](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#custom-node-support-guide) for official details.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/backend/manager.mdx)

[Previous](http://docs.comfy.org/custom-nodes/backend/lifecycle)

[Datatypes  
\
Next](http://docs.comfy.org/custom-nodes/backend/datatypes)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Using ComfyUI Manager](http://docs.comfy.org#using-comfyui-manager)
- [ComfyUI Manager files](http://docs.comfy.org#comfyui-manager-files)

<!-- END Get_Started/custom-nodes/backend/manager.md -->


<!-- BEGIN Get_Started/custom-nodes/backend/more_on_inputs.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
  
  - [Properties](http://docs.comfy.org/custom-nodes/backend/server_overview)
  - [Lifecycle](http://docs.comfy.org/custom-nodes/backend/lifecycle)
  - [Publishing to the Manager](http://docs.comfy.org/custom-nodes/backend/manager)
  - [Datatypes](http://docs.comfy.org/custom-nodes/backend/datatypes)
  - [Images, Latents, and Masks](http://docs.comfy.org/custom-nodes/backend/images_and_masks)
  - [Hidden and Flexible inputs](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)
  - [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)
  - [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion)
  - [Data lists](http://docs.comfy.org/custom-nodes/backend/lists)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/backend/snippets)
  - [Working with torch.Tensor](http://docs.comfy.org/custom-nodes/backend/tensors)
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Hidden and Flexible inputs

# Hidden and Flexible inputs

## [​](http://docs.comfy.org#hidden-inputs) Hidden inputs

Alongside the `required` and `optional` inputs, which create corresponding inputs or widgets on the client-side, there are three `hidden` input options which allow the custom node to request certain information from the server.

These are accessed by returning a value for `hidden` in the `INPUT_TYPES` `dict`, with the signature `dict[str,str]`, containing one or more of `PROMPT`, `EXTRA_PNGINFO`, or `UNIQUE_ID`

```python
@classmethod
def INPUT_TYPES(s):
    return {
        "required": {...},
        "optional": {...},
        "hidden": {
            "unique_id": "UNIQUE_ID",
            "prompt": "PROMPT", 
            "extra_pnginfo": "EXTRA_PNGINFO",
        }
    }
```

### [​](http://docs.comfy.org#unique-id) UNIQUE\_ID

`UNIQUE_ID` is the unique identifier of the node, and matches the `id` property of the node on the client side. It is commonly used in client-server communications (see [messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages#getting-node-id)).

### [​](http://docs.comfy.org#prompt) PROMPT

`PROMPT` is the complete prompt sent by the client to the server. See [the prompt object](http://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking#prompt) for a full description.

### [​](http://docs.comfy.org#extra-pnginfo) EXTRA\_PNGINFO

`EXTRA_PNGINFO` is a dictionary that will be copied into the metadata of any `.png` files saved. Custom nodes can store additional information in this dictionary for saving (or as a way to communicate with a downstream node).

Note that if Comfy is started with the `disable_metadata` option, this data won’t be saved.

### [​](http://docs.comfy.org#dynprompt) DYNPROMPT

`DYNPROMPT` is an instance of `comfy_execution.graph.DynamicPrompt`. It differs from `PROMPT` in that it may mutate during the course of execution in response to [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion).

`DYNPROMPT` should only be used for advanced cases (like implementing loops in custom nodes).

## [​](http://docs.comfy.org#flexible-inputs) Flexible inputs

### [​](http://docs.comfy.org#custom-datatypes) Custom datatypes

If you want to pass data between your own custom nodes, you may find it helpful to define a custom datatype. This is (almost) as simple as just choosing a name for the datatype, which should be a unique string in upper case, such as `CHEESE`.

You can then use `CHEESE` in your node `INPUT_TYPES` and `RETURN_TYPES`, and the Comfy client will only allow `CHEESE` outputs to connect to a `CHEESE` input. `CHEESE` can be any python object.

The only point to note is that because the Comfy client doesn’t know about `CHEESE` you need (unless you define a custom widget for `CHEESE`, which is a topic for another day), to force it to be an input rather than a widget. This can be done with the `forceInput` option in the input options dictionary:

```python
@classmethod
def INPUT_TYPES(s):
    return {
        "required": { "my_cheese": ("CHEESE", {"forceInput":True}) }
    }
```

### [​](http://docs.comfy.org#wildcard-inputs) Wildcard inputs

```python
@classmethod
def INPUT_TYPES(s):
    return {
        "required": { "anything": ("*",{})},
    }

@classmethod
def VALIDATE_INPUTS(s, input_types):
    return True
```

The frontend allows `*` to indicate that an input can be connected to any source. Because this is not officially supported by the backend, you can skip the backend validation of types by accepting a parameter named `input_types` in your `VALIDATE_INPUTS` function. (See [VALIDATE\_INPUTS](http://docs.comfy.org/server_overview#validate-inputs) for more information.) It’s up to the node to make sense of the data that is passed.

### [​](http://docs.comfy.org#dynamically-created-inputs) Dynamically created inputs

If inputs are dynamically created on the client side, they can’t be defined in the Python source code. In order to access this data we need an `optional` dictionary that allows Comfy to pass data with arbitrary names. Since the Comfy server

```python
class ContainsAnyDict(dict):
    def __contains__(self, key):
        return True
...

@classmethod
def INPUT_TYPES(s):
    return {
        "required": {},
        "optional": ContainsAnyDict()
    }
...

def main_method(self, **kwargs):
    # the dynamically created input data will be in the dictionary kwargs

```

Hat tip to rgthree for this pythonic trick!

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/backend/more_on_inputs.mdx)

[Previous](http://docs.comfy.org/custom-nodes/backend/images_and_masks)

[Lazy Evaluation  
\
Next](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Hidden inputs](http://docs.comfy.org#hidden-inputs)
- [UNIQUE\_ID](http://docs.comfy.org#unique-id)
- [PROMPT](http://docs.comfy.org#prompt)
- [EXTRA\_PNGINFO](http://docs.comfy.org#extra-pnginfo)
- [DYNPROMPT](http://docs.comfy.org#dynprompt)
- [Flexible inputs](http://docs.comfy.org#flexible-inputs)
- [Custom datatypes](http://docs.comfy.org#custom-datatypes)
- [Wildcard inputs](http://docs.comfy.org#wildcard-inputs)
- [Dynamically created inputs](http://docs.comfy.org#dynamically-created-inputs)

<!-- END Get_Started/custom-nodes/backend/more_on_inputs.md -->


<!-- BEGIN Get_Started/custom-nodes/backend/server_overview.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
  
  - [Properties](http://docs.comfy.org/custom-nodes/backend/server_overview)
  - [Lifecycle](http://docs.comfy.org/custom-nodes/backend/lifecycle)
  - [Publishing to the Manager](http://docs.comfy.org/custom-nodes/backend/manager)
  - [Datatypes](http://docs.comfy.org/custom-nodes/backend/datatypes)
  - [Images, Latents, and Masks](http://docs.comfy.org/custom-nodes/backend/images_and_masks)
  - [Hidden and Flexible inputs](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)
  - [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)
  - [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion)
  - [Data lists](http://docs.comfy.org/custom-nodes/backend/lists)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/backend/snippets)
  - [Working with torch.Tensor](http://docs.comfy.org/custom-nodes/backend/tensors)
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Properties

# Properties

Properties of a custom node

### [​](http://docs.comfy.org#simple-example) Simple Example

Here’s the code for the Invert Image Node, which gives an overview of the key concepts in custom node development.

```python
class InvertImageNode:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": { "image_in" : ("IMAGE", {}) },
        }

    RETURN_TYPES = ("IMAGE",)
    RETURN_NAMES = ("image_out",)
    CATEGORY = "examples"
    FUNCTION = "invert"

    def invert(self, image_in):
        image_out = 1 - image_in
        return (image_out,)
```

### [​](http://docs.comfy.org#main-properties) Main properties

Every custom node is a Python class, with the following key properties:

#### [​](http://docs.comfy.org#input-types) INPUT\_TYPES

`INPUT_TYPES`, as the name suggests, defines the inputs for the node. The method returns a `dict` which *must* contain the key `required`, and *may* also include the keys `optional` and/or `hidden`. The only difference between `required` and `optional` inputs is that `optional` inputs can be left unconnected. For more information on `hidden` inputs, see [Hidden Inputs](http://docs.comfy.org/more_on_inputs#hidden-inputs).

Each key has, as its value, another `dict`, in which key-value pairs specify the names and types of the inputs. The types are defined by a `tuple`, the first element of which defines the data type, and the second element of which is a `dict` of additional parameters.

Here we have just one required input, named `image_in`, of type `IMAGE`, with no additional parameters.

Note that unlike the next few attributes, this `INPUT_TYPES` is a `@classmethod`. This is so that the options in dropdown widgets (like the name of the checkpoint to be loaded) can be computed by Comfy at run time. We’ll go into this more later.

#### [​](http://docs.comfy.org#return-types) RETURN\_TYPES

A `tuple` of `str` defining the data types returned by the node. If the node has no outputs this must still be provided `RETURN_TYPES = ()`

If you have exactly one output, remember the trailing comma: `RETURN_TYPES = ("IMAGE",)`. This is required for Python to make it a `tuple`

#### [​](http://docs.comfy.org#return-names) RETURN\_NAMES

The names to be used to label the outputs. This is optional; if omitted, the names are simply the `RETURN_TYPES` in lowercase.

#### [​](http://docs.comfy.org#category) CATEGORY

Where the node will be found in the ComfyUI **Add Node** menu. Submenus can be specified as a path, eg. `examples/trivial`.

#### [​](http://docs.comfy.org#function) FUNCTION

The name of the Python function in the class that should be called when the node is executed.

The function is called with named arguments. All `required` (and `hidden`) inputs will be included; `optional` inputs will be included only if they are connected, so you should provide default values for them in the function definition (or capture them with `**kwargs`).

The function returns a tuple corresponding to the `RETURN_TYPES`. This is required even if nothing is returned (`return ()`). Again, if you only have one output, remember that trailing comma `return (image_out,)`!

### [​](http://docs.comfy.org#execution-control-extras) Execution Control Extras

A great feature of Comfy is that it caches outputs, and only executes nodes that might produce a different result than the previous run. This can greatly speed up lots of workflows.

In essence this works by identifying which nodes produce an output (these, notably the Image Preview and Save Image nodes, are always executed), and then working backwards to identify which nodes provide data that might have changed since the last run.

Two optional features of a custom node assist in this process.

#### [​](http://docs.comfy.org#output-node) OUTPUT\_NODE

By default, a node is not considered an output. Set `OUTPUT_NODE = True` to specify that it is.

#### [​](http://docs.comfy.org#is-changed) IS\_CHANGED

By default, Comfy considers that a node has changed if any of its inputs or widgets have changed. This is normally correct, but you may need to override this if, for instance, the node uses a random number (and does not specify a seed - it’s best practice to have a seed input in this case so that the user can control reproducability and avoid unecessary execution), or loads an input that may have changed externally, or sometimes ignores inputs (so doesn’t need to execute just because those inputs changed).

Despite the name, IS\_CHANGED should not return a `bool`

`IS_CHANGED` is passed the same arguments as the main function defined by `FUNCTION`, and can return any Python object. This object is compared with the one returned in the previous run (if any) and the node will be considered to have changed if `is_changed != is_changed_old` (this code is in `execution.py` if you need to dig).

Since `True == True`, a node that returns `True` to say it has changed will be considered not to have! I’m sure this would be changed in the Comfy code if it wasn’t for the fact that it might break existing nodes to do so.

To specify that your node should always be considered to have changed (which you should avoid if possible, since it stops Comfy optimising what gets run), `return float("NaN")`. This returns a `NaN` value, which is not equal to anything, even another `NaN`.

A good example of actually checking for changes is the code from the built-in LoadImage node, which loads the image and returns a hash

```python
    @classmethod
    def IS_CHANGED(s, image):
        image_path = folder_paths.get_annotated_filepath(image)
        m = hashlib.sha256()
        with open(image_path, 'rb') as f:
            m.update(f.read())
        return m.digest().hex()
```

### [​](http://docs.comfy.org#other-attributes) Other attributes

There are three other attributes that can be used to modify the default Comfy treatment of a node.

#### [​](http://docs.comfy.org#input-is-list%2C-output-is-list) INPUT\_IS\_LIST, OUTPUT\_IS\_LIST

These are used to control sequential processing of data, and are described [later](http://docs.comfy.org/lists.mdx).

### [​](http://docs.comfy.org#validate-inputs) VALIDATE\_INPUTS

If a class method `VALIDATE_INPUTS` is defined, it will be called before the workflow begins execution. `VALIDATE_INPUTS` should return `True` if the inputs are valid, or a message (as a `str`) describing the error (which will prevent execution).

#### [​](http://docs.comfy.org#validating-constants) Validating Constants

Note that `VALIDATE_INPUTS` will only receive inputs that are defined as constants within the workflow. Any inputs that are received from other nodes will *not* be available in `VALIDATE_INPUTS`.

`VALIDATE_INPUTS` is called with only the inputs that its signature requests (those returned by `inspect.getfullargspec(obj_class.VALIDATE_INPUTS).args`). Any inputs which are received in this way will *not* run through the default validation rules. For example, in the following snippet, the front-end will use the specified `min` and `max` values of the `foo` input, but the back-end will not enforce it.

```python
class CustomNode:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": { "foo" : ("INT", {"min": 0, "max": 10}) },
        }

    @classmethod
    def VALIDATE_INPUTS(cls, foo):
        # YOLO, anything goes!
        return True
```

Additionally, if the function takes a `**kwargs` input, it will receive *all* available inputs and all of them will skip validation as if specified explicitly.

#### [​](http://docs.comfy.org#validating-types) Validating Types

If the `VALIDATE_INPUTS` method receives an argument named `input_types`, it will be passed a dictionary in which the key is the name of each input which is connected to an output from another node and the value is the type of that output.

When this argument is present, all default validation of input types is skipped. Here’s an example making use of the fact that the front-end allows for the specification of multiple types:

```python
class AddNumbers:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "input1" : ("INT,FLOAT", {"min": 0, "max": 1000})
                "input2" : ("INT,FLOAT", {"min": 0, "max": 1000})
            },
        }

    @classmethod
    def VALIDATE_INPUTS(cls, input_types):
        # The min and max of input1 and input2 are still validated because
        # we didn't take `input1` or `input2` as arguments
        if input_types["input1"] not in ("INT", "FLOAT"):
            return "input1 must be an INT or FLOAT type"
        if input_types["input2"] not in ("INT", "FLOAT"):
            return "input2 must be an INT or FLOAT type"
        return True
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/backend/server_overview.mdx)

[Previous](http://docs.comfy.org/custom-nodes/walkthrough)

[Lifecycle  
\
Next](http://docs.comfy.org/custom-nodes/backend/lifecycle)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Simple Example](http://docs.comfy.org#simple-example)
- [Main properties](http://docs.comfy.org#main-properties)
- [INPUT\_TYPES](http://docs.comfy.org#input-types)
- [RETURN\_TYPES](http://docs.comfy.org#return-types)
- [RETURN\_NAMES](http://docs.comfy.org#return-names)
- [CATEGORY](http://docs.comfy.org#category)
- [FUNCTION](http://docs.comfy.org#function)
- [Execution Control Extras](http://docs.comfy.org#execution-control-extras)
- [OUTPUT\_NODE](http://docs.comfy.org#output-node)
- [IS\_CHANGED](http://docs.comfy.org#is-changed)
- [Other attributes](http://docs.comfy.org#other-attributes)
- [INPUT\_IS\_LIST, OUTPUT\_IS\_LIST](http://docs.comfy.org#input-is-list%2C-output-is-list)
- [VALIDATE\_INPUTS](http://docs.comfy.org#validate-inputs)
- [Validating Constants](http://docs.comfy.org#validating-constants)
- [Validating Types](http://docs.comfy.org#validating-types)

<!-- END Get_Started/custom-nodes/backend/server_overview.md -->


<!-- BEGIN Get_Started/custom-nodes/backend/snippets.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
  
  - [Properties](http://docs.comfy.org/custom-nodes/backend/server_overview)
  - [Lifecycle](http://docs.comfy.org/custom-nodes/backend/lifecycle)
  - [Publishing to the Manager](http://docs.comfy.org/custom-nodes/backend/manager)
  - [Datatypes](http://docs.comfy.org/custom-nodes/backend/datatypes)
  - [Images, Latents, and Masks](http://docs.comfy.org/custom-nodes/backend/images_and_masks)
  - [Hidden and Flexible inputs](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)
  - [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)
  - [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion)
  - [Data lists](http://docs.comfy.org/custom-nodes/backend/lists)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/backend/snippets)
  - [Working with torch.Tensor](http://docs.comfy.org/custom-nodes/backend/tensors)
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Annotated Examples

# Annotated Examples

A growing collection of fragments of example code…

## [​](http://docs.comfy.org#images-and-masks) Images and Masks

### [​](http://docs.comfy.org#load-an-image) Load an image

Load an image into a batch of size 1 (based on `LoadImage` source code in `nodes.py`)

```python
i = Image.open(image_path)
i = ImageOps.exif_transpose(i)
if i.mode == 'I':
    i = i.point(lambda i: i * (1 / 255))
image = i.convert("RGB")
image = np.array(image).astype(np.float32) / 255.0
image = torch.from_numpy(image)[None,]
```

### [​](http://docs.comfy.org#save-an-image-batch) Save an image batch

Save a batch of images (based on `SaveImage` source code in `nodes.py`)

```python
for (batch_number, image) in enumerate(images):
    i = 255. * image.cpu().numpy()
    img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))
    filepath = # some path that takes the batch number into account
    img.save(filepath)
```

### [​](http://docs.comfy.org#invert-a-mask) Invert a mask

Inverting a mask is a straightforward process. Since masks are normalised to the range \[0,1]:

```python
mask = 1.0 - mask
```

### [​](http://docs.comfy.org#convert-a-mask-to-image-shape) Convert a mask to Image shape

```python
# We want [B,H,W,C] with C = 1
if len(mask.shape)==2: # we have [H,W], so insert B and C as dimension 1
    mask = mask[None,:,:,None]
elif len(mask.shape)==3 and mask.shape[2]==1: # we have [H,W,C]
    mask = mask[None,:,:,:]
elif len(mask.shape)==3:                      # we have [B,H,W]
    mask = mask[:,:,:,None]
```

### [​](http://docs.comfy.org#using-masks-as-transparency-layers) Using Masks as Transparency Layers

When used for tasks like inpainting or segmentation, the MASK’s values will eventually be rounded to the nearest integer so that they are binary — 0 indicating regions to be ignored and 1 indicating regions to be targeted. However, this doesn’t happen until the MASK is passed to those nodes. This flexibility allows you to use MASKs as as you would in digital photography contexts as a transparency layer:

```python
# Invert mask back to original transparency layer
mask = 1.0 - mask

# Unsqueeze the `C` (channels) dimension
mask = mask.unsqueeze(-1)

# Concatenate ("cat") along the `C` dimension
rgba_image = torch.cat((rgb_image, mask), dim=-1)
```

## [​](http://docs.comfy.org#noise) Noise

### [​](http://docs.comfy.org#creating-noise-variations) Creating noise variations

Here’s an example of creating a noise object which mixes the noise from two sources. This could be used to create slight noise variations by varying `weight2`.

```python
class Noise_MixedNoise:
    def __init__(self, nosie1, noise2, weight2):
        self.noise1  = noise1
        self.noise2  = noise2
        self.weight2 = weight2

    @property
    def seed(self): return self.noise1.seed

    def generate_noise(self, input_latent:torch.Tensor) -> torch.Tensor:
        noise1 = self.noise1.generate_noise(input_latent)
        noise2 = self.noise2.generate_noise(input_latent)
        return noise1 * (1.0-self.weight2) + noise2 * (self.weight2)
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/backend/snippets.mdx)

[Previous](http://docs.comfy.org/custom-nodes/backend/lists)

[Working with torch.Tensor  
\
Next](http://docs.comfy.org/custom-nodes/backend/tensors)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Images and Masks](http://docs.comfy.org#images-and-masks)
- [Load an image](http://docs.comfy.org#load-an-image)
- [Save an image batch](http://docs.comfy.org#save-an-image-batch)
- [Invert a mask](http://docs.comfy.org#invert-a-mask)
- [Convert a mask to Image shape](http://docs.comfy.org#convert-a-mask-to-image-shape)
- [Using Masks as Transparency Layers](http://docs.comfy.org#using-masks-as-transparency-layers)
- [Noise](http://docs.comfy.org#noise)
- [Creating noise variations](http://docs.comfy.org#creating-noise-variations)

<!-- END Get_Started/custom-nodes/backend/snippets.md -->


<!-- BEGIN Get_Started/custom-nodes/backend/tensors.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
  
  - [Properties](http://docs.comfy.org/custom-nodes/backend/server_overview)
  - [Lifecycle](http://docs.comfy.org/custom-nodes/backend/lifecycle)
  - [Publishing to the Manager](http://docs.comfy.org/custom-nodes/backend/manager)
  - [Datatypes](http://docs.comfy.org/custom-nodes/backend/datatypes)
  - [Images, Latents, and Masks](http://docs.comfy.org/custom-nodes/backend/images_and_masks)
  - [Hidden and Flexible inputs](http://docs.comfy.org/custom-nodes/backend/more_on_inputs)
  - [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation)
  - [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion)
  - [Data lists](http://docs.comfy.org/custom-nodes/backend/lists)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/backend/snippets)
  - [Working with torch.Tensor](http://docs.comfy.org/custom-nodes/backend/tensors)
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Working with torch.Tensor

# Working with torch.Tensor

## [​](http://docs.comfy.org#pytorch%2C-tensors%2C-and-torch-tensor) pytorch, tensors, and torch.Tensor

All the core number crunching in Comfy is done by [pytorch](https://pytorch.org/). If your custom nodes are going to get into the guts of stable diffusion you will need to become familiar with this library, which is way beyond the scope of this introduction.

However, many custom nodes will need to manipulate images, latents and masks, each of which are represented internally as `torch.Tensor`, so you’ll want to bookmark the [documentation for torch.Tensor](https://pytorch.org/docs/stable/tensors.html).

### [​](http://docs.comfy.org#what-is-a-tensor%3F) What is a Tensor?

`torch.Tensor` represents a tensor, which is the mathematical generalization of a vector or matrix to any number of dimensions. A tensor’s *rank* is the number of dimensions it has (so a vector has *rank* 1, a matrix *rank* 2); its *shape* describes the size of each dimension.

So an RGB image (of height H and width W) might be thought of as three arrays (one for each color channel), each measuring H x W, which could be represented as a tensor with *shape* `[H,W,3]`. In Comfy images almost always come in a batch (even if the batch only contains a single image). `torch` always places the batch dimension first, so Comfy images have *shape* `[B,H,W,3]`, generally written as `[B,H,W,C]` where C stands for Channels.

### [​](http://docs.comfy.org#squeeze%2C-unsqueeze%2C-and-reshape) squeeze, unsqueeze, and reshape

If a tensor has a dimension of size 1 (known as a collapsed dimension), it is equivalent to the same tensor with that dimension removed (a batch with 1 image is just an image). Removing such a collapsed dimension is referred to as squeezing, and inserting one is known as unsqueezing.

Some torch code, and some custom node authors, will return a squeezed tensor when a dimension is collapsed - such as when a batch has only one member. This is a common cause of bugs!

To represent the same data in a different shape is referred to as reshaping. This often requires you to know the underlying data structure, so handle with care!

### [​](http://docs.comfy.org#important-notation) Important notation

`torch.Tensor` supports most Python slice notation, iteration, and other common list-like operations. A tensor also has a `.shape` attribute which returns its size as a `torch.Size` (which is a subclass of `tuple` and can be treated as such).

There are some other important bits of notation you’ll often see (several of these are less common standard Python notation, seen much more frequently when dealing with tensors)

- `torch.Tensor` supports the use of `None` in slice notation to indicate the insertion of a dimension of size 1.
- `:` is frequently used when slicing a tensor; this simply means ‘keep the whole dimension’. It’s like using `a[start:end]` in Python, but omitting the start point and end point.
- `...` represents ‘the whole of an unspecified number of dimensions’. So `a[0, ...]` would extract the first item from a batch regardless of the number of dimensions.
- in methods which require a shape to be passed, it is often passed as a `tuple` of the dimensions, in which a single dimension can be given the size `-1`, indicating that the size of this dimension should be calculated based on the total size of the data.

```python
>>> a = torch.Tensor((1,2))
>>> a.shape
torch.Size([2])
>>> a[:,None].shape 
torch.Size([2, 1])
>>> a.reshape((1,-1)).shape
torch.Size([1, 2])
```

### [​](http://docs.comfy.org#elementwise-operations) Elementwise operations

Many binary on `torch.Tensor` (including ’+’, ’-’, ’\*’, ’/’ and ’==’) are applied elementwise (independantly applied to each element). The operands must be *either* two tensors of the same shape, *or* a tensor and a scalar. So:

```python
>>> import torch
>>> a = torch.Tensor((1,2))
>>> b = torch.Tensor((3,2))
>>> a*b
tensor([3., 4.])
>>> a/b
tensor([0.3333, 1.0000])
>>> a==b
tensor([False,  True])
>>> a==1
tensor([ True, False])
>>> c = torch.Tensor((3,2,1)) 
>>> a==c
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
RuntimeError: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 0
```

### [​](http://docs.comfy.org#tensor-truthiness) Tensor truthiness

The ‘truthiness’ value of a Tensor is not the same as that of Python lists.

You may be familiar with the truthy value of a Python list as `True` for any non-empty list, and `False` for `None` or `[]`. By contrast A `torch.Tensor` (with more than one elements) does not have a defined truthy value. Instead you need to use `.all()` or `.any()` to combine the elementwise truthiness:

```python
>>> a = torch.Tensor((1,2))
>>> print("yes" if a else "no")
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
RuntimeError: Boolean value of Tensor with more than one value is ambiguous
>>> a.all()
tensor(False)
>>> a.any()
tensor(True)
```

This also means that you need to use `if a is not None:` not `if a:` to determine if a tensor variable has been set.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/backend/tensors.mdx)

[Previous](http://docs.comfy.org/custom-nodes/backend/snippets)

[Javascript Extensions  
\
Next](http://docs.comfy.org/custom-nodes/js/javascript_overview)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [pytorch, tensors, and torch.Tensor](http://docs.comfy.org#pytorch%2C-tensors%2C-and-torch-tensor)
- [What is a Tensor?](http://docs.comfy.org#what-is-a-tensor%3F)
- [squeeze, unsqueeze, and reshape](http://docs.comfy.org#squeeze%2C-unsqueeze%2C-and-reshape)
- [Important notation](http://docs.comfy.org#important-notation)
- [Elementwise operations](http://docs.comfy.org#elementwise-operations)
- [Tensor truthiness](http://docs.comfy.org#tensor-truthiness)

<!-- END Get_Started/custom-nodes/backend/tensors.md -->


<!-- BEGIN Get_Started/custom-nodes/js/javascript_examples.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
  
  - [Javascript Extensions](http://docs.comfy.org/custom-nodes/js/javascript_overview)
  - [Comfy Hooks](http://docs.comfy.org/custom-nodes/js/javascript_hooks)
  - [Comfy Objects](http://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking)
  - [Settings](http://docs.comfy.org/custom-nodes/js/javascript_settings)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/js/javascript_examples)
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Annotated Examples

# Annotated Examples

A growing collection of fragments of example code…

## [​](http://docs.comfy.org#right-click-menus) Right click menus

### [​](http://docs.comfy.org#background-menu) Background menu

The main background menu (right-click on the canvas) is generated by a call to  
`LGraph.getCanvasMenuOptions`. One way to add your own menu options is to hijack this call:

```javascript
/* in setup() */
    const original_getCanvasMenuOptions = LGraphCanvas.prototype.getCanvasMenuOptions;
    LGraphCanvas.prototype.getCanvasMenuOptions = function () {
        // get the basic options 
        const options = original_getCanvasMenuOptions.apply(this, arguments);
        options.push(null); // inserts a divider
        options.push({
            content: "The text for the menu",
            callback: async () => {
                // do whatever
            }
        })
        return options;
    }
```

### [​](http://docs.comfy.org#node-menu) Node menu

When you right click on a node, the menu is similarly generated by `node.getExtraMenuOptions`. But instead of returning an options object, this one gets it passed in…

```javascript
/* in beforeRegisterNodeDef() */
if (nodeType?.comfyClass=="MyNodeClass") { 
    const original_getExtraMenuOptions = nodeType.prototype.getExtraMenuOptions;
    nodeType.prototype.getExtraMenuOptions = function(_, options) {
        original_getExtraMenuOptions?.apply(this, arguments);
        options.push({
            content: "Do something fun",
            callback: async () => {
                // fun thing
            }
        })
    }   
}
```

### [​](http://docs.comfy.org#submenus) Submenus

If you want a submenu, provide a callback which uses `LiteGraph.ContextMenu` to create it:

```javascript
function make_submenu(value, options, e, menu, node) {
    const submenu = new LiteGraph.ContextMenu(
        ["option 1", "option 2", "option 3"],
        { 
            event: e, 
            callback: function (v) { 
                // do something with v (=="option x")
            }, 
            parentMenu: menu, 
            node:node
        }
    )
}

/* ... */
    options.push(
        {
            content: "Menu with options",
            has_submenu: true,
            callback: make_submenu,
        }
    )
```

## [​](http://docs.comfy.org#capture-ui-events) Capture UI events

This works just like you’d expect - find the UI element in the DOM and add an eventListener. `setup()` is a good place to do this, since the page has fully loaded. For instance, to detect a click on the ‘Queue’ button:

```javascript
function queue_button_pressed() { console.log("Queue button was pressed!") }
document.getElementById("queue-button").addEventListener("click", queue_button_pressed);
```

## [​](http://docs.comfy.org#detect-when-a-workflow-starts) Detect when a workflow starts

This is one of many `api` events:

```javascript
import { api } from "../../scripts/api.js";
/* in setup() */
    function on_execution_start() { 
        /* do whatever */
    }
    api.addEventListener("execution_start", on_execution_start);
```

## [​](http://docs.comfy.org#detect-an-interrupted-workflow) Detect an interrupted workflow

A simple example of hijacking the api:

```javascript
import { api } from "../../scripts/api.js";
/* in setup() */
    const original_api_interrupt = api.interrupt;
    api.interrupt = function () {
        /* Do something before the original method is called */
        original_api_interrupt.apply(this, arguments);
        /* Or after */
    }
```

## [​](http://docs.comfy.org#catch-clicks-on-your-node) Catch clicks on your node

`node` has a mouseDown method you can hijack. This time we’re careful to pass on any return value.

```javascript
async nodeCreated(node) {
    if (node?.comfyClass === "My Node Name") {
        const original_onMouseDown = node.onMouseDown;
        node.onMouseDown = function( e, pos, canvas ) {
            alert("ouch!");
            return original_onMouseDown?.apply(this, arguments);
        }        
    }
}
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/js/javascript_examples.mdx)

[Previous](http://docs.comfy.org/custom-nodes/js/javascript_settings)

[Workflow templates  
\
Next](http://docs.comfy.org/custom-nodes/workflow_templates)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Right click menus](http://docs.comfy.org#right-click-menus)
- [Background menu](http://docs.comfy.org#background-menu)
- [Node menu](http://docs.comfy.org#node-menu)
- [Submenus](http://docs.comfy.org#submenus)
- [Capture UI events](http://docs.comfy.org#capture-ui-events)
- [Detect when a workflow starts](http://docs.comfy.org#detect-when-a-workflow-starts)
- [Detect an interrupted workflow](http://docs.comfy.org#detect-an-interrupted-workflow)
- [Catch clicks on your node](http://docs.comfy.org#catch-clicks-on-your-node)

<!-- END Get_Started/custom-nodes/js/javascript_examples.md -->


<!-- BEGIN Get_Started/custom-nodes/js/javascript_hooks.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
  
  - [Javascript Extensions](http://docs.comfy.org/custom-nodes/js/javascript_overview)
  - [Comfy Hooks](http://docs.comfy.org/custom-nodes/js/javascript_hooks)
  - [Comfy Objects](http://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking)
  - [Settings](http://docs.comfy.org/custom-nodes/js/javascript_settings)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/js/javascript_examples)
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Comfy Hooks

# Comfy Hooks

## [​](http://docs.comfy.org#extension-hooks) Extension hooks

At various points during Comfy execution, the application calls `#invokeExtensionsAsync` or `#invokeExtensions` with the name of a hook. These invoke, on all registered extensions, the appropriately named method (if present), such as `setup` in the example above.

Comfy provides a variety of hooks for custom extension code to use to modify client behavior.

These hooks are called during creation and modification of the Comfy client side elements.  
Events during workflow execution are handled by the `apiUpdateHandlers`

A few of the most significant hooks are described below. As Comfy is being actively developed, from time to time additional hooks are added, so search for `#invokeExtensions` in `app.js` to find all available hooks.

See also the [sequence](http://docs.comfy.org/_sites/docs.comfy.org/custom-nodes/js/javascript_hooks#call-sequences) in which hooks are invoked.

### [​](http://docs.comfy.org#commonly-used-hooks) Commonly used hooks

Start with `beforeRegisterNodeDef`, which is used by the majority of extensions, and is often the only one needed.

#### [​](http://docs.comfy.org#beforeregisternodedef) beforeRegisterNodeDef()

Called once for each node type (the list of nodes available in the `AddNode` menu), and is used to modify the bahaviour of the node.

```javascript
async beforeRegisterNodeDef(nodeType, nodeData, app) 
```

The object passed in the `nodeType` parameter essentially serves as a template for all nodes that will be created of this type, so modifications made to `nodeType.prototype` will apply to all nodes of this type. `nodeData` is an encapsulation of aspects of the node defined in the Python code, such as its category, inputs, and outputs. `app` is a reference to the main Comfy app object (which you have already imported anyway!)

This method is called, on each registered extension, for *every* node type, not just the ones added by that extension.

The usual idiom is to check `nodeType.ComfyClass`, which holds the Python class name corresponding to this node, to see if you need to modify the node. Often this means modifying the custom nodes that you have added, although you may sometimes need to modify the behavior of other nodes (or other custom nodes might modify yours!), in which case care should be taken to ensure interoperability.

Since other extensions may also modify nodes, aim to write code that makes as few assumptions as possible. And play nicely - isolate your changes wherever possible.

A very common idiom in `beforeRegisterNodeDef` is to ‘hijack’ an existing method:

```javascript
async beforeRegisterNodeDef(nodeType, nodeData, app) {
	if (nodeType.comfyClass=="MyNodeClass") { 
		const onConnectionsChange = nodeType.prototype.onConnectionsChange;
		nodeType.prototype.onConnectionsChange = function (side,slot,connect,link_info,output) {     
			const r = onConnectionsChange?.apply(this, arguments);   
			console.log("Someone changed my connection!");
			return r;
		}
	}
}
```

In this idiom the existing prototype method is stored, and then replaced. The replacement calls the original method (the `?.apply` ensures that if there wasn’t one this is still safe) and then performs additional operations. Depending on your code logic, you may need to place the `apply` elsewhere in your replacement code, or even make calling it conditional.

When hijacking a method in this way, you will want to look at the core comfy code (breakpoints are your friend) to check and conform with the method signature.

#### [​](http://docs.comfy.org#nodecreated) nodeCreated()

```javascript
async nodeCreated(node)
```

Called when a specific instance of a node gets created (right at the end of the `ComfyNode()` function on `nodeType` which serves as a constructor). In this hook you can make modifications to individual instances of your node.

Changes that apply to all instances are better added to the prototype in `beforeRegisterNodeDef` as described above.

#### [​](http://docs.comfy.org#init) init()

```javascript
async init()
```

Called when the Comfy webpage is loaded (or reloaded). The call is made after the graph object has been created, but before any nodes are registered or created. It can be used to modify core Comfy behavior by hijacking methods of the app, or of the graph (a `LiteGraph` object). This is discussed further in [Comfy Objects](http://docs.comfy.org/javascript_objects_and_hijacking).

With great power comes great responsibility. Hijacking core behavior makes it more likely your nodes will be incompatible with other custom nodes, or future Comfy updates

#### [​](http://docs.comfy.org#setup) setup()

```javascript
async setup()
```

Called at the end of the startup process. A good place to add event listeners (either for Comfy events, or DOM events), or adding to the global menus, both of which are discussed elsewhere.

To do something when a workflow has loaded, use `afterConfigureGraph`, not `setup`

### [​](http://docs.comfy.org#call-sequences) Call sequences

These sequences were obtained by insert logging code into the Comfy `app.js` file. You may find similar code helpful in understanding the execution flow.

```javascript
/* approx line 220 at time of writing: */
	#invokeExtensions(method, ...args) {
		console.log(`invokeExtensions      ${method}`) // this line added
		// ...
	}
/* approx line 250 at time of writing: */
	async #invokeExtensionsAsync(method, ...args) {
		console.log(`invokeExtensionsAsync ${method}`) // this line added
		// ...
	}
```

#### [​](http://docs.comfy.org#web-page-load) Web page load

```plaintext
invokeExtensionsAsync init
invokeExtensionsAsync addCustomNodeDefs
invokeExtensionsAsync getCustomWidgets
invokeExtensionsAsync beforeRegisterNodeDef    [repeated multiple times]
invokeExtensionsAsync registerCustomNodes
invokeExtensionsAsync beforeConfigureGraph
invokeExtensionsAsync nodeCreated
invokeExtensions      loadedGraphNode
invokeExtensionsAsync afterConfigureGraph
invokeExtensionsAsync setup
```

#### [​](http://docs.comfy.org#loading-workflow) Loading workflow

```plaintext
invokeExtensionsAsync beforeConfigureGraph
invokeExtensionsAsync beforeRegisterNodeDef   [zero, one, or multiple times]
invokeExtensionsAsync nodeCreated             [repeated multiple times]
invokeExtensions      loadedGraphNode         [repeated multiple times]
invokeExtensionsAsync afterConfigureGraph
```

#### [​](http://docs.comfy.org#adding-new-node) Adding new node

```plaintext
invokeExtensionsAsync nodeCreated
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/js/javascript_hooks.mdx)

[Previous](http://docs.comfy.org/custom-nodes/js/javascript_overview)

[Comfy Objects  
\
Next](http://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Extension hooks](http://docs.comfy.org#extension-hooks)
- [Commonly used hooks](http://docs.comfy.org#commonly-used-hooks)
- [beforeRegisterNodeDef()](http://docs.comfy.org#beforeregisternodedef)
- [nodeCreated()](http://docs.comfy.org#nodecreated)
- [init()](http://docs.comfy.org#init)
- [setup()](http://docs.comfy.org#setup)
- [Call sequences](http://docs.comfy.org#call-sequences)
- [Web page load](http://docs.comfy.org#web-page-load)
- [Loading workflow](http://docs.comfy.org#loading-workflow)
- [Adding new node](http://docs.comfy.org#adding-new-node)

<!-- END Get_Started/custom-nodes/js/javascript_hooks.md -->


<!-- BEGIN Get_Started/custom-nodes/js/javascript_objects_and_hijacking.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
  
  - [Javascript Extensions](http://docs.comfy.org/custom-nodes/js/javascript_overview)
  - [Comfy Hooks](http://docs.comfy.org/custom-nodes/js/javascript_hooks)
  - [Comfy Objects](http://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking)
  - [Settings](http://docs.comfy.org/custom-nodes/js/javascript_settings)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/js/javascript_examples)
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Comfy Objects

# Comfy Objects

## [​](http://docs.comfy.org#litegraph) LiteGraph

The Comfy UI is built on top of [LiteGraph](https://github.com/jagenjo/litegraph.js). Much of the Comfy functionality is provided by LiteGraph, so if developing more complex nodes you will probably find it helpful to clone that repository and browse the documentation, which can be found at `doc/index.html`.

## [​](http://docs.comfy.org#comfyapp) ComfyApp

The `app` object (always accessible by `import { app } from "../../scripts/app.js";`) represents the Comfy application running in the browser, and contains a number of useful properties and functions, some of which are listed below.

Hijacking functions on `app` is not recommended, as Comfy is under constant development, and core behavior may change.

### [​](http://docs.comfy.org#properties) Properties

Important properties of `app` include (this is not an exhaustive list):

propertycontents`canvas`An LGraphCanvas object, representing the current user interface. It contains some potentially interesting properties, such as `node_over` and `selected_nodes`.`canvasEl`The DOM `<canvas>` element`graph`A reference to the LGraph object describing the current graph`runningNodeId`During execution, the node currently being executed`ui`Provides access to some UI elements, such as the queue, menu, and dialogs

`canvas` (for graphical elements) and `graph` (for logical connections) are probably the ones you are most likely to want to access.

### [​](http://docs.comfy.org#functions) Functions

Again, there are many. A few significant ones are:

functionnotesgraphToPromptConvert the graph into a prompt that can be sent to the Python serverloadGraphDataLoad a graphqueuePromptSubmit a prompt to the queueregisterExtensionYou’ve seen this one - used to add an extension

## [​](http://docs.comfy.org#lgraph) LGraph

The `LGraph` object is part of the LiteGraph framework, and represents the current logical state of the graph (nodes and links). If you want to manipulate the graph, the LiteGraph documentation (at `doc/index.html` if you clone `https://github.com/jagenjo/litegraph.js`) describes the functions you will need.

You can use `graph` to obtain details of nodes and links, for example:

```javascript
const ComfyNode_object_for_my_node = app.graph._nodes_by_id(my_node_id) 
ComfyNode_object_for_my_node.inputs.forEach(input => {
    const link_id = input.link;
    if (link_id) {
        const LLink_object = app.graph.links[link_id]
        const id_of_upstream_node = LLink_object.origin_id
        // etc
    }
});
```

## [​](http://docs.comfy.org#llink) LLink

The `LLink` object, accessible through `graph.links`, represents a single link in the graph, from node `link.origin_id` output slot `link.origin_slot` to node `link.target_id` slot `link.target_slot`. It also has a string representing the data type, in `link.type`, and `link.id`.

`LLink`s are created in the `connect` method of a `LGraphNode` (of which `ComfyNode` is a subclass).

Avoid creating your own LLink objects - use the LiteGraph functions instead.

## [​](http://docs.comfy.org#comfynode) ComfyNode

`ComfyNode` is a subclass of `LGraphNode`, and the LiteGraph documentation is therefore helpful for more generic operations. However, Comfy has significantly extended the LiteGraph core behavior, and also does not make use of all LiteGraph functionality.

The description that follows applies to a normal node. Group nodes, primitive nodes, notes, and redirect nodes have different properties.

A `ComfyNode` object represents a node in the current workflow. It has a number of important properties that you may wish to make use of, a very large number of functions that you may wish to use, or hijack to modify behavior.

To get a more complete sense of the node object, you may find it helpful to insert the following code into your extension and place a breakpoint on the `console.log` command. When you then create a new node you can use your favorite debugger to interrogate the node.

```javascript
async nodeCreated(node) {
    console.log("nodeCreated")
}
```

### [​](http://docs.comfy.org#properties-2) Properties

propertycontents`bgcolor`The background color of the node, or undefined for the default`comfyClass`The Python class representing the node`flags`A dictionary that may contain flags related to the state of the node. In particular, `flags.collapsed` is true for collapsed nodes.`graph`A reference to the LGraph object`id`A unique id`input_type`A list of the input types (eg “STRING”, “MODEL”, “CLIP” etc). Generally matches the Python INPUT\_TYPES`inputs`A list of inputs (discussed below)`mode`Normally 0, set to 2 if the node is muted and 4 if the node is bypassed. Values of 1 and 3 are not used by Comfy`order`The node’s position in the execution order. Set by `LGraph.computeExecutionOrder()` when the prompt is submitted`pos`The \[x,y] position of the node on the canvas`properties`A dictionary containing `"Node name for S&R"`, used by LiteGraph`properties_info`The type and default value of entries in `properties``size`The width and height of the node on the canvas`title`Display Title`type`The unique name (from Python) of the node class`widgets`A list of widgets (discussed below)`widgets_values`A list of the current values of widgets

### [​](http://docs.comfy.org#functions-2) Functions

There are a very large number of functions (85, last time I counted). A selection are listed below. Most of these functions are unmodified from the LiteGraph core code.

#### [​](http://docs.comfy.org#inputs%2C-outputs%2C-widgets) Inputs, Outputs, Widgets

functionnotesInputs / OutputsMost have output methods with the equivalent names: s/In/Out/`addInput`Create a new input, defined by name and type`addInputs`Array version of `addInput``findInputSlot`Find the slot index from the input name`findInputSlotByType`Find an input matching the type. Options to prefer, or only use, free slots`removeInput`By slot index`getInputNode`Get the node connected to this input. The output equivalent is `getOutputNodes` and returns a list`getInputLink`Get the LLink connected to this input. No output equivalentWidgets`addWidget`Add a standard Comfy widget`addCustomWidget`Add a custom widget (defined in the `getComfyWidgets` hook)`addDOMWidget`Add a widget defined by a DOM element`convertWidgetToInput`Convert a widget to an input if allowed by `isConvertableWidget` (in `widgetInputs.js`)

#### [​](http://docs.comfy.org#connections) Connections

functionnotes`connect`Connect this node’s output to another node’s input`connectByType`Connect output to another node by specifying the type - connects to first available matching slot`connectByTypeOutput`Connect input to another node output by type`disconnectInput`Remove any link into the input (specified by name or index)`disconnectOutput`Disconnect an output from a specified node’s input`onConnectionChange`Called on each node. `side==1` if it’s an input on this node`onConnectInput`Called *before* a connection is made. If this returns `false`, the connection is refused

#### [​](http://docs.comfy.org#display) Display

functionnotes`setDirtyCanvas`Specify that the foreground (nodes) and/or background (links and images) need to be redrawn`onDrawBackground`Called with a `CanvasRenderingContext2D` object to draw the background. Used by Comfy to render images`onDrawForeground`Called with a `CanvasRenderingContext2D` object to draw the node.`getTitle`The title to be displayed.`collapse`Toggles the collapsed state of the node.

`collapse` is badly named; it *toggles* the collapsed state. It takes a boolean parameter, which can be used to override `node.collapsable === false`.

#### [​](http://docs.comfy.org#other) Other

functionnotes`changeMode`Use to set the node to bypassed (`mode == 4`) or not (`mode == 0`)

## [​](http://docs.comfy.org#inputs-and-widgets) Inputs and Widgets

Inputs and Widgets represent the two ways that data can be fed into a node. In general a widget can be converted to an input, but not all inputs can be converted to a widget (as many datatypes can’t be entered through a UI element).

`node.inputs` is a list of the current inputs (colored dots on the left hand side of the node), specifying their `.name`, `.type`, and `.link` (a reference to the connected `LLink` in `app.graph.links`).

If an input is a widget which has been converted, it also holds a reference to the, now inactive, widget in `.widget`.

`node.widgets` is a list of all widgets, whether or not they have been converted to an input. A widget has:

property/functionnotes`callback`A function called when the widget value is changed`last_y`The vertical position of the widget in the node`name`The (unique within a node) widget name`options`As specified in the Python code (such as default, min, and max)`type`The name of the widget type (see below) in lowercase`value`The current widget value. This is a property with `get` and `set` methods

### [​](http://docs.comfy.org#widget-types) Widget Types

`app.widgets` is a dictionary of currently registered widget types, keyed in the UPPER CASE version of the name of the type. Build in Comfy widgets types include the self explanatory `BOOLEAN`, `INT`, and `FLOAT`, as well as `STRING` (which comes in two flavours, single line and multiline), `COMBO` for dropdown selection from a list, and `IMAGEUPLOAD`, used in Load Image nodes.

Custom widget types can be added by providing a `getCustomWidgets` method in your extension.

### [​](http://docs.comfy.org#linked-widgets) Linked widgets

Widgets can also be linked - the built in behavior of `seed` and `control_after_generate`, for example. A linked widget has `.type = 'base_widget_type:base_widget_name'`; so `control_after_generate` may have type `int:seed`.

## [​](http://docs.comfy.org#prompt) Prompt

When you press the `Queue Prompt` button in Comfy, the `app.graphToPrompt()` method is called to convert the current graph into a prompt that can be sent to the server.

`app.graphToPrompt` returns an object (refered to herein as `prompt`) with two properties, `output` and `workflow`.

### [​](http://docs.comfy.org#output) output

`prompt.output` maps from the `node_id` of each node in the graph to an object with two properties.

- `prompt.output[node_id].class_type`, the unique name of the custom node class, as defined in the Python code
- `prompt.output[node_id].inputs`, which contains the value of each input (or widget) as a map from the input name to:
  
  - the selected value, if it is a widget, or
  - an array containing (`upstream_node_id`, `upstream_node_output_slot`) if there is a link connected to the input, or
  - undefined, if it is a widget that has been converted to an input and is not connected
  - other unconnected inputs are not included in `.inputs`

Note that the `upstream_node_id` in the array describing a connected input is represented as a string, not an integer.

### [​](http://docs.comfy.org#workflow) workflow

`prompt.workflow` contains the following properties:

- `config` - a dictionary of additional configuration options (empty by default)
- `extra` - a dictionary containing extra information about the workflow. By default it contains:
  
  - `extra.ds` - describes the current view of the graph (`scale` and `offset`)
- `groups` - all groups in the workflow
- `last_link_id` - the id of the last link added
- `last_node_id` - the id of the last node added
- `links` - a list of all links in the graph. Each entry is an array of five integers and one string:
  
  - (`link_id`, `upstream_node_id`, `upstream_node_output_slot`, `downstream_node_id`, `downstream_node_input_slot`, `data type`)
- `nodes` - a list of all nodes in the graph. Each entry is a map of a subset of the properties of the node as described [above](http://docs.comfy.org/_sites/docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking#comfynode)
  
  - The following properties are included: `flags`, `id`, `inputs`, `mode`, `order`, `pos`, `properties`, `size`, `type`, `widgets_values`
  - In addition, unless a node has no outputs, there is an `outputs` property, which is a list of the outputs of the node, each of which contains:
    
    - `name` - the name of the output
    - `type` - the data type of the output
    - `links` - a list of the `link_id` of all links from this output (if there are no connections, may be an empty list, or null),
    - `shape` - the shape used to draw the output (default 3 for a dot)
    - `slot_index` - the slot number of the output
- `version` - the LiteGraph version number (at time of writing, `0.4`)

`nodes.output` is absent for nodes with no outputs, not an empty list.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/js/javascript_objects_and_hijacking.mdx)

[Previous](http://docs.comfy.org/custom-nodes/js/javascript_hooks)

[Settings  
\
Next](http://docs.comfy.org/custom-nodes/js/javascript_settings)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [LiteGraph](http://docs.comfy.org#litegraph)
- [ComfyApp](http://docs.comfy.org#comfyapp)
- [Properties](http://docs.comfy.org#properties)
- [Functions](http://docs.comfy.org#functions)
- [LGraph](http://docs.comfy.org#lgraph)
- [LLink](http://docs.comfy.org#llink)
- [ComfyNode](http://docs.comfy.org#comfynode)
- [Properties](http://docs.comfy.org#properties-2)
- [Functions](http://docs.comfy.org#functions-2)
- [Inputs, Outputs, Widgets](http://docs.comfy.org#inputs%2C-outputs%2C-widgets)
- [Connections](http://docs.comfy.org#connections)
- [Display](http://docs.comfy.org#display)
- [Other](http://docs.comfy.org#other)
- [Inputs and Widgets](http://docs.comfy.org#inputs-and-widgets)
- [Widget Types](http://docs.comfy.org#widget-types)
- [Linked widgets](http://docs.comfy.org#linked-widgets)
- [Prompt](http://docs.comfy.org#prompt)
- [output](http://docs.comfy.org#output)
- [workflow](http://docs.comfy.org#workflow)

<!-- END Get_Started/custom-nodes/js/javascript_objects_and_hijacking.md -->


<!-- BEGIN Get_Started/custom-nodes/js/javascript_overview.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
  
  - [Javascript Extensions](http://docs.comfy.org/custom-nodes/js/javascript_overview)
  - [Comfy Hooks](http://docs.comfy.org/custom-nodes/js/javascript_hooks)
  - [Comfy Objects](http://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking)
  - [Settings](http://docs.comfy.org/custom-nodes/js/javascript_settings)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/js/javascript_examples)
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Javascript Extensions

# Javascript Extensions

## [​](http://docs.comfy.org#extending-the-comfy-client) Extending the Comfy Client

Comfy can be modified through an extensions mechanism. To add an extension you need to:

- Export `WEB_DIRECTORY` from your Python module,
- Place one or more `.js` files into that directory,
- Use `app.registerExtension` to register your extension.

These three steps are below. Once you know how to add an extension, look through the [hooks](http://docs.comfy.org/javascript_hooks) available to get your code called, a description of various [Comfy objects](http://docs.comfy.org/javascript_objects_and_hijacking) you might need, or jump straight to some [example code snippets](http://docs.comfy.org/javascript_examples).

### [​](http://docs.comfy.org#exporting-web-directory) Exporting `WEB_DIRECTORY`

The Comfy web client can be extended by creating a subdirectory in your custom node directory, conventionally called `js`, and exporting `WEB_DIRECTORY` - so your `__init_.py` will include something like:

```python
WEB_DIRECTORY = "./js"
__all__ = ["NODE_CLASS_MAPPINGS", "NODE_DISPLAY_NAME_MAPPINGS", "WEB_DIRECTORY"]
```

### [​](http://docs.comfy.org#including-js-files) Including `.js` files

All Javascript `.js` files will be loaded by the browser as the Comfy webpage loads. You don’t need to specify the file your extension is in.

*Only* `.js` files will be added to the webpage. Other resources (such as `.css` files) can be accessed at `extensions/custom_node_subfolder/the_file.css` and added programmatically.

That path does *not* include the name of the subfolder. The value of `WEB_DIRECTORY` is inserted by the server.

### [​](http://docs.comfy.org#registering-an-extension) Registering an extension

The basic structure of an extension follows is to import the main Comfy `app` object, and call `app.registerExtension`, passing a dictionary that contains a unique `name`, and one or more functions to be called by hooks in the Comfy code.

A complete, trivial, and annoying, extension might look like this:

```javascript
import { app } from "../../scripts/app.js";
app.registerExtension({ 
	name: "a.unique.name.for.a.useless.extension",
	async setup() { 
		alert("Setup complete!")
	},
})
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/js/javascript_overview.mdx)

[Previous](http://docs.comfy.org/custom-nodes/backend/tensors)

[Comfy Hooks  
\
Next](http://docs.comfy.org/custom-nodes/js/javascript_hooks)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Extending the Comfy Client](http://docs.comfy.org#extending-the-comfy-client)
- [Exporting WEB\_DIRECTORY](http://docs.comfy.org#exporting-web-directory)
- [Including .js files](http://docs.comfy.org#including-js-files)
- [Registering an extension](http://docs.comfy.org#registering-an-extension)

<!-- END Get_Started/custom-nodes/js/javascript_overview.md -->


<!-- BEGIN Get_Started/custom-nodes/js/javascript_settings.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
  
  - [Javascript Extensions](http://docs.comfy.org/custom-nodes/js/javascript_overview)
  - [Comfy Hooks](http://docs.comfy.org/custom-nodes/js/javascript_hooks)
  - [Comfy Objects](http://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking)
  - [Settings](http://docs.comfy.org/custom-nodes/js/javascript_settings)
  - [Annotated Examples](http://docs.comfy.org/custom-nodes/js/javascript_examples)
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Settings

# Settings

You can provide a settings object to ComfyUI that will show up when the user opens the ComfyUI settings panel.

## [​](http://docs.comfy.org#basic-operation) Basic operation

### [​](http://docs.comfy.org#add-a-setting) Add a setting

```javascript
import { app } from "../../scripts/app.js";

app.registerExtension({
    name: "My Extension",
    settings: [
        {
            id: "example.boolean",
            name: "Example boolean setting",
            type: "boolean",
            defaultValue: false,
        },
    ],
});
```

The `id` must be unique across all extensions and will be used to fetch values.

If you do not [provide a category](http://docs.comfy.org/_sites/docs.comfy.org/custom-nodes/js/javascript_settings#categories), then the `id` will be split by `.` to determine where it appears in the settings panel.

- If your `id` doesn’t contain any `.` then it will appear in the “Other” category and your `id` will be used as the section heading.
- If your `id` contains at least one `.` then the leftmost part will be used as the setting category and the second part will be used as the section heading. Any further parts are ignored.

### [​](http://docs.comfy.org#read-a-setting) Read a setting

```javascript
import { app } from "../../scripts/app.js";

if (app.extensionManager.setting.get('example.boolean')) {
    console.log("Setting is enabled.");
} else {
    console.log("Setting is disabled.");
}
```

### [​](http://docs.comfy.org#react-to-changes) React to changes

The `onChange()` event handler will be called as soon as the user changes the setting in the settings panel.

This will also be called when the extension is registered, on every page load.

```javascript
{
    id: "example.boolean",
    name: "Example boolean setting",
    type: "boolean",
    defaultValue: false,
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### [​](http://docs.comfy.org#write-a-setting) Write a setting

```javascript
import { app } from "../../scripts/app.js";

try {
    await app.extensionManager.setting.set("example.boolean", true);
} catch (error) {
    console.error(`Error changing setting: ${error}`);
}
```

### [​](http://docs.comfy.org#extra-configuration) Extra configuration

The setting types are based on [PrimeVue](https://primevue.org/) components. Props described in the PrimeVue documentation can be defined for ComfyUI settings by adding them in an `attrs` field.

For instance, this adds increment/decrement buttons to a number input:

```javascript
{
    id: "example.number",
    name: "Example number setting",
    type: "number",
    defaultValue: 0,
    attrs: {
        showButtons: true,
    },
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

## [​](http://docs.comfy.org#types) Types

### [​](http://docs.comfy.org#boolean) Boolean

This shows an on/off toggle.

Based on the [ToggleSwitch PrimeVue component](https://primevue.org/toggleswitch/).

```javascript
{
    id: "example.boolean",
    name: "Example boolean setting",
    type: "boolean",
    defaultValue: false,
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### [​](http://docs.comfy.org#text) Text

This is freeform text.

Based on the [InputText PrimeVue component](https://primevue.org/inputtext/).

```javascript
{
    id: "example.text",
    name: "Example text setting",
    type: "text",
    defaultValue: "Foo",
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### [​](http://docs.comfy.org#number) Number

This for entering numbers.

To allow decimal places, set the `maxFractionDigits` attribute to a number greater than zero.

Based on the [InputNumber PrimeVue component](https://primevue.org/inputnumber/).

```javascript
{
    id: "example.number",
    name: "Example number setting",
    type: "number",
    defaultValue: 42,
    attrs: {
        showButtons: true,
        maxFractionDigits: 1,
    },
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### [​](http://docs.comfy.org#slider) Slider

This lets the user enter a number directly or via a slider.

Based on the [Slider PrimeVue component](https://primevue.org/slider/). Ranges are not supported.

```javascript
{
    id: "example.slider",
    name: "Example slider setting",
    type: "slider",
    attrs: {
        min: -10,
        max: 10,
        step: 0.5,
    },
    defaultValue: 0,
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### [​](http://docs.comfy.org#combo) Combo

This lets the user pick from a drop-down list of values.

You can provide options either as a plain string or as an object with `text` and `value` fields. If you only provide a plain string, then it will be used for both.

You can let the user enter freeform text by supplying the `editable: true` attribute, or search by supplying the `filter: true` attribute.

Based on the [Select PrimeVue component](https://primevue.org/select/). Groups are not supported.

```javascript
{
    id: "example.combo",
    name: "Example combo setting",
    type: "combo",
    defaultValue: "first",
    options: [
        { text: "My first option", value: "first" },
        "My second option",
    ],
    attrs: {
        editable: true,
        filter: true,
    },
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### [​](http://docs.comfy.org#color) Color

This lets the user select a color from a color picker or type in a hex reference.

Note that the format requires six full hex digits - three digit shorthand does not work.

Based on the [ColorPicker PrimeVue component](https://primevue.org/colorpicker/).

```javascript
{
    id: "example.color",
    name: "Example color setting",
    type: "color",
    defaultValue: "ff0000",
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### [​](http://docs.comfy.org#image) Image

This lets the user upload an image.

The setting will be saved as a [data URL](https://developer.mozilla.org/en-US/docs/Web/URI/Schemes/data).

Based on the [FileUpload PrimeVue component](https://primevue.org/fileupload/).

```javascript
{
    id: "example.image",
    name: "Example image setting",
    type: "image",
    onChange: (newVal, oldVal) => {
        console.log(`Setting was changed from ${oldVal} to ${newVal}`);
    },
}
```

### [​](http://docs.comfy.org#hidden) Hidden

Hidden settings aren’t displayed in the settings panel, but you can read and write to them from your code.

```javascript
{
    id: "example.hidden",
    name: "Example hidden setting",
    type: "hidden",
}
```

## [​](http://docs.comfy.org#other) Other

### [​](http://docs.comfy.org#categories) Categories

You can specify the categorisation of your setting separately to the `id`. This means you can change the categorisation and naming without changing the `id` and losing the values that have already been set by users.

```javascript
{
    id: "example.boolean",
    name: "Example boolean setting",
    type: "boolean",
    defaultValue: false,
    category: ["Category name", "Section heading", "Setting label"],
}
```

### [​](http://docs.comfy.org#tooltips) Tooltips

You can add extra contextual help with the `tooltip` field. This adds a small ℹ︎ icon after the field name that will show the help text when the user hovers over it.

```javascript
{
    id: "example.boolean",
    name: "Example boolean setting",
    type: "boolean",
    defaultValue: false,
    tooltip: "This is some helpful information",
}
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/js/javascript_settings.mdx)

[Previous](http://docs.comfy.org/custom-nodes/js/javascript_objects_and_hijacking)

[Annotated Examples  
\
Next](http://docs.comfy.org/custom-nodes/js/javascript_examples)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Basic operation](http://docs.comfy.org#basic-operation)
- [Add a setting](http://docs.comfy.org#add-a-setting)
- [Read a setting](http://docs.comfy.org#read-a-setting)
- [React to changes](http://docs.comfy.org#react-to-changes)
- [Write a setting](http://docs.comfy.org#write-a-setting)
- [Extra configuration](http://docs.comfy.org#extra-configuration)
- [Types](http://docs.comfy.org#types)
- [Boolean](http://docs.comfy.org#boolean)
- [Text](http://docs.comfy.org#text)
- [Number](http://docs.comfy.org#number)
- [Slider](http://docs.comfy.org#slider)
- [Combo](http://docs.comfy.org#combo)
- [Color](http://docs.comfy.org#color)
- [Image](http://docs.comfy.org#image)
- [Hidden](http://docs.comfy.org#hidden)
- [Other](http://docs.comfy.org#other)
- [Categories](http://docs.comfy.org#categories)
- [Tooltips](http://docs.comfy.org#tooltips)

<!-- END Get_Started/custom-nodes/js/javascript_settings.md -->


<!-- BEGIN Get_Started/custom-nodes/overview.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Overview

# Overview

Custom nodes allow you to implement new features and share them with the wider community.

A custom node is like any Comfy node: it takes input, does something to it, and produces an output. While some custom nodes perform highly complex tasks, many just do one thing. Here’s an example of a simple node that takes an image and inverts it.

## [​](http://docs.comfy.org#client-server-model) Client-Server Model

Comfy runs in a client-server model. The server, written in Python, handles all the real work: data-processing, models, image diffusion etc. The client, written in Javascript, handles the user interface.

Comfy can also be used in API mode, in which a workflow is sent to the server by a non-Comfy client (such as another UI, or a command line script).

Custom nodes can be placed into one of four categories:

### [​](http://docs.comfy.org#server-side-only) Server side only

The majority of Custom Nodes run purely on the server side, by defining a Python class that specifies the input and output types, and provides a function that can be called to process inputs and produce an output.

### [​](http://docs.comfy.org#client-side-only) Client side only

A few Custom Nodes provide a modification to the client UI, but do not add core functionality. Despite the name, they may not even add new nodes to the system.

### [​](http://docs.comfy.org#independent-client-and-server) Independent Client and Server

Custom nodes may provide additional server features, and additional (related) UI features (such as a new widget to deal with a new data type). In most cases, communication between the client and server can be handled by the Comfy data flow control.

### [​](http://docs.comfy.org#connected-client-and-server) Connected Client and Server

In a small number of cases, the UI features and the server need to interact with each other directly.

Any node that requires Client-Server communication will not be compatible with use through the API.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/overview.mdx)

[Previous](http://docs.comfy.org/comfy-cli/reference)

[Getting Started  
\
Next](http://docs.comfy.org/custom-nodes/walkthrough)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Client-Server Model](http://docs.comfy.org#client-server-model)
- [Server side only](http://docs.comfy.org#server-side-only)
- [Client side only](http://docs.comfy.org#client-side-only)
- [Independent Client and Server](http://docs.comfy.org#independent-client-and-server)
- [Connected Client and Server](http://docs.comfy.org#connected-client-and-server)

<!-- END Get_Started/custom-nodes/overview.md -->


<!-- BEGIN Get_Started/custom-nodes/tips.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Tips

# Tips

### [​](http://docs.comfy.org#recommended-development-lifecycle) Recommended Development Lifecycle

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/tips.mdx)

[Previous](http://docs.comfy.org/custom-nodes/workflow_templates)

[Overview  
\
Next](http://docs.comfy.org/registry/overview)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Recommended Development Lifecycle](http://docs.comfy.org#recommended-development-lifecycle)

<!-- END Get_Started/custom-nodes/tips.md -->


<!-- BEGIN Get_Started/custom-nodes/walkthrough.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Getting Started

# Getting Started

This page will take you step-by-step through the process of creating a custom node.

Our example will take a batch of images, and return one of the images. Initially, the node will return the image which is, on average, the lightest in color; we’ll then extend it to have a range of selection criteria, and then finally add some client side code.

This page assumes very little knowledge of Python or Javascript.

After this walkthrough, dive into the details of [backend code](http://docs.comfy.org/backend/server_overview), and [frontend code](http://docs.comfy.org/backend/server_overview).

## [​](http://docs.comfy.org#write-a-basic-node) Write a basic node

### [​](http://docs.comfy.org#prerequisites) Prerequisites

- A working ComfyUI [installation](http://docs.comfy.org/installation/manual_install). For development, we recommend installing ComfyUI manually.
- A working comfy-cli [installation](http://docs.comfy.org/comfy-cli/getting-started).

### [​](http://docs.comfy.org#setting-up) Setting up

```bash
cd ComfyUI/custom_nodes
comfy node scaffold
```

After answering a few questions, you’ll have a new directory set up.

```bash
 ~  % comfy node scaffold
You've downloaded .cookiecutters/cookiecutter-comfy-extension before. Is it okay to delete and re-download it? [y/n] (y): y
  [1/9] full_name (): Comfy
  [2/9] email (you@gmail.com): me@comfy.org
  [3/9] github_username (your_github_username): comfy
  [4/9] project_name (My Custom Nodepack): FirstComfyNode
  [5/9] project_slug (firstcomfynode): 
  [6/9] project_short_description (A collection of custom nodes for ComfyUI): 
  [7/9] version (0.0.1): 
  [8/9] Select open_source_license
    1 - GNU General Public License v3
    2 - MIT license
    3 - BSD license
    4 - ISC license
    5 - Apache Software License 2.0
    6 - Not open source
    Choose from [1/2/3/4/5/6] (1): 1
  [9/9] include_web_directory_for_custom_javascript [y/n] (n): y
Initialized empty Git repository in firstcomfynode/.git/
✓ Custom node project created successfully!
```

### [​](http://docs.comfy.org#defining-the-node) Defining the node

Add the following code to the end of `src/nodes.py`:

src/nodes.py

```python
class ImageSelector:
    CATEGORY = "example"
    @classmethod    
    def INPUT_TYPES(s):
        return { "required":  { "images": ("IMAGE",), } }
    RETURN_TYPES = ("IMAGE",)
    FUNCTION = "choose_image"
```

The basic structure of a custom node is described in detail [here](http://docs.comfy.org/custom-nodes/backend/server_overview).

A custom node is defined using a Python class, which must include these four things: `CATEGORY`, which specifies where in the add new node menu the custom node will be located, `INPUT_TYPES`, which is a class method defining what inputs the node will take (see [later](http://docs.comfy.org/custom-nodes/backend/server_overview#input-types) for details of the dictionary returned), `RETURN_TYPES`, which defines what outputs the node will produce, and `FUNCTION`, the name of the function that will be called when the node is executed.

Notice that the data type for input and output is `IMAGE` (singular) even though we expect to receive a batch of images, and return just one. In Comfy, `IMAGE` means image batch, and a single image is treated as a batch of size 1.

### [​](http://docs.comfy.org#the-main-function) The main function

The main function, `choose_image`, receives named arguments as defined in `INPUT_TYPES`, and returns a `tuple` as defined in `RETURN_TYPES`. Since we’re dealing with images, which are internally stored as `torch.Tensor`,

```python
import torch
```

Then add the function to your class. The datatype for image is `torch.Tensor` with shape `[B,H,W,C]`, where `B` is the batch size and `C` is the number of channels - 3, for RGB. If we iterate over such a tensor, we will get a series of `B` tensors of shape `[H,W,C]`. The `.flatten()` method turns this into a one dimensional tensor, of length `H*W*C`, `torch.mean()` takes the mean, and `.item()` turns a single value tensor into a Python float.

```python
def choose_image(self, images):
    brightness = list(torch.mean(image.flatten()).item() for image in images)
    brightest = brightness.index(max(brightness))
    result = images[brightest].unsqueeze(0)
    return (result,)
```

Notes on those last two lines:

- `images[brightest]` will return a Tensor of shape `[H,W,C]`. `unsqueeze` is used to insert a (length 1) dimension at, in this case, dimension zero, to give us `[B,H,W,C]` with `B=1`: a single image.
- in `return (result,)`, the trailing comma is essential to ensure you return a tuple.

### [​](http://docs.comfy.org#register-the-node) Register the node

To make Comfy recognize the new node, it must be available at the package level. Modify the `NODE_CLASS_MAPPINGS` variable at the end of `src/nodes.py`. You must restart ComfyUI to see any changes.

src/nodes.py

```python

NODE_CLASS_MAPPINGS = {
    "Example" : Example,
    "Image Selector" : ImageSelector,
}

# Optionally, you can rename the node in the `NODE_DISPLAY_NAME_MAPPINGS` dictionary.
NODE_DISPLAY_NAME_MAPPINGS = {
    "Example": "Example Node",
    "Image Selector": "Image Selector",
}
```

For a detailed explanation of how ComfyUI discovers and loads custom nodes, see the [node lifecycle documentation](http://docs.comfy.org/custom-nodes/backend/lifecycle).

## [​](http://docs.comfy.org#add-some-options) Add some options

That node is maybe a bit boring, so we might add some options; a widget that allows you to choose the brightest image, or the reddest, bluest, or greenest. Edit your `INPUT_TYPES` to look like:

```python
@classmethod    
def INPUT_TYPES(s):
    return { "required":  { "images": ("IMAGE",), 
                            "mode": (["brightest", "reddest", "greenest", "bluest"],)} }
```

Then update the main function. We’ll use a fairly naive definition of ‘reddest’ as being the average `R` value of the pixels divided by the average of all three colors. So:

```python
def choose_image(self, images, mode):
    batch_size = images.shape[0]
    brightness = list(torch.mean(image.flatten()).item() for image in images)
    if (mode=="brightest"):
        scores = brightness
    else:
        channel = 0 if mode=="reddest" else (1 if mode=="greenest" else 2)
        absolute = list(torch.mean(image[:,:,channel].flatten()).item() for image in images)
        scores = list( absolute[i]/(brightness[i]+1e-8) for i in range(batch_size) )
    best = scores.index(max(scores))
    result = images[best].unsqueeze(0)
    return (result,)
```

## [​](http://docs.comfy.org#tweak-the-ui) Tweak the UI

Maybe we’d like a bit of visual feedback, so let’s send a little text message to be displayed.

### [​](http://docs.comfy.org#send-a-message-from-server) Send a message from server

This requires two lines to be added to the Python code:

```python
from server import PromptServer
```

and, at the end of the `choose_image` method, add a line to send a message to the front end (`send_sync` takes a message type, which should be unique, and a dictionary)

```python
PromptServer.instance.send_sync("example.imageselector.textmessage", {"message":f"Picked image {best+1}"})
return (result,)
```

### [​](http://docs.comfy.org#write-a-client-extension) Write a client extension

To add some Javascript to the client, create a subdirectory, `web/js` in your custom node directory, and modify the end of `__init__.py` to tell Comfy about it by exporting `WEB_DIRECTORY`:

```python
WEB_DIRECTORY = "./web/js"
__all__ = ['NODE_CLASS_MAPPINGS', 'WEB_DIRECTORY']
```

The client extension is saved as a `.js` file in the `web/js` subdirectory, so create `image_selector/web/js/imageSelector.js` with the code below. (For more, see [client side coding](http://docs.comfy.org/js/javascript_overview)).

```javascript
app.registerExtension({
	name: "example.imageselector",
    async setup() {
        function messageHandler(event) { alert(event.detail.message); }
        app.api.addEventListener("example.imageselector.textmessage", messageHandler);
    },
})
```

All we’ve done is register an extension and add a listener for the message type we are sending in the `setup()` method. This reads the dictionary we sent (which is stored in `event.detail`).

Stop the Comfy server, start it again, reload the webpage, and run your workflow.

### [​](http://docs.comfy.org#the-complete-example) The complete example

The complete example is available [here](https://gist.github.com/robinjhuang/fbf54b7715091c7b478724fc4dffbd03). You can download the example workflow [JSON file](https://github.com/Comfy-Org/docs/blob/main/public/workflow.json) or view it below:

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/walkthrough.mdx)

[Previous](http://docs.comfy.org/custom-nodes/overview)

[PropertiesProperties of a custom node  
\
Next](http://docs.comfy.org/custom-nodes/backend/server_overview)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Write a basic node](http://docs.comfy.org#write-a-basic-node)
- [Prerequisites](http://docs.comfy.org#prerequisites)
- [Setting up](http://docs.comfy.org#setting-up)
- [Defining the node](http://docs.comfy.org#defining-the-node)
- [The main function](http://docs.comfy.org#the-main-function)
- [Register the node](http://docs.comfy.org#register-the-node)
- [Add some options](http://docs.comfy.org#add-some-options)
- [Tweak the UI](http://docs.comfy.org#tweak-the-ui)
- [Send a message from server](http://docs.comfy.org#send-a-message-from-server)
- [Write a client extension](http://docs.comfy.org#write-a-client-extension)
- [The complete example](http://docs.comfy.org#the-complete-example)

<!-- END Get_Started/custom-nodes/walkthrough.md -->


<!-- BEGIN Get_Started/custom-nodes/workflow_templates.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Workflow templates

# Workflow templates

If you have example workflow files associated with your custom nodes then ComfyUI can show these to the user in the template browser (`Workflow`/`Browse Templates` menu). Workflow templates are a great way to support people getting started with your nodes.

All you have to do as a node developer is to create an `example_workflows` folder and place the `json` files there. Optionally you can place `jpg` files with the same name to be shown as the template thumbnail.

Under the hood ComfyUI statically serves these files along with an endpoint (`/api/workflow_templates`) that returns the collection of workflow templates.

## [​](http://docs.comfy.org#example) Example

Under `ComfyUI-MyCustomNodeModule/example_workflows/` directory:

- `My_example_workflow_1.json`
- `My_example_workflow_1.jpg`
- `My_example_workflow_2.json`

In this example ComfyUI’s template browser shows a category called `ComfyUI-MyCustomNodeModule` with two items, one of which has a thumbnail.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/custom-nodes/workflow_templates.mdx)

[Previous](http://docs.comfy.org/custom-nodes/js/javascript_examples)

[Tips  
\
Next](http://docs.comfy.org/custom-nodes/tips)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Example](http://docs.comfy.org#example)

<!-- END Get_Started/custom-nodes/workflow_templates.md -->


<!-- BEGIN Get_Started/essentials/comfyui-server/comms_messages.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Messages

# Messages

## [​](http://docs.comfy.org#messages) Messages

During execution (or when the state of the queue changes), the `PromptExecutor` sends messages back to the client through the `send_sync` method of `PromptServer`.

These messages are received by a socket event listener defined in `api.js` (at time of writing around line 90, or search for `this.socket.addEventListener`), which creates a `CustomEvent` object for any known message type, and dispatches it to any registered listeners.

An extension can register to receive events (normally done in the `setup()` function) following the standard Javascript idiom:

```javascript
api.addEventListener(message_type, messageHandler);
```

If the `message_type` is not one of the built in ones, it will be added to the list of known message types automatically. The message `messageHandler` will be called with a `CustomEvent` object, which extends the event raised by the socket to add a `.detail` property, which is a dictionary of the data sent by the server. So usage is generally along the lines of:

```javascript
function messageHandler(event) {
    if (event.detail.node == aNodeIdThatIsInteresting) {
        // do something with event.detail.other_things
    }
}
```

### [​](http://docs.comfy.org#built-in-message-types) Built in message types

During execution (or when the state of the queue changes), the `PromptExecutor` sends the following messages back to the client through the `send_sync` method of `PromptServer`. An extension can register as a listener for any of these.

eventwhendata`execution_start`When a prompt is about to run`prompt_id``execution_error`When an error occurs during execution`prompt_id`, plus additional information`execution_interrupted`When execution is stopped by a node raising `InterruptProcessingException``prompt_id`, `node_id`, `node_type` and `executed` (a list of executed nodes)`execution_cached`At the start of execution`prompt_id`, `nodes` (a list of nodes which are being skipped because their cached outputs can be used)`executing`When a new node is about to be executed`node` (node id or `None` to indicate completion), `prompt_id``executed`When a node returns a ui element`node` (node id), `prompt_id`, `output``progress`During execution of a node that implements the required hook`node` (node id), `prompt_id`, `value`, `max``status`When the state of the queue changes`exec_info`, a dictionary holding `queue_remaining`, the number of entries in the queue

### [​](http://docs.comfy.org#using-executed) Using executed

Despite the name, an `executed` message is not sent whenever a node completes execution (unlike `executing`), but only when the node returns a ui update.

To do this, the main function needs to return a dictionary instead of a tuple:

```python
# at the end of my main method
        return { "ui":a_new_dictionary, "result": the_tuple_of_output_values }
```

`a_new_dictionary` will then be sent as the value of `output` in an `executed` message. The `result` key can be omitted if the node has no outputs (see, for instance, the code for `SaveImage` in `nodes.py`)

### [​](http://docs.comfy.org#custom-message-types) Custom message types

As indicated above, on the client side, a custom message type can be added simply by registering as a listener for a unique message type name.

```javascript
api.addEventListener("my.custom.message", messageHandler);
```

On the server, the code is equally simple:

```python
from server import PromptServer
# then, in your main execution function (normally)
        PromptServer.instance.send_sync("my.custom.message", a_dictionary)
```

#### [​](http://docs.comfy.org#getting-node-id) Getting node\_id

Most of the built-in messages include the current node id in the value of `node`. It’s likely that you will want to do the same.

The node\_id is available on the server side through a hidden input, which is obtained with the `hidden` key in the `INPUT_TYPES` dictionary:

```python
    @classmethod    
    def INPUT_TYPES(s):
        return {"required" : { }, # whatever your required inputs are 
                "hidden": { "node_id": "UNIQUE_ID" } } # Add the hidden key

    def my_main_function(self, required_inputs, node_id):
        # do some things
        PromptServer.instance.send_sync("my.custom.message", {"node": node_id, "other_things": etc})
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/comfyui-server/comms_messages.mdx)

[Previous](http://docs.comfy.org/essentials/comfyui-server/comms_overview)

[Execution Model Inversion Guide  
\
Next](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Messages](http://docs.comfy.org#messages)
- [Built in message types](http://docs.comfy.org#built-in-message-types)
- [Using executed](http://docs.comfy.org#using-executed)
- [Custom message types](http://docs.comfy.org#custom-message-types)
- [Getting node\_id](http://docs.comfy.org#getting-node-id)

<!-- END Get_Started/essentials/comfyui-server/comms_messages.md -->


<!-- BEGIN Get_Started/essentials/comfyui-server/comms_overview.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Server Overview

# Server Overview

## [​](http://docs.comfy.org#overview) Overview

The Comfy server runs on top of the [aiohttp framework](https://docs.aiohttp.org/), which in turn uses [asyncio](https://pypi.org/project/asyncio/).

Messages from the server to the client are sent by socket messages through the `send_sync` method of the server, which is an instance of `PromptServer` (defined in `server.py`). They are processed by a socket event listener registered in `api.js`. See [messages](http://docs.comfy.org/comms_messages).

Messages from the client to the server are sent by the `api.fetchApi()` method defined in `api.js`, and are handled by http routes defined by the server. See [routes](http://docs.comfy.org/comms_routes).

The client submits the whole workflow (widget values and all) when you queue a request. The server does not receive any changes you make after you send a request to the queue. If you want to modify server behavior during execution, you’ll need routes.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/comfyui-server/comms_overview.mdx)

[Messages  
\
Next](http://docs.comfy.org/essentials/comfyui-server/comms_messages)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Overview](http://docs.comfy.org#overview)

<!-- END Get_Started/essentials/comfyui-server/comms_overview.md -->


<!-- BEGIN Get_Started/essentials/comfyui-server/execution_model_inversion_guide.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Execution Model Inversion Guide

# Execution Model Inversion Guide

[PR #2666](https://github.com/comfyanonymous/ComfyUI/pull/2666) inverts the execution model from a back-to-front recursive model to a front-to-back topological sort. While most custom nodes should continue to “just work”, this page is intended to serve as a guide for custom node creators to the things that *could* break.

## [​](http://docs.comfy.org#breaking-changes) Breaking Changes

### [​](http://docs.comfy.org#monkey-patching) Monkey Patching

Any code that monkey patched the execution model is likely to stop working. Note that the performance of execution with this PR exceeds that with the most popular monkey patches, so many of them will be unnecessary.

### [​](http://docs.comfy.org#optional-input-validation) Optional Input Validation

Prior to this PR, only nodes that were connected to outputs exclusively through a string of `"required"` inputs were actually validated. If you had custom nodes that were only ever connected to `"optional"` inputs, you previously wouldn’t have been seeing that they failed validation.

If your nodes’ outputs could already be connected to `"required"` inputs, it is unlikely that anything in this section applies to you. It will primarily apply to custom node authors who use custom types and exclusively use `"optional"` inputs.

Here are some of the things that could cause you to fail validation along with recommended solutions:

- Use of reserved [Additional Parameters](http://docs.comfy.org/custom-nodes/backend/datatypes#additional-parameters) like `min` and `max` on types that aren’t comparable (e.g. dictionaries) in order to configure custom widgets.
  
  - Change the additional parameters used to non-reserved keys like `uiMin` and `uiMax`. *(Recommended Solution)*
    
    ```python
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "my_size": ("VEC2", {"uiMin": 0.0, "uiMax": 1.0}),
            }
        }
    ```
  - Define a custom [VALIDATE\_INPUTS](http://docs.comfy.org/custom-nodes/backend/server_overview#validate-inputs) function with this input so validation of it is skipped. *(Quick Solution)*
    
    ```python
    @classmethod
    def VALIDATE_INPUTS(cls, my_size):
        return True
    ```
- Use of composite types (e.g. `CUSTOM_A,CUSTOM_B`)
  
  - (When used as output) Define and use a wrapper like `MakeSmartType` [seen here in the PR’s unit tests](https://github.com/comfyanonymous/ComfyUI/pull/2666/files#diff-714643f1fdb6f8798c45f77ab10d212ca7f41dd71bbe55069f1f9f146a8f0cb9R2)
    
    ```python
    class MyCustomNode:
    
        @classmethod
        def INPUT_TYPES(cls):
            return {
                "required": {
                    "input": (MakeSmartType("FOO,BAR"), {}),
                }
            }
    
        RETURN_TYPES = (MakeSmartType("FOO,BAR"),)
    
        # ...
    ```
  - (When used as input) Define a custom[VALIDATE\_INPUTS](http://docs.comfy.org/custom-nodes/backend/server_overview#validate-inputs) function that takes a `input_types` argument so type validation is skipped.
    
    ```python
    @classmethod
    def VALIDATE_INPUTS(cls, input_types):
        return True
    ```
  - (Supports both, convenient) Define and use the `@VariantSupport` decorator [seen here in the PR’s unit tests](https://github.com/comfyanonymous/ComfyUI/pull/2666/files#diff-714643f1fdb6f8798c45f77ab10d212ca7f41dd71bbe55069f1f9f146a8f0cb9R15)
    
    ```python
    @VariantSupport
    class MyCustomNode:
    
        @classmethod
        def INPUT_TYPES(cls):
            return {
                "required": {
                    "input": ("FOO,BAR", {}),
                }
            }
        
        RETURN_TYPES = (MakeSmartType("FOO,BAR"),)
    
        # ...
    ```
- The use of lists (e.g. `[1, 2, 3]`) as constants in the graph definition (e.g. to represent a const `VEC3` input). This would have required a front-end extension before. Previously, lists of size exactly `2` would have failed anyway — they would have been treated as broken links.
  
  - Wrap the lists in a dictionary like `{ "value": [1, 2, 3] }`

### [​](http://docs.comfy.org#execution-order) Execution Order

Execution order has always changed depending on which nodes happen to have which IDs, but it may now change depending on which values are cached as well. In general, the execution order should be considered non-deterministic and subject to change (beyond what is enforced by the graph’s structure).

Don’t rely on the execution order.

*HIC SUNT DRACONES*

## [​](http://docs.comfy.org#new-functionality) New Functionality

### [​](http://docs.comfy.org#validation-changes) Validation Changes

A number of features were added to the `VALIDATE_INPUTS` function in order to lessen the impact of the [Optional Input Validation](http://docs.comfy.org/_sites/docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide#optional-input-validation) mentioned above.

- Default validation will now be skipped for inputs which are received by the `VALIDATE_INPUTS` function.
- The `VALIDATE_INPUTS` function can now take `**kwargs` which causes all inputs to be treated as validated by the node creator.
- The `VALIDATE_INPUTS` function can take an input named `input_types`. This input will be a dict mapping each input (connected via a link) to the type of the connected output. When this argument exists, type validation for the node’s inputs is skipped.

You can read more at [VALIDATE\_INPUTS](http://docs.comfy.org/custom-nodes/backend/server_overview#validate-inputs).

### [​](http://docs.comfy.org#lazy-evaluation) Lazy Evaluation

Inputs can be evaluated lazily (i.e. you can wait to see if they are needed before evaluating the attached node and all its ancestors). See [Lazy Evaluation](http://docs.comfy.org/custom-nodes/backend/lazy_evaluation) for more information.

### [​](http://docs.comfy.org#node-expansion) Node Expansion

At runtime, nodes can expand into a subgraph of nodes. This is what allows loops to be implemented (via tail-recursion). See [Node Expansion](http://docs.comfy.org/custom-nodes/backend/expansion) for more information.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/comfyui-server/execution_model_inversion_guide.mdx)

[Previous](http://docs.comfy.org/essentials/comfyui-server/comms_messages)

[Getting Started  
\
Next](http://docs.comfy.org/comfy-cli/getting-started)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Breaking Changes](http://docs.comfy.org#breaking-changes)
- [Monkey Patching](http://docs.comfy.org#monkey-patching)
- [Optional Input Validation](http://docs.comfy.org#optional-input-validation)
- [Execution Order](http://docs.comfy.org#execution-order)
- [New Functionality](http://docs.comfy.org#new-functionality)
- [Validation Changes](http://docs.comfy.org#validation-changes)
- [Lazy Evaluation](http://docs.comfy.org#lazy-evaluation)
- [Node Expansion](http://docs.comfy.org#node-expansion)

<!-- END Get_Started/essentials/comfyui-server/execution_model_inversion_guide.md -->


<!-- BEGIN Get_Started/essentials/core-concepts/dependencies.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Dependencies

# Dependencies

Understand dependencies in ComfyUI

## [​](http://docs.comfy.org#a-workflow-file-depends-on-other-files) A workflow file depends on other files

We often obtain various workflow files from the community, but frequently find that the workflow cannot run directly after loading. This is because a workflow file depends on other files besides the workflow itself, such as media asset inputs, models, custom nodes, related Python dependencies, etc. ComfyUI workflows can only run normally when all relevant dependencies are satisfied.

ComfyUI workflow dependencies mainly fall into the following categories:

- Assets (media files including audio, video, images, and other inputs)
- Custom nodes
- Python dependencies
- Models (such as Stable Diffusion models, etc.)

## [​](http://docs.comfy.org#assets) Assets

An AI model is an example of an ***asset***. In media production, an asset is some media file that supplies input data. For example, a video editing program operates on movie files stored on disk. The editing program’s project file holds links to these movie file assets, allowing non-destructive editing that doesn’t alter the original movie files.

ComfyUI works the same way. A workflow can only run if all of the required assets are found and loaded. Generative AI models, images, movies, and sounds are some examples of assets that a workflow might depend upon. These are therefore known as ***dependent assets*** or ***asset dependencies***.

## [​](http://docs.comfy.org#custom-nodes) Custom Nodes

Custom nodes are an important component of ComfyUI that extend its functionality. They are created by the community and can be installed to add new capabilities to your workflows.

## [​](http://docs.comfy.org#python-dependencies) Python Dependencies

ComfyUI is a Python-based project. We build a standalone Python environment to run ComfyUI, and all related dependencies are installed in this isolated Python environment.

### [​](http://docs.comfy.org#comfyui-dependencies) ComfyUI Dependencies

You can view ComfyUI’s current dependencies in the [requirements.txt](https://github.com/comfyanonymous/ComfyUI/blob/master/requirements.txt) file:

```text
comfyui-frontend-package==1.14.5
torch
torchsde
torchvision
torchaudio
numpy>=1.25.0
einops
transformers>=4.28.1
tokenizers>=0.13.3
sentencepiece
safetensors>=0.4.2
aiohttp>=3.11.8
yarl>=1.18.0
pyyaml
Pillow
scipy
tqdm
psutil

#non essential dependencies:
kornia>=0.7.1
spandrel
soundfile
av
```

As ComfyUI evolves, we may adjust dependencies accordingly, such as adding new dependencies or removing ones that are no longer needed. So if you use Git to update ComfyUI, you need to run the following command in the corresponding environment after pulling the latest updates:

```bash
pip install -r requirements.txt
```

This ensures that ComfyUI’s dependencies are up to date for proper operation. You can also modify specific package dependency versions to upgrade or downgrade certain dependencies.

Additionally, ComfyUI’s frontend [ComfyUI\_frontend](https://github.com/Comfy-Org/ComfyUI_frontend) is currently maintained as a separate project. We update the `comfyui-frontend-package` dependency version after the corresponding version stabilizes. If you need to switch to a different frontend version, you can check the version information [here](https://pypi.org/project/comfyui-frontend-package/#history).

### [​](http://docs.comfy.org#custom-node-dependencies) Custom Node Dependencies

Thanks to the efforts of many authors in the ComfyUI community, we can extend ComfyUI’s functionality by using different custom nodes, enabling impressive creativity.

Typically, each custom node has its own dependencies and a separate `requirements.txt` file. If you use [ComfyUI Manager](https://github.com/ltdrdata/ComfyUI-Manager) to install custom nodes, ComfyUI Manager will usually automatically install the corresponding dependencies.

There are also cases where you need to install dependencies manually. Currently, all custom nodes are installed in the `ComfyUI/custom_nodes` directory.

You need to navigate to the corresponding plugin directory in your ComfyUI Python environment and run `pip install -r requirements.txt` to install the dependencies.

If you’re using the [Windows Portable version](http://docs.comfy.org/installation/comfyui_portable_windows), you can use the following command in the `ComfyUI_windows_portable` directory:

```plaintext
python_embeded\python.exe -m pip install -r ComfyUI\custom_nodes\<custom_node_name>\requirements.txt
```

to install the dependencies for the corresponding node.

### [​](http://docs.comfy.org#dependency-conflicts) Dependency Conflicts

Dependency conflicts are a common issue when using ComfyUI. You might find that after installing or updating a custom node, previously installed custom nodes can no longer be found in ComfyUI’s node library, or error pop-ups appear. One possible reason is dependency conflicts.

There can be many reasons for dependency conflicts, such as:

1. Custom node version locking

Some plugins may fix the exact version of a dependency library (e.g., `open_clip_torch==2.26.1`), while other plugins may require a higher version (e.g., `open_clip_torch>=2.29.0`), making it impossible to satisfy both version requirements simultaneously.

**Solution**: You can try changing the fixed version dependency to a range constraint, such as `open_clip_torch>=2.26.1`, and then reinstall the dependencies to resolve these issues.

2. Environment pollution

During the installation of custom node dependencies, it may overwrite versions of libraries already installed by other plugins. For example, multiple plugins may depend on `PyTorch` but require different CUDA versions, and the later installed plugin will break the existing environment.

**Solutions**:

- You can try manually installing specific versions of dependencies in the Python virtual environment to resolve such issues.
- Or create different Python virtual environments for different plugins to resolve these issues.
- Try installing plugins one by one, restarting ComfyUI after each installation to observe if dependency conflicts occur.

<!--THE END-->

3. Custom node dependency versions incompatible with ComfyUI dependency versions

These types of dependency conflicts may be more difficult to resolve, and you may need to upgrade/downgrade ComfyUI or change the dependency versions of custom nodes to resolve these issues.

**Solution**: These types of dependency conflicts may be more difficult to resolve, and you may need to upgrade/downgrade ComfyUI or change the dependency versions of custom nodes to resolve these issues.

## [​](http://docs.comfy.org#models) Models

Models are a significant asset dependency for ComfyUI. Various custom nodes and workflows are built around specific models, such as the Stable Diffusion series, Flux series, Ltxv, and others. These models are an essential foundation for creation with ComfyUI, so we need to ensure that the models we use are properly available. Typically, our models are saved in the corresponding directory under `ComfyUI/models/`. Of course, you can also create an [extra\_model\_paths.yaml](https://github.com/comfyanonymous/ComfyUI/blob/master/extra_model_paths.yaml.example) by modifying the template to make additional model paths recognized by ComfyUI. This allows multiple ComfyUI instances to share the same model library, reducing disk usage.

## [​](http://docs.comfy.org#software) Software

An advanced application like ComfyUI also has ***software dependencies***. These are libraries of programming code and data that are required for the application to run. Custom nodes are examples of software dependencies. On an even more fundamental level, the Python programming environment is the ultimate dependency for ComfyUI. The correct version of Python is required to run a particular version of ComfyUI. Updates to Python, ComfyUI, and custom nodes can all be handled from the **ComfyUI Manager** window.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/core-concepts/dependencies.mdx)

[Previous](http://docs.comfy.org/essentials/core-concepts/models)

[ShortcutsKeyboard and mouse shortcuts for ComfyUI and related settings  
\
Next](http://docs.comfy.org/interface/shortcuts)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [A workflow file depends on other files](http://docs.comfy.org#a-workflow-file-depends-on-other-files)
- [Assets](http://docs.comfy.org#assets)
- [Custom Nodes](http://docs.comfy.org#custom-nodes)
- [Python Dependencies](http://docs.comfy.org#python-dependencies)
- [ComfyUI Dependencies](http://docs.comfy.org#comfyui-dependencies)
- [Custom Node Dependencies](http://docs.comfy.org#custom-node-dependencies)
- [Dependency Conflicts](http://docs.comfy.org#dependency-conflicts)
- [Models](http://docs.comfy.org#models)
- [Software](http://docs.comfy.org#software)

<!-- END Get_Started/essentials/core-concepts/dependencies.md -->


<!-- BEGIN Get_Started/essentials/core-concepts/links.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Links

# Links

Understand connection links in ComfyUI

As ComfyUI is still in rapid iteration and development, we are continuously improving it every day. Therefore, some operations mentioned in this article may change or be omitted. Please refer to the actual interface. If you find changes in actual operations, it may be due to our iterative updates. You can also fork [this repo](https://github.com/Comfy-Org/docs) and help us improve this documentation.

## [​](http://docs.comfy.org#links-connect-nodes) Links connect nodes

In the terminology of ComfyUI, the lines or curves between nodes are called ***links***. They’re also known as ***connections*** or wires. Links can be displayed in several ways, such as curves, right angles, straight lines, or completely hidden.

You can modify the link style in **Setup Menu** —&gt; **Display (Lite Graph)** —&gt; **Graph** —&gt; **Link Render Mode**.

You can also temporarily hide links in the **Canvas Menu**.

Link display is crucial. Depending on the situation, it may be necessary to see all links. Especially when learning, sharing, or even just understanding workflows, the visibility of links enables users to follow the flow of data through the graph. For packaged workflows that aren’t intended to be altered, it might make sense to hide the links to reduce clutter.

### [​](http://docs.comfy.org#reroute-node) Reroute node

If legibility of the graph structure is important, then link wires can be manually routed in the 2D space of the graph with a tiny node called **Reroute**. Its purpose is to position the beginning and/or end points of link wires to ensure visibility. We can design a workflow so that link wires don’t pass behind nodes, don’t cross other link wires, and so on.

We are also continuously improving the native reroute functionality in litegraph. We recommend using this feature in the future to reorganize connections.

## [​](http://docs.comfy.org#color-coding) Color-coding

The data type of node properties is indicated by color coding of input/output ports and link connection wires. We can always tell which inputs and outputs can be connected to one another by their color. Ports can only be connected to other ports of the same color to ensure matching data types.

Common data types:

Data typeColordiffusion modellavenderCLIP modelyellowVAE modelroseconditioningorangelatent imagepinkpixel imagebluemaskgreennumber (integer or float)light greenmeshbright green

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/core-concepts/links.mdx)

[Previous](http://docs.comfy.org/essentials/core-concepts/properties)

[Mask EditorLearn how to use the Mask Editor in ComfyUI, including settings and usage instructions  
\
Next](http://docs.comfy.org/interface/maskeditor)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Links connect nodes](http://docs.comfy.org#links-connect-nodes)
- [Reroute node](http://docs.comfy.org#reroute-node)
- [Color-coding](http://docs.comfy.org#color-coding)

<!-- END Get_Started/essentials/core-concepts/links.md -->


<!-- BEGIN Get_Started/essentials/core-concepts/models.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Models

# Models

## [​](http://docs.comfy.org#models-are-essential) Models are essential

Models are essential building blocks for media generation workflows. They can be combined and mixed to achieve different creative effects.

The word ***model*** has many different meanings. Here, it means a data file carrying information that is required for a node graph to do its work. Specifically, it’s a data structure that *models* some function. As a verb, to model something means to represent it or provide an example.

The primary example of a model data file in ComfyUI is an AI ***diffusion model***. This is a large set of data that represents the complex relationships among text strings and images, making it possible to translate words into pictures or vice versa. Other examples of common models used for image generation are language models such as CLIP, and upscaling models such as RealESRGAN.

## [​](http://docs.comfy.org#model-files) Model files

Model files are absolutely required for generative media production. Nothing can happen in a workflow if the model files are not found. Models are not included in the ComfyUI installation, but ComfyUI can often automatically download and install missing model files. Many models can be downloaded and installed from the **ComfyUI Manager** window. Models can also be found at websites such as [huggingface.co](https://huggingface.co), [civitai.green](https://civitai.green), and [github.com](https://github.com).

### [​](http://docs.comfy.org#using-models-in-comfyui) Using Models in ComfyUI

1. Download and place them in the ComfyUI program directory
   
   1. Within the **models** folder, you’ll find subfolders for various types of models, such as **checkpoints**
   2. The **ComfyUI Manager** helps to automate the process of searching, downloading, and installing
   3. Restart ComfyUI if it’s running
2. In your workflow, create the node appropriate to the model type, e.g. **Load Checkpoint**, **Load LoRA**, **Load VAE**
3. In the loader node, choose the model you wish to use
4. Connect the loader node to other nodes in your workflow

### [​](http://docs.comfy.org#file-size) File size

Models can be extremely large files relative to image files. A typical uncompressed image may require a few megabytes of disk storage. Generative AI models can be tens of thousands of times larger, up to tens of gigabytes per model. They take up a great deal of disk space and take a long time to transfer over a network.

## [​](http://docs.comfy.org#model-training-and-refinement) Model training and refinement

A generative AI model is created by training a machine learning program on a very large set of data, such as pairs of images and text descriptions. An AI model doesn’t store the training data explicitly, but rather it stores the correlations that are implicit within the data.

Organizations and companies such as Stability AI and Black Forest Labs release “base” models that carry large amounts of generic information. These are general purpose generative AI models. Commonly, the base models need to be ***refined*** in order to get high quality generative outputs. A dedicated community of people work to refine the base models. The new, refined models produce better output, provide new or different functionality, and/or use fewer resources. Refined models can usually be run on systems with less computing power and/or memory.

## [​](http://docs.comfy.org#auxiliary-models) Auxiliary models

Model functionality can be extended with auxiliary models. For example, art directing a text-to-image workflow to achieve a specific result may be difficult or impossible using a diffusion model alone. Additional models can refine a diffusion model within the workflow graph to produce desired results. Examples include **LoRA** (Low Rank Adaptation), a small model that is trained on a specific subject; **ControlNet**, a model that helps control composition using a guide image; and **Inpainting**, a model that allows certain diffusion models to generate new content within an existing image.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/core-concepts/models.mdx)

[Previous](http://docs.comfy.org/interface/maskeditor)

[DependenciesUnderstand dependencies in ComfyUI  
\
Next](http://docs.comfy.org/essentials/core-concepts/dependencies)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Models are essential](http://docs.comfy.org#models-are-essential)
- [Model files](http://docs.comfy.org#model-files)
- [Using Models in ComfyUI](http://docs.comfy.org#using-models-in-comfyui)
- [File size](http://docs.comfy.org#file-size)
- [Model training and refinement](http://docs.comfy.org#model-training-and-refinement)
- [Auxiliary models](http://docs.comfy.org#auxiliary-models)

<!-- END Get_Started/essentials/core-concepts/models.md -->


<!-- BEGIN Get_Started/essentials/core-concepts/nodes.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Nodes

# Nodes

Understand the concept of a node in ComfyUI.

In ComfyUI, nodes are the fundamental building blocks for executing tasks. Each node is an independently built module, whether it’s a **Comfy Core** node or a **Custom Node**, with its own unique functionality. Nodes connect to each other through links, allowing us to build complex functionality like assembling LEGO blocks. The combinations of different nodes create the unlimited possibilities of ComfyUI.

For example, in the K-Sampler node, you can see it has multiple inputs and outputs, and also includes multiple parameter settings. These parameters determine the logic of node execution. Behind each node is well-written Python logic, allowing you to achieve corresponding functionality without having to write code yourself.

As ComfyUI is still in rapid iteration and development, we are continuously improving it every day. Therefore, some operations mentioned in this article may change or be omitted. Please refer to the actual interface. If you find changes in actual operations, it may be due to our iterative updates. You can also fork [this repo](https://github.com/Comfy-Org/docs) and help us improve this documentation.

## [​](http://docs.comfy.org#nodes-perform-operations) Nodes perform operations

In computer science, a ***node*** is a container for information, usually including programmed instructions to perform some task. Nodes almost never exist in isolation, they’re almost always connected to other nodes in a networked graph. In ComfyUI, nodes take the visual form of boxes that are connected to each other.

ComfyUI nodes are usually ***function operators***. This means that they operate on some data to perform a function. A function is a process that accepts input data, performs some operation on it, and produces output data. In other words, nodes do some work, contributing to the completion of a task such as generating an image. So ComfyUI nodes almost always have at least one input or output, and usually have multiple inputs and outputs.

## [​](http://docs.comfy.org#different-node-states) Different Node States

In ComfyUI, nodes have multiple states. Here are some common node states:

1. **Normal State**: The default state
2. **Running State**: The running state, typically displayed when a node is executing after you start running the workflow
3. **Error State**: Node error, typically displayed after running the workflow if there’s a problem with the node’s input, indicated by red marking of the erroneous input node. You need to fix the problematic input to ensure the workflow runs correctly
4. **Missing State**: This state usually appears after importing workflows, with two possibilities:
   
   - Comfy Core native node missing: This usually happens because ComfyUI has been updated, but you’re using an older version of ComfyUI. You need to update ComfyUI to resolve this issue
   - Custom node missing: The workflow uses custom nodes developed by third-party authors, but your local ComfyUI version doesn’t have these custom nodes installed. You can use [ComfyUI-Manager](https://github.com/Comfy-Org/ComfyUI-Manager) to find and install the missing custom nodes

## [​](http://docs.comfy.org#connections-between-nodes) Connections Between Nodes

In ComfyUI, nodes are connected through [links](http://docs.comfy.org/essentials/core-concepts/links), allowing data of the same type to flow between different processing units to achieve the final result.

Each node receives some input, processes it through its module, and converts it to corresponding output. Connections between different nodes must conform to the data type requirements. In ComfyUI, we use different colors to distinguish node data types. Below are some basic data types:

Data typeColordiffusion modellavenderCLIP modelyellowVAE modelroseconditioningorangelatent imagepinkpixel imagebluemaskgreennumber (integer or float)light greenmeshbright green

As ComfyUI evolves, we may expand to more data types to meet the needs of more scenarios.

### [​](http://docs.comfy.org#connecting-and-disconnecting-nodes) Connecting and Disconnecting Nodes

**Connecting**: Drag from the output point of one node to the input of the same color on another node to connect them **Disconnecting**: Click on the input endpoint and drag the mouse left button to disconnect, or cancel the connection through the midpoint menu of the link

## [​](http://docs.comfy.org#node-appearance) Node Appearance

We provide various style settings for you to customize the appearance of nodes:

- Modify styles
- Double-click the node title to modify the node name
- Switch node inputs between input sockets and widgets through the context menu
- Resize the node using the bottom right corner

### [​](http://docs.comfy.org#node-badges) Node Badges

We provide multiple node badge display features, such as:

- Node ID
- Node source

Currently, **Comfy Core nodes** use a fox icon for display, while custom nodes use their names. This way you can quickly understand which node package a node comes from.

You can set the corresponding display in the menu:

## [​](http://docs.comfy.org#node-context-menus) Node Context Menus

Node context menus are mainly divided into two types:

- Context menu for the node itself
- Context menu for inputs/outputs

### [​](http://docs.comfy.org#node-context-menu) Node Context Menu

By right-clicking on a node, you can expand the corresponding node context menu:

In the node’s right-click context menu, you can:

- Adjust the node’s color style
- Modify the title
- Clone, copy, or delete the node
- Set the node’s mode

In this menu, besides appearance-related settings, the following menu operations are important:

- **Mode**: Set the node’s mode: Always, Never, Bypass
- **Toggle between Widget and Input mode for node inputs**: Switch between widget and input mode for node inputs

#### [​](http://docs.comfy.org#mode) Mode

For modes, you may notice that we currently provide: Always, Never, On Event, On Trigger - four modes, but actually only **Always** and **Never** are effective. **On Event** and **On Trigger** are currently ineffective as we haven’t fully implemented this feature. Additionally, you can understand **Bypass** as a mode. Below is an explanation of the available modes:

- **Always**: The default node mode. The node will execute whenever it runs for the first time or when any of its inputs change since the last execution
- **Never**: The node will never execute under any circumstances, as if it’s been deleted. Subsequent nodes cannot read or receive any data from it
- **Bypass**: The node will never execute under any circumstances, but subsequent nodes can still try to obtain data that hasn’t been processed by this node

Below is a comparison of the `Never` and `Bypass` modes:

In this comparison example, you can see that both workflows apply two LoRA models simultaneously, with the difference being that one `Load LoRA` node is set to `Never` mode while the other is set to `Bypass` mode.

- The node set to `Never` mode causes subsequent nodes to show errors because they don’t receive any input data
- The node set to `Bypass` mode still allows subsequent nodes to receive unprocessed data, so they load the output data from the first `Load LoRA` node, allowing the subsequent workflow to continue running normally

#### [​](http://docs.comfy.org#switching-between-widget-and-input-mode-for-node-inputs) Switching Between Widget and Input Mode for Node Inputs

In some cases, we need to use output results from other nodes as input. In this case, we can switch between widget and input mode for node inputs.

Here’s a very simple example:

By switching the K-Sampler’s Seed from widget to input mode, multiple nodes can share the same seed, achieving variable uniformity across multiple samplers. Comparing the first node with the subsequent two nodes, you can see that the seed in the latter two nodes is in input mode. You can also convert it back to widget mode:

After frontend version v1.16.0, we improved this feature. Now you only need to directly connect the input line to the corresponding widget to complete this process

> Say goodbye to annoying widget &lt;&gt; socket conversion starting from frontend version v1.16.0! Now each widget just always have an associated input socket by default [#ComfyUI](https://twitter.com/hashtag/ComfyUI?src=hash&ref_src=twsrc%5Etfw) [pic.twitter.com/sP9HHKyGYW](https://t.co/sP9HHKyGYW)
> 
> — Chenlei Hu (@HclHno3) [April 7, 2025](https://twitter.com/HclHno3/status/1909059259536375961?ref_src=twsrc%5Etfw)

### [​](http://docs.comfy.org#input%2Foutput-context-menu) Input/Output Context Menu

This context menu is mainly related to the data type of the corresponding input/output:

When dragging the input/output of a node, if a connection appears but you haven’t connected to another node’s input or output, releasing the mouse will pop up a context menu for the input/output, used to quickly add related types of nodes. You can adjust the number of node suggestions in the settings:

## [​](http://docs.comfy.org#node-selection-toolbox) Node Selection Toolbox

The **Node Selection Toolbox** is a floating tool that provides quick operations for nodes. When you select a node, it hovers above the selected node. Through this toolbox, you can:

- Change the node’s color
- Quickly set the node to Bypass mode (not execute during runtime)
- Lock the node
- Delete the node

Of course, these functions can also be found in the right-click menu of the corresponding node. The node selection toolbox just provides a shortcut operation. If you want to disable this feature, you can turn it off in the settings.

## [​](http://docs.comfy.org#node-groups) Node Groups

In ComfyUI, you can select multiple parts of a workflow simultaneously, then use the right-click menu to merge them into a node group, making that part a reusable module that can be repeatedly called in your ComfyUI.

## [​](http://docs.comfy.org#custom-nodes) Custom Nodes

ComfyUI includes many powerful nodes in the base installation package. These are known as **Comfy Core** nodes. Additionally, the ComfyUI community has created an amazing array of [***custom nodes***](https://registry.comfy.org) to perform a wide variety of functions.

## [​](http://docs.comfy.org#comfyui-manager) ComfyUI Manager

The **ComfyUI Manager** window makes it easy to perform custom node management tasks such as search, install, update, disable, and uninstall. The Manager is included in the ComfyUI desktop application, but not in the ComfyUI server application.

### [​](http://docs.comfy.org#installing-the-comfyui-manager) Installing the ComfyUI Manager

If you’re running the ComfyUI server application, you need to install the Manager. If ComfyUI is running, shut it down before proceeding.

The first step is to install Git, a command-line application for software version control. Git will download the ComfyUI Manager from [github.com](https://github.com). Download Git from [git-scm.com](https://git-scm.com/) and install it.

Once Git is installed, navigate to the ComfyUI server program directory, to the folder labeled **custom\_nodes**. Open up a command window or terminal. Make sure that the command line displays the current directory path as **custom\_nodes**. Enter the following command. This will download the Manager. Technically, this is known as *cloning a Git repository*.

```bash
git clone https://github.com/ltdrdata/ComfyUI-Manager.git
```

For details or special cases, see [ComfyUI Manager Install](https://github.com/ltdrdata/ComfyUI-Manager?tab=readme-ov-file#installation).

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/core-concepts/nodes.mdx)

[Previous](http://docs.comfy.org/essentials/core-concepts/workflow)

[Properties  
\
Next](http://docs.comfy.org/essentials/core-concepts/properties)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Nodes perform operations](http://docs.comfy.org#nodes-perform-operations)
- [Different Node States](http://docs.comfy.org#different-node-states)
- [Connections Between Nodes](http://docs.comfy.org#connections-between-nodes)
- [Connecting and Disconnecting Nodes](http://docs.comfy.org#connecting-and-disconnecting-nodes)
- [Node Appearance](http://docs.comfy.org#node-appearance)
- [Node Badges](http://docs.comfy.org#node-badges)
- [Node Context Menus](http://docs.comfy.org#node-context-menus)
- [Node Context Menu](http://docs.comfy.org#node-context-menu)
- [Mode](http://docs.comfy.org#mode)
- [Switching Between Widget and Input Mode for Node Inputs](http://docs.comfy.org#switching-between-widget-and-input-mode-for-node-inputs)
- [Input/Output Context Menu](http://docs.comfy.org#input%2Foutput-context-menu)
- [Node Selection Toolbox](http://docs.comfy.org#node-selection-toolbox)
- [Node Groups](http://docs.comfy.org#node-groups)
- [Custom Nodes](http://docs.comfy.org#custom-nodes)
- [ComfyUI Manager](http://docs.comfy.org#comfyui-manager)
- [Installing the ComfyUI Manager](http://docs.comfy.org#installing-the-comfyui-manager)

<!-- END Get_Started/essentials/core-concepts/nodes.md -->


<!-- BEGIN Get_Started/essentials/core-concepts/properties.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Properties

# Properties

## [​](http://docs.comfy.org#nodes-are-containers-for-properties) Nodes are containers for properties

Nodes usually have ***properties***. Also known as ***parameters*** or ***attributes***, node properties are variables that can be changed. Some properties can be adjusted manually by the user, using a data entry field called a ***widget***. Other properties can be driven automatically by other nodes connected to the property ***input slot*** or port. Usually, a property can be converted from widget to input and vice versa, allowing users to control property values manually or automatically.

Properties can take many forms and hold many different types of information. For example, a **Load Checkpoint** node has a single property:  the file path to the generative model checkpoint file. A **KSampler** node has multiple properties such as the number of sampling **steps**, **CFG** scale, **sampler\_name**, etc.

## [​](http://docs.comfy.org#data-types) Data types

Information can come in many different forms, called ***data types***. For example, alphanumeric text is known as a ***string***, a whole number is an ***integer***, and a number with a decimal point is known as a ***floating point*** number or ***float***. New data types are always being added to ComfyUI.

ComfyUI is written in the Python scripting language, which is very forgiving about data types. By contrast, the ComfyUI environment is very ***strongly typed***. This means that different data types can’t be mixed up. For example, we can’t connect an image output to an integer input. This is a huge benefit to users, guiding them to proper workflow construction and preventing program errors.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/core-concepts/properties.mdx)

[Previous](http://docs.comfy.org/essentials/core-concepts/nodes)

[LinksUnderstand connection links in ComfyUI  
\
Next](http://docs.comfy.org/essentials/core-concepts/links)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Nodes are containers for properties](http://docs.comfy.org#nodes-are-containers-for-properties)
- [Data types](http://docs.comfy.org#data-types)

<!-- END Get_Started/essentials/core-concepts/properties.md -->


<!-- BEGIN Get_Started/essentials/core-concepts/workflow.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Workflow

# Workflow

## [​](http://docs.comfy.org#a-graph-of-nodes) A graph of nodes

ComfyUI is an environment for building and running generative content ***workflows***. In this context, a workflow is defined as a collection of program objects called ***nodes*** that are connected to each other, forming a network. This network is also known as a ***graph***.

A ComfyUI workflow can generate any type of media: image, video, audio, AI model, AI agent, and so on.

## [​](http://docs.comfy.org#sample-workflows) Sample workflows

To get started, try out some of the [official workflows](https://comfyanonymous.github.io/ComfyUI_examples). These use only the Core nodes included in the ComfyUI installation. A thriving community of developers has created a rich [ecosystem](https://registry.comfy.org) of custom nodes to extend the functionality of ComfyUI.

### [​](http://docs.comfy.org#simple-example) Simple Example

## [​](http://docs.comfy.org#visual-programming) Visual programming

A node-based computer program like ComfyUI provides a level of power and flexibility that can’t be achieved with traditional menu- and button-driven applications. The ComfyUI node graph is not limited by the tools provided in a traditional computer application. It’s a high-level ***visual programming environment*** allowing users to design complex systems without needing to write program code or understand advanced mathematics.

Many other computer applications use this same node graph paradigm. Examples include the compositing application called Nuke, the 3D programs Maya and Blender, the Unreal real-time graphics engine, and the interactive media authoring program called Max.

### [​](http://docs.comfy.org#more-complex-example) More Complex Example

## [​](http://docs.comfy.org#procedural-framework) Procedural framework

Another term used to describe a node-based application is ***procedural framework***. Procedural means generative: some procedure or algorithm is employed to generate content such as a 3D model or a musical composition.

ComfyUI is all of these things: a node graph, a visual programming environment, and a procedural framework. What makes ComfyUI different (and amazing!) is that its radically open structure allows us to generate any type of media asset such as picture, movie, sound, 3D model, AI model, etc.

In the context of ComfyUI, the term ***workflow*** is a synonym for the node network or graph. It corresponds to the ***scene graph*** in a 3D or multimedia program: the network of all of the nodes within a particular disk file. 3D programs call this a ***scene file***. Video editing, compositing, and multimedia programs usually call it a ***project file***.

## [​](http://docs.comfy.org#saving-workflows) Saving workflows

The ComfyUI workflow is automatically saved in the metadata of any generated image, allowing users to open and use the graph that generated the image. A workflow can also be stored in a human-readable text file that follows the JSON data format. This is necessary for media formats that don’t support metadata. ComfyUI workflows stored as JSON files are very small, allowing convenient versioning, archiving, and sharing of graphs, independently of any generated media.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/essentials/core-concepts/workflow.mdx)

[Previous](http://docs.comfy.org/interface/credits)

[NodesUnderstand the concept of a node in ComfyUI.  
\
Next](http://docs.comfy.org/essentials/core-concepts/nodes)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [A graph of nodes](http://docs.comfy.org#a-graph-of-nodes)
- [Sample workflows](http://docs.comfy.org#sample-workflows)
- [Simple Example](http://docs.comfy.org#simple-example)
- [Visual programming](http://docs.comfy.org#visual-programming)
- [More Complex Example](http://docs.comfy.org#more-complex-example)
- [Procedural framework](http://docs.comfy.org#procedural-framework)
- [Saving workflows](http://docs.comfy.org#saving-workflows)

<!-- END Get_Started/essentials/core-concepts/workflow.md -->


<!-- BEGIN Get_Started/interface/credits.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Credits Management

# Credits Management

In this article, we will introduce ComfyUI’s credit management features, including how to obtain, use, and view credits.

The credit system was added to support the `API Nodes`, as calling closed-source AI models requires token consumption, so proper credit management is necessary. By default, the credits interface is not displayed. Please first log in to your ComfyUI account in `Settings` -&gt; `User`, and then you can view your associated account’s credit information in `Settings` -&gt; `Credits`.

ComfyUI will always remain fully open-source and free for local users.

## [​](http://docs.comfy.org#how-to-purchase-credits%3F) How to Purchase Credits?

Below is a demonstration video for purchasing credits:

Detailed steps are as follows:

1

Log in to your ComfyUI account

Log in to your ComfyUI account in `Settings` -&gt; `User`

2

Go to \`Settings\` -&gt; \`Credits\` to purchase credits

After logging in, you should see the `Credits` option added to the menu

Go to `Settings` -&gt; `Credits` to purchase credits

3

Set the amount of credits to purchase

In the popup, set the purchase amount and click the `Buy` button

4

Make payment through Stripe

On the payment page, please follow these steps:

1. Select the currency for payment
2. Confirm that the email is the same as your ComfyUI registration email
3. Choose your payment method

<!--THE END-->

- Credit Card
- WeChat (only supported when paying in USD)

<!--THE END-->

4. Click the `Pay` button or the `Generate QR Code` button to complete the payment process

5

Complete payment and check your credit balance

After completing the payment, please return to `Menu` -&gt; `Credits` to check if your balance has been updated. Try refreshing the interface or restarting if necessary

## [​](http://docs.comfy.org#frequently-asked-questions) Frequently Asked Questions

Can credits go negative?

No, when your credit balance is negative, you will not be able to run API Nodes

Can I get a refund for unused credits?

Currently, we do not support refunds

How do I check my current balance and usage?

Click on `Settings` -&gt; `Credits` to see your current balance and access the `Credit History` entry

Can I share my credits with other users?

You can log into the same account on multiple devices, but we do not support sharing credits with other users

How do I know how many credits I've consumed each time?

Due to different image sizes and generation quantities, the `Tokens` and `Credits` consumed each time vary. In `Settings` -&gt; `Credits`, you can see the credits consumed each time and the corresponding credit history

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/interface/credits.mdx)

[Previous](http://docs.comfy.org/interface/user)

[Workflow  
\
Next](http://docs.comfy.org/essentials/core-concepts/workflow)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [How to Purchase Credits?](http://docs.comfy.org#how-to-purchase-credits%3F)
- [Frequently Asked Questions](http://docs.comfy.org#frequently-asked-questions)

<!-- END Get_Started/interface/credits.md -->


<!-- BEGIN Get_Started/interface/maskeditor.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Mask Editor - Create and Edit Masks in ComfyUI

# Mask Editor - Create and Edit Masks in ComfyUI

Learn how to use the Mask Editor in ComfyUI, including settings and usage instructions

The Mask Editor is a very useful feature in ComfyUI that allows users to create and edit masks within images without needing to use other applications.

The Mask Editor is currently triggered through the `Load Image` node. After uploading an image, you can right-click on the node and select `Open in MaskEditor` from the menu to open the Mask Editor.

You can then click with your mouse on the image to create and edit masks.

## [​](http://docs.comfy.org#demo-video) Demo Video

Your browser does not support the video tag.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/interface/maskeditor.mdx)

[Previous](http://docs.comfy.org/essentials/core-concepts/links)

[Models  
\
Next](http://docs.comfy.org/essentials/core-concepts/models)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Demo Video](http://docs.comfy.org#demo-video)

<!-- END Get_Started/interface/maskeditor.md -->


<!-- BEGIN Get_Started/interface/overview.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Interface Overview

# ComfyUI Interface Overview

In this article, we will briefly introduce the basic user interface of ComfyUI, familiarizing you with the various parts of the ComfyUI interface.

The visual interface is currently the way most users utilize ComfyUI to call the [ComfyUI Server](http://docs.comfy.org/essentials/comfyui-server/comms_overview) to generate corresponding media resources. It provides a visual interface for users to operate and organize workflows, debug workflows, and create amazing works.

Typically, when you start the ComfyUI server, you will see an interface like this:

If you are an earlier user, you may have seen the previous menu interface like this:

Currently, the [ComfyUI frontend](https://github.com/Comfy-Org/ComfyUI_frontend) is a separate project, released and maintained as an independent pip package. If you want to contribute, you can fork this [repository](https://github.com/Comfy-Org/ComfyUI_frontend) and submit a pull request.

## [​](http://docs.comfy.org#localization-support) Localization Support

Currently, ComfyUI supports: English, Chinese, Russian, French, Japanese, and Korean. If you need to switch the interface language to your preferred language, you can click the **Settings gear icon** and then select your desired language under `Comfy` —&gt; `Locale`.

## [​](http://docs.comfy.org#new-menu-interface) New Menu Interface

### [​](http://docs.comfy.org#workspace-areas) Workspace Areas

Below are the main interface areas of ComfyUI and a brief introduction to each part.

Currently, apart from the main workflow interface, the ComfyUI interface is mainly divided into the following parts:

1. Menu Bar: Provides workflow, editing, help menus, workflow execution, ComfyUI Manager entry, etc.
2. Sidebar Panel Switch Buttons: Used to switch between workflow history queue, node library, model library, local user workflow browsing, etc.
3. Theme Switch Button: Quickly switch between ComfyUI’s default dark theme and light theme
4. Settings: Click to open the settings button
5. Canvas Menu: Provides zoom in, zoom out, and auto-fit operations for the ComfyUI canvas

### [​](http://docs.comfy.org#menu-bar-functions) Menu Bar Functions

The image above shows the corresponding functions of the top menu bar, including common features, which we will explain in detail in the specific function usage section.

### [​](http://docs.comfy.org#sidebar-panel-buttons) Sidebar Panel Buttons

In the current ComfyUI, we provide four side panels with the following functions:

1. Workflow History Queue (Queue): All queue information for ComfyUI executing media content generation
2. Node Library: All nodes in ComfyUI, including `Comfy Core` and your installed custom nodes, can be found here
3. Model Library: Models in your local `ComfyUI/models` directory can be found here
4. Local User Workflows (Workflows): Your locally saved workflows can be found here

## [​](http://docs.comfy.org#old-menu-version) Old Menu Version

Currently, ComfyUI enables the new interface by default. If you prefer to use the old interface, you can click the **Settings gear icon** and then set `Use new menu` to `disabled` under `Comfy` —&gt; `Menu` to switch to the old menu version.

The old menu interface only supports English.

The function annotations for the old menu interface are explained below:

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/interface/overview.mdx)

[Previous](http://docs.comfy.org/get_started/first_generation)

[Account ManagementIn this article, we will introduce the account management features of ComfyUI, including login, registration, logout, and other operations.  
\
Next](http://docs.comfy.org/interface/user)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Localization Support](http://docs.comfy.org#localization-support)
- [New Menu Interface](http://docs.comfy.org#new-menu-interface)
- [Workspace Areas](http://docs.comfy.org#workspace-areas)
- [Menu Bar Functions](http://docs.comfy.org#menu-bar-functions)
- [Sidebar Panel Buttons](http://docs.comfy.org#sidebar-panel-buttons)
- [Old Menu Version](http://docs.comfy.org#old-menu-version)

<!-- END Get_Started/interface/overview.md -->


<!-- BEGIN Get_Started/interface/shortcuts.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Keyboard Shortcuts and Custom Settings

# ComfyUI Keyboard Shortcuts and Custom Settings

Keyboard and mouse shortcuts for ComfyUI and related settings

Currently, ComfyUI supports custom keyboard shortcuts. You can set the shortcuts by clicking on `Settings (gear icon)` —&gt; `Keybinding`.

In the corresponding menu, you can see all the current shortcut settings for ComfyUI. Click the `edit icon` before the corresponding command to customize the shortcut.

Below is the current list of shortcuts for ComfyUI, which you can customize as needed.

- Windows/Linux
- MacOS

ShortcutCommandCtrl + EnterQueue up current graph for generationCtrl + Shift + EnterQueue up current graph as first for generationCtrl + Z / Ctrl + YUndo/RedoCtrl + SSave workflowCtrl + OLoad workflowCtrl + ASelect all nodesAlt + CCollapse/uncollapse selected nodesCtrl + MMute/unmute selected nodesCtrl + BBypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)Delete  
BackspaceDelete selected nodesCtrl + Delete  
Ctrl + BackspaceDelete the current graphSpaceMove the canvas around when held and moving the cursorCtrl + Click  
Shift + ClickAdd clicked node to selectionCtrl + C/Ctrl + VCopy and paste selected nodes (without maintaining connections to outputs of unselected nodes)Ctrl + C/Ctrl + Shift + VCopy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes)Shift + DragMove multiple selected nodes at the same timeCtrl + DLoad default graphQToggle visibility of the queueHToggle visibility of historyRRefresh graphDouble-Click LMBQuick search for nodes to add

ShortcutCommandCtrl + EnterQueue up current graph for generationCtrl + Shift + EnterQueue up current graph as first for generationCtrl + Z / Ctrl + YUndo/RedoCtrl + SSave workflowCtrl + OLoad workflowCtrl + ASelect all nodesAlt + CCollapse/uncollapse selected nodesCtrl + MMute/unmute selected nodesCtrl + BBypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)Delete  
BackspaceDelete selected nodesCtrl + Delete  
Ctrl + BackspaceDelete the current graphSpaceMove the canvas around when held and moving the cursorCtrl + Click  
Shift + ClickAdd clicked node to selectionCtrl + C/Ctrl + VCopy and paste selected nodes (without maintaining connections to outputs of unselected nodes)Ctrl + C/Ctrl + Shift + VCopy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes)Shift + DragMove multiple selected nodes at the same timeCtrl + DLoad default graphQToggle visibility of the queueHToggle visibility of historyRRefresh graphDouble-Click LMBQuick search for nodes to add

KeybindExplanationCmd ⌘ + EnterQueue up current graph for generationCmd ⌘ + Shift + EnterQueue up current graph as first for generationCmd ⌘ + Z/Cmd ⌘ + YUndo/RedoCmd ⌘ + SSave workflowCmd ⌘ + OLoad workflowCmd ⌘ + ASelect all nodesOpt ⌥ + CCollapse/uncollapse selected nodesCmd ⌘ + MMute/unmute selected nodesCmd ⌘ + BBypass selected nodes (acts like the node was removed from the graph and the wires reconnected through)Delete  
BackspaceDelete selected nodesCmd ⌘ + Delete  
Cmd ⌘ + BackspaceDelete the current graphSpaceMove the canvas around when held and moving the cursorCmd ⌘ + Click  
Shift + ClickAdd clicked node to selectionCmd ⌘ + C / Cmd ⌘ + VCopy and paste selected nodes (without maintaining connections to outputs of unselected nodes)Cmd ⌘ + C / Cmd ⌘ + Shift + VCopy and paste selected nodes (maintaining connections from outputs of unselected nodes to inputs of pasted nodes)Shift + DragMove multiple selected nodes at the same timeCmd ⌘ + DLoad default graphQToggle visibility of the queueHToggle visibility of historyRRefresh graphDouble-Click LMBQuick search for nodes to add

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/interface/shortcuts.mdx)

[Previous](http://docs.comfy.org/essentials/core-concepts/dependencies)

[Text to ImageThis guide will help you understand the concept of text-to-image in AI art generation and complete a text-to-image workflow in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/basic/text-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

<!-- END Get_Started/interface/shortcuts.md -->


<!-- BEGIN Get_Started/interface/user.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Account Management

# Account Management

In this article, we will introduce the account management features of ComfyUI, including login, registration, logout, and other operations.

The account system was added to support `API Nodes`, which allow calls to closed-source model APIs, greatly expanding the possibilities of ComfyUI. Since the corresponding API calls consume Tokens, we have added a corresponding user system.

Log in through `Settings` -&gt; `User`:

Currently, we support the following login methods:

- Email login
- Google login
- Github login

If this is your first login, please create an account first.

You may need to use at least [ComfyUI v0.3.0](https://github.com/comfyanonymous/ComfyUI/releases/tag/v0.3.30) to use the account system, and ensure that the corresponding front-end version is at least `1.17.11`. Sometimes the front-end may fail to install and revert to an older version, so please check in `Settings` -&gt; `About` to see if the front-end version is greater than `1.17.11`.

In some regions, you may experience login timeouts or failures due to network restrictions preventing normal access to the login API. Before logging in, please **ensure that your network environment does not restrict access to the corresponding API**, and that you can access websites like Google or Github normally.

After logging in, the login button will be displayed in the top menu bar of the ComfyUI interface, and you can open the corresponding login interface through this button. You can also log out of your account in the settings menu.

## [​](http://docs.comfy.org#common-questions-about-accounts) Common Questions About Accounts

Are there any restrictions on login devices?

We do not impose restrictions on login devices. You can log into your account on any device, but please note that your account information may be accessed by other devices, so do not log into your account on public devices.

Why can't I access login via LAN IP?

Currently, we do not support logging in via LAN IP because the security of LAN environments is uncontrollable. Therefore, the current version does not fully support LAN IP login. In the future, we may consider handling LAN situations appropriately.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/interface/user.mdx)

[Previous](http://docs.comfy.org/interface/overview)

[Credits ManagementIn this article, we will introduce ComfyUI's credit management features, including how to obtain, use, and view credits.  
\
Next](http://docs.comfy.org/interface/credits)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Common Questions About Accounts](http://docs.comfy.org#common-questions-about-accounts)

<!-- END Get_Started/interface/user.md -->


<!-- BEGIN Get_Started/registry/cicd.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Custom Node CI/CD

# Custom Node CI/CD

## [​](http://docs.comfy.org#introduction) Introduction

When making changes to custom nodes, it’s not uncommon to break things in Comfy or other custom nodes. It is often unrealistic to test on every operating system and different configurations of Pytorch.

### [​](http://docs.comfy.org#run-comfy-workflows-using-github-actions) Run Comfy Workflows using Github Actions

[Comfy-Action](https://github.com/Comfy-Org/comfy-action) allows you to run a Comfy workflow.json file on Github Actions. It supports downloading models, custom nodes, and runs on Linux/Mac/Windows.

### [​](http://docs.comfy.org#results) Results

Output files are uploaded to the [CI/CD Dashboard](https://comfyci.org) and can be viewed as a last step before commiting new changes or publishing new versions of the custom node.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/registry/cicd.mdx)

[Previous](http://docs.comfy.org/registry/standards)

[pyproject.toml  
\
Next](http://docs.comfy.org/registry/specifications)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Introduction](http://docs.comfy.org#introduction)
- [Run Comfy Workflows using Github Actions](http://docs.comfy.org#run-comfy-workflows-using-github-actions)
- [Results](http://docs.comfy.org#results)

<!-- END Get_Started/registry/cicd.md -->


<!-- BEGIN Get_Started/registry/overview.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Overview

# Overview

## [​](http://docs.comfy.org#introduction) Introduction

The Registry is a public collection of custom nodes. Developers can publish, version, deprecate, and track metrics related to their custom nodes. ComfyUI users can discover, install, and rate custom nodes from the registry.

## [​](http://docs.comfy.org#why-use-the-registry%3F) Why use the Registry?

The Comfy Registry helps the community by standardizing the development of custom nodes:

  **Node Versioning:** Developers frequently publish new versions of their custom nodes which often break workflows that rely on them. With registry nodes being [semantically versioned](https://semver.org/), users can now choose to safely upgrade, deprecate, or lock their node versions in place, knowing in advance how their actions will impact their workflows. The workflow JSON will store the version of the node used, so you can always reliably reproduce your workflows.

  **Node Security:** The registry will serve as a backend for the [ComfyUI-manager](https://github.com/ltdrdata/ComfyUI-Manager). All nodes will be scanned for malicious behaviour such as custom pip wheels, arbitrary system calls, etc. Nodes that pass these checks will have a verification flag () beside their name on the UI-manager. For a list of security standards, see the [standards](http://docs.comfy.org/registry/standards).

  **Search:** Search across all nodes on the Registry to find existing nodes for your workflow.x

## [​](http://docs.comfy.org#publishing-nodes) Publishing Nodes

Get started publishing your first node by following the [tutorial](http://docs.comfy.org/registry/publishing).

## [​](http://docs.comfy.org#frequently-asked-questions) Frequently Asked Questions

Do registry nodes have unique identifiers?

Yes, a custom node on the Registry has a globally unique name and this allows Comfy Workflow JSON files to uniquely identify any custom node without collisions.

Are there any restrictions on what I can publish?

Check the [standards](http://docs.comfy.org/registry/standards) for more information.

How do you ensure node stability?

Once a custom node version is published, it cannot be changed. This ensures that users can rely on the stability of the custom node over time.

How are nodes versioned?

Custom nodes are versioned using [semantic versioning](https://semver.org/). This allows users to understand the impact of upgrading to a new version.

How do I deprecate a node version?

You can deprecate a version in the Comfy Registry website by clicking **More Actions &gt; Deprecate**. Users who installed this version will be shown the deprecation message and be encouraged to upgrade to a newer version.

Deprecating versions is useful when an issue is discovered after publishing.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/registry/overview.mdx)

[Previous](http://docs.comfy.org/custom-nodes/tips)

[Publishing Nodes  
\
Next](http://docs.comfy.org/registry/publishing)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Introduction](http://docs.comfy.org#introduction)
- [Why use the Registry?](http://docs.comfy.org#why-use-the-registry%3F)
- [Publishing Nodes](http://docs.comfy.org#publishing-nodes)
- [Frequently Asked Questions](http://docs.comfy.org#frequently-asked-questions)

<!-- END Get_Started/registry/overview.md -->


<!-- BEGIN Get_Started/registry/publishing.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Publishing Nodes

# Publishing Nodes

## [​](http://docs.comfy.org#set-up-a-registry-account) Set up a Registry Account

Follow the steps below to set up a registry account and publish your first node.

### [​](http://docs.comfy.org#watch-a-tutorial) Watch a Tutorial

### [​](http://docs.comfy.org#create-a-publisher) Create a Publisher

A publisher is an identity that can publish custom nodes to the registry. Every custom node needs to include a publisher identifier in the pyproject.toml [file](http://docs.comfy.org).

Go to [Comfy Registry](https://registry.comfy.org), and create a publisher account. Your publisher id is globally unique, and cannot be changed later because it is used in the URL of your custom node.

Your publisher id is found after the `@` symbol on your profile page.

### [​](http://docs.comfy.org#create-an-api-key-for-publishing) Create an API Key for publishing

Go [here](https://registry.comfy.org/nodes) and click on the publisher you want to create an API key for. This will be used to publish a custom node via the CLI.

Name the API key and save it somewhere safe. If you lose it, you’ll have to create a new key.

### [​](http://docs.comfy.org#add-metadata) Add Metadata

Have you installed the comfy-cli? [Do that first](http://docs.comfy.org/comfy-cli/getting-started).

```bash
comfy node init
```

This command will generate the following metadata:

```toml
# pyproject.toml
[project]
name = "" # Unique identifier for your node. Immutable after creation.
description = ""
version = "1.0.0" # Custom Node version. Must be semantically versioned.
license = { file = "LICENSE.txt" }
dependencies  = [] # Filled in from requirements.txt

[project.urls]
Repository = "https://github.com/..."

[tool.comfy]
PublisherId = "" # TODO (fill in Publisher ID from Comfy Registry Website).
DisplayName = "" # Display name for the Custom Node. Can be changed later.
Icon = "https://example.com/icon.png" # SVG, PNG, JPG or GIF (MAX. 800x400px)
```

Add this file to your repository. Check the [specifications](http://docs.comfy.org/registry/specifications) for more information on the pyproject.toml file.

## [​](http://docs.comfy.org#publish-to-the-registry) Publish to the Registry

### [​](http://docs.comfy.org#option-1%3A-comfy-cli) Option 1: Comfy CLI

Run the command below to manually publish your node to the registry.

```bash
comfy node publish
```

You’ll be prompted for the API key.

```bash
API Key for publisher '<publisher id>': ****************************************************

...Version 1.0.0 Published. 
See it here: https://registry.comfy.org/publisherId/your-node
```

Keep in mind that the API key is hidden by default.

When copy-pasting, your API key might have an additional \\x16 at the back when using CTRL+V (for Windows), eg: \*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\\x16.

It is recommended to copy-paste your API key via right-clicking instead.

### [​](http://docs.comfy.org#option-2%3A-github-actions) Option 2: Github Actions

Automatically publish your node through github actions.

1

Set up a Github Secret

Go to Settings -&gt; Secrets and Variables -&gt; Actions -&gt; Under Secrets Tab and Repository secrets -&gt; New Repository Secret.

Create a secret called `REGISTRY_ACCESS_TOKEN` and store your API key as the value.

2

Create a Github Action

Copy the code below and paste it here `/.github/workflows/publish_action.yml`

```bash
name: Publish to Comfy registry
on:
  workflow_dispatch:
  push:
    branches:
      - main
    paths:
      - "pyproject.toml"

jobs:
  publish-node:
    name: Publish Custom Node to registry
    runs-on: ubuntu-latest
    steps:
      - name: Check out code
        uses: actions/checkout@v4
      - name: Publish Custom Node
        uses: Comfy-Org/publish-node-action@main
        with:
          personal_access_token: ${{ secrets.REGISTRY_ACCESS_TOKEN }} ## Add your own personal access token to your Github Repository secrets and reference it here.
```

If your working branch is named something besides `main`, such as `master`, add the name under the branches section.

3

Test the Github Action

Push an update to your `pyproject.toml`’s version number. You should see your updated node on the registry.

The github action will automatically run every time you push an update to your `pyproject.toml` file

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/registry/publishing.mdx)

[Previous](http://docs.comfy.org/registry/overview)

[StandardsSecurity and other standards for publishing to the Registry  
\
Next](http://docs.comfy.org/registry/standards)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Set up a Registry Account](http://docs.comfy.org#set-up-a-registry-account)
- [Watch a Tutorial](http://docs.comfy.org#watch-a-tutorial)
- [Create a Publisher](http://docs.comfy.org#create-a-publisher)
- [Create an API Key for publishing](http://docs.comfy.org#create-an-api-key-for-publishing)
- [Add Metadata](http://docs.comfy.org#add-metadata)
- [Publish to the Registry](http://docs.comfy.org#publish-to-the-registry)
- [Option 1: Comfy CLI](http://docs.comfy.org#option-1%3A-comfy-cli)
- [Option 2: Github Actions](http://docs.comfy.org#option-2%3A-github-actions)

<!-- END Get_Started/registry/publishing.md -->


<!-- BEGIN Get_Started/registry/specifications.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

pyproject.toml

# pyproject.toml

# [​](http://docs.comfy.org#node-id) Node ID

The node id (“name” field in toml file) uniquely identifies the custom node, and will be used in URLs from the registry. Users can also install the node by referencing the name.

`comfy node install <node-id>`

The node id must be less than 100 characters and can only contain alphanumeric characters, hyphens, underscores, and periods. There should not be consecutive special characters and the id cannot start with a number or special character.

Comparison of node ids is case-insensitive. See the official [python documentation](https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#name) for more information.

We recommend using a short name for your node id, and don’t include “ComfyUI” in the name.

# [​](http://docs.comfy.org#version-number) Version Number

The registry uses [semantic versioning](https://semver.org/) which indicates the specific release of a custom node through a three-digit version number X.Y.Z.

X - **MAJOR** change that breaks previous updates

Y - **MINOR** change that adds new features and is backwards compatible

Z - **PATCH** change that fixes a bug

# [​](http://docs.comfy.org#license) License

An optional field that expects a relative path to your license file (usually named `LICENSE` or `LICENSE.txt`).

- `license = { file = "LICENSE" }` ✅
- `license = "LICENSE"` ❌

Alternatively, it can also be referenced by name. Common licenses include [MIT](https://opensource.org/license/mit), [GPL](https://www.gnu.org/licenses/gpl-3.0.en.html), or [Apache](https://www.apache.org/licenses/LICENSE-2.0).

- `license = {text = "MIT License"}` ✅
- `license = "MIT LICENSE"` ❌

Read up more on toml file standards [here](https://packaging.python.org/en/latest/guides/writing-pyproject-toml/#license)

# [​](http://docs.comfy.org#publisher-id) Publisher ID

The publisher id uniquely identifies a publisher and is ideally the same as your github username. It is also referred to as the username on the registry and can be found after the `@` symbol on the profile page.

# [​](http://docs.comfy.org#icon) Icon

An optional field that expects a icon URL. It accepts extensions SVG, PNG, JPG or GIF with a maximum resolution of 800px by 400px.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/registry/specifications.mdx)

[Previous](http://docs.comfy.org/registry/cicd)

[Workflow JSONJSON schema for a ComfyUI workflow.  
\
Next](http://docs.comfy.org/specs/workflow_json)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Node ID](http://docs.comfy.org#node-id)
- [Version Number](http://docs.comfy.org#version-number)
- [License](http://docs.comfy.org#license)
- [Publisher ID](http://docs.comfy.org#publisher-id)
- [Icon](http://docs.comfy.org#icon)

<!-- END Get_Started/registry/specifications.md -->


<!-- BEGIN Get_Started/registry/standards.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Standards

# Standards

Security and other standards for publishing to the Registry

## [​](http://docs.comfy.org#base-standards) Base Standards

### [​](http://docs.comfy.org#1-community-value) 1. Community Value

Custom nodes must provide valuable functionality to the ComfyUI community

Avoid:

- Excessive self-promotion
- Impersonation or misleading behavior
- Malicious behavior
- Self-promotion is permitted only within your designated settings menu section
- Top and side menus should contain only useful functionality

### [​](http://docs.comfy.org#2-node-compatibility) 2. Node Compatibility

Do not interfere with other custom nodes’ operations (installation, updates, removal)

- For dependencies on other custom nodes:
  
  - Display clear warnings when dependent functionality is used
  - Provide example workflows demonstrating required nodes

### [​](http://docs.comfy.org#3-legal-compliance) 3. Legal Compliance

Must comply with all applicable laws and regulations

### [​](http://docs.comfy.org#5-quality-requirements) 5. Quality Requirements

Nodes must be fully functional, well documented, and actively maintained.

### [​](http://docs.comfy.org#6-fork-guidelines) 6. Fork Guidelines

Forked nodes must:

- Have clearly distinct names from original
- Provide significant differences in functionality or code

Below are standards that must be met to publish custom nodes to the registry.

## [​](http://docs.comfy.org#security-standards) Security Standards

Custom nodes should be secure. We will start working with custom nodes that violate these standards to be rewritten. If there is some major functionality that should be exposed by core, please request it in the [rfcs repo](https://github.com/comfy-org/rfcs).

### [​](http://docs.comfy.org#eval%2Fexec-calls) eval/exec Calls

#### [​](http://docs.comfy.org#policy) Policy

The use of `eval` and `exec` functions is prohibited in custom nodes due to security concerns.

#### [​](http://docs.comfy.org#reasoning) Reasoning

These functions can enable arbitrary code execution, creating potential Remote Code Execution (RCE) vulnerabilities when processing user inputs. Workflows containing nodes that pass user inputs into `eval` or `exec` could be exploited for various cyberattacks, including:

- Keylogging
- Ransomware
- Other malicious code execution

### [​](http://docs.comfy.org#subprocess-for-pip-install) subprocess for pip install

#### [​](http://docs.comfy.org#policy-2) Policy

Runtime package installation through subprocess calls is not permitted.

#### [​](http://docs.comfy.org#reasoning-2) Reasoning

- First item ComfyUI manager will ship with ComfyUI and lets the user install dependencies
- Centralized dependency management improves security and user experience
- Helps prevent potential supply chain attacks
- Eliminates need for multiple ComfyUI reloads

### [​](http://docs.comfy.org#code-obfuscation) Code Obfuscation

#### [​](http://docs.comfy.org#policy-3) Policy

Code obfuscation is prohibited in custom nodes.

#### [​](http://docs.comfy.org#reasoning-3) Reasoning

Obfuscated code:

- Impossible to review and likely to be malicious

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/registry/standards.mdx)

[Previous](http://docs.comfy.org/registry/publishing)

[Custom Node CI/CD  
\
Next](http://docs.comfy.org/registry/cicd)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Base Standards](http://docs.comfy.org#base-standards)
- [1. Community Value](http://docs.comfy.org#1-community-value)
- [2. Node Compatibility](http://docs.comfy.org#2-node-compatibility)
- [3. Legal Compliance](http://docs.comfy.org#3-legal-compliance)
- [5. Quality Requirements](http://docs.comfy.org#5-quality-requirements)
- [6. Fork Guidelines](http://docs.comfy.org#6-fork-guidelines)
- [Security Standards](http://docs.comfy.org#security-standards)
- [eval/exec Calls](http://docs.comfy.org#eval%2Fexec-calls)
- [Policy](http://docs.comfy.org#policy)
- [Reasoning](http://docs.comfy.org#reasoning)
- [subprocess for pip install](http://docs.comfy.org#subprocess-for-pip-install)
- [Policy](http://docs.comfy.org#policy-2)
- [Reasoning](http://docs.comfy.org#reasoning-2)
- [Code Obfuscation](http://docs.comfy.org#code-obfuscation)
- [Policy](http://docs.comfy.org#policy-3)
- [Reasoning](http://docs.comfy.org#reasoning-3)

<!-- END Get_Started/registry/standards.md -->


<!-- BEGIN Get_Started/specs/nodedef_json.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions
  
  - [Node Definition JSON](http://docs.comfy.org/specs/nodedef_json)
  - [Node Definition JSON 1.0](http://docs.comfy.org/specs/nodedef_json_1_0)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Node Definition JSON

# Node Definition JSON

JSON schema for a ComfyUI Node.

The node definition JSON is defined using [JSON Schema](https://json-schema.org/). Changes to this schema will be discussed in the [rfcs repo](https://github.com/comfy-org/rfcs).

## [​](http://docs.comfy.org#v2-0-latest) v2.0 (Latest)

Node Definition v2.0

```json
{
  "$ref": "#/definitions/ComfyNodeDefV2",
  "definitions": {
    "ComfyNodeDefV2": {
      "type": "object",
      "properties": {
        "inputs": {
          "type": "object",
          "additionalProperties": {
            "anyOf": [
              {
                "type": "object",
                "properties": {
                  "default": {
                    "anyOf": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "array",
                        "items": {
                          "type": "number"
                        }
                      }
                    ]
                  },
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "min": {
                    "type": "number"
                  },
                  "max": {
                    "type": "number"
                  },
                  "step": {
                    "type": "number"
                  },
                  "display": {
                    "type": "string",
                    "enum": [
                      "slider",
                      "number",
                      "knob"
                    ]
                  },
                  "control_after_generate": {
                    "type": "boolean"
                  },
                  "type": {
                    "type": "string",
                    "const": "INT"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {
                    "anyOf": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "array",
                        "items": {
                          "type": "number"
                        }
                      }
                    ]
                  },
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "min": {
                    "type": "number"
                  },
                  "max": {
                    "type": "number"
                  },
                  "step": {
                    "type": "number"
                  },
                  "display": {
                    "type": "string",
                    "enum": [
                      "slider",
                      "number",
                      "knob"
                    ]
                  },
                  "round": {
                    "anyOf": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "boolean",
                        "const": false
                      }
                    ]
                  },
                  "type": {
                    "type": "string",
                    "const": "FLOAT"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {
                    "type": "boolean"
                  },
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "label_on": {
                    "type": "string"
                  },
                  "label_off": {
                    "type": "string"
                  },
                  "type": {
                    "type": "string",
                    "const": "BOOLEAN"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {
                    "type": "string"
                  },
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "multiline": {
                    "type": "boolean"
                  },
                  "dynamicPrompts": {
                    "type": "boolean"
                  },
                  "defaultVal": {
                    "type": "string"
                  },
                  "placeholder": {
                    "type": "string"
                  },
                  "type": {
                    "type": "string",
                    "const": "STRING"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {},
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "control_after_generate": {
                    "type": "boolean"
                  },
                  "image_upload": {
                    "type": "boolean"
                  },
                  "image_folder": {
                    "type": "string",
                    "enum": [
                      "input",
                      "output",
                      "temp"
                    ]
                  },
                  "allow_batch": {
                    "type": "boolean"
                  },
                  "video_upload": {
                    "type": "boolean"
                  },
                  "remote": {
                    "type": "object",
                    "properties": {
                      "route": {
                        "anyOf": [
                          {
                            "type": "string",
                            "format": "uri"
                          },
                          {
                            "type": "string",
                            "pattern": "^\\/"
                          }
                        ]
                      },
                      "refresh": {
                        "anyOf": [
                          {
                            "type": "number",
                            "minimum": -9007199254740991,
                            "maximum": 9007199254740991
                          },
                          {
                            "type": "number",
                            "maximum": 9007199254740991,
                            "minimum": -9007199254740991
                          }
                        ]
                      },
                      "response_key": {
                        "type": "string"
                      },
                      "query_params": {
                        "type": "object",
                        "additionalProperties": {
                          "type": "string"
                        }
                      },
                      "refresh_button": {
                        "type": "boolean"
                      },
                      "control_after_refresh": {
                        "type": "string",
                        "enum": [
                          "first",
                          "last"
                        ]
                      },
                      "timeout": {
                        "type": "number",
                        "minimum": 0
                      },
                      "max_retries": {
                        "type": "number",
                        "minimum": 0
                      }
                    },
                    "required": [
                      "route"
                    ],
                    "additionalProperties": false
                  },
                  "options": {
                    "type": "array",
                    "items": {
                      "type": [
                        "string",
                        "number"
                      ]
                    }
                  },
                  "type": {
                    "type": "string",
                    "const": "COMBO"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              },
              {
                "type": "object",
                "properties": {
                  "default": {},
                  "defaultInput": {
                    "type": "boolean"
                  },
                  "forceInput": {
                    "type": "boolean"
                  },
                  "tooltip": {
                    "type": "string"
                  },
                  "hidden": {
                    "type": "boolean"
                  },
                  "advanced": {
                    "type": "boolean"
                  },
                  "rawLink": {
                    "type": "boolean"
                  },
                  "lazy": {
                    "type": "boolean"
                  },
                  "type": {
                    "type": "string"
                  },
                  "name": {
                    "type": "string"
                  },
                  "isOptional": {
                    "type": "boolean"
                  }
                },
                "required": [
                  "type",
                  "name"
                ],
                "additionalProperties": true
              }
            ]
          }
        },
        "outputs": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "index": {
                "type": "number"
              },
              "name": {
                "type": "string"
              },
              "type": {
                "type": "string"
              },
              "is_list": {
                "type": "boolean"
              },
              "options": {
                "type": "array"
              },
              "tooltip": {
                "type": "string"
              }
            },
            "required": [
              "index",
              "name",
              "type",
              "is_list"
            ],
            "additionalProperties": false
          }
        },
        "hidden": {
          "type": "object",
          "additionalProperties": {}
        },
        "name": {
          "type": "string"
        },
        "display_name": {
          "type": "string"
        },
        "description": {
          "type": "string"
        },
        "category": {
          "type": "string"
        },
        "output_node": {
          "type": "boolean"
        },
        "python_module": {
          "type": "string"
        },
        "deprecated": {
          "type": "boolean"
        },
        "experimental": {
          "type": "boolean"
        }
      },
      "required": [
        "inputs",
        "outputs",
        "name",
        "display_name",
        "description",
        "category",
        "output_node",
        "python_module"
      ],
      "additionalProperties": false
    }
  },
  "$schema": "http://json-schema.org/draft-07/schema#"
}
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/specs/nodedef_json.mdx)

[Previous](http://docs.comfy.org/specs/workflow_json_0.4)

[Node Definition JSON 1.0JSON schema for a ComfyUI Node.  
\
Next](http://docs.comfy.org/specs/nodedef_json_1_0)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [v2.0 (Latest)](http://docs.comfy.org#v2-0-latest)

<!-- END Get_Started/specs/nodedef_json.md -->


<!-- BEGIN Get_Started/specs/nodedef_json_1_0.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
- Node Definitions
  
  - [Node Definition JSON](http://docs.comfy.org/specs/nodedef_json)
  - [Node Definition JSON 1.0](http://docs.comfy.org/specs/nodedef_json_1_0)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Node Definition JSON 1.0

# Node Definition JSON 1.0

JSON schema for a ComfyUI Node.

## [​](http://docs.comfy.org#v1-0) v1.0

Node Definition v1.0

```json
{
  "$ref": "#/definitions/ComfyNodeDefV1",
  "definitions": {
    "ComfyNodeDefV1": {
      "type": "object",
      "properties": {
        "input": {
          "type": "object",
          "properties": {
            "required": {
              "type": "object",
              "additionalProperties": {
                "anyOf": [
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "INT"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "array",
                                    "items": {
                                      "type": "number"
                                    }
                                  }
                                ]
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "min": {
                                "type": "number"
                              },
                              "max": {
                                "type": "number"
                              },
                              "step": {
                                "type": "number"
                              },
                              "display": {
                                "type": "string",
                                "enum": [
                                  "slider",
                                  "number",
                                  "knob"
                                ]
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "FLOAT"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "array",
                                    "items": {
                                      "type": "number"
                                    }
                                  }
                                ]
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "min": {
                                "type": "number"
                              },
                              "max": {
                                "type": "number"
                              },
                              "step": {
                                "type": "number"
                              },
                              "display": {
                                "type": "string",
                                "enum": [
                                  "slider",
                                  "number",
                                  "knob"
                                ]
                              },
                              "round": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "boolean",
                                    "const": false
                                  }
                                ]
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "BOOLEAN"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "type": "boolean"
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "label_on": {
                                "type": "string"
                              },
                              "label_off": {
                                "type": "string"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "STRING"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "type": "string"
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "multiline": {
                                "type": "boolean"
                              },
                              "dynamicPrompts": {
                                "type": "boolean"
                              },
                              "defaultVal": {
                                "type": "string"
                              },
                              "placeholder": {
                                "type": "string"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "array",
                        "items": {
                          "type": [
                            "string",
                            "number"
                          ]
                        }
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              },
                              "image_upload": {
                                "type": "boolean"
                              },
                              "image_folder": {
                                "type": "string",
                                "enum": [
                                  "input",
                                  "output",
                                  "temp"
                                ]
                              },
                              "allow_batch": {
                                "type": "boolean"
                              },
                              "video_upload": {
                                "type": "boolean"
                              },
                              "remote": {
                                "type": "object",
                                "properties": {
                                  "route": {
                                    "anyOf": [
                                      {
                                        "type": "string",
                                        "format": "uri"
                                      },
                                      {
                                        "type": "string",
                                        "pattern": "^\\/"
                                      }
                                    ]
                                  },
                                  "refresh": {
                                    "anyOf": [
                                      {
                                        "type": "number",
                                        "minimum": -9007199254740991,
                                        "maximum": 9007199254740991
                                      },
                                      {
                                        "type": "number",
                                        "maximum": 9007199254740991,
                                        "minimum": -9007199254740991
                                      }
                                    ]
                                  },
                                  "response_key": {
                                    "type": "string"
                                  },
                                  "query_params": {
                                    "type": "object",
                                    "additionalProperties": {
                                      "type": "string"
                                    }
                                  },
                                  "refresh_button": {
                                    "type": "boolean"
                                  },
                                  "control_after_refresh": {
                                    "type": "string",
                                    "enum": [
                                      "first",
                                      "last"
                                    ]
                                  },
                                  "timeout": {
                                    "type": "number",
                                    "minimum": 0
                                  },
                                  "max_retries": {
                                    "type": "number",
                                    "minimum": 0
                                  }
                                },
                                "required": [
                                  "route"
                                ],
                                "additionalProperties": false
                              },
                              "options": {
                                "type": "array",
                                "items": {
                                  "type": [
                                    "string",
                                    "number"
                                  ]
                                }
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "COMBO"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              },
                              "image_upload": {
                                "type": "boolean"
                              },
                              "image_folder": {
                                "type": "string",
                                "enum": [
                                  "input",
                                  "output",
                                  "temp"
                                ]
                              },
                              "allow_batch": {
                                "type": "boolean"
                              },
                              "video_upload": {
                                "type": "boolean"
                              },
                              "remote": {
                                "type": "object",
                                "properties": {
                                  "route": {
                                    "anyOf": [
                                      {
                                        "type": "string",
                                        "format": "uri"
                                      },
                                      {
                                        "type": "string",
                                        "pattern": "^\\/"
                                      }
                                    ]
                                  },
                                  "refresh": {
                                    "anyOf": [
                                      {
                                        "type": "number",
                                        "minimum": -9007199254740991,
                                        "maximum": 9007199254740991
                                      },
                                      {
                                        "type": "number",
                                        "maximum": 9007199254740991,
                                        "minimum": -9007199254740991
                                      }
                                    ]
                                  },
                                  "response_key": {
                                    "type": "string"
                                  },
                                  "query_params": {
                                    "type": "object",
                                    "additionalProperties": {
                                      "type": "string"
                                    }
                                  },
                                  "refresh_button": {
                                    "type": "boolean"
                                  },
                                  "control_after_refresh": {
                                    "type": "string",
                                    "enum": [
                                      "first",
                                      "last"
                                    ]
                                  },
                                  "timeout": {
                                    "type": "number",
                                    "minimum": 0
                                  },
                                  "max_retries": {
                                    "type": "number",
                                    "minimum": 0
                                  }
                                },
                                "required": [
                                  "route"
                                ],
                                "additionalProperties": false
                              },
                              "options": {
                                "type": "array",
                                "items": {
                                  "type": [
                                    "string",
                                    "number"
                                  ]
                                }
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  }
                ]
              }
            },
            "optional": {
              "type": "object",
              "additionalProperties": {
                "anyOf": [
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "INT"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "array",
                                    "items": {
                                      "type": "number"
                                    }
                                  }
                                ]
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "min": {
                                "type": "number"
                              },
                              "max": {
                                "type": "number"
                              },
                              "step": {
                                "type": "number"
                              },
                              "display": {
                                "type": "string",
                                "enum": [
                                  "slider",
                                  "number",
                                  "knob"
                                ]
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "FLOAT"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "array",
                                    "items": {
                                      "type": "number"
                                    }
                                  }
                                ]
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "min": {
                                "type": "number"
                              },
                              "max": {
                                "type": "number"
                              },
                              "step": {
                                "type": "number"
                              },
                              "display": {
                                "type": "string",
                                "enum": [
                                  "slider",
                                  "number",
                                  "knob"
                                ]
                              },
                              "round": {
                                "anyOf": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "boolean",
                                    "const": false
                                  }
                                ]
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "BOOLEAN"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "type": "boolean"
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "label_on": {
                                "type": "string"
                              },
                              "label_off": {
                                "type": "string"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "STRING"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {
                                "type": "string"
                              },
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "multiline": {
                                "type": "boolean"
                              },
                              "dynamicPrompts": {
                                "type": "boolean"
                              },
                              "defaultVal": {
                                "type": "string"
                              },
                              "placeholder": {
                                "type": "string"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "array",
                        "items": {
                          "type": [
                            "string",
                            "number"
                          ]
                        }
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              },
                              "image_upload": {
                                "type": "boolean"
                              },
                              "image_folder": {
                                "type": "string",
                                "enum": [
                                  "input",
                                  "output",
                                  "temp"
                                ]
                              },
                              "allow_batch": {
                                "type": "boolean"
                              },
                              "video_upload": {
                                "type": "boolean"
                              },
                              "remote": {
                                "type": "object",
                                "properties": {
                                  "route": {
                                    "anyOf": [
                                      {
                                        "type": "string",
                                        "format": "uri"
                                      },
                                      {
                                        "type": "string",
                                        "pattern": "^\\/"
                                      }
                                    ]
                                  },
                                  "refresh": {
                                    "anyOf": [
                                      {
                                        "type": "number",
                                        "minimum": -9007199254740991,
                                        "maximum": 9007199254740991
                                      },
                                      {
                                        "type": "number",
                                        "maximum": 9007199254740991,
                                        "minimum": -9007199254740991
                                      }
                                    ]
                                  },
                                  "response_key": {
                                    "type": "string"
                                  },
                                  "query_params": {
                                    "type": "object",
                                    "additionalProperties": {
                                      "type": "string"
                                    }
                                  },
                                  "refresh_button": {
                                    "type": "boolean"
                                  },
                                  "control_after_refresh": {
                                    "type": "string",
                                    "enum": [
                                      "first",
                                      "last"
                                    ]
                                  },
                                  "timeout": {
                                    "type": "number",
                                    "minimum": 0
                                  },
                                  "max_retries": {
                                    "type": "number",
                                    "minimum": 0
                                  }
                                },
                                "required": [
                                  "route"
                                ],
                                "additionalProperties": false
                              },
                              "options": {
                                "type": "array",
                                "items": {
                                  "type": [
                                    "string",
                                    "number"
                                  ]
                                }
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string",
                        "const": "COMBO"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              },
                              "control_after_generate": {
                                "type": "boolean"
                              },
                              "image_upload": {
                                "type": "boolean"
                              },
                              "image_folder": {
                                "type": "string",
                                "enum": [
                                  "input",
                                  "output",
                                  "temp"
                                ]
                              },
                              "allow_batch": {
                                "type": "boolean"
                              },
                              "video_upload": {
                                "type": "boolean"
                              },
                              "remote": {
                                "type": "object",
                                "properties": {
                                  "route": {
                                    "anyOf": [
                                      {
                                        "type": "string",
                                        "format": "uri"
                                      },
                                      {
                                        "type": "string",
                                        "pattern": "^\\/"
                                      }
                                    ]
                                  },
                                  "refresh": {
                                    "anyOf": [
                                      {
                                        "type": "number",
                                        "minimum": -9007199254740991,
                                        "maximum": 9007199254740991
                                      },
                                      {
                                        "type": "number",
                                        "maximum": 9007199254740991,
                                        "minimum": -9007199254740991
                                      }
                                    ]
                                  },
                                  "response_key": {
                                    "type": "string"
                                  },
                                  "query_params": {
                                    "type": "object",
                                    "additionalProperties": {
                                      "type": "string"
                                    }
                                  },
                                  "refresh_button": {
                                    "type": "boolean"
                                  },
                                  "control_after_refresh": {
                                    "type": "string",
                                    "enum": [
                                      "first",
                                      "last"
                                    ]
                                  },
                                  "timeout": {
                                    "type": "number",
                                    "minimum": 0
                                  },
                                  "max_retries": {
                                    "type": "number",
                                    "minimum": 0
                                  }
                                },
                                "required": [
                                  "route"
                                ],
                                "additionalProperties": false
                              },
                              "options": {
                                "type": "array",
                                "items": {
                                  "type": [
                                    "string",
                                    "number"
                                  ]
                                }
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "string"
                      },
                      {
                        "anyOf": [
                          {
                            "not": {}
                          },
                          {
                            "type": "object",
                            "properties": {
                              "default": {},
                              "defaultInput": {
                                "type": "boolean"
                              },
                              "forceInput": {
                                "type": "boolean"
                              },
                              "tooltip": {
                                "type": "string"
                              },
                              "hidden": {
                                "type": "boolean"
                              },
                              "advanced": {
                                "type": "boolean"
                              },
                              "rawLink": {
                                "type": "boolean"
                              },
                              "lazy": {
                                "type": "boolean"
                              }
                            },
                            "additionalProperties": true
                          }
                        ]
                      }
                    ]
                  }
                ]
              }
            },
            "hidden": {
              "type": "object",
              "additionalProperties": {}
            }
          },
          "additionalProperties": false
        },
        "output": {
          "type": "array",
          "items": {
            "anyOf": [
              {
                "type": "string"
              },
              {
                "type": "array",
                "items": {
                  "type": [
                    "string",
                    "number"
                  ]
                }
              }
            ]
          }
        },
        "output_is_list": {
          "type": "array",
          "items": {
            "type": "boolean"
          }
        },
        "output_name": {
          "type": "array",
          "items": {
            "type": "string"
          }
        },
        "output_tooltips": {
          "type": "array",
          "items": {
            "type": "string"
          }
        },
        "name": {
          "type": "string"
        },
        "display_name": {
          "type": "string"
        },
        "description": {
          "type": "string"
        },
        "category": {
          "type": "string"
        },
        "output_node": {
          "type": "boolean"
        },
        "python_module": {
          "type": "string"
        },
        "deprecated": {
          "type": "boolean"
        },
        "experimental": {
          "type": "boolean"
        }
      },
      "required": [
        "name",
        "display_name",
        "description",
        "category",
        "output_node",
        "python_module"
      ],
      "additionalProperties": false
    }
  },
  "$schema": "http://json-schema.org/draft-07/schema#"
}
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/specs/nodedef_json_1_0.mdx)

[Previous  
\
Node Definition JSONJSON schema for a ComfyUI Node.](http://docs.comfy.org/specs/nodedef_json)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [v1.0](http://docs.comfy.org#v1-0)

<!-- END Get_Started/specs/nodedef_json_1_0.md -->


<!-- BEGIN Get_Started/specs/workflow_json.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
  
  - [Workflow JSON](http://docs.comfy.org/specs/workflow_json)
  - [Workflow JSON 0.4](http://docs.comfy.org/specs/workflow_json_0.4)
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Workflow JSON

# Workflow JSON

JSON schema for a ComfyUI workflow.

The workflow JSON is defined using [JSON Schema](https://json-schema.org/). Changes to this schema will be discussed in the [rfcs repo](https://github.com/comfy-org/rfcs).

## [​](http://docs.comfy.org#version-1-0-latest) Version 1.0 (Latest)

ComfyUI Workflow v1.0

```json
{
  "$ref": "#/definitions/ComfyWorkflow1_0",
  "definitions": {
    "ComfyWorkflow1_0": {
      "type": "object",
      "properties": {
        "version": {
          "type": "number",
          "const": 1
        },
        "config": {
          "anyOf": [
            {
              "anyOf": [
                {
                  "not": {}
                },
                {
                  "type": "object",
                  "properties": {
                    "links_ontop": {
                      "type": "boolean"
                    },
                    "align_to_grid": {
                      "type": "boolean"
                    }
                  },
                  "additionalProperties": true
                }
              ]
            },
            {
              "type": "null"
            }
          ]
        },
        "state": {
          "type": "object",
          "properties": {
            "lastGroupid": {
              "type": "number"
            },
            "lastNodeId": {
              "type": "number"
            },
            "lastLinkId": {
              "type": "number"
            },
            "lastRerouteId": {
              "type": "number"
            }
          },
          "additionalProperties": true
        },
        "groups": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "title": {
                "type": "string"
              },
              "bounding": {
                "type": "array",
                "minItems": 4,
                "maxItems": 4,
                "items": [
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  }
                ]
              },
              "color": {
                "type": "string"
              },
              "font_size": {
                "type": "number"
              },
              "locked": {
                "type": "boolean"
              }
            },
            "required": [
              "title",
              "bounding"
            ],
            "additionalProperties": true
          }
        },
        "nodes": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "id": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "type": {
                "type": "string"
              },
              "pos": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "size": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "flags": {
                "type": "object",
                "properties": {
                  "collapsed": {
                    "type": "boolean"
                  },
                  "pinned": {
                    "type": "boolean"
                  },
                  "allow_interaction": {
                    "type": "boolean"
                  },
                  "horizontal": {
                    "type": "boolean"
                  },
                  "skip_repeated_outputs": {
                    "type": "boolean"
                  }
                },
                "additionalProperties": true
              },
              "order": {
                "type": "number"
              },
              "mode": {
                "type": "number"
              },
              "inputs": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "type": {
                      "anyOf": [
                        {
                          "type": "string"
                        },
                        {
                          "type": "array",
                          "items": {
                            "type": "string"
                          }
                        },
                        {
                          "type": "number"
                        }
                      ]
                    },
                    "link": {
                      "type": [
                        "number",
                        "null"
                      ]
                    },
                    "slot_index": {
                      "anyOf": [
                        {
                          "type": "integer"
                        },
                        {
                          "type": "string"
                        }
                      ]
                    }
                  },
                  "required": [
                    "name",
                    "type"
                  ],
                  "additionalProperties": true
                }
              },
              "outputs": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "type": {
                      "anyOf": [
                        {
                          "type": "string"
                        },
                        {
                          "type": "array",
                          "items": {
                            "type": "string"
                          }
                        },
                        {
                          "type": "number"
                        }
                      ]
                    },
                    "links": {
                      "anyOf": [
                        {
                          "type": "array",
                          "items": {
                            "type": "number"
                          }
                        },
                        {
                          "type": "null"
                        }
                      ]
                    },
                    "slot_index": {
                      "anyOf": [
                        {
                          "type": "integer"
                        },
                        {
                          "type": "string"
                        }
                      ]
                    }
                  },
                  "required": [
                    "name",
                    "type"
                  ],
                  "additionalProperties": true
                }
              },
              "properties": {
                "type": "object",
                "properties": {
                  "Node name for S&R": {
                    "type": "string"
                  }
                },
                "additionalProperties": true
              },
              "widgets_values": {
                "anyOf": [
                  {
                    "type": "array"
                  },
                  {
                    "type": "object",
                    "additionalProperties": {}
                  }
                ]
              },
              "color": {
                "type": "string"
              },
              "bgcolor": {
                "type": "string"
              }
            },
            "required": [
              "id",
              "type",
              "pos",
              "size",
              "flags",
              "order",
              "mode",
              "properties"
            ],
            "additionalProperties": true
          }
        },
        "links": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "id": {
                "type": "number"
              },
              "origin_id": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "origin_slot": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "target_id": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "target_slot": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "type": {
                "anyOf": [
                  {
                    "type": "string"
                  },
                  {
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  {
                    "type": "number"
                  }
                ]
              },
              "parentId": {
                "type": "number"
              }
            },
            "required": [
              "id",
              "origin_id",
              "origin_slot",
              "target_id",
              "target_slot",
              "type"
            ],
            "additionalProperties": true
          }
        },
        "reroutes": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "id": {
                "type": "number"
              },
              "parentId": {
                "type": "number"
              },
              "pos": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "linkIds": {
                "anyOf": [
                  {
                    "type": "array",
                    "items": {
                      "type": "number"
                    }
                  },
                  {
                    "type": "null"
                  }
                ]
              }
            },
            "required": [
              "id",
              "pos"
            ],
            "additionalProperties": true
          }
        },
        "extra": {
          "anyOf": [
            {
              "anyOf": [
                {
                  "not": {}
                },
                {
                  "type": "object",
                  "properties": {
                    "ds": {
                      "type": "object",
                      "properties": {
                        "scale": {
                          "type": "number"
                        },
                        "offset": {
                          "anyOf": [
                            {
                              "type": "object",
                              "properties": {
                                "0": {
                                  "type": "number"
                                },
                                "1": {
                                  "type": "number"
                                }
                              },
                              "required": [
                                "0",
                                "1"
                              ],
                              "additionalProperties": true
                            },
                            {
                              "type": "array",
                              "minItems": 2,
                              "maxItems": 2,
                              "items": [
                                {
                                  "type": "number"
                                },
                                {
                                  "type": "number"
                                }
                              ]
                            }
                          ]
                        }
                      },
                      "required": [
                        "scale",
                        "offset"
                      ],
                      "additionalProperties": true
                    },
                    "info": {
                      "type": "object",
                      "properties": {
                        "name": {
                          "type": "string"
                        },
                        "author": {
                          "type": "string"
                        },
                        "description": {
                          "type": "string"
                        },
                        "version": {
                          "type": "string"
                        },
                        "created": {
                          "type": "string"
                        },
                        "modified": {
                          "type": "string"
                        },
                        "software": {
                          "type": "string"
                        }
                      },
                      "required": [
                        "name",
                        "author",
                        "description",
                        "version",
                        "created",
                        "modified",
                        "software"
                      ],
                      "additionalProperties": true
                    },
                    "linkExtensions": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "id": {
                            "type": "number"
                          },
                          "parentId": {
                            "type": "number"
                          }
                        },
                        "required": [
                          "id",
                          "parentId"
                        ],
                        "additionalProperties": true
                      }
                    },
                    "reroutes": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "id": {
                            "type": "number"
                          },
                          "parentId": {
                            "type": "number"
                          },
                          "pos": {
                            "anyOf": [
                              {
                                "type": "object",
                                "properties": {
                                  "0": {
                                    "type": "number"
                                  },
                                  "1": {
                                    "type": "number"
                                  }
                                },
                                "required": [
                                  "0",
                                  "1"
                                ],
                                "additionalProperties": true
                              },
                              {
                                "type": "array",
                                "minItems": 2,
                                "maxItems": 2,
                                "items": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "number"
                                  }
                                ]
                              }
                            ]
                          },
                          "linkIds": {
                            "anyOf": [
                              {
                                "type": "array",
                                "items": {
                                  "type": "number"
                                }
                              },
                              {
                                "type": "null"
                              }
                            ]
                          }
                        },
                        "required": [
                          "id",
                          "pos"
                        ],
                        "additionalProperties": true
                      }
                    }
                  },
                  "additionalProperties": true
                }
              ]
            },
            {
              "type": "null"
            }
          ]
        },
        "models": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "name": {
                "type": "string"
              },
              "url": {
                "type": "string",
                "format": "uri"
              },
              "hash": {
                "type": "string"
              },
              "hash_type": {
                "type": "string"
              },
              "directory": {
                "type": "string"
              }
            },
            "required": [
              "name",
              "url",
              "directory"
            ],
            "additionalProperties": false
          }
        }
      },
      "required": [
        "version",
        "state",
        "nodes"
      ],
      "additionalProperties": true
    }
  },
  "$schema": "http://json-schema.org/draft-07/schema#"
}
```

## [​](http://docs.comfy.org#older-versions) Older versions

- [0.4](http://docs.comfy.org/workflow_json_0.4.mdx)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/specs/workflow_json.mdx)

[Previous](http://docs.comfy.org/registry/specifications)

[Workflow JSON 0.4JSON schema for a ComfyUI workflow.  
\
Next](http://docs.comfy.org/specs/workflow_json_0.4)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Version 1.0 (Latest)](http://docs.comfy.org#version-1-0-latest)
- [Older versions](http://docs.comfy.org#older-versions)

<!-- END Get_Started/specs/workflow_json.md -->


<!-- BEGIN Get_Started/specs/workflow_json_0.4.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Development

##### ComfyUI Server

- [Server Overview](http://docs.comfy.org/essentials/comfyui-server/comms_overview)
- [Messages](http://docs.comfy.org/essentials/comfyui-server/comms_messages)
- [Execution Model Inversion Guide](http://docs.comfy.org/essentials/comfyui-server/execution_model_inversion_guide)

##### CLI

- [Getting Started](http://docs.comfy.org/comfy-cli/getting-started)
- [Reference](http://docs.comfy.org/comfy-cli/reference)

##### Develop Custom Nodes

- [Overview](http://docs.comfy.org/custom-nodes/overview)
- [Getting Started](http://docs.comfy.org/custom-nodes/walkthrough)
- Backend
- UI
- [Workflow templates](http://docs.comfy.org/custom-nodes/workflow_templates)
- [Tips](http://docs.comfy.org/custom-nodes/tips)

##### Registry

- [Overview](http://docs.comfy.org/registry/overview)
- [Publishing Nodes](http://docs.comfy.org/registry/publishing)
- [Standards](http://docs.comfy.org/registry/standards)
- [Custom Node CI/CD](http://docs.comfy.org/registry/cicd)
- [pyproject.toml](http://docs.comfy.org/registry/specifications)
- [](http://docs.comfy.org/)

##### Specifications

- Workflow JSON
  
  - [Workflow JSON](http://docs.comfy.org/specs/workflow_json)
  - [Workflow JSON 0.4](http://docs.comfy.org/specs/workflow_json_0.4)
- Node Definitions

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

Workflow JSON 0.4

# Workflow JSON 0.4

JSON schema for a ComfyUI workflow.

## [​](http://docs.comfy.org#v0-4) v0.4

```json
{
  "$ref": "#/definitions/ComfyWorkflow0_4",
  "definitions": {
    "ComfyWorkflow0_4": {
      "type": "object",
      "properties": {
        "last_node_id": {
          "anyOf": [
            {
              "type": "integer"
            },
            {
              "type": "string"
            }
          ]
        },
        "last_link_id": {
          "type": "number"
        },
        "nodes": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "id": {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              "type": {
                "type": "string"
              },
              "pos": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "size": {
                "anyOf": [
                  {
                    "type": "object",
                    "properties": {
                      "0": {
                        "type": "number"
                      },
                      "1": {
                        "type": "number"
                      }
                    },
                    "required": [
                      "0",
                      "1"
                    ],
                    "additionalProperties": true
                  },
                  {
                    "type": "array",
                    "minItems": 2,
                    "maxItems": 2,
                    "items": [
                      {
                        "type": "number"
                      },
                      {
                        "type": "number"
                      }
                    ]
                  }
                ]
              },
              "flags": {
                "type": "object",
                "properties": {
                  "collapsed": {
                    "type": "boolean"
                  },
                  "pinned": {
                    "type": "boolean"
                  },
                  "allow_interaction": {
                    "type": "boolean"
                  },
                  "horizontal": {
                    "type": "boolean"
                  },
                  "skip_repeated_outputs": {
                    "type": "boolean"
                  }
                },
                "additionalProperties": true
              },
              "order": {
                "type": "number"
              },
              "mode": {
                "type": "number"
              },
              "inputs": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "type": {
                      "anyOf": [
                        {
                          "type": "string"
                        },
                        {
                          "type": "array",
                          "items": {
                            "type": "string"
                          }
                        },
                        {
                          "type": "number"
                        }
                      ]
                    },
                    "link": {
                      "type": [
                        "number",
                        "null"
                      ]
                    },
                    "slot_index": {
                      "anyOf": [
                        {
                          "type": "integer"
                        },
                        {
                          "type": "string"
                        }
                      ]
                    }
                  },
                  "required": [
                    "name",
                    "type"
                  ],
                  "additionalProperties": true
                }
              },
              "outputs": {
                "type": "array",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "type": {
                      "anyOf": [
                        {
                          "type": "string"
                        },
                        {
                          "type": "array",
                          "items": {
                            "type": "string"
                          }
                        },
                        {
                          "type": "number"
                        }
                      ]
                    },
                    "links": {
                      "anyOf": [
                        {
                          "type": "array",
                          "items": {
                            "type": "number"
                          }
                        },
                        {
                          "type": "null"
                        }
                      ]
                    },
                    "slot_index": {
                      "anyOf": [
                        {
                          "type": "integer"
                        },
                        {
                          "type": "string"
                        }
                      ]
                    }
                  },
                  "required": [
                    "name",
                    "type"
                  ],
                  "additionalProperties": true
                }
              },
              "properties": {
                "type": "object",
                "properties": {
                  "Node name for S&R": {
                    "type": "string"
                  }
                },
                "additionalProperties": true
              },
              "widgets_values": {
                "anyOf": [
                  {
                    "type": "array"
                  },
                  {
                    "type": "object",
                    "additionalProperties": {}
                  }
                ]
              },
              "color": {
                "type": "string"
              },
              "bgcolor": {
                "type": "string"
              }
            },
            "required": [
              "id",
              "type",
              "pos",
              "size",
              "flags",
              "order",
              "mode",
              "properties"
            ],
            "additionalProperties": true
          }
        },
        "links": {
          "type": "array",
          "items": {
            "type": "array",
            "minItems": 6,
            "maxItems": 6,
            "items": [
              {
                "type": "number"
              },
              {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              {
                "anyOf": [
                  {
                    "type": "integer"
                  },
                  {
                    "type": "string"
                  }
                ]
              },
              {
                "anyOf": [
                  {
                    "type": "string"
                  },
                  {
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  },
                  {
                    "type": "number"
                  }
                ]
              }
            ]
          }
        },
        "groups": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "title": {
                "type": "string"
              },
              "bounding": {
                "type": "array",
                "minItems": 4,
                "maxItems": 4,
                "items": [
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  },
                  {
                    "type": "number"
                  }
                ]
              },
              "color": {
                "type": "string"
              },
              "font_size": {
                "type": "number"
              },
              "locked": {
                "type": "boolean"
              }
            },
            "required": [
              "title",
              "bounding"
            ],
            "additionalProperties": true
          }
        },
        "config": {
          "anyOf": [
            {
              "anyOf": [
                {
                  "not": {}
                },
                {
                  "type": "object",
                  "properties": {
                    "links_ontop": {
                      "type": "boolean"
                    },
                    "align_to_grid": {
                      "type": "boolean"
                    }
                  },
                  "additionalProperties": true
                }
              ]
            },
            {
              "type": "null"
            }
          ]
        },
        "extra": {
          "anyOf": [
            {
              "anyOf": [
                {
                  "not": {}
                },
                {
                  "type": "object",
                  "properties": {
                    "ds": {
                      "type": "object",
                      "properties": {
                        "scale": {
                          "type": "number"
                        },
                        "offset": {
                          "anyOf": [
                            {
                              "type": "object",
                              "properties": {
                                "0": {
                                  "type": "number"
                                },
                                "1": {
                                  "type": "number"
                                }
                              },
                              "required": [
                                "0",
                                "1"
                              ],
                              "additionalProperties": true
                            },
                            {
                              "type": "array",
                              "minItems": 2,
                              "maxItems": 2,
                              "items": [
                                {
                                  "type": "number"
                                },
                                {
                                  "type": "number"
                                }
                              ]
                            }
                          ]
                        }
                      },
                      "required": [
                        "scale",
                        "offset"
                      ],
                      "additionalProperties": true
                    },
                    "info": {
                      "type": "object",
                      "properties": {
                        "name": {
                          "type": "string"
                        },
                        "author": {
                          "type": "string"
                        },
                        "description": {
                          "type": "string"
                        },
                        "version": {
                          "type": "string"
                        },
                        "created": {
                          "type": "string"
                        },
                        "modified": {
                          "type": "string"
                        },
                        "software": {
                          "type": "string"
                        }
                      },
                      "required": [
                        "name",
                        "author",
                        "description",
                        "version",
                        "created",
                        "modified",
                        "software"
                      ],
                      "additionalProperties": true
                    },
                    "linkExtensions": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "id": {
                            "type": "number"
                          },
                          "parentId": {
                            "type": "number"
                          }
                        },
                        "required": [
                          "id",
                          "parentId"
                        ],
                        "additionalProperties": true
                      }
                    },
                    "reroutes": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "id": {
                            "type": "number"
                          },
                          "parentId": {
                            "type": "number"
                          },
                          "pos": {
                            "anyOf": [
                              {
                                "type": "object",
                                "properties": {
                                  "0": {
                                    "type": "number"
                                  },
                                  "1": {
                                    "type": "number"
                                  }
                                },
                                "required": [
                                  "0",
                                  "1"
                                ],
                                "additionalProperties": true
                              },
                              {
                                "type": "array",
                                "minItems": 2,
                                "maxItems": 2,
                                "items": [
                                  {
                                    "type": "number"
                                  },
                                  {
                                    "type": "number"
                                  }
                                ]
                              }
                            ]
                          },
                          "linkIds": {
                            "anyOf": [
                              {
                                "type": "array",
                                "items": {
                                  "type": "number"
                                }
                              },
                              {
                                "type": "null"
                              }
                            ]
                          }
                        },
                        "required": [
                          "id",
                          "pos"
                        ],
                        "additionalProperties": true
                      }
                    }
                  },
                  "additionalProperties": true
                }
              ]
            },
            {
              "type": "null"
            }
          ]
        },
        "version": {
          "type": "number"
        },
        "models": {
          "type": "array",
          "items": {
            "type": "object",
            "properties": {
              "name": {
                "type": "string"
              },
              "url": {
                "type": "string",
                "format": "uri"
              },
              "hash": {
                "type": "string"
              },
              "hash_type": {
                "type": "string"
              },
              "directory": {
                "type": "string"
              }
            },
            "required": [
              "name",
              "url",
              "directory"
            ],
            "additionalProperties": false
          }
        }
      },
      "required": [
        "last_node_id",
        "last_link_id",
        "nodes",
        "links",
        "version"
      ],
      "additionalProperties": true
    }
  },
  "$schema": "http://json-schema.org/draft-07/schema#"
}
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/specs/workflow_json_0.4.mdx)

[Previous](http://docs.comfy.org/specs/workflow_json)

[Node Definition JSONJSON schema for a ComfyUI Node.  
\
Next](http://docs.comfy.org/specs/nodedef_json)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [v0.4](http://docs.comfy.org#v0-4)

<!-- END Get_Started/specs/workflow_json_0.4.md -->


<!-- BEGIN Get_Started/tutorials/3d/hunyuan3D-2.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
  
  - [Hunyuan3D-2](http://docs.comfy.org/tutorials/3d/hunyuan3D-2)
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Hunyuan3D-2 Examples

# ComfyUI Hunyuan3D-2 Examples

This guide will demonstrate how to use Hunyuan3D-2 in ComfyUI to generate 3D assets.

# [​](http://docs.comfy.org#hunyuan3d-2-0-introduction) Hunyuan3D 2.0 Introduction

[Hunyuan3D 2.0](https://github.com/Tencent/Hunyuan3D-2) is an open-source 3D asset generation model released by Tencent, capable of generating high-fidelity 3D models with high-resolution texture maps through text or images.

Hunyuan3D 2.0 adopts a two-stage generation approach, first generating a geometry model without textures, then synthesizing high-resolution texture maps. This effectively separates the complexity of shape and texture generation. Below are the two core components of Hunyuan3D 2.0:

1. **Geometry Generation Model (Hunyuan3D-DiT)**: Based on a flow diffusion Transformer architecture, it generates untextured geometric models that precisely match input conditions.
2. **Texture Generation Model (Hunyuan3D-Paint)**: Combines geometric conditions and multi-view diffusion techniques to add high-resolution textures to models, supporting PBR materials.

**Key Advantages**

- **High-Precision Generation**: Sharp geometric structures, rich texture colors, support for PBR material generation, achieving near-realistic lighting effects.
- **Diverse Usage Methods**: Provides code calls, Blender plugins, Gradio applications, and online experience through the official website, suitable for different user needs.
- **Lightweight and Compatibility**: The Hunyuan3D-2mini model requires only 5GB VRAM, the standard version needs 6GB VRAM for shape generation, and the complete process (shape + texture) requires only 12GB VRAM.

Recently (March 18, 2025), Hunyuan3D 2.0 also introduced a multi-view shape generation model (Hunyuan3D-2mv), which supports generating more detailed geometric structures from inputs at different angles.

This example includes three workflows:

- Using Hunyuan3D-2mv with multiple view inputs to generate 3D models
- Using Hunyuan3D-2mv-turbo with multiple view inputs to generate 3D models
- Using Hunyuan3D-2 with a single view input to generate 3D models

ComfyUI now natively supports Hunyuan3D-2mv, but does not yet support texture and material generation. Please make sure you have updated to the latest version of [ComfyUI](https://github.com/comfyanonymous/ComfyUI) before starting.

The workflow example PNG images in this tutorial contain workflow JSON in their metadata:

- You can drag them directly into ComfyUI
- Or use the menu `Workflows` -&gt; `Open (ctrl+o)`

This will load the corresponding workflow and prompt you to download the required models. The generated `.glb` format models will be output to the `ComfyUI/output/mesh` folder.

## [​](http://docs.comfy.org#comfyui-hunyuan3d-2mv-workflow-example) ComfyUI Hunyuan3D-2mv Workflow Example

In the Hunyuan3D-2mv workflow, we’ll use multi-view images to generate a 3D model. Note that multiple view images are not mandatory in this workflow - you can use only the `front` view image to generate a 3D model.

### [​](http://docs.comfy.org#1-workflow) 1. Workflow

Please download the images below and drag into ComfyUI to load the workflow.

Download the images below we will use them as input images.

In this example, the input images have already been preprocessed to remove excess background. In actual use, you can use custom nodes like [ComfyUI\_essentials](https://github.com/cubiq/ComfyUI_essentials) to automatically remove excess background.

### [​](http://docs.comfy.org#2-manual-model-installation) 2. Manual Model Installation

Download the model below and save it to the corresponding ComfyUI folder

- hunyuan3d-dit-v2-mv: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2-mv.safetensors`

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── hunyuan3d-dit-v2-mv.safetensors  // renamed file
```

### [​](http://docs.comfy.org#3-steps-to-run-the-workflow) 3. Steps to Run the Workflow

1. Ensure that the Image Only Checkpoint Loader(img2vid model) has loaded our downloaded and renamed `hunyuan3d-dit-v2-mv.safetensors` model
2. Load the corresponding view images in each of the `Load Image` nodes
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

If you need to add more views, make sure to load other view images in the `Hunyuan3Dv2ConditioningMultiView` node, and ensure that you load the corresponding view images in the `Load Image` nodes.

## [​](http://docs.comfy.org#hunyuan3d-2mv-turbo-workflow) Hunyuan3D-2mv-turbo Workflow

In the Hunyuan3D-2mv-turbo workflow, we’ll use the Hunyuan3D-2mv-turbo model to generate 3D models. This model is a step distillation version of Hunyuan3D-2mv, allowing for faster 3D model generation. In this version of the workflow, we set `cfg` to 1.0 and add a `flux guidance` node to control the `distilled cfg` generation.

### [​](http://docs.comfy.org#1-workflow-2) 1. Workflow

Please download the images below and drag into ComfyUI to load the workflow.

Download the images below we will use them as input images.

### [​](http://docs.comfy.org#2-manual-model-installation-2) 2. Manual Model Installation

Download the model below and save it to the corresponding ComfyUI folder

- hunyuan3d-dit-v2-mv-turbo: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2mv/resolve/main/hunyuan3d-dit-v2-mv-turbo/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2-mv-turbo.safetensors`

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── hunyuan3d-dit-v2-mv-turbo.safetensors  // renamed file
```

### [​](http://docs.comfy.org#3-steps-to-run-the-workflow-2) 3. Steps to Run the Workflow

1. Ensure that the `Image Only Checkpoint Loader(img2vid model)` node has loaded our renamed `hunyuan3d-dit-v2-mv-turbo.safetensors` model
2. Load the corresponding view images in each of the `Load Image` nodes
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## [​](http://docs.comfy.org#hunyuan3d-2-single-view-workflow) Hunyuan3D-2 Single View Workflow

In the Hunyuan3D-2 workflow, we’ll use the Hunyuan3D-2 model to generate 3D models. This model is not a multi-view model. In this workflow, we use the `Hunyuan3Dv2Conditioning` node instead of the `Hunyuan3Dv2ConditioningMultiView` node.

### [​](http://docs.comfy.org#1-workflow-3) 1. Workflow

Please download the image below and drag it into ComfyUI to load the workflow.

Download the image below we will use it as input image.

### [​](http://docs.comfy.org#2-manual-model-installation-3) 2. Manual Model Installation

Download the model below and save it to the corresponding ComfyUI folder

- hunyuan3d-dit-v2-0: [model.fp16.safetensors](https://huggingface.co/tencent/Hunyuan3D-2/resolve/main/hunyuan3d-dit-v2-0/model.fp16.safetensors?download=true) - after downloading, you can rename it to `hunyuan3d-dit-v2.safetensors`

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── hunyuan3d-dit-v2.safetensors  // renamed file
```

### [​](http://docs.comfy.org#3-steps-to-run-the-workflow-3) 3. Steps to Run the Workflow

1. Ensure that the `Image Only Checkpoint Loader(img2vid model)` node has loaded our renamed `hunyuan3d-dit-v2.safetensors` model
2. Load the image in the `Load Image` node
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## [​](http://docs.comfy.org#community-resources) Community Resources

Below are ComfyUI community resources related to Hunyuan3D-2

- [ComfyUI-Hunyuan3DWrapper](https://github.com/kijai/ComfyUI-Hunyuan3DWrapper)
- [Kijai/Hunyuan3D-2\_safetensors](https://huggingface.co/Kijai/Hunyuan3D-2_safetensors/tree/main)
- [ComfyUI-3D-Pack](https://github.com/MrForExample/ComfyUI-3D-Pack)

## [​](http://docs.comfy.org#hunyuan3d-2-0-open-source-model-series) Hunyuan3D 2.0 Open-Source Model Series

Currently, Hunyuan3D 2.0 has open-sourced multiple models covering the complete 3D generation process. You can visit [Hunyuan3D-2](https://github.com/Tencent/Hunyuan3D-2) for more information.

**Hunyuan3D-2mini Series**

ModelDescriptionDateParametersHuggingfaceHunyuan3D-DiT-v2-miniMini Image to Shape Model2025-03-180.6B[Visit](https://huggingface.co/tencent/Hunyuan3D-2mini/tree/main/hunyuan3d-dit-v2-mini)

**Hunyuan3D-2mv Series**

ModelDescriptionDateParametersHuggingfaceHunyuan3D-DiT-v2-mv-FastGuidance Distillation Version, can halve DIT inference time2025-03-181.1B[Visit](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv-fast)Hunyuan3D-DiT-v2-mvMulti-view Image to Shape Model, suitable for 3D creation requiring multiple angles to understand the scene2025-03-181.1B[Visit](https://huggingface.co/tencent/Hunyuan3D-2mv/tree/main/hunyuan3d-dit-v2-mv)

**Hunyuan3D-2 Series**

ModelDescriptionDateParametersHuggingfaceHunyuan3D-DiT-v2-0-FastGuidance Distillation Model2025-02-031.1B[Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0-fast)Hunyuan3D-DiT-v2-0Image to Shape Model2025-01-211.1B[Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-dit-v2-0)Hunyuan3D-Paint-v2-0Texture Generation Model2025-01-211.3B[Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-paint-v2-0)Hunyuan3D-Delight-v2-0Image Delight Model2025-01-211.3B[Visit](https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-delight-v2-0)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/3d/hunyuan3D-2.mdx)

[Previous](http://docs.comfy.org/tutorials/image/hidream/hidream-e1)

[LTX-Video  
\
Next](http://docs.comfy.org/tutorials/video/ltxv)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Hunyuan3D 2.0 Introduction](http://docs.comfy.org#hunyuan3d-2-0-introduction)
- [ComfyUI Hunyuan3D-2mv Workflow Example](http://docs.comfy.org#comfyui-hunyuan3d-2mv-workflow-example)
- [1. Workflow](http://docs.comfy.org#1-workflow)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow)
- [Hunyuan3D-2mv-turbo Workflow](http://docs.comfy.org#hunyuan3d-2mv-turbo-workflow)
- [1. Workflow](http://docs.comfy.org#1-workflow-2)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation-2)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow-2)
- [Hunyuan3D-2 Single View Workflow](http://docs.comfy.org#hunyuan3d-2-single-view-workflow)
- [1. Workflow](http://docs.comfy.org#1-workflow-3)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation-3)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow-3)
- [Community Resources](http://docs.comfy.org#community-resources)
- [Hunyuan3D 2.0 Open-Source Model Series](http://docs.comfy.org#hunyuan3d-2-0-open-source-model-series)

<!-- END Get_Started/tutorials/3d/hunyuan3D-2.md -->


<!-- BEGIN Get_Started/tutorials/audio/ace-step/ace-step-v1.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
  
  - [ACE-Step Music Generation](http://docs.comfy.org/tutorials/audio/ace-step/ace-step-v1)
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI ACE-Step Native Example

# ComfyUI ACE-Step Native Example

This guide will help you create dynamic music using the ACE-Step model in ComfyUI

ACE-Step is an open-source foundational music generation model jointly developed by Chinese team StepFun and ACE Studio, aimed at providing music creators with efficient, flexible and high-quality music generation and editing tools.

The model is released under the [Apache-2.0](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-license) license and is free for commercial use.

As a powerful music generation foundation, ACE-Step provides rich extensibility. Through fine-tuning techniques like LoRA and ControlNet, developers can customize the model according to their actual needs. Whether it’s audio editing, vocal synthesis, accompaniment production, voice cloning or style transfer applications, ACE-Step provides stable and reliable technical support. This flexible architecture greatly simplifies the development process of music AI applications, allowing more creators to quickly apply AI technology to music creation.

Currently, ACE-Step has released related training code, including LoRA model training, and the corresponding ControlNet training code will be released in the future. You can visit their [Github](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-roadmap) to learn more details.

## [​](http://docs.comfy.org#ace-step-comfyui-text-to-audio-generation-workflow-example) ACE-Step ComfyUI Text-to-Audio Generation Workflow Example

### [​](http://docs.comfy.org#1-download-workflow-and-related-models) 1. Download Workflow and Related Models

Click the button below to download the corresponding workflow file. Drag it into ComfyUI to load the workflow information. The workflow includes model download information.

Click the button below to download the corresponding workflow file. Drag it into ComfyUI to load the workflow information. The workflow includes model download information.

[Download Json Format Workflow File](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/audio/ace-step/ace_step_1_t2m.json)

You can also manually download [ace\_step\_v1\_3.5b.safetensors](https://huggingface.co/Comfy-Org/ACE-Step_ComfyUI_repackaged/blob/main/all_in_one/ace_step_v1_3.5b.safetensors) and save it to the `ComfyUI/models/checkpoints` folder

### [​](http://docs.comfy.org#2-complete-the-workflow-step-by-step) 2. Complete the Workflow Step by Step

1. Ensure the `Load Checkpoints` node has loaded the `ace_step_v1_3.5b.safetensors` model
2. Input corresponding music styles etc. in the `tags` field of `TextEncodeAceStepAudio`
3. Input corresponding lyrics in the `lyrics` field of `TextEncodeAceStepAudio`
4. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the generation
5. After the generation is complete, you can view the generated audio in the `Save Audio` node. You can click to play and preview. The audio will also be saved to `ComfyUI/output/audio` (subdirectory determined by the `Save Audio` node).

## [​](http://docs.comfy.org#ace-step-comfyui-audio-to-audio-workflow) ACE-Step ComfyUI Audio-to-Audio Workflow

Similar to image-to-image workflows, you can input a piece of music and use the workflow below to resample and generate music. You can also adjust the difference from the original audio by controlling the `denoise` parameter in the `Ksampler`.

### [​](http://docs.comfy.org#1-download-workflow-file) 1. Download Workflow File

Click the button below to download the corresponding workflow file. Drag it into ComfyUI to load the workflow information.

[Download Json Format Workflow File](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/audio/ace-stepace_step_1_m2m_editing.json)

### [​](http://docs.comfy.org#2-complete-the-workflow-step-by-step-2) 2. Complete the Workflow Step by Step

1. Ensure the `Load Checkpoints` node has loaded the `ace_step_v1_3.5b.safetensors` model
2. Upload the music you want to edit in the `LoadAudio` node (you can use the results generated from the text-to-audio workflow in this article)
3. Input corresponding music styles etc. in the `tags` field of `TextEncodeAceStepAudio`
4. Input corresponding lyrics in the `lyrics` field of `TextEncodeAceStepAudio`, you can refer to the prompt guide part (still updating) or the lyrics examples on the ACE-Step project page
5. Modify the `denoise` parameter in the `Ksampler` node to adjust the amount of noise added during sampling, which controls the similarity to the original audio (smaller values result in greater similarity to the original audio; if set to `1.00`, it can be considered as if there is no audio input)
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the audio generation
7. After the generation is complete, you can view the generated audio in the `Save Audio` node. You can click to play and preview. The audio will also be saved to `ComfyUI/output/audio` (subdirectory determined by the `Save Audio` node).

You can also implement the lyrics modification and editing functionality from the ACE-Step project page, modifying the original lyrics to change the audio effect.

### [​](http://docs.comfy.org#3-lyrics-modification-and-editing-example) 3. Lyrics Modification and Editing Example

\[To be updated]

## [​](http://docs.comfy.org#ace-step-prompt-guide) ACE-Step Prompt Guide

ACE currently uses two types of prompts: `tags` and `lyrics`.

- `tags`: Mainly used to describe music styles, scenes, etc. Similar to prompts we use for other generations, they primarily describe the overall style and requirements of the audio, separated by English commas
- `lyrics`: Mainly used to describe lyrics, supporting lyric structure tags such as \[verse], \[chorus], and \[bridge] to distinguish different parts of the lyrics. You can also input instrument names for purely instrumental music

You can find rich examples of `tags` and `lyrics` on the [ACE-Step model homepage](https://ace-step.github.io/). You can refer to these examples to try corresponding prompts. This document’s prompt guide is organized based on the project to help you quickly try combinations to achieve your desired effect.

### [​](http://docs.comfy.org#tags-prompt) Tags (prompt)

#### [​](http://docs.comfy.org#mainstream-music-styles) Mainstream Music Styles

Use short tag combinations to generate specific music styles

- electronic
- rock
- pop
- funk
- soul
- cyberpunk
- Acid jazz
- electro
- em (electronic music)
- soft electric drums
- melodic

#### [​](http://docs.comfy.org#scene-types) Scene Types

Combine specific usage scenarios and atmospheres to generate music that matches the corresponding mood

- background music for parties
- radio broadcasts
- workout playlists

#### [​](http://docs.comfy.org#instrumental-elements) Instrumental Elements

- saxophone
- jazz
- piano, violin

#### [​](http://docs.comfy.org#vocal-types) Vocal Types

- female voice
- male voice
- clean vocals

#### [​](http://docs.comfy.org#professional-terms) Professional Terms

Use some professional terms commonly used in music to precisely control music effects

- 110 bpm (beats per minute is 110)
- fast tempo
- slow tempo
- loops
- fills
- acoustic guitar
- electric bass

### [​](http://docs.comfy.org#lyrics) Lyrics

#### [​](http://docs.comfy.org#lyric-structure-tags) Lyric Structure Tags

- \[outro]
- \[verse]
- \[chorus]
- \[bridge]

#### [​](http://docs.comfy.org#multilingual-support) Multilingual Support

- ACE-Step V1 supports multiple languages. When used, ACE-Step converts different languages into English letters and then generates music.
- In ComfyUI, we haven’t fully implemented the conversion of all languages to English letters. Currently, only [Japanese hiragana and katakana characters](https://github.com/comfyanonymous/ComfyUI/commit/5d3cc85e13833aeb6ef9242cdae243083e30c6fc) are implemented. So if you need to use multiple languages for music generation, you need to first convert the corresponding language to English letters, and then input the language code abbreviation at the beginning of the `lyrics`, such as Chinese `[zh]`, Korean `[ko]`, etc.

For example:

```plaintext
[zh]ni hao
[ko]an nyeong
```

Currently, ACE-Step supports 19 languages, but the following ten languages have better support:

- English
- Chinese: \[zh]
- Russian: \[ru]
- Spanish: \[es]
- Japanese: \[ja]
- German: \[de]
- French: \[fr]
- Portuguese: \[pt]
- Italian: \[it]
- Korean: \[ko]

The language tags above have not been fully tested at the time of writing this documentation. If any language tag is incorrect, please [submit an issue to our documentation repository](https://github.com/Comfy-Org/docs/issues) and we will make timely corrections.

## [​](http://docs.comfy.org#ace-step-related-resources) ACE-Step Related Resources

- [Project Page](https://ace-step.github.io/)
- [Hugging Face](https://huggingface.co/ACE-Step/ACE-Step-v1-3.5B)
- [GitHub](https://github.com/ace-step/ACE-Step)
- [Training Scripts](https://github.com/ace-step/ACE-Step?tab=readme-ov-file#-train)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/audio/ace-step/ace-step-v1.mdx)

[Previous](http://docs.comfy.org/tutorials/video/wan/wan-flf)

[OverviewIn this article, we will introduce ComfyUI's API Nodes and related information.  
\
Next](http://docs.comfy.org/tutorials/api-nodes/overview)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [ACE-Step ComfyUI Text-to-Audio Generation Workflow Example](http://docs.comfy.org#ace-step-comfyui-text-to-audio-generation-workflow-example)
- [1. Download Workflow and Related Models](http://docs.comfy.org#1-download-workflow-and-related-models)
- [2. Complete the Workflow Step by Step](http://docs.comfy.org#2-complete-the-workflow-step-by-step)
- [ACE-Step ComfyUI Audio-to-Audio Workflow](http://docs.comfy.org#ace-step-comfyui-audio-to-audio-workflow)
- [1. Download Workflow File](http://docs.comfy.org#1-download-workflow-file)
- [2. Complete the Workflow Step by Step](http://docs.comfy.org#2-complete-the-workflow-step-by-step-2)
- [3. Lyrics Modification and Editing Example](http://docs.comfy.org#3-lyrics-modification-and-editing-example)
- [ACE-Step Prompt Guide](http://docs.comfy.org#ace-step-prompt-guide)
- [Tags (prompt)](http://docs.comfy.org#tags-prompt)
- [Mainstream Music Styles](http://docs.comfy.org#mainstream-music-styles)
- [Scene Types](http://docs.comfy.org#scene-types)
- [Instrumental Elements](http://docs.comfy.org#instrumental-elements)
- [Vocal Types](http://docs.comfy.org#vocal-types)
- [Professional Terms](http://docs.comfy.org#professional-terms)
- [Lyrics](http://docs.comfy.org#lyrics)
- [Lyric Structure Tags](http://docs.comfy.org#lyric-structure-tags)
- [Multilingual Support](http://docs.comfy.org#multilingual-support)
- [ACE-Step Related Resources](http://docs.comfy.org#ace-step-related-resources)

<!-- END Get_Started/tutorials/audio/ace-step/ace-step-v1.md -->


<!-- BEGIN Get_Started/tutorials/basic/image-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
  
  - [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image)
  - [Image to Image](http://docs.comfy.org/tutorials/basic/image-to-image)
  - [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint)
  - [Outpaint](http://docs.comfy.org/tutorials/basic/outpaint)
  - [Upscale](http://docs.comfy.org/tutorials/basic/upscale)
  - [LoRA](http://docs.comfy.org/tutorials/basic/lora)
  - [Multiple LoRAs](http://docs.comfy.org/tutorials/basic/multiple-loras)
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Image to Image Workflow

# ComfyUI Image to Image Workflow

This guide will help you understand and complete an image to image workflow

## [​](http://docs.comfy.org#what-is-image-to-image) What is Image to Image

Image to Image is a workflow in ComfyUI that allows users to input an image and generate a new image based on it.

Image to Image can be used in scenarios such as:

- Converting original image styles, like transforming realistic photos into artistic styles
- Converting line art into realistic images
- Image restoration
- Colorizing old photos
- … and other scenarios

To explain it with an analogy: It’s like asking an artist to create a specific piece based on your reference image.

If you carefully compare this tutorial with the [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image) tutorial, you’ll notice that the Image to Image process is very similar to Text to Image, just with an additional input reference image as a condition. In Text to Image, we let the artist (image model) create freely based on our prompts, while in Image to Image, we let the artist create based on both our reference image and prompts.

## [​](http://docs.comfy.org#comfyui-image-to-image-workflow-example-guide) ComfyUI Image to Image Workflow Example Guide

### [​](http://docs.comfy.org#model-installation) Model Installation

Download the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) file and put it in your `ComfyUI/models/checkpoints` folder.

### [​](http://docs.comfy.org#image-to-image-workflow-and-input-image) Image to Image Workflow and Input Image

Download the image below and **drag it into ComfyUI** to load the workflow:

Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`.

Download the image below and we will use it as the input image:

### [​](http://docs.comfy.org#complete-the-workflow-step-by-step) Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

1. Ensure `Load Checkpoint` loads **v1-5-pruned-emaonly-fp16.safetensors**
2. Upload the input image to the `Load Image` node
3. Click `Queue` or press `Ctrl/Cmd + Enter` to generate

## [​](http://docs.comfy.org#key-points-of-image-to-image-workflow) Key Points of Image to Image Workflow

The key to the Image to Image workflow lies in the `denoise` parameter in the `KSampler` node, which should be **less than 1**

If you’ve adjusted the `denoise` parameter and generated images, you’ll notice:

- The smaller the `denoise` value, the smaller the difference between the generated image and the reference image
- The larger the `denoise` value, the larger the difference between the generated image and the reference image

This is because `denoise` determines the strength of noise added to the latent space image after converting the reference image. If `denoise` is 1, the latent space image will become completely random noise, making it the same as the latent space generated by the `empty latent image` node, losing all characteristics of the reference image.

For the corresponding principles, please refer to the principle explanation in the [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image) tutorial.

## [​](http://docs.comfy.org#try-it-yourself) Try It Yourself

1. Try modifying the `denoise` parameter in the **KSampler** node, gradually changing it from 1 to 0, and observe the changes in the generated images
2. Replace with your own prompts and reference images to generate your own image effects

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/basic/image-to-image.mdx)

[Previous](http://docs.comfy.org/tutorials/basic/text-to-image)

[InpaintThis guide will introduce you to the inpainting workflow in ComfyUI, walk you through an inpainting example, and cover topics like using the mask editor  
\
Next](http://docs.comfy.org/tutorials/basic/inpaint)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [What is Image to Image](http://docs.comfy.org#what-is-image-to-image)
- [ComfyUI Image to Image Workflow Example Guide](http://docs.comfy.org#comfyui-image-to-image-workflow-example-guide)
- [Model Installation](http://docs.comfy.org#model-installation)
- [Image to Image Workflow and Input Image](http://docs.comfy.org#image-to-image-workflow-and-input-image)
- [Complete the Workflow Step by Step](http://docs.comfy.org#complete-the-workflow-step-by-step)
- [Key Points of Image to Image Workflow](http://docs.comfy.org#key-points-of-image-to-image-workflow)
- [Try It Yourself](http://docs.comfy.org#try-it-yourself)

<!-- END Get_Started/tutorials/basic/image-to-image.md -->


<!-- BEGIN Get_Started/tutorials/basic/inpaint.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
  
  - [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image)
  - [Image to Image](http://docs.comfy.org/tutorials/basic/image-to-image)
  - [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint)
  - [Outpaint](http://docs.comfy.org/tutorials/basic/outpaint)
  - [Upscale](http://docs.comfy.org/tutorials/basic/upscale)
  - [LoRA](http://docs.comfy.org/tutorials/basic/lora)
  - [Multiple LoRAs](http://docs.comfy.org/tutorials/basic/multiple-loras)
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Inpainting Workflow

# ComfyUI Inpainting Workflow

This guide will introduce you to the inpainting workflow in ComfyUI, walk you through an inpainting example, and cover topics like using the mask editor

This article will introduce the concept of inpainting in AI image generation and guide you through creating an inpainting workflow in ComfyUI. We’ll cover:

- Using inpainting workflows to modify images
- Using the ComfyUI mask editor to draw masks
- `VAE Encoder (for Inpainting)` node

## [​](http://docs.comfy.org#about-inpainting) About Inpainting

In AI image generation, we often encounter situations where we’re satisfied with the overall image but there are elements we don’t want or that contain errors. Simply regenerating might produce a completely different image, so using inpainting to fix specific parts becomes very useful.

It’s like having an **artist (AI model)** paint a picture, but we’re still not satisfied with the specific details. We need to tell the artist **which areas to adjust (mask)**, and then let them **repaint (inpaint)** according to our requirements.

Common inpainting scenarios include:

- **Defect Repair:** Removing unwanted objects, fixing incorrect AI-generated body parts, etc.
- **Detail Optimization:** Precisely adjusting local elements (like modifying clothing textures, adjusting facial expressions)
- And other scenarios

## [​](http://docs.comfy.org#comfyui-inpainting-workflow-example) ComfyUI Inpainting Workflow Example

### [​](http://docs.comfy.org#model-and-resource-preparation) Model and Resource Preparation

#### [​](http://docs.comfy.org#1-model-installation) 1. Model Installation

Download the [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors) file and put it in your `ComfyUI/models/checkpoints` folder:

#### [​](http://docs.comfy.org#2-inpainting-asset) 2. Inpainting Asset

Please download the following image which we’ll use as input:

This image already contains an alpha channel (transparency mask), so you don’t need to manually draw a mask. This tutorial will also cover how to use the mask editor to draw masks.

#### [​](http://docs.comfy.org#3-inpainting-workflow) 3. Inpainting Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:

Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`.

### [​](http://docs.comfy.org#comfyui-inpainting-workflow-example-explanation) ComfyUI Inpainting Workflow Example Explanation

Follow the steps in the diagram below to ensure the workflow runs correctly.

1. Ensure `Load Checkpoint` loads `512-inpainting-ema.safetensors`
2. Upload the input image to the `Load Image` node
3. Click `Queue` or use `Ctrl + Enter` to generate

For comparison, here’s the result using the [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) model:

You will find that the results generated by the [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors) model have better inpainting effects and more natural transitions. This is because this model is specifically designed for inpainting, which helps us better control the generation area, resulting in improved inpainting effects.

Do you remember the analogy we’ve been using? Different models are like artists with varying abilities, but each artist has their own limits. Choosing the right model can help you achieve better generation results.

You can try these approaches to achieve better results:

1. Modify positive and negative prompts with more specific descriptions
2. Try multiple runs using different seeds in the `KSampler` for different generation results
3. After learning about the mask editor in this tutorial, you can re-inpaint the generated results to achieve satisfactory outcomes.

Next, we’ll learn about using the **Mask Editor**. While our input image already includes an `alpha` transparency channel (the area we want to edit), so manual mask drawing isn’t necessary, you’ll often use the Mask Editor to create masks in practical applications.

### [​](http://docs.comfy.org#using-the-mask-editor) Using the Mask Editor

First right-click the `Save Image` node and select `Copy(Clipspace)`:

Then right-click the **Load Image** node and select `Paste(Clipspace)`:

Right-click the **Load Image** node again and select `Open in MaskEditor`:

1. Adjust brush parameters on the right panel
2. Use eraser to correct mistakes
3. Click `Save` when finished

The drawn content will be used as a Mask input to the VAE Encoder (for Inpainting) node for encoding

Then try adjusting your prompts and generating again until you achieve satisfactory results.

## [​](http://docs.comfy.org#vae-encoder-for-inpainting-node) VAE Encoder (for Inpainting) Node

Comparing this workflow with [Text-to-Image](http://docs.comfy.org/tutorials/basic/text-to-image) and [Image-to-Image](http://docs.comfy.org/tutorials/basic/image-to-image), you’ll notice the main differences are in the VAE section’s conditional inputs. In this workflow, we use the **VAE Encoder (for Inpainting)** node, specifically designed for inpainting to help us better control the generation area and achieve better results.

**Input Types**

Parameter NameFunction`pixels`Input image to be encoded into latent space.`vae`VAE model used to encode the image from pixel space to latent space.`mask`Image mask specifying which areas need modification.`grow_mask_by`Pixel value to expand the original mask outward, ensuring a transition area around the mask to avoid hard edges between inpainted and original areas.

**Output Types**

Parameter NameFunction`latent`Image encoded into latent space by the VAE.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/basic/inpaint.mdx)

[Previous](http://docs.comfy.org/tutorials/basic/image-to-image)

[OutpaintThis guide will introduce you to the outpainting workflow in ComfyUI and walk you through an outpainting example  
\
Next](http://docs.comfy.org/tutorials/basic/outpaint)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [About Inpainting](http://docs.comfy.org#about-inpainting)
- [ComfyUI Inpainting Workflow Example](http://docs.comfy.org#comfyui-inpainting-workflow-example)
- [Model and Resource Preparation](http://docs.comfy.org#model-and-resource-preparation)
- [1. Model Installation](http://docs.comfy.org#1-model-installation)
- [2. Inpainting Asset](http://docs.comfy.org#2-inpainting-asset)
- [3. Inpainting Workflow](http://docs.comfy.org#3-inpainting-workflow)
- [ComfyUI Inpainting Workflow Example Explanation](http://docs.comfy.org#comfyui-inpainting-workflow-example-explanation)
- [Using the Mask Editor](http://docs.comfy.org#using-the-mask-editor)
- [VAE Encoder (for Inpainting) Node](http://docs.comfy.org#vae-encoder-for-inpainting-node)

<!-- END Get_Started/tutorials/basic/inpaint.md -->


<!-- BEGIN Get_Started/tutorials/basic/lora.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
  
  - [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image)
  - [Image to Image](http://docs.comfy.org/tutorials/basic/image-to-image)
  - [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint)
  - [Outpaint](http://docs.comfy.org/tutorials/basic/outpaint)
  - [Upscale](http://docs.comfy.org/tutorials/basic/upscale)
  - [LoRA](http://docs.comfy.org/tutorials/basic/lora)
  - [Multiple LoRAs](http://docs.comfy.org/tutorials/basic/multiple-loras)
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI LoRA Example

# ComfyUI LoRA Example

This guide will help you understand and use a single LoRA model

**LoRA (Low-Rank Adaptation)** is an efficient technique for fine-tuning large generative models like Stable Diffusion. It introduces trainable low-rank matrices to the pre-trained model, adjusting only a portion of parameters rather than retraining the entire model, thus achieving optimization for specific tasks at a lower computational cost. Compared to base models like SD1.5, LoRA models are smaller and easier to train.

The image above compares generation with the same parameters using [dreamshaper\_8](https://civitai.com/models/4384?modelVersionId=128713) directly versus using the [blindbox\_V1Mix](https://civitai.com/models/25995/blindbox) LoRA model. As you can see, by using a LoRA model, we can generate images in different styles without adjusting the base model.

We will demonstrate how to use a LoRA model. All LoRA variants: Lycoris, loha, lokr, locon, etc… are used in the same way.

In this example, we will learn how to load and use a LoRA model in [ComfyUI](https://github.com/comfyanonymous/ComfyUI), covering the following topics:

1. Installing a LoRA model
2. Generating images using a LoRA model
3. A simple introduction to the `Load LoRA` node

## [​](http://docs.comfy.org#required-model-installation) Required Model Installation

Download the [dreamshaper\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model&format=SafeTensor&size=pruned&fp=fp16) file and put it in your `ComfyUI/models/checkpoints` folder.

Download the [blindbox\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model&format=SafeTensor&size=full&fp=fp16) file and put it in your `ComfyUI/models/loras` folder.

## [​](http://docs.comfy.org#lora-workflow-file) LoRA Workflow File

Download the image below and **drag it into ComfyUI** to load the workflow.

Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`.

## [​](http://docs.comfy.org#complete-the-workflow-step-by-step) Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

1. Ensure `Load Checkpoint` loads `dreamshaper_8.safetensors`
2. Ensure `Load LoRA` loads `blindbox_V1Mix.safetensors`
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to generate the image

## [​](http://docs.comfy.org#load-lora-node-introduction) Load LoRA Node Introduction

Models in the `ComfyUI\models\loras` folder will be detected by ComfyUI and can be loaded using this node.

### [​](http://docs.comfy.org#input-types) Input Types

Parameter NameFunction`model`Connect to the base model`clip`Connect to the CLIP model`lora_name`Select the LoRA model to load and use`strength_model`Affects how strongly the LoRA influences the model weights; higher values make the LoRA style stronger`strength_clip`Affects how strongly the LoRA influences the CLIP text embeddings

### [​](http://docs.comfy.org#output-types) Output Types

Parameter NameFunction`model`Outputs the model with LoRA adjustments applied`clip`Outputs the CLIP model with LoRA adjustments applied

This node supports chain connections, allowing multiple `Load LoRA` nodes to be linked in series to apply multiple LoRA models. For more details, please refer to [ComfyUI Multiple LoRAs Example](http://docs.comfy.org/tutorials/basic/multiple-loras)

## [​](http://docs.comfy.org#try-it-yourself) Try It Yourself

1. Try modifying the prompt or adjusting different parameters of the `Load LoRA` node, such as `strength_model`, to observe changes in the generated images and become familiar with the `Load LoRA` node.
2. Visit [CivitAI](https://civitai.com/models) to download other kinds of LoRA models and try using them.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/basic/lora.mdx)

[Previous](http://docs.comfy.org/tutorials/basic/upscale)

[Multiple LoRAsThis guide demonstrates how to apply multiple LoRA models simultaneously in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/basic/multiple-loras)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Required Model Installation](http://docs.comfy.org#required-model-installation)
- [LoRA Workflow File](http://docs.comfy.org#lora-workflow-file)
- [Complete the Workflow Step by Step](http://docs.comfy.org#complete-the-workflow-step-by-step)
- [Load LoRA Node Introduction](http://docs.comfy.org#load-lora-node-introduction)
- [Input Types](http://docs.comfy.org#input-types)
- [Output Types](http://docs.comfy.org#output-types)
- [Try It Yourself](http://docs.comfy.org#try-it-yourself)

<!-- END Get_Started/tutorials/basic/lora.md -->


<!-- BEGIN Get_Started/tutorials/basic/multiple-loras.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
  
  - [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image)
  - [Image to Image](http://docs.comfy.org/tutorials/basic/image-to-image)
  - [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint)
  - [Outpaint](http://docs.comfy.org/tutorials/basic/outpaint)
  - [Upscale](http://docs.comfy.org/tutorials/basic/upscale)
  - [LoRA](http://docs.comfy.org/tutorials/basic/lora)
  - [Multiple LoRAs](http://docs.comfy.org/tutorials/basic/multiple-loras)
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Multiple LoRAs Example

# ComfyUI Multiple LoRAs Example

This guide demonstrates how to apply multiple LoRA models simultaneously in ComfyUI

In our [ComfyUI LoRA Example](http://docs.comfy.org/tutorials/basic/lora), we introduced how to load and use a single LoRA model, mentioning the node’s chain connection capability.

This tutorial demonstrates chaining multiple `Load LoRA` nodes to apply two LoRA models simultaneously: [blindbox\_V1Mix](https://civitai.com/models/25995?modelVersionId=32988) and [MoXinV1](https://civitai.com/models/12597?modelVersionId=14856).

The comparison below shows individual effects of these LoRAs using identical parameters:

By chaining multiple LoRA models, we achieve a blended style in the final output:

## [​](http://docs.comfy.org#model-installation) Model Installation

Download the [dreamshaper\_8.safetensors](https://civitai.com/api/download/models/128713?type=Model&format=SafeTensor&size=pruned&fp=fp16) file and put it in your `ComfyUI/models/checkpoints` folder.

Download the [blindbox\_V1Mix.safetensors](https://civitai.com/api/download/models/32988?type=Model&format=SafeTensor&size=full&fp=fp16) file and put it in your `ComfyUI/models/loras` folder.

Download the [MoXinV1.safetensors](https://civitai.com/api/download/models/14856?type=Model&format=SafeTensor&size=full&fp=fp16) file and put it in your `ComfyUI/models/loras` folder.

## [​](http://docs.comfy.org#multi-lora-workflow) Multi-LoRA Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:

Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`.

## [​](http://docs.comfy.org#complete-the-workflow-step-by-step) Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

1. Ensure `Load Checkpoint` loads **dreamshaper\_8.safetensors**
2. Ensure first `Load LoRA` loads **blindbox\_V1Mix.safetensors**
3. Ensure second `Load LoRA` loads **MoXinV1.safetensors**
4. Click `Queue` or press `Ctrl/Cmd + Enter` to generate

## [​](http://docs.comfy.org#try-it-yourself) Try It Yourself

1. Adjust `strength_model` values in both `Load LoRA` nodes to control each LoRA’s influence
2. Explore [CivitAI](https://civitai.com/models) for additional LoRAs and create custom combinations

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/basic/multiple-loras.mdx)

[Previous](http://docs.comfy.org/tutorials/basic/lora)

[ControlNetThis guide will introduce you to the basic concepts of ControlNet and demonstrate how to generate corresponding images in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/controlnet/controlnet)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Model Installation](http://docs.comfy.org#model-installation)
- [Multi-LoRA Workflow](http://docs.comfy.org#multi-lora-workflow)
- [Complete the Workflow Step by Step](http://docs.comfy.org#complete-the-workflow-step-by-step)
- [Try It Yourself](http://docs.comfy.org#try-it-yourself)

<!-- END Get_Started/tutorials/basic/multiple-loras.md -->


<!-- BEGIN Get_Started/tutorials/basic/outpaint.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
  
  - [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image)
  - [Image to Image](http://docs.comfy.org/tutorials/basic/image-to-image)
  - [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint)
  - [Outpaint](http://docs.comfy.org/tutorials/basic/outpaint)
  - [Upscale](http://docs.comfy.org/tutorials/basic/upscale)
  - [LoRA](http://docs.comfy.org/tutorials/basic/lora)
  - [Multiple LoRAs](http://docs.comfy.org/tutorials/basic/multiple-loras)
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Outpainting Workflow Example

# ComfyUI Outpainting Workflow Example

This guide will introduce you to the outpainting workflow in ComfyUI and walk you through an outpainting example

This guide will introduce you to the concept of outpainting in AI image generation and how to create an outpainting workflow in ComfyUI. We will cover:

- Using outpainting workflow to extend an image
- Understanding and using outpainting-related nodes in ComfyUI
- Mastering the basic outpainting process

## [​](http://docs.comfy.org#about-outpainting) About Outpainting

In AI image generation, we often encounter situations where an existing image has good composition but the canvas area is too small, requiring us to extend the canvas to get a larger scene. This is where outpainting comes in.

Basically, it requires similar content to [Inpainting](http://docs.comfy.org/tutorials/basic/inpaint), but we use different nodes to **build the mask**.

Outpainting applications include:

- **Scene Extension:** Expand the scene range of the original image to show a more complete environment
- **Composition Adjustment:** Optimize the overall composition by extending the canvas
- **Content Addition:** Add more related scene elements to the original image

## [​](http://docs.comfy.org#comfyui-outpainting-workflow-example-explanation) ComfyUI Outpainting Workflow Example Explanation

### [​](http://docs.comfy.org#preparation) Preparation

#### [​](http://docs.comfy.org#1-model-installation) 1. Model Installation

Download the following model file and save it to `ComfyUI/models/checkpoints` directory:

- [512-inpainting-ema.safetensors](https://huggingface.co/stabilityai/stable-diffusion-2-inpainting/blob/main/512-inpainting-ema.safetensors)

#### [​](http://docs.comfy.org#2-input-image) 2. Input Image

Prepare an image you want to extend. In this example, we will use the following image:

#### [​](http://docs.comfy.org#3-outpainting-workflow) 3. Outpainting Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:

Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`.

### [​](http://docs.comfy.org#outpainting-workflow-usage-explanation) Outpainting Workflow Usage Explanation

The key steps of the outpainting workflow are as follows:

1. Load the locally installed model file in the `Load Checkpoint` node
2. Click the `Upload` button in the `Load Image` node to upload your image
3. Click the `Queue` button or use the shortcut `Ctrl + Enter` to execute the image generation

In this workflow, we mainly use the `Pad Image for outpainting` node to control the direction and range of image extension. This is actually an [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint.mdx) workflow, but we use different nodes to build the mask.

### [​](http://docs.comfy.org#pad-image-for-outpainting-node) Pad Image for outpainting Node

This node accepts an input image and outputs an extended image with a corresponding mask, where the mask is built based on the node parameters.

#### [​](http://docs.comfy.org#input-parameters) Input Parameters

Parameter NameFunction`image`Input image`left`Left padding amount`top`Top padding amount`right`Right padding amount`bottom`Bottom padding amount`feathering`Controls the smoothness of the transition between the original image and the added padding, higher values create smoother transitions

#### [​](http://docs.comfy.org#output-parameters) Output Parameters

Parameter NameFunction`image`Output `image` represents the padded image`mask`Output `mask` indicates the original image area and the added padding area

#### [​](http://docs.comfy.org#node-output-content) Node Output Content

After processing by the `Pad Image for outpainting` node, the output image and mask preview are as follows:

You can see the corresponding output results:

- The `Image` output is the extended image
- The `Mask` output is the mask marking the extension areas

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/basic/outpaint.mdx)

[Previous](http://docs.comfy.org/tutorials/basic/inpaint)

[UpscaleThis guide explains the concept of image upscaling in AI drawing and demonstrates how to implement an image upscaling workflow in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/basic/upscale)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [About Outpainting](http://docs.comfy.org#about-outpainting)
- [ComfyUI Outpainting Workflow Example Explanation](http://docs.comfy.org#comfyui-outpainting-workflow-example-explanation)
- [Preparation](http://docs.comfy.org#preparation)
- [1. Model Installation](http://docs.comfy.org#1-model-installation)
- [2. Input Image](http://docs.comfy.org#2-input-image)
- [3. Outpainting Workflow](http://docs.comfy.org#3-outpainting-workflow)
- [Outpainting Workflow Usage Explanation](http://docs.comfy.org#outpainting-workflow-usage-explanation)
- [Pad Image for outpainting Node](http://docs.comfy.org#pad-image-for-outpainting-node)
- [Input Parameters](http://docs.comfy.org#input-parameters)
- [Output Parameters](http://docs.comfy.org#output-parameters)
- [Node Output Content](http://docs.comfy.org#node-output-content)

<!-- END Get_Started/tutorials/basic/outpaint.md -->


<!-- BEGIN Get_Started/tutorials/basic/text-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
  
  - [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image)
  - [Image to Image](http://docs.comfy.org/tutorials/basic/image-to-image)
  - [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint)
  - [Outpaint](http://docs.comfy.org/tutorials/basic/outpaint)
  - [Upscale](http://docs.comfy.org/tutorials/basic/upscale)
  - [LoRA](http://docs.comfy.org/tutorials/basic/lora)
  - [Multiple LoRAs](http://docs.comfy.org/tutorials/basic/multiple-loras)
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Text to Image Workflow

# ComfyUI Text to Image Workflow

This guide will help you understand the concept of text-to-image in AI art generation and complete a text-to-image workflow in ComfyUI

This guide aims to introduce you to ComfyUI’s text-to-image workflow and help you understand the functionality and usage of various ComfyUI nodes.

In this document, we will:

- Complete a text-to-image workflow
- Gain a basic understanding of diffusion model principles
- Learn about the functions and roles of workflow nodes
- Get an initial understanding of the SD1.5 model

We’ll start by running a text-to-image workflow, followed by explanations of related concepts. Please choose the relevant sections based on your needs.

## [​](http://docs.comfy.org#about-text-to-image) About Text to Image

**Text to Image** is a fundamental process in AI art generation that creates images from text descriptions, with **diffusion models** at its core.

The text-to-image process requires the following elements:

- **Artist:** The image generation model
- **Canvas:** The latent space
- **Image Requirements (Prompts):** Including positive prompts (elements you want in the image) and negative prompts (elements you don’t want)

This text-to-image generation process can be simply understood as telling your requirements (positive and negative prompts) to an **artist (the image model)**, who then creates what you want based on these requirements.

## [​](http://docs.comfy.org#comfyui-text-to-image-workflow-example-guide) ComfyUI Text to Image Workflow Example Guide

### [​](http://docs.comfy.org#1-preparation) 1. Preparation

Ensure you have at least one SD1.5 model file in your `ComfyUI/models/checkpoints` folder, such as [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors)

If you haven’t installed it yet, please refer to the model installation section in [Getting Started with ComfyUI AI Art Generation](http://docs.comfy.org/get_started/first_generation).

### [​](http://docs.comfy.org#2-loading-the-text-to-image-workflow) 2. Loading the Text to Image Workflow

Download the image below and **drag it into ComfyUI** to load the workflow:

Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`.

### [​](http://docs.comfy.org#3-loading-the-model-and-generating-your-first-image) 3. Loading the Model and Generating Your First Image

After installing the image model, follow the steps in the image below to load the model and generate your first image

Follow these steps according to the image numbers:

1. In the **Load Checkpoint** node, use the arrows or click the text area to ensure **v1-5-pruned-emaonly-fp16.safetensors** is selected, and the left/right arrows don’t show **null** text
2. Click the `Queue` button or use the shortcut `Ctrl + Enter` to execute image generation

After the process completes, you should see the resulting image in the **Save Image** node interface, which you can right-click to save locally

If you’re not satisfied with the result, try running the generation multiple times. Each time you run it, **KSampler** will use a different random seed based on the `seed` parameter, so each generation will produce different results

### [​](http://docs.comfy.org#4-start-experimenting) 4. Start Experimenting

Try modifying the text in the **CLIP Text Encoder**

The `Positive` connection to the KSampler node represents positive prompts, while the `Negative` connection represents negative prompts

Here are some basic prompting principles for the SD1.5 model:

- Use English whenever possible
- Separate prompts with English commas `,`
- Use phrases rather than long sentences
- Use specific descriptions
- Use expressions like `(golden hour:1.2)` to increase the weight of specific keywords, making them more likely to appear in the image. `1.2` is the weight, `golden hour` is the keyword
- Use keywords like `masterpiece, best quality, 4k` to improve generation quality

Here are several prompt examples you can try, or use your own prompts for generation:

**1. Anime Style**

Positive prompts:

```plaintext
anime style, 1girl with long pink hair, cherry blossom background, studio ghibli aesthetic, soft lighting, intricate details

masterpiece, best quality, 4k
```

Negative prompts:

```plaintext
low quality, blurry, deformed hands, extra fingers
```

**2. Realistic Style**

Positive prompts:

```plaintext
(ultra realistic portrait:1.3), (elegant woman in crimson silk dress:1.2), 
full body, soft cinematic lighting, (golden hour:1.2), 
(fujifilm XT4:1.1), shallow depth of field, 
(skin texture details:1.3), (film grain:1.1), 
gentle wind flow, warm color grading, (perfect facial symmetry:1.3)
```

Negative prompts:

```plaintext
(deformed, cartoon, anime, doll, plastic skin, overexposed, blurry, extra fingers)
```

**3. Specific Artist Style**

Positive prompts:

```plaintext
fantasy elf, detailed character, glowing magic, vibrant colors, long flowing hair, elegant armor, ethereal beauty, mystical forest, magical aura, high detail, soft lighting, fantasy portrait, Artgerm style
```

Negative prompts:

```plaintext
blurry, low detail, cartoonish, unrealistic anatomy, out of focus, cluttered, flat lighting
```

## [​](http://docs.comfy.org#text-to-image-working-principles) Text to Image Working Principles

The entire text-to-image process can be understood as a **reverse diffusion process**. The [v1-5-pruned-emaonly-fp16.safetensors](https://huggingface.co/Comfy-Org/stable-diffusion-v1-5-archive/blob/main/v1-5-pruned-emaonly-fp16.safetensors) we downloaded is a pre-trained model that can **generate target images from pure Gaussian noise**. We only need to input our prompts, and it can generate target images through denoising random noise.

We need to understand two concepts:

1. **Latent Space:** Latent Space is an abstract data representation method in diffusion models. Converting images from pixel space to latent space reduces storage space and makes it easier to train diffusion models and reduce denoising complexity. It’s like architects using blueprints (latent space) for design rather than designing directly on the building (pixel space), maintaining structural features while significantly reducing modification costs
2. **Pixel Space:** Pixel Space is the storage space for images, which is the final image we see, used to store pixel values.

If you want to learn more about diffusion models, you can read these papers:

- [Denoising Diffusion Probabilistic Models (DDPM)](https://arxiv.org/pdf/2006.11239)
- [Denoising Diffusion Implicit Models (DDIM)](https://arxiv.org/pdf/2010.02502)
- [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/pdf/2112.10752)

## [​](http://docs.comfy.org#comfyui-text-to-image-workflow-node-explanation) ComfyUI Text to Image Workflow Node Explanation

### [​](http://docs.comfy.org#a-load-checkpoint-node) A. Load Checkpoint Node

This node is typically used to load the image generation model. A `checkpoint` usually contains three components: `MODEL (UNet)`, `CLIP`, and `VAE`

- `MODEL (UNet)`: The UNet model responsible for noise prediction and image generation during the diffusion process
- `CLIP`: The text encoder that converts our text prompts into vectors that the model can understand, as the model cannot directly understand text prompts
- `VAE`: The Variational AutoEncoder that converts images between pixel space and latent space, as diffusion models work in latent space while our images are in pixel space

### [​](http://docs.comfy.org#b-empty-latent-image-node) B. Empty Latent Image Node

Defines a latent space that outputs to the KSampler node. The Empty Latent Image node constructs a **pure noise latent space**

You can think of its function as defining the canvas size, which determines the dimensions of our final generated image

### [​](http://docs.comfy.org#c-clip-text-encoder-node) C. CLIP Text Encoder Node

Used to encode prompts, which are your requirements for the image

- The `Positive` condition input connected to the KSampler node represents positive prompts (elements you want in the image)
- The `Negative` condition input connected to the KSampler node represents negative prompts (elements you don’t want in the image)

The prompts are encoded into semantic vectors by the `CLIP` component from the `Load Checkpoint` node and output as conditions to the KSampler node

### [​](http://docs.comfy.org#d-ksampler-node) D. KSampler Node

The **KSampler** is the core of the entire workflow, where the entire noise denoising process occurs, ultimately outputting a latent space image

Here’s an explanation of the KSampler node parameters:

Parameter NameDescriptionFunction**model**Diffusion model used for denoisingDetermines the style and quality of generated images**positive**Positive prompt condition encodingGuides generation to include specified elements**negative**Negative prompt condition encodingSuppresses unwanted content**latent\_image**Latent space image to be denoisedServes as the input carrier for noise initialization**seed**Random seed for noise generationControls generation randomness**control\_after\_generate**Seed control mode after generationDetermines seed variation pattern in batch generation**steps**Number of denoising iterationsMore steps mean finer details but longer processing time**cfg**Classifier-free guidance scaleControls prompt constraint strength (too high leads to overfitting)**sampler\_name**Sampling algorithm nameDetermines the mathematical method for denoising path**scheduler**Scheduler typeControls noise decay rate and step size allocation**denoise**Denoising strength coefficientControls noise strength added to latent space, 0.0 preserves original input features, 1.0 is complete noise

In the KSampler node, the latent space uses `seed` as an initialization parameter to construct random noise, and semantic vectors `Positive` and `Negative` are input as conditions to the diffusion model

Then, based on the number of denoising steps specified by the `steps` parameter, denoising is performed. Each denoising step uses the denoising strength coefficient specified by the `denoise` parameter to denoise the latent space and generate a new latent space image

### [​](http://docs.comfy.org#e-vae-decode-node) E. VAE Decode Node

Converts the latent space image output from the **KSampler** into a pixel space image

### [​](http://docs.comfy.org#f-save-image-node) F. Save Image Node

Previews and saves the decoded image from latent space to the local `ComfyUI/output` folder

## [​](http://docs.comfy.org#introduction-to-sd1-5-model) Introduction to SD1.5 Model

**SD1.5 (Stable Diffusion 1.5)** is an AI image generation model developed by [Stability AI](https://stability.ai/). It’s the foundational version of the Stable Diffusion series, trained on **512×512** resolution images, making it particularly good at generating images at this resolution. With a size of about 4GB, it runs smoothly on **consumer-grade GPUs (e.g., 6GB VRAM)**. Currently, SD1.5 has a rich ecosystem, supporting various plugins (like ControlNet, LoRA) and optimization tools. As a milestone model in AI art generation, SD1.5 remains the best entry-level choice thanks to its open-source nature, lightweight architecture, and rich ecosystem. Although newer versions like SDXL/SD3 have been released, its value for consumer-grade hardware remains unmatched.

### [​](http://docs.comfy.org#basic-information) Basic Information

- **Release Date**: October 2022
- **Core Architecture**: Based on Latent Diffusion Model (LDM)
- **Training Data**: LAION-Aesthetics v2.5 dataset (approximately 590M training steps)
- **Open Source Features**: Fully open-source model/code/training data

### [​](http://docs.comfy.org#advantages-and-limitations) Advantages and Limitations

Model Advantages:

- Lightweight: Small size, only about 4GB, runs smoothly on consumer GPUs
- Low Entry Barrier: Supports a wide range of plugins and optimization tools
- Mature Ecosystem: Extensive plugin and tool support
- Fast Generation: Smooth operation on consumer GPUs

Model Limitations:

- Detail Handling: Hands/complex lighting prone to distortion
- Resolution Limits: Quality degrades for direct 1024x1024 generation
- Prompt Dependency: Requires precise English descriptions for control

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/basic/text-to-image.mdx)

[Previous](http://docs.comfy.org/interface/shortcuts)

[Image to ImageThis guide will help you understand and complete an image to image workflow  
\
Next](http://docs.comfy.org/tutorials/basic/image-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [About Text to Image](http://docs.comfy.org#about-text-to-image)
- [ComfyUI Text to Image Workflow Example Guide](http://docs.comfy.org#comfyui-text-to-image-workflow-example-guide)
- [1. Preparation](http://docs.comfy.org#1-preparation)
- [2. Loading the Text to Image Workflow](http://docs.comfy.org#2-loading-the-text-to-image-workflow)
- [3. Loading the Model and Generating Your First Image](http://docs.comfy.org#3-loading-the-model-and-generating-your-first-image)
- [4. Start Experimenting](http://docs.comfy.org#4-start-experimenting)
- [Text to Image Working Principles](http://docs.comfy.org#text-to-image-working-principles)
- [ComfyUI Text to Image Workflow Node Explanation](http://docs.comfy.org#comfyui-text-to-image-workflow-node-explanation)
- [A. Load Checkpoint Node](http://docs.comfy.org#a-load-checkpoint-node)
- [B. Empty Latent Image Node](http://docs.comfy.org#b-empty-latent-image-node)
- [C. CLIP Text Encoder Node](http://docs.comfy.org#c-clip-text-encoder-node)
- [D. KSampler Node](http://docs.comfy.org#d-ksampler-node)
- [E. VAE Decode Node](http://docs.comfy.org#e-vae-decode-node)
- [F. Save Image Node](http://docs.comfy.org#f-save-image-node)
- [Introduction to SD1.5 Model](http://docs.comfy.org#introduction-to-sd1-5-model)
- [Basic Information](http://docs.comfy.org#basic-information)
- [Advantages and Limitations](http://docs.comfy.org#advantages-and-limitations)

<!-- END Get_Started/tutorials/basic/text-to-image.md -->


<!-- BEGIN Get_Started/tutorials/basic/upscale.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
  
  - [Text to Image](http://docs.comfy.org/tutorials/basic/text-to-image)
  - [Image to Image](http://docs.comfy.org/tutorials/basic/image-to-image)
  - [Inpaint](http://docs.comfy.org/tutorials/basic/inpaint)
  - [Outpaint](http://docs.comfy.org/tutorials/basic/outpaint)
  - [Upscale](http://docs.comfy.org/tutorials/basic/upscale)
  - [LoRA](http://docs.comfy.org/tutorials/basic/lora)
  - [Multiple LoRAs](http://docs.comfy.org/tutorials/basic/multiple-loras)
- ControlNet
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Image Upscale Workflow

# ComfyUI Image Upscale Workflow

This guide explains the concept of image upscaling in AI drawing and demonstrates how to implement an image upscaling workflow in ComfyUI

## [​](http://docs.comfy.org#what-is-image-upscaling%3F) What is Image Upscaling?

Image Upscaling is the process of converting low-resolution images to high-resolution using algorithms. Unlike traditional interpolation methods, AI upscaling models (like ESRGAN) can intelligently reconstruct details while maintaining image quality.

For instance, the default SD1.5 model often struggles with large-size image generation. To achieve high-resolution results,we typically generate smaller images first and then use upscaling techniques.

This article covers one of many upscaling methods in ComfyUI. In this tutorial, we’ll guide you through:

1. Downloading and installing upscaling models
2. Performing basic image upscaling
3. Combining text-to-image workflows with upscaling

## [​](http://docs.comfy.org#upscaling-workflow) Upscaling Workflow

### [​](http://docs.comfy.org#model-installation) Model Installation

Required ESRGAN models download:

1

Visit OpenModelDB

Visit [OpenModelDB](https://openmodeldb.info/) to search and download upscaling models (e.g., RealESRGAN)

As shown:

1. Filter models by image type using the category selector
2. The model’s magnification factor is indicated in the top-right corner (e.g., 2x in the screenshot)

We’ll use the [4x-ESRGAN](https://openmodeldb.info/models/4x-ESRGAN) model for this tutorial. Click the `Download` button on the model detail page.

2

Save Model Files in Directory

Save the model file (.pth) in `ComfyUI/models/upscale_models` directory

### [​](http://docs.comfy.org#workflow-and-assets) Workflow and Assets

Download and drag the following image into ComfyUI to load the basic upscaling workflow:

Images containing workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`.

Use this image in smaller size as input:

### [​](http://docs.comfy.org#complete-the-workflow-step-by-step) Complete the Workflow Step by Step

Follow the steps in the diagram below to ensure the workflow runs correctly.

1. Ensure `Load Upscale Model` loads `4x-ESRGAN.pth`
2. Upload the input image to the `Load Image` node
3. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to generate the image

The core components are the `Load Upscale Model` and `Upscale Image (Using Model)` nodes, which receive an image input and upscale it using the selected model.

## [​](http://docs.comfy.org#text-to-image-combined-workflow) Text-to-Image Combined Workflow

After mastering basic upscaling, we can combine it with the [text-to-image](http://docs.comfy.org/tutorials/basic/text-to-image) workflow. For text-to-image basics, refer to the [text-to-image tutorial](http://docs.comfy.org/tutorials/basic/text-to-image).

Download and drag this image into ComfyUI to load the combined workflow:

This workflow connects the text-to-image output image directly to the upscaling nodes for final processing.

## [​](http://docs.comfy.org#additional-tips) Additional Tips

Model characteristics:

- **RealESRGAN**: General-purpose upscaling
- **BSRGAN**: Excels with text and sharp edges
- **SwinIR**: Preserves natural textures, ideal for landscapes

<!--THE END-->

1. **Chained Upscaling**: Combine multiple upscale nodes (e.g., 2x → 4x) for ultra-high magnification
2. **Hybrid Workflow**: Connect upscale nodes after generation for “generate+enhance” pipelines
3. **Comparative Testing**: Different models perform better on specific image types - test multiple options

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/basic/upscale.mdx)

[Previous](http://docs.comfy.org/tutorials/basic/outpaint)

[LoRAThis guide will help you understand and use a single LoRA model  
\
Next](http://docs.comfy.org/tutorials/basic/lora)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [What is Image Upscaling?](http://docs.comfy.org#what-is-image-upscaling%3F)
- [Upscaling Workflow](http://docs.comfy.org#upscaling-workflow)
- [Model Installation](http://docs.comfy.org#model-installation)
- [Workflow and Assets](http://docs.comfy.org#workflow-and-assets)
- [Complete the Workflow Step by Step](http://docs.comfy.org#complete-the-workflow-step-by-step)
- [Text-to-Image Combined Workflow](http://docs.comfy.org#text-to-image-combined-workflow)
- [Additional Tips](http://docs.comfy.org#additional-tips)

<!-- END Get_Started/tutorials/basic/upscale.md -->


<!-- BEGIN Get_Started/tutorials/controlnet/controlnet.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
  
  - [ControlNet](http://docs.comfy.org/tutorials/controlnet/controlnet)
  - [Pose ControlNet](http://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass)
  - [Depth ControlNet](http://docs.comfy.org/tutorials/controlnet/depth-controlnet)
  - [Depth T2I Adapter](http://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter)
  - [Mixing ControlNet](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets)
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI ControlNet Usage Example

# ComfyUI ControlNet Usage Example

This guide will introduce you to the basic concepts of ControlNet and demonstrate how to generate corresponding images in ComfyUI

Achieving precise control over image creation in AI image generation cannot be done with just one click. It typically requires numerous generation attempts to produce a satisfactory image. However, the emergence of **ControlNet** has effectively addressed this challenge.

ControlNet is a conditional control generation model based on diffusion models (such as Stable Diffusion), first proposed by [Lvmin Zhang](https://lllyasviel.github.io/) and Maneesh Agrawala et al. in 2023 in the paper [Adding Conditional Control to Text-to-Image Diffusion Models](https://arxiv.org/abs/2302.05543).

ControlNet models significantly enhance the controllability of image generation and the ability to reproduce details by introducing multimodal input conditions, such as edge detection maps, depth maps, and pose keypoints.

These conditioning constraints make image generation more controllable, allowing multiple ControlNet models to be used simultaneously during the drawing process for better results.

Before ControlNet, we could only rely on the model to generate images repeatedly until we were satisfied with the results, which involved a lot of randomness.

With the advent of ControlNet, we can control image generation by introducing additional conditions. For example, we can use a simple sketch to guide the image generation process, producing images that closely align with our sketch.

In this example, we will guide you through installing and using ControlNet models in [ComfyUI](https://github.com/comfyanonymous/ComfyUI), and complete a sketch-controlled image generation example.

The workflows for other types of ControlNet V1.1 models are similar to this example. You only need to select the appropriate model and upload the corresponding reference image based on your needs.

## [​](http://docs.comfy.org#controlnet-image-preprocessing-information) ControlNet Image Preprocessing Information

Different types of ControlNet models typically require different types of reference images:

> Image source: [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

Since the current **Comfy Core** nodes do not include all types of **preprocessors**, in the actual examples in this documentation, we will provide pre-processed images. However, in practical use, you may need to use custom nodes to preprocess images to meet the requirements of different ControlNet models. Below are some relevant custom nodes:

- [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
- [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

## [​](http://docs.comfy.org#comfyui-controlnet-workflow-example-explanation) ComfyUI ControlNet Workflow Example Explanation

### [​](http://docs.comfy.org#1-controlnet-workflow-assets) 1. ControlNet Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`. This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.

Please download the image below, which we will use as input:

### [​](http://docs.comfy.org#2-manual-model-installation) 2. Manual Model Installation

If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:

- [dreamCreationVirtual3DECommerce\_v10.safetensors](https://civitai.com/api/download/models/731340?type=Model&format=SafeTensor&size=full&fp=fp16)
- [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)
- [control\_v11p\_sd15\_scribble\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── dreamCreationVirtual3DECommerce_v10.safetensors
│   ├── vae/
│   │   └── vae-ft-mse-840000-ema-pruned.safetensors
│   └── controlnet/
│       └── control_v11p_sd15_scribble_fp16.safetensors
```

In this example, you could also use the VAE model embedded in dreamCreationVirtual3DECommerce\_v10.safetensors, but we’re following the model author’s recommendation to use a separate VAE model.

### [​](http://docs.comfy.org#3-step-by-step-workflow-execution) 3. Step-by-Step Workflow Execution

1. Ensure that `Load Checkpoint` can load **dreamCreationVirtual3DECommerce\_v10.safetensors**
2. Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**
3. Click `Upload` in the `Load Image` node to upload the input image provided earlier
4. Ensure that `Load ControlNet` can load **control\_v11p\_sd15\_scribble\_fp16.safetensors**
5. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## [​](http://docs.comfy.org#related-node-explanations) Related Node Explanations

### [​](http://docs.comfy.org#load-controlnet-node-explanation) Load ControlNet Node Explanation

Models located in `ComfyUI\models\controlnet` will be detected by ComfyUI and can be loaded through this node.

### [​](http://docs.comfy.org#apply-controlnet-node-explanation) Apply ControlNet Node Explanation

This node accepts the ControlNet model loaded by `load controlnet` and generates corresponding control conditions based on the input image.

**Input Types**

Parameter NameFunction`positive`Positive conditioning`negative`Negative conditioning`control_net`The ControlNet model to be applied`image`Preprocessed image used as reference for ControlNet application`vae`VAE model input`strength`Strength of ControlNet application; higher values increase ControlNet’s influence on the generated image`start_percent`Determines when to start applying ControlNet as a percentage; e.g., 0.2 means ControlNet guidance begins when 20% of diffusion is complete`end_percent`Determines when to stop applying ControlNet as a percentage; e.g., 0.8 means ControlNet guidance stops when 80% of diffusion is complete

**Output Types**

Parameter NameFunction`positive`Positive conditioning data processed by ControlNet`negative`Negative conditioning data processed by ControlNet

You can use chain connections to apply multiple ControlNet models, as shown in the image below. You can also refer to the [Mixing ControlNet Models](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets.mdx) guide to learn more about combining multiple ControlNet models.

You might see the `Apply ControlNet(Old)` node in some early workflows, which is an early version of the ControlNet node. It is currently deprecated and not visible by default in search and node lists. To enable it, go to **Settings** —&gt; **comfy** —&gt; **Node** and enable the `Show deprecated nodes in search` option. However, it’s recommended to use the new node.

## [​](http://docs.comfy.org#start-your-exploration) Start Your Exploration

1. Try creating similar sketches, or even draw your own, and use ControlNet models to generate images to experience the benefits of ControlNet.
2. Adjust the `Control Strength` parameter in the Apply ControlNet node to control the influence of the ControlNet model on the generated image.
3. Visit the [ControlNet-v1-1\_fp16\_safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/tree/main) repository to download other types of ControlNet models and try using them to generate images.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/controlnet/controlnet.mdx)

[Previous](http://docs.comfy.org/tutorials/basic/multiple-loras)

[Pose ControlNetThis guide will introduce you to the basic concepts of Pose ControlNet, and demonstrate how to generate large-sized images in ComfyUI using a two-pass generation approach  
\
Next](http://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [ControlNet Image Preprocessing Information](http://docs.comfy.org#controlnet-image-preprocessing-information)
- [ComfyUI ControlNet Workflow Example Explanation](http://docs.comfy.org#comfyui-controlnet-workflow-example-explanation)
- [1. ControlNet Workflow Assets](http://docs.comfy.org#1-controlnet-workflow-assets)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation)
- [3. Step-by-Step Workflow Execution](http://docs.comfy.org#3-step-by-step-workflow-execution)
- [Related Node Explanations](http://docs.comfy.org#related-node-explanations)
- [Load ControlNet Node Explanation](http://docs.comfy.org#load-controlnet-node-explanation)
- [Apply ControlNet Node Explanation](http://docs.comfy.org#apply-controlnet-node-explanation)
- [Start Your Exploration](http://docs.comfy.org#start-your-exploration)

<!-- END Get_Started/tutorials/controlnet/controlnet.md -->


<!-- BEGIN Get_Started/tutorials/controlnet/depth-controlnet.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
  
  - [ControlNet](http://docs.comfy.org/tutorials/controlnet/controlnet)
  - [Pose ControlNet](http://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass)
  - [Depth ControlNet](http://docs.comfy.org/tutorials/controlnet/depth-controlnet)
  - [Depth T2I Adapter](http://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter)
  - [Mixing ControlNet](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets)
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Depth ControlNet Usage Example

# ComfyUI Depth ControlNet Usage Example

This guide will introduce you to the basic concepts of Depth ControlNet and demonstrate how to generate corresponding images in ComfyUI

## [​](http://docs.comfy.org#introduction-to-depth-maps-and-depth-controlnet) Introduction to Depth Maps and Depth ControlNet

A depth map is a special type of image that uses grayscale values to represent the distance between objects in a scene and the observer or camera. In a depth map, the grayscale value is inversely proportional to distance: brighter areas (closer to white) indicate objects that are closer, while darker areas (closer to black) indicate objects that are farther away.

Depth ControlNet is a ControlNet model specifically trained to understand and utilize depth map information. It helps AI correctly interpret spatial relationships, ensuring that generated images conform to the spatial structure specified by the depth map, thereby enabling precise control over three-dimensional spatial layouts.

### [​](http://docs.comfy.org#application-scenarios-for-depth-maps-with-controlnet) Application Scenarios for Depth Maps with ControlNet

Depth maps have numerous applications in various scenarios:

1. **Portrait Scenes**: Control the spatial relationship between subjects and backgrounds, avoiding distortion in critical areas such as faces
2. **Landscape Scenes**: Control the hierarchical relationships between foreground, middle ground, and background
3. **Architectural Scenes**: Control the spatial structure and perspective relationships of buildings
4. **Product Showcase**: Control the separation and spatial positioning of products against their backgrounds

In this example, we will use a depth map to generate an architectural visualization scene.

## [​](http://docs.comfy.org#comfyui-controlnet-workflow-example-explanation) ComfyUI ControlNet Workflow Example Explanation

### [​](http://docs.comfy.org#1-controlnet-workflow-assets) 1. ControlNet Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`. This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.

Please download the image below, which we will use as input:

### [​](http://docs.comfy.org#2-model-installation) 2. Model Installation

If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:

- [architecturerealmix\_v11.safetensors](https://civitai.com/api/download/models/431755?type=Model&format=SafeTensor&size=full&fp=fp16)
- [control\_v11f1p\_sd15\_depth\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11f1p_sd15_depth_fp16.safetensors?download=true)

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── architecturerealmix_v11.safetensors
│   └── controlnet/
│       └── control_v11f1p_sd15_depth_fp16.safetensors
```

### [​](http://docs.comfy.org#3-step-by-step-workflow-execution) 3. Step-by-Step Workflow Execution

1. Ensure that `Load Checkpoint` can load **architecturerealmix\_v11.safetensors**
2. Ensure that `Load ControlNet` can load **control\_v11f1p\_sd15\_depth\_fp16.safetensors**
3. Click `Upload` in the `Load Image` node to upload the depth image provided earlier
4. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## [​](http://docs.comfy.org#combining-depth-control-with-other-techniques) Combining Depth Control with Other Techniques

Based on different creative needs, you can combine Depth ControlNet with other types of ControlNet to achieve better results:

1. **Depth + Lineart**: Maintain spatial relationships while reinforcing outlines, suitable for architecture, products, and character design
2. **Depth + Pose**: Control character posture while maintaining correct spatial relationships, suitable for character scenes

For more information on using multiple ControlNet models together, please refer to the [Mixing ControlNet](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets.mdx) example.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/controlnet/depth-controlnet.mdx)

[Previous](http://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass)

[Depth T2I AdapterThis guide will introduce you to the basic concepts of Depth T2I Adapter and demonstrate how to generate corresponding images in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Introduction to Depth Maps and Depth ControlNet](http://docs.comfy.org#introduction-to-depth-maps-and-depth-controlnet)
- [Application Scenarios for Depth Maps with ControlNet](http://docs.comfy.org#application-scenarios-for-depth-maps-with-controlnet)
- [ComfyUI ControlNet Workflow Example Explanation](http://docs.comfy.org#comfyui-controlnet-workflow-example-explanation)
- [1. ControlNet Workflow Assets](http://docs.comfy.org#1-controlnet-workflow-assets)
- [2. Model Installation](http://docs.comfy.org#2-model-installation)
- [3. Step-by-Step Workflow Execution](http://docs.comfy.org#3-step-by-step-workflow-execution)
- [Combining Depth Control with Other Techniques](http://docs.comfy.org#combining-depth-control-with-other-techniques)

<!-- END Get_Started/tutorials/controlnet/depth-controlnet.md -->


<!-- BEGIN Get_Started/tutorials/controlnet/depth-t2i-adapter.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
  
  - [ControlNet](http://docs.comfy.org/tutorials/controlnet/controlnet)
  - [Pose ControlNet](http://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass)
  - [Depth ControlNet](http://docs.comfy.org/tutorials/controlnet/depth-controlnet)
  - [Depth T2I Adapter](http://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter)
  - [Mixing ControlNet](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets)
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Depth T2I Adapter Usage Example

# ComfyUI Depth T2I Adapter Usage Example

This guide will introduce you to the basic concepts of Depth T2I Adapter and demonstrate how to generate corresponding images in ComfyUI

## [​](http://docs.comfy.org#introduction-to-t2i-adapter) Introduction to T2I Adapter

[T2I-Adapter](https://huggingface.co/TencentARC/T2I-Adapter) is a lightweight adapter developed by [Tencent ARC Lab](https://github.com/TencentARC) designed to enhance the structural, color, and style control capabilities of text-to-image generation models (such as Stable Diffusion). It works by aligning external conditions (such as edge detection maps, depth maps, sketches, or color reference images) with the model’s internal features, achieving high-precision control without modifying the original model structure. With only about 77M parameters (approximately 300MB in size), its inference speed is about 3 times faster than [ControlNet](https://github.com/lllyasviel/ControlNet-v1-1-nightly), and it supports multiple condition combinations (such as sketch + color grid). Application scenarios include line art to image conversion, color style transfer, multi-element scene generation, and more.

### [​](http://docs.comfy.org#comparison-between-t2i-adapter-and-controlnet) Comparison Between T2I Adapter and ControlNet

Although their functions are similar, there are notable differences in implementation and application:

1. **Lightweight Design**: T2I Adapter has fewer parameters and a smaller memory footprint
2. **Inference Speed**: T2I Adapter is typically about 3 times faster than ControlNet
3. **Control Precision**: ControlNet offers more precise control in certain scenarios, while T2I Adapter is more suitable for lightweight control
4. **Multi-condition Combination**: T2I Adapter shows more significant resource advantages when combining multiple conditions

### [​](http://docs.comfy.org#main-types-of-t2i-adapter) Main Types of T2I Adapter

T2I Adapter provides various types to control different aspects:

- **Depth**: Controls the spatial structure and depth relationships in images
- **Line Art (Canny/Sketch)**: Controls image edges and lines
- **Keypose**: Controls character poses and actions
- **Segmentation (Seg)**: Controls scene layout through semantic segmentation
- **Color**: Controls the overall color scheme of images

In ComfyUI, using T2I Adapter is similar to [ControlNet](http://docs.comfy.org/tutorials/controlnet/controlnet.mdx) in terms of interface and workflow. In this example, we will demonstrate how to use a depth T2I Adapter to control an interior scene.

## [​](http://docs.comfy.org#value-of-depth-t2i-adapter-applications) Value of Depth T2I Adapter Applications

Depth maps have several important applications in image generation:

1. **Spatial Layout Control**: Accurately describes three-dimensional spatial structures, suitable for interior design and architectural visualization
2. **Object Positioning**: Controls the relative position and size of objects in a scene, suitable for product showcases and scene construction
3. **Perspective Relationships**: Maintains reasonable perspective and proportions, suitable for landscape and urban scene generation
4. **Light and Shadow Layout**: Natural light and shadow distribution based on depth information, enhancing realism

We will use interior design as an example to demonstrate how to use the depth T2I Adapter, but these techniques are applicable to other scenarios as well.

## [​](http://docs.comfy.org#comfyui-depth-t2i-adapter-workflow-example-explanation) ComfyUI Depth T2I Adapter Workflow Example Explanation

### [​](http://docs.comfy.org#1-depth-t2i-adapter-workflow-assets) 1. Depth T2I Adapter Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`. This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.

Please download the image below, which we will use as input:

### [​](http://docs.comfy.org#2-model-installation) 2. Model Installation

If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:

- [interiordesignsuperm\_v2.safetensors](https://civitai.com/api/download/models/93152?type=Model&format=SafeTensor&size=full&fp=fp16)
- [t2iadapter\_depth\_sd15v2.pth](https://huggingface.co/TencentARC/T2I-Adapter/resolve/main/models/t2iadapter_depth_sd15v2.pth?download=true)

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── interiordesignsuperm_v2.safetensors
│   └── controlnet/
│       └── t2iadapter_depth_sd15v2.pth
```

### [​](http://docs.comfy.org#3-step-by-step-workflow-execution) 3. Step-by-Step Workflow Execution

1. Ensure that `Load Checkpoint` can load **interiordesignsuperm\_v2.safetensors**
2. Ensure that `Load ControlNet` can load **t2iadapter\_depth\_sd15v2.pth**
3. Click `Upload` in the `Load Image` node to upload the input image provided earlier
4. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## [​](http://docs.comfy.org#general-tips-for-using-t2i-adapter) General Tips for Using T2I Adapter

### [​](http://docs.comfy.org#input-image-quality-optimization) Input Image Quality Optimization

Regardless of the application scenario, high-quality input images are key to successfully using T2I Adapter:

1. **Moderate Contrast**: Control images (such as depth maps, line art) should have clear contrast, but not excessively extreme
2. **Clear Boundaries**: Ensure that major structures and element boundaries are clearly distinguishable in the control image
3. **Noise Control**: Try to avoid excessive noise in control images, especially for depth maps and line art
4. **Reasonable Layout**: Control images should have a reasonable spatial layout and element distribution

## [​](http://docs.comfy.org#characteristics-of-t2i-adapter-usage) Characteristics of T2I Adapter Usage

One major advantage of T2I Adapter is its ability to easily combine multiple conditions for complex control effects:

1. **Depth + Edge**: Control spatial layout while maintaining clear structural edges, suitable for architecture and interior design
2. **Line Art + Color**: Control shapes while specifying color schemes, suitable for character design and illustrations
3. **Pose + Segmentation**: Control character actions while defining scene areas, suitable for complex narrative scenes

Mixing different T2I Adapters, or combining them with other control methods (such as ControlNet, regional prompts, etc.), can further expand creative possibilities. To achieve mixing, simply chain multiple `Apply ControlNet` nodes together in the same way as described in [Mixing ControlNet](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets.mdx).

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/controlnet/depth-t2i-adapter.mdx)

[Previous](http://docs.comfy.org/tutorials/controlnet/depth-controlnet)

[Mixing ControlNetIn this example, we will demonstrate how to mix multiple ControlNets and learn to use multiple ControlNet models to control image generation  
\
Next](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Introduction to T2I Adapter](http://docs.comfy.org#introduction-to-t2i-adapter)
- [Comparison Between T2I Adapter and ControlNet](http://docs.comfy.org#comparison-between-t2i-adapter-and-controlnet)
- [Main Types of T2I Adapter](http://docs.comfy.org#main-types-of-t2i-adapter)
- [Value of Depth T2I Adapter Applications](http://docs.comfy.org#value-of-depth-t2i-adapter-applications)
- [ComfyUI Depth T2I Adapter Workflow Example Explanation](http://docs.comfy.org#comfyui-depth-t2i-adapter-workflow-example-explanation)
- [1. Depth T2I Adapter Workflow Assets](http://docs.comfy.org#1-depth-t2i-adapter-workflow-assets)
- [2. Model Installation](http://docs.comfy.org#2-model-installation)
- [3. Step-by-Step Workflow Execution](http://docs.comfy.org#3-step-by-step-workflow-execution)
- [General Tips for Using T2I Adapter](http://docs.comfy.org#general-tips-for-using-t2i-adapter)
- [Input Image Quality Optimization](http://docs.comfy.org#input-image-quality-optimization)
- [Characteristics of T2I Adapter Usage](http://docs.comfy.org#characteristics-of-t2i-adapter-usage)

<!-- END Get_Started/tutorials/controlnet/depth-t2i-adapter.md -->


<!-- BEGIN Get_Started/tutorials/controlnet/mixing-controlnets.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
  
  - [ControlNet](http://docs.comfy.org/tutorials/controlnet/controlnet)
  - [Pose ControlNet](http://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass)
  - [Depth ControlNet](http://docs.comfy.org/tutorials/controlnet/depth-controlnet)
  - [Depth T2I Adapter](http://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter)
  - [Mixing ControlNet](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets)
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Mixing ControlNet Examples

# ComfyUI Mixing ControlNet Examples

In this example, we will demonstrate how to mix multiple ControlNets and learn to use multiple ControlNet models to control image generation

In AI image generation, a single control condition often fails to meet the requirements of complex scenes. Mixing multiple ControlNets allows you to control different regions or aspects of an image simultaneously, achieving more precise control over image generation.

In certain scenarios, mixing ControlNets can leverage the characteristics of different control conditions to achieve more refined conditional control:

1. **Scene Complexity**: Complex scenes require multiple control conditions working together
2. **Fine-grained Control**: By adjusting the strength parameter of each ControlNet, you can precisely control the degree of influence for each part
3. **Complementary Effects**: Different types of ControlNets can complement each other, compensating for the limitations of single controls
4. **Creative Expression**: Combining different controls can produce unique creative effects

### [​](http://docs.comfy.org#how-to-mix-controlnets) How to Mix ControlNets

When mixing multiple ControlNets, each ControlNet influences the image generation process according to its applied area. ComfyUI enables multiple ControlNet conditions to be applied sequentially in a layered manner through chain connections in the `Apply ControlNet` node:

## [​](http://docs.comfy.org#comfyui-controlnet-regional-division-mixing-example) ComfyUI ControlNet Regional Division Mixing Example

In this example, we will use a combination of **Pose ControlNet** and **Scribble ControlNet** to generate a scene containing multiple elements: a character on the left controlled by Pose ControlNet and a cat on a scooter on the right controlled by Scribble ControlNet.

### [​](http://docs.comfy.org#1-controlnet-mixing-workflow-assets) 1. ControlNet Mixing Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

This workflow image contains Metadata, and can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`. The system will automatically detect and prompt to download the required models.

Input pose image (controls the character pose on the left):

Input scribble image (controls the cat and scooter on the right):

### [​](http://docs.comfy.org#2-manual-model-installation) 2. Manual Model Installation

If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:

- [awpainting\_v14.safetensors](https://civitai.com/api/download/models/624939?type=Model&format=SafeTensor&size=full&fp=fp16)
- [control\_v11p\_sd15\_scribble\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_scribble_fp16.safetensors?download=true)
- [control\_v11p\_sd15\_openpose\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)
- [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── awpainting_v14.safetensors
│   ├── controlnet/
│   │   └── control_v11p_sd15_scribble_fp16.safetensors
│   │   └── control_v11p_sd15_openpose_fp16.safetensors
│   ├── vae/
│   │   └── vae-ft-mse-840000-ema-pruned.safetensors
```

### [​](http://docs.comfy.org#3-step-by-step-workflow-execution) 3. Step-by-Step Workflow Execution

Follow these steps according to the numbered markers in the image:

1. Ensure that `Load Checkpoint` can load **awpainting\_v14.safetensors**
2. Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**

First ControlNet group using the Openpose model: 3. Ensure that `Load ControlNet Model` loads **control\_v11p\_sd15\_openpose\_fp16.safetensors** 4. Click `Upload` in the `Load Image` node to upload the pose image provided earlier

Second ControlNet group using the Scribble model: 5. Ensure that `Load ControlNet Model` loads **control\_v11p\_sd15\_scribble\_fp16.safetensors** 6. Click `Upload` in the `Load Image` node to upload the scribble image provided earlier 7. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## [​](http://docs.comfy.org#workflow-explanation) Workflow Explanation

#### [​](http://docs.comfy.org#strength-balance) Strength Balance

When controlling different regions of an image, balancing the strength parameters is particularly important:

- If the ControlNet strength for one region is significantly higher than another, it may cause that region’s control effect to overpower and suppress the other region
- It’s recommended to set similar strength values for ControlNets controlling different regions, for example, both set to 1.0

#### [​](http://docs.comfy.org#prompt-techniques) Prompt Techniques

For regional division mixing, the prompt needs to include descriptions of both regions:

```plaintext
"A woman in red dress, a cat riding a scooter, detailed background, high quality"
```

Such a prompt covers both the character and the cat on the scooter, ensuring the model pays attention to both control regions.

## [​](http://docs.comfy.org#multi-dimensional-control-applications-for-a-single-subject) Multi-dimensional Control Applications for a Single Subject

In addition to the regional division mixing shown in this example, another common mixing approach is to apply multi-dimensional control to the same subject. For example:

- **Pose + Depth**: Control character posture and spatial sense
- **Pose + Canny**: Control character posture and edge details
- **Pose + Reference**: Control character posture while referencing a specific style

In this type of application, reference images for multiple ControlNets should be aligned to the same subject, and their strengths should be adjusted to ensure proper balance.

By combining different types of ControlNets and specifying their control regions, you can achieve precise control over elements in your image.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/controlnet/mixing-controlnets.mdx)

[Previous](http://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter)

[Flux.1 Text-to-ImageThis guide provides a brief introduction to the Flux.1 model and guides you through using the Flux.1 model for text-to-image generation with examples including the full version and the FP8 Checkpoint version.  
\
Next](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [How to Mix ControlNets](http://docs.comfy.org#how-to-mix-controlnets)
- [ComfyUI ControlNet Regional Division Mixing Example](http://docs.comfy.org#comfyui-controlnet-regional-division-mixing-example)
- [1. ControlNet Mixing Workflow Assets](http://docs.comfy.org#1-controlnet-mixing-workflow-assets)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation)
- [3. Step-by-Step Workflow Execution](http://docs.comfy.org#3-step-by-step-workflow-execution)
- [Workflow Explanation](http://docs.comfy.org#workflow-explanation)
- [Strength Balance](http://docs.comfy.org#strength-balance)
- [Prompt Techniques](http://docs.comfy.org#prompt-techniques)
- [Multi-dimensional Control Applications for a Single Subject](http://docs.comfy.org#multi-dimensional-control-applications-for-a-single-subject)

<!-- END Get_Started/tutorials/controlnet/mixing-controlnets.md -->


<!-- BEGIN Get_Started/tutorials/controlnet/pose-controlnet-2-pass.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
  
  - [ControlNet](http://docs.comfy.org/tutorials/controlnet/controlnet)
  - [Pose ControlNet](http://docs.comfy.org/tutorials/controlnet/pose-controlnet-2-pass)
  - [Depth ControlNet](http://docs.comfy.org/tutorials/controlnet/depth-controlnet)
  - [Depth T2I Adapter](http://docs.comfy.org/tutorials/controlnet/depth-t2i-adapter)
  - [Mixing ControlNet](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets)
- Flux
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Pose ControlNet Usage Example

# ComfyUI Pose ControlNet Usage Example

This guide will introduce you to the basic concepts of Pose ControlNet, and demonstrate how to generate large-sized images in ComfyUI using a two-pass generation approach

## [​](http://docs.comfy.org#introduction-to-openpose) Introduction to OpenPose

[OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) is an open-source real-time multi-person pose estimation system developed by Carnegie Mellon University (CMU), representing a significant breakthrough in the field of computer vision. The system can simultaneously detect multiple people in an image, capturing:

- **Body skeleton**: 18 keypoints, including head, shoulders, elbows, wrists, hips, knees, and ankles
- **Facial expressions**: 70 facial keypoints for capturing micro-expressions and facial contours
- **Hand details**: 21 hand keypoints for precisely expressing finger positions and gestures
- **Foot posture**: 6 foot keypoints, recording standing postures and movement details

In AI image generation, skeleton structure maps generated by OpenPose serve as conditional inputs for ControlNet, enabling precise control over the posture, actions, and expressions of generated characters. This allows us to generate realistic human figures with expected poses and actions, greatly improving the controllability and practical value of AI-generated content. Particularly for early Stable Diffusion 1.5 series models, skeletal maps generated by OpenPose can effectively prevent issues with distorted character actions, limbs, and expressions.

## [​](http://docs.comfy.org#comfyui-2-pass-pose-controlnet-usage-example) ComfyUI 2-Pass Pose ControlNet Usage Example

### [​](http://docs.comfy.org#1-pose-controlnet-workflow-assets) 1. Pose ControlNet Workflow Assets

Please download the workflow image below and drag it into ComfyUI to load the workflow:

Images with workflow JSON in their metadata can be directly dragged into ComfyUI or loaded using the menu `Workflows` -&gt; `Open (ctrl+o)`. This image already includes download links for the corresponding models, and dragging it into ComfyUI will automatically prompt for downloads.

Please download the image below, which we will use as input:

### [​](http://docs.comfy.org#2-manual-model-installation) 2. Manual Model Installation

If your network cannot successfully complete the automatic download of the corresponding models, please try manually downloading the models below and placing them in the specified directories:

- [control\_v11p\_sd15\_openpose\_fp16.safetensors](https://huggingface.co/comfyanonymous/ControlNet-v1-1_fp16_safetensors/resolve/main/control_v11p_sd15_openpose_fp16.safetensors?download=true)
- [majicmixRealistic\_v7.safetensors](https://civitai.com/api/download/models/176425?type=Model&format=SafeTensor&size=pruned&fp=fp16)
- [japaneseStyleRealistic\_v20.safetensors](https://civitai.com/api/download/models/85426?type=Model&format=SafeTensor&size=pruned&fp=fp16)
- [vae-ft-mse-840000-ema-pruned.safetensors](https://huggingface.co/stabilityai/sd-vae-ft-mse-original/resolve/main/vae-ft-mse-840000-ema-pruned.safetensors?download=true)

```plaintext
ComfyUI/
├── models/
│   ├── checkpoints/
│   │   └── majicmixRealistic_v7.safetensors
│   │   └── japaneseStyleRealistic_v20.safetensors
│   ├── vae/
│   │   └── vae-ft-mse-840000-ema-pruned.safetensors
│   └── controlnet/
│       └── control_v11p_sd15_openpose_fp16.safetensors
```

### [​](http://docs.comfy.org#3-step-by-step-workflow-execution) 3. Step-by-Step Workflow Execution

Follow these steps according to the numbered markers in the image:

1. Ensure that `Load Checkpoint` can load **majicmixRealistic\_v7.safetensors**
2. Ensure that `Load VAE` can load **vae-ft-mse-840000-ema-pruned.safetensors**
3. Ensure that `Load ControlNet Model` can load **control\_v11p\_sd15\_openpose\_fp16.safetensors**
4. Click the select button in the `Load Image` node to upload the pose input image provided earlier, or use your own OpenPose skeleton map
5. Ensure that `Load Checkpoint` can load **japaneseStyleRealistic\_v20.safetensors**
6. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## [​](http://docs.comfy.org#explanation-of-the-pose-controlnet-2-pass-workflow) Explanation of the Pose ControlNet 2-Pass Workflow

This workflow uses a two-pass image generation approach, dividing the image creation process into two phases:

### [​](http://docs.comfy.org#first-phase%3A-basic-pose-image-generation) First Phase: Basic Pose Image Generation

In the first phase, the **majicmixRealistic\_v7** model is combined with Pose ControlNet to generate an initial character pose image:

1. First, load the majicmixRealistic\_v7 model via the `Load Checkpoint` node
2. Load the pose control model through the `Load ControlNet Model` node
3. The input pose image is fed into the `Apply ControlNet` node and combined with positive and negative prompt conditions
4. The first `KSampler` node (typically using 20-30 steps) generates a basic character pose image
5. The pixel-space image for the first phase is obtained through `VAE Decode`

This phase primarily focuses on correct character posture, pose, and basic structure, ensuring that the generated character conforms to the input skeletal pose.

### [​](http://docs.comfy.org#second-phase%3A-style-optimization-and-detail-enhancement) Second Phase: Style Optimization and Detail Enhancement

In the second phase, the output image from the first phase is used as a reference, with the **japaneseStyleRealistic\_v20** model performing stylization and detail enhancement:

1. The image generated in the first phase creates a larger resolution latent space through the `Upscale latent` node
2. The second `Load Checkpoint` loads the japaneseStyleRealistic\_v20 model, which focuses on details and style
3. The second `KSampler` node uses a lower `denoise` strength (typically 0.4-0.6) for refinement, preserving the basic structure from the first phase
4. Finally, a higher quality, larger resolution image is output through the second `VAE Decode` and `Save Image` nodes

This phase primarily focuses on style consistency, detail richness, and enhancing overall image quality.

## [​](http://docs.comfy.org#advantages-of-2-pass-image-generation) Advantages of 2-Pass Image Generation

Compared to single-pass generation, the two-pass image generation method offers the following advantages:

1. **Higher Resolution**: Two-pass processing can generate high-resolution images beyond the capabilities of single-pass generation
2. **Style Blending**: Can combine advantages of different models, such as using a realistic model in the first phase and a stylized model in the second phase
3. **Better Details**: The second phase can focus on optimizing details without having to worry about overall structure
4. **Precise Control**: Once pose control is completed in the first phase, the second phase can focus on refining style and details
5. **Reduced GPU Load**: Generating in two passes allows for high-quality large images with limited GPU resources

To learn more about techniques for mixing multiple ControlNets, please refer to the [Mixing ControlNet Models](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets.mdx) tutorial.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/controlnet/pose-controlnet-2-pass.mdx)

[Previous](http://docs.comfy.org/tutorials/controlnet/controlnet)

[Depth ControlNetThis guide will introduce you to the basic concepts of Depth ControlNet and demonstrate how to generate corresponding images in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/controlnet/depth-controlnet)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Introduction to OpenPose](http://docs.comfy.org#introduction-to-openpose)
- [ComfyUI 2-Pass Pose ControlNet Usage Example](http://docs.comfy.org#comfyui-2-pass-pose-controlnet-usage-example)
- [1. Pose ControlNet Workflow Assets](http://docs.comfy.org#1-pose-controlnet-workflow-assets)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation)
- [3. Step-by-Step Workflow Execution](http://docs.comfy.org#3-step-by-step-workflow-execution)
- [Explanation of the Pose ControlNet 2-Pass Workflow](http://docs.comfy.org#explanation-of-the-pose-controlnet-2-pass-workflow)
- [First Phase: Basic Pose Image Generation](http://docs.comfy.org#first-phase%3A-basic-pose-image-generation)
- [Second Phase: Style Optimization and Detail Enhancement](http://docs.comfy.org#second-phase%3A-style-optimization-and-detail-enhancement)
- [Advantages of 2-Pass Image Generation](http://docs.comfy.org#advantages-of-2-pass-image-generation)

<!-- END Get_Started/tutorials/controlnet/pose-controlnet-2-pass.md -->


<!-- BEGIN Get_Started/tutorials/flux/flux-1-controlnet.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
  
  - [Flux.1 Text-to-Image](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image)
  - [Flux.1 fill dev](http://docs.comfy.org/tutorials/flux/flux-1-fill-dev)
  - [Flux.1 ControlNet](http://docs.comfy.org/tutorials/flux/flux-1-controlnet)
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Flux.1 ControlNet Examples

# ComfyUI Flux.1 ControlNet Examples

This guide will demonstrate workflow examples using Flux.1 ControlNet.

## [​](http://docs.comfy.org#flux-1-controlnet-model-introduction) FLUX.1 ControlNet Model Introduction

FLUX.1 Canny and Depth are two powerful models from the [FLUX.1 Tools](https://blackforestlabs.ai/flux-1-tools/) launched by [Black Forest Labs](https://blackforestlabs.ai/). This toolkit is designed to add control and guidance capabilities to FLUX.1, enabling users to modify and recreate real or generated images.

**FLUX.1-Depth-dev** and **FLUX.1-Canny-dev** are both 12B parameter Rectified Flow Transformer models that can generate images based on text descriptions while maintaining the structural features of the input image. The Depth version maintains the spatial structure of the source image through depth map extraction techniques, while the Canny version uses edge detection techniques to preserve the structural features of the source image, allowing users to choose the appropriate control method based on different needs.

Both models have the following features:

- Top-tier output quality and detail representation
- Excellent prompt following ability while maintaining consistency with the original image
- Trained using guided distillation techniques for improved efficiency
- Open weights for the research community
- API interfaces (pro version) and open-source weights (dev version)

Additionally, Black Forest Labs also provides **FLUX.1-Depth-dev-lora** and **FLUX.1-Canny-dev-lora** adapter versions extracted from the complete models. These can be applied to the FLUX.1 \[dev] base model to provide similar functionality with smaller file size, especially suitable for resource-constrained environments.

We will use the full version of **FLUX.1-Canny-dev** and **FLUX.1-Depth-dev-lora** to complete the workflow examples.

All workflow images’s Metadata contains the corresponding model download information. You can load the workflows by:

- Dragging them directly into ComfyUI
- Or using the menu `Workflows` -&gt; `Open（ctrl+o）`

If you’re not using the Desktop Version or some models can’t be downloaded automatically, please refer to the manual installation sections to save the model files to the corresponding folder.

For image preprocessors, you can use the following custom nodes to complete image preprocessing. In this example, we will provide processed images as input.

- [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
- [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

## [​](http://docs.comfy.org#flux-1-canny-dev-complete-version-workflow) FLUX.1-Canny-dev Complete Version Workflow

### [​](http://docs.comfy.org#1-workflow-and-asset) 1. Workflow and Asset

Please download the workflow image below and drag it into ComfyUI to load the workflow

Please download the image below, which we will use as the input image

### [​](http://docs.comfy.org#2-manual-models-installation) 2. Manual Models Installation

If you have previously used the [complete version of Flux related workflows](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image), then you only need to download the **flux1-canny-dev.safetensors** model file. Since you need to first agree to the terms of [black-forest-labs/FLUX.1-Canny-dev](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev), please visit the [black-forest-labs/FLUX.1-Canny-dev](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev) page and make sure you have agreed to the corresponding terms as shown in the image below.

Complete model list:

- [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
- [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
- [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
- [flux1-canny-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev/resolve/main/flux1-canny-dev.safetensors?download=true) (Please ensure you have agreed to the corresponding repo’s terms)

File storage location:

```plaintext
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── t5xxl_fp16.safetensors
│   ├── vae/
│   │   └── ae.safetensors
│   └── diffusion_models/
│       └── flux1-canny-dev.safetensors
```

### [​](http://docs.comfy.org#3-step-by-step-workflow-execution) 3. Step-by-Step Workflow Execution

1. Make sure `ae.safetensors` is loaded in the `Load VAE` node
2. Make sure `flux1-canny-dev.safetensors` is loaded in the `Load Diffusion Model` node
3. Make sure the following models are loaded in the `DualCLIPLoader` node:
   
   - clip\_name1: t5xxl\_fp16.safetensors
   - clip\_name2: clip\_l.safetensors
4. Upload the provided input image in the `Load Image` node
5. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

### [​](http://docs.comfy.org#4-start-your-experimentation) 4. Start Your Experimentation

Try using the [FLUX.1-Depth-dev](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev) model to complete the Depth version of the workflow

You can use the image below as input

Or use the following custom nodes to complete image preprocessing:

- [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet)
- [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

## [​](http://docs.comfy.org#flux-1-depth-dev-lora-workflow) FLUX.1-Depth-dev-lora Workflow

The LoRA version workflow builds on the complete version by adding the LoRA model. Compared to the [complete version of the Flux workflow](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image), it adds nodes for loading and using the corresponding LoRA model.

### [​](http://docs.comfy.org#1-workflow-and-asset-2) 1. Workflow and Asset

Please download the workflow image below and drag it into ComfyUI to load the workflow

Please download the image below, which we will use as the input image

### [​](http://docs.comfy.org#2-manual-model-download) 2. Manual Model Download

If you have previously used the [complete version of Flux related workflows](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image), then you only need to download the **flux1-depth-dev-lora.safetensors** model file.

Complete model list:

- [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
- [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
- [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
- [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors?download=true)
- [flux1-depth-dev-lora.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Depth-dev-lora/resolve/main/flux1-depth-dev-lora.safetensors?download=true)

File storage location:

```plaintext
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── t5xxl_fp16.safetensors
│   ├── vae/
│   │   └── ae.safetensors
│   ├── diffusion_models/
│   │   └── flux1-dev.safetensors
│   └── loras/
│       └── flux1-depth-dev-lora.safetensors
```

### [​](http://docs.comfy.org#3-step-by-step-workflow-execution-2) 3. Step-by-Step Workflow Execution

1. Make sure `flux1-dev.safetensors` is loaded in the `Load Diffusion Model` node
2. Make sure `flux1-depth-dev-lora.safetensors` is loaded in the `LoraLoaderModelOnly` node
3. Make sure the following models are loaded in the `DualCLIPLoader` node:
   
   - clip\_name1: t5xxl\_fp16.safetensors
   - clip\_name2: clip\_l.safetensors
4. Upload the provided input image in the `Load Image` node
5. Make sure `ae.safetensors` is loaded in the `Load VAE` node
6. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

### [​](http://docs.comfy.org#4-start-your-experimentation-2) 4. Start Your Experimentation

Try using the [FLUX.1-Canny-dev-lora](https://huggingface.co/black-forest-labs/FLUX.1-Canny-dev-lora) model to complete the Canny version of the workflow

Use [ComfyUI-Advanced-ControlNet](https://github.com/Kosinkadink/ComfyUI-Advanced-ControlNet) or [ComfyUI ControlNet aux](https://github.com/Fannovel16/comfyui_controlnet_aux) to complete image preprocessing

## [​](http://docs.comfy.org#community-versions-of-flux-controlnets) Community Versions of Flux Controlnets

XLab and InstantX + Shakker Labs have released Controlnets for Flux.

**InstantX:**

- [FLUX.1-dev-Controlnet-Canny](https://huggingface.co/InstantX/FLUX.1-dev-Controlnet-Canny/blob/main/diffusion_pytorch_model.safetensors)
- [FLUX.1-dev-ControlNet-Depth](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Depth/blob/main/diffusion_pytorch_model.safetensors)
- [FLUX.1-dev-ControlNet-Union-Pro](https://huggingface.co/Shakker-Labs/FLUX.1-dev-ControlNet-Union-Pro/blob/main/diffusion_pytorch_model.safetensors)

**XLab**: [flux-controlnet-collections](https://huggingface.co/XLabs-AI/flux-controlnet-collections)

Place these files in the `ComfyUI/models/controlnet` directory.

You can visit [Flux Controlnet Example](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/flux_controlnet_example.png) to get the corresponding workflow image, and use the image from [here](https://raw.githubusercontent.com/comfyanonymous/ComfyUI_examples/refs/heads/master/flux/girl_in_field.png) as the input image.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/flux/flux-1-controlnet.mdx)

[Previous](http://docs.comfy.org/tutorials/flux/flux-1-fill-dev)

[HiDream-I1This guide will walk you through completing a ComfyUI native HiDream-I1 text-to-image workflow example  
\
Next](http://docs.comfy.org/tutorials/image/hidream/hidream-i1)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [FLUX.1 ControlNet Model Introduction](http://docs.comfy.org#flux-1-controlnet-model-introduction)
- [FLUX.1-Canny-dev Complete Version Workflow](http://docs.comfy.org#flux-1-canny-dev-complete-version-workflow)
- [1. Workflow and Asset](http://docs.comfy.org#1-workflow-and-asset)
- [2. Manual Models Installation](http://docs.comfy.org#2-manual-models-installation)
- [3. Step-by-Step Workflow Execution](http://docs.comfy.org#3-step-by-step-workflow-execution)
- [4. Start Your Experimentation](http://docs.comfy.org#4-start-your-experimentation)
- [FLUX.1-Depth-dev-lora Workflow](http://docs.comfy.org#flux-1-depth-dev-lora-workflow)
- [1. Workflow and Asset](http://docs.comfy.org#1-workflow-and-asset-2)
- [2. Manual Model Download](http://docs.comfy.org#2-manual-model-download)
- [3. Step-by-Step Workflow Execution](http://docs.comfy.org#3-step-by-step-workflow-execution-2)
- [4. Start Your Experimentation](http://docs.comfy.org#4-start-your-experimentation-2)
- [Community Versions of Flux Controlnets](http://docs.comfy.org#community-versions-of-flux-controlnets)

<!-- END Get_Started/tutorials/flux/flux-1-controlnet.md -->


<!-- BEGIN Get_Started/tutorials/flux/flux-1-fill-dev.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
  
  - [Flux.1 Text-to-Image](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image)
  - [Flux.1 fill dev](http://docs.comfy.org/tutorials/flux/flux-1-fill-dev)
  - [Flux.1 ControlNet](http://docs.comfy.org/tutorials/flux/flux-1-controlnet)
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Flux.1 fill dev Example

# ComfyUI Flux.1 fill dev Example

This guide demonstrates how to use Flux.1 fill dev to create Inpainting and Outpainting workflows.

## [​](http://docs.comfy.org#introduction-to-flux-1-fill-dev-model) Introduction to Flux.1 fill dev Model

Flux.1 fill dev is one of the core tools in the [FLUX.1 Tools suite](https://blackforestlabs.ai/flux-1-tools/) launched by [Black Forest Labs](https://blackforestlabs.ai/), specifically designed for image inpainting and outpainting.

Key features of Flux.1 fill dev:

- Powerful image inpainting and outpainting capabilities, with results second only to the commercial version FLUX.1 Fill \[pro].
- Excellent prompt understanding and following ability, precisely capturing user intent while maintaining high consistency with the original image.
- Advanced guided distillation training technology, making the model more efficient while maintaining high-quality output.
- Friendly licensing terms, with generated outputs usable for personal, scientific, and commercial purposes, please refer to the [FLUX.1 \[dev\] non-commercial license](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/LICENSE.md) for details.

Open Source Repository: [FLUX.1 \[dev\]](https://huggingface.co/black-forest-labs/FLUX.1-dev)

This guide will demonstrate inpainting and outpainting workflows based on the Flux.1 fill dev model. If you’re not familiar with inpainting and outpainting workflows, you can refer to [ComfyUI Layout Inpainting Example](http://docs.comfy.org/tutorials/basic/inpaint) and [ComfyUI Image Extension Example](http://docs.comfy.org/tutorials/basic/outpaint) for some related explanations.

## [​](http://docs.comfy.org#flux-1-fill-dev-and-related-models-installation) Flux.1 Fill dev and related models installation

Before we begin, let’s complete the installation of the Flux.1 Fill dev model files. The inpainting and outpainting workflows will use exactly the same model files. If you’ve previously used the full version of the [Flux.1 Text-to-Image workflow](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image), then you only need to download the **flux1-fill-dev.safetensors** model file in this section.

However, since downloading the corresponding model requires agreeing to the corresponding usage agreement, please visit the [black-forest-labs/FLUX.1-Fill-dev](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev) page and make sure you have agreed to the corresponding agreement as shown in the image below.

Complete model list:

- [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
- [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true)
- [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
- [flux1-fill-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-Fill-dev/resolve/main/flux1-fill-dev.safetensors?download=true)

File storage location:

```plaintext
ComfyUI/
├── models/
│   ├── text_encoders/
│   │    ├── clip_l.safetensors
│   │    └── t5xxl_fp16.safetensors
│   ├── vae/
│   │    └── ae.safetensors
│   └── diffusion_models/
│        └── flux1-fill-dev.safetensors
```

## [​](http://docs.comfy.org#flux-1-fill-dev-inpainting-workflow) Flux.1 Fill dev inpainting workflow

### [​](http://docs.comfy.org#1-inpainting-workflow-and-asset) 1. Inpainting workflow and asset

Please download the image below and drag it into ComfyUI to load the corresponding workflow

Please download the image below, we will use it as the input image

The corresponding image already contains an alpha channel, so you don’t need to draw a mask separately. If you want to draw your own mask, please [click here](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/flux/inpaint/flux_fill_inpaint_input_original.png) to get the image without a mask, and refer to the MaskEditor usage section in the [ComfyUI Layout Inpainting Example](http://docs.comfy.org/tutorials/basic/inpaint#using-the-mask-editor) to learn how to draw a mask in the `Load Image` node.

### [​](http://docs.comfy.org#2-steps-to-run-the-workflow) 2. Steps to run the workflow

1. Ensure the `Load Diffusion Model` node has `flux1-fill-dev.safetensors` loaded.
2. Ensure the `DualCLIPLoader` node has the following models loaded:
   
   - clip\_name1: `t5xxl_fp16.safetensors`
   - clip\_name2: `clip_l.safetensors`
3. Ensure the `Load VAE` node has `ae.safetensors` loaded.
4. Upload the input image provided in the document to the `Load Image` node; if you’re using the version without a mask, remember to complete the mask drawing using the mask editor
5. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## [​](http://docs.comfy.org#flux-1-fill-dev-outpainting-workflow) Flux.1 Fill dev Outpainting Workflow

### [​](http://docs.comfy.org#1-outpainting-workflow-and-asset) 1. Outpainting workflow and asset

Please download the image below and drag it into ComfyUI to load the corresponding workflow

Please download the image below, we will use it as the input image

### [​](http://docs.comfy.org#2-steps-to-run-the-workflow-2) 2. Steps to run the workflow

1. Ensure the `Load Diffusion Model` node has `flux1-fill-dev.safetensors` loaded.
2. Ensure the `DualCLIPLoader` node has the following models loaded:
   
   - clip\_name1: `t5xxl_fp16.safetensors`
   - clip\_name2: `clip_l.safetensors`
3. Ensure the `Load VAE` node has `ae.safetensors` loaded.
4. Upload the input image provided in the document to the `Load Image` node
5. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/flux/flux-1-fill-dev.mdx)

[Previous](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image)

[Flux.1 ControlNetThis guide will demonstrate workflow examples using Flux.1 ControlNet.  
\
Next](http://docs.comfy.org/tutorials/flux/flux-1-controlnet)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Introduction to Flux.1 fill dev Model](http://docs.comfy.org#introduction-to-flux-1-fill-dev-model)
- [Flux.1 Fill dev and related models installation](http://docs.comfy.org#flux-1-fill-dev-and-related-models-installation)
- [Flux.1 Fill dev inpainting workflow](http://docs.comfy.org#flux-1-fill-dev-inpainting-workflow)
- [1. Inpainting workflow and asset](http://docs.comfy.org#1-inpainting-workflow-and-asset)
- [2. Steps to run the workflow](http://docs.comfy.org#2-steps-to-run-the-workflow)
- [Flux.1 Fill dev Outpainting Workflow](http://docs.comfy.org#flux-1-fill-dev-outpainting-workflow)
- [1. Outpainting workflow and asset](http://docs.comfy.org#1-outpainting-workflow-and-asset)
- [2. Steps to run the workflow](http://docs.comfy.org#2-steps-to-run-the-workflow-2)

<!-- END Get_Started/tutorials/flux/flux-1-fill-dev.md -->


<!-- BEGIN Get_Started/tutorials/flux/flux-1-text-to-image.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
  
  - [Flux.1 Text-to-Image](http://docs.comfy.org/tutorials/flux/flux-1-text-to-image)
  - [Flux.1 fill dev](http://docs.comfy.org/tutorials/flux/flux-1-fill-dev)
  - [Flux.1 ControlNet](http://docs.comfy.org/tutorials/flux/flux-1-controlnet)
- Image
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Flux.1 Text-to-Image Workflow Example

# ComfyUI Flux.1 Text-to-Image Workflow Example

This guide provides a brief introduction to the Flux.1 model and guides you through using the Flux.1 model for text-to-image generation with examples including the full version and the FP8 Checkpoint version.

Flux is one of the largest open-source text-to-image generation models, with 12B parameters and an original file size of approximately 23GB. It was developed by [Black Forest Labs](https://blackforestlabs.ai/), a team founded by former Stable Diffusion team members. Flux is known for its excellent image quality and flexibility, capable of generating high-quality, diverse images.

Currently, the Flux.1 model has several main versions:

- **Flux.1 Pro:** The best performing model, closed-source, only available through API calls.
- [**Flux.1 \[dev\]：**](https://huggingface.co/black-forest-labs/FLUX.1-dev) Open-source but limited to non-commercial use, distilled from the Pro version, with performance close to the Pro version.
- [**Flux.1 \[schnell\]：**](https://huggingface.co/black-forest-labs/FLUX.1-schnell) Uses the Apache2.0 license, requires only 4 steps to generate images, suitable for low-spec hardware.

**Flux.1 Model Features**

- **Hybrid Architecture:** Combines the advantages of Transformer networks and diffusion models, effectively integrating text and image information, improving the alignment accuracy between generated images and prompts, with excellent fidelity to complex prompts.
- **Parameter Scale:** Flux has 12B parameters, capturing more complex pattern relationships and generating more realistic, diverse images.
- **Supports Multiple Styles:** Supports diverse styles, with excellent performance for various types of images.

In this example, we’ll introduce text-to-image examples using both Flux.1 Dev and Flux.1 Schnell versions, including the full version model and the simplified FP8 Checkpoint version.

- **Flux Full Version:** Best performance, but requires larger VRAM resources and installation of multiple model files.
- **Flux FP8 Checkpoint:** Requires only one fp8 version of the model, but quality is slightly reduced compared to the full version.

All workflow images’s Metadata contains the corresponding model download information. You can load the workflows by:

- Dragging them directly into ComfyUI
- Or using the menu `Workflows` -&gt; `Open（ctrl+o）`

If you’re not using the Desktop Version or some models can’t be downloaded automatically, please refer to the manual installation sections to save the model files to the corresponding folder. Make sure your ComfyUI is updated to the latest version before starting.

## [​](http://docs.comfy.org#flux-1-full-version-text-to-image-example) Flux.1 Full Version Text-to-Image Example

If you can’t download models from [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev), make sure you’ve logged into Huggingface and agreed to the corresponding repository’s license agreement.

### [​](http://docs.comfy.org#flux-1-dev) Flux.1 Dev

#### [​](http://docs.comfy.org#1-workflow-file) 1. Workflow File

Please download the image below and drag it into ComfyUI to load the workflow.

#### [​](http://docs.comfy.org#2-manual-model-installation) 2. Manual Model Installation

- The `flux1-dev.safetensors` file requires agreeing to the [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev) agreement before downloading via browser.
- If your VRAM is low, you can try using [t5xxl\_fp8\_e4m3fn.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors?download=true) to replace the `t5xxl_fp16.safetensors` file.

Please download the following model files:

- [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
- [t5xxl\_fp16.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors?download=true) Recommended when your VRAM is greater than 32GB.
- [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
- [flux1-dev.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-dev/resolve/main/flux1-dev.safetensors)

Storage location:

```plaintext
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── t5xxl_fp16.safetensors
│   ├── vae/
│   │   └── ae.safetensors
│   └── diffusion_models/
│       └── flux1-dev.safetensors
```

#### [​](http://docs.comfy.org#3-steps-to-run-the-workflow) 3. Steps to Run the Workflow

Please refer to the image below to ensure all model files are loaded correctly

1. Ensure the `DualCLIPLoader` node has the following models loaded:
   
   - clip\_name1: t5xxl\_fp16.safetensors
   - clip\_name2: clip\_l.safetensors
2. Ensure the `Load Diffusion Model` node has `flux1-dev.safetensors` loaded
3. Make sure the `Load VAE` node has `ae.safetensors` loaded
4. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

Thanks to Flux’s excellent prompt following capability, we don’t need any negative prompts

### [​](http://docs.comfy.org#flux-1-schnell) Flux.1 Schnell

#### [​](http://docs.comfy.org#1-workflow-file-2) 1. Workflow File

Please download the image below and drag it into ComfyUI to load the workflow.

#### [​](http://docs.comfy.org#2-manual-models-installation) 2. Manual Models Installation

In this workflow, only two model files are different from the Flux1 Dev version workflow. For t5xxl, you can still use the fp16 version for better results.

- **t5xxl\_fp16.safetensors** -&gt; **t5xxl\_fp8.safetensors**
- **flux1-dev.safetensors** -&gt; **flux1-schnell.safetensors**

Complete model file list:

- [clip\_l.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors?download=true)
- [t5xxl\_fp8\_e4m3fn.safetensors](https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp8_e4m3fn.safetensors?download=true)
- [ae.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors?download=true)
- [flux1-schnell.safetensors](https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/flux1-schnell.safetensors)

File storage location:

```plaintext
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── t5xxl_fp8_e4m3fn.safetensors
│   ├── vae/
│   │   └── ae.safetensors
│   └── diffusion_models/
│       └── flux1-schnell.safetensors
```

#### [​](http://docs.comfy.org#3-steps-to-run-the-workflow-2) 3. Steps to Run the Workflow

1. Ensure the `DualCLIPLoader` node has the following models loaded:
   
   - clip\_name1: t5xxl\_fp8\_e4m3fn.safetensors
   - clip\_name2: clip\_l.safetensors
2. Ensure the `Load Diffusion Model` node has `flux1-schnell.safetensors` loaded
3. Ensure the `Load VAE` node has `ae.safetensors` loaded
4. Click the `Queue` button, or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## [​](http://docs.comfy.org#flux-1-fp8-checkpoint-version-text-to-image-example) Flux.1 FP8 Checkpoint Version Text-to-Image Example

The fp8 version is a quantized version of the original Flux.1 fp16 version. To some extent, the quality of this version will be lower than that of the fp16 version, but it also requires less VRAM, and you only need to install one model file to try running it.

### [​](http://docs.comfy.org#flux-1-dev-2) Flux.1 Dev

Please download the image below and drag it into ComfyUI to load the workflow.

Please download [flux1-dev-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-dev/resolve/main/flux1-dev-fp8.safetensors?download=true) and save it to the `ComfyUI/models/checkpoints/` directory.

Ensure that the corresponding `Load Checkpoint` node loads `flux1-dev-fp8.safetensors`, and you can try to run the workflow.

### [​](http://docs.comfy.org#flux-1-schnell-2) Flux.1 Schnell

Please download the image below and drag it into ComfyUI to load the workflow.

Please download [flux1-schnell-fp8.safetensors](https://huggingface.co/Comfy-Org/flux1-schnell/resolve/main/flux1-schnell-fp8.safetensors?download=true) and save it to the `ComfyUI/models/checkpoints/` directory.

Ensure that the corresponding `Load Checkpoint` node loads `flux1-schnell-fp8.safetensors`, and you can try to run the workflow.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/flux/flux-1-text-to-image.mdx)

[Previous](http://docs.comfy.org/tutorials/controlnet/mixing-controlnets)

[Flux.1 fill devThis guide demonstrates how to use Flux.1 fill dev to create Inpainting and Outpainting workflows.  
\
Next](http://docs.comfy.org/tutorials/flux/flux-1-fill-dev)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Flux.1 Full Version Text-to-Image Example](http://docs.comfy.org#flux-1-full-version-text-to-image-example)
- [Flux.1 Dev](http://docs.comfy.org#flux-1-dev)
- [1. Workflow File](http://docs.comfy.org#1-workflow-file)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow)
- [Flux.1 Schnell](http://docs.comfy.org#flux-1-schnell)
- [1. Workflow File](http://docs.comfy.org#1-workflow-file-2)
- [2. Manual Models Installation](http://docs.comfy.org#2-manual-models-installation)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow-2)
- [Flux.1 FP8 Checkpoint Version Text-to-Image Example](http://docs.comfy.org#flux-1-fp8-checkpoint-version-text-to-image-example)
- [Flux.1 Dev](http://docs.comfy.org#flux-1-dev-2)
- [Flux.1 Schnell](http://docs.comfy.org#flux-1-schnell-2)

<!-- END Get_Started/tutorials/flux/flux-1-text-to-image.md -->


<!-- BEGIN Get_Started/tutorials/image/hidream/hidream-e1.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
  
  - HiDream
    
    - [HiDream-I1](http://docs.comfy.org/tutorials/image/hidream/hidream-i1)
    - [HiDream-e1](http://docs.comfy.org/tutorials/image/hidream/hidream-e1)
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Native HiDream-e1 Workflow Example

# ComfyUI Native HiDream-e1 Workflow Example

This guide will help you understand and complete the ComfyUI native HiDream-I1 text-to-image workflow example

HiDream-E1 is an interactive image editing large model officially open-sourced by HiDream-ai on April 28, 2025, built based on HiDream-I1.

It allows you to edit images using natural language. The model is released under the [MIT License](https://github.com/HiDream-ai/HiDream-E1?tab=MIT-1-ov-file), supporting use in personal projects, scientific research, and commercial applications. In combination with the previously released [hidream-i1](http://docs.comfy.org/zh-CN/tutorials/advanced/hidream), it enables **creative capabilities from image generation to editing**.

**ComfyUI now natively supports HiDream E1**. In this guide, we will help you complete the workflow example of using HiDream E1 in ComfyUI.

For reference, this workflow takes about 500s for the first run and 370s for the second run with 28 sampling steps on Google Colab L4 with 22.5GB VRAM.

### [​](http://docs.comfy.org#hidream-e1-information) HiDream-E1 Information

**HiDream-E1 Model Download** Currently, HiDream provides a full version. Here is the model information:

NameInference StepsResolutionHuggingFace RepositoryHiDream-E1-Full28768x768[🤗 HiDream-E1-Full](https://huggingface.co/HiDream-ai/HiDream-E1-Full)

-[Github](https://github.com/HiDream-ai/HiDream-E1)

## [​](http://docs.comfy.org#comfyui-native-hidream-e1-workflow-example) ComfyUI Native HiDream-e1 Workflow Example

Please upgrade your ComfyUI to the latest version (latest commit) before starting to ensure your ComfyUI has the relevant support.

### [​](http://docs.comfy.org#1-download-hidream-e1-workflow-and-related-files) 1. Download HiDream-e1 Workflow and Related Files

#### [​](http://docs.comfy.org#1-1-download-workflow-file) 1.1 Download Workflow File

Please download the image below and drag it into ComfyUI. The workflow already contains model download information, and after loading, it will prompt you to download the corresponding models.

#### [​](http://docs.comfy.org#1-2-download-input-image) 1.2 Download Input Image

Please download the image below, which we will use as input

### [​](http://docs.comfy.org#2-manual-model-installation-for-hidream-e1-related-models) 2. Manual Model Installation for HiDream-e1 Related Models

All models mentioned in this guide can be found [here](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/tree/main/split_files). Please download the corresponding files and save them to the appropriate folders.

The following model files are shared models that we will use. Please click the corresponding links to download and save according to the model file storage location. We will guide you to download the corresponding **diffusion models** in the workflow.

**text\_encoders**:

- [clip\_l\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors)
- [clip\_g\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_g_hidream.safetensors)
- [t5xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors) This model has been used in many workflows, you may have already downloaded this file.
- [llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors)

**VAE**

- [ae.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/vae/ae.safetensors) This is Flux’s VAE model. If you have used Flux workflows before, you may have already downloaded this file.

**diffusion models**

- [hidream\_e1\_full\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_e1_full_bf16.safetensors)

Model file storage location

```plaintext
📂 ComfyUI/
├── 📂 models/
│   ├── 📂 text_encoders/
│   │   ├─── clip_l_hidream.safetensors
│   │   ├─── clip_g_hidream.safetensors
│   │   ├─── t5xxl_fp8_e4m3fn_scaled.safetensors
│   │   └─── llama_3.1_8b_instruct_fp8_scaled.safetensors
│   └── 📂 vae/
│   │   └── ae.safetensors
│   └── 📂 diffusion_models/
│       └── hidream_e1_full_bf16.safetensors   
```

### [​](http://docs.comfy.org#3-complete-the-hidream-e1-workflow-step-by-step) 3. Complete the HiDream-e1 Workflow Step by Step

Follow these steps to complete the workflow:

1. Make sure the `Load Diffusion Model` node has loaded the `hidream_e1_full_bf16.safetensors` model
2. Ensure that the four corresponding text encoders are correctly loaded in the `QuadrupleCLIPLoader`
   
   - clip\_l\_hidream.safetensors
   - clip\_g\_hidream.safetensors
   - t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   - llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node is using the `ae.safetensors` file
4. Load the input image we downloaded earlier in the `Load Image` node
5. (Important) Enter **the prompt for how you want to modify the image** in the `Empty Text Encoder(Positive)` node
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to generate the image

### [​](http://docs.comfy.org#additional-notes-on-comfyui-hidream-e1-workflow) Additional Notes on ComfyUI HiDream-e1 Workflow

- You may need to modify the prompt multiple times or generate multiple times to get better results
- This model has difficulty maintaining consistency when changing image styles, so try to make your prompts as complete as possible
- As the model supports a resolution of 768\*768, in actual testing with other dimensions, the image performance is poor or even significantly different at other dimensions

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/image/hidream/hidream-e1.mdx)

[Previous](http://docs.comfy.org/tutorials/image/hidream/hidream-i1)

[Hunyuan3D-2This guide will demonstrate how to use Hunyuan3D-2 in ComfyUI to generate 3D assets.  
\
Next](http://docs.comfy.org/tutorials/3d/hunyuan3D-2)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [HiDream-E1 Information](http://docs.comfy.org#hidream-e1-information)
- [ComfyUI Native HiDream-e1 Workflow Example](http://docs.comfy.org#comfyui-native-hidream-e1-workflow-example)
- [1. Download HiDream-e1 Workflow and Related Files](http://docs.comfy.org#1-download-hidream-e1-workflow-and-related-files)
- [1.1 Download Workflow File](http://docs.comfy.org#1-1-download-workflow-file)
- [1.2 Download Input Image](http://docs.comfy.org#1-2-download-input-image)
- [2. Manual Model Installation for HiDream-e1 Related Models](http://docs.comfy.org#2-manual-model-installation-for-hidream-e1-related-models)
- [3. Complete the HiDream-e1 Workflow Step by Step](http://docs.comfy.org#3-complete-the-hidream-e1-workflow-step-by-step)
- [Additional Notes on ComfyUI HiDream-e1 Workflow](http://docs.comfy.org#additional-notes-on-comfyui-hidream-e1-workflow)

<!-- END Get_Started/tutorials/image/hidream/hidream-e1.md -->


<!-- BEGIN Get_Started/tutorials/image/hidream/hidream-i1.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
  
  - HiDream
    
    - [HiDream-I1](http://docs.comfy.org/tutorials/image/hidream/hidream-i1)
    - [HiDream-e1](http://docs.comfy.org/tutorials/image/hidream/hidream-e1)
- 3D
- Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Native HiDream-I1 Text-to-Image Workflow Example

# ComfyUI Native HiDream-I1 Text-to-Image Workflow Example

This guide will walk you through completing a ComfyUI native HiDream-I1 text-to-image workflow example

HiDream-I1 is a text-to-image model officially open-sourced by HiDream-ai on April 7, 2025. The model has 17B parameters and is released under the [MIT license](https://github.com/HiDream-ai/HiDream-I1/blob/main/LICENSE), supporting personal projects, scientific research, and commercial use. It currently performs excellently in multiple benchmark tests.

## [​](http://docs.comfy.org#model-features) Model Features

**Hybrid Architecture Design** A combination of Diffusion Transformer (DiT) and Mixture of Experts (MoE) architecture:

- Based on Diffusion Transformer (DiT), with dual-stream MMDiT modules processing multimodal information and single-stream DiT modules optimizing global consistency.
- Dynamic routing mechanism flexibly allocates computing resources, enhancing complex scene processing capabilities and delivering excellent performance in color restoration, edge processing, and other details.

**Multimodal Text Encoder Integration** Integrates four text encoders:

- OpenCLIP ViT-bigG, OpenAI CLIP ViT-L (visual semantic alignment)
- T5-XXL (long text parsing)
- Llama-3.1-8B-Instruct (instruction understanding) This combination achieves SOTA performance in complex semantic parsing of colors, quantities, spatial relationships, etc., with Chinese prompt support significantly outperforming similar open-source models.

**Original Model Versions**

HiDream-ai provides three versions of the HiDream-I1 model to meet different needs. Below are the links to the original model repositories:

Model NameDescriptionInference StepsRepository LinkHiDream-I1-FullFull version50[🤗 HiDream-I1-Full](https://huggingface.co/HiDream-ai/HiDream-I1-Full)HiDream-I1-DevDistilled dev28[🤗 HiDream-I1-Dev](https://huggingface.co/HiDream-ai/HiDream-I1-Dev)HiDream-I1-FastDistilled fast16[🤗 HiDream-I1-Fast](https://huggingface.co/HiDream-ai/HiDream-I1-Fast)

## [​](http://docs.comfy.org#about-this-workflow-example) About This Workflow Example

In this example, we will use the repackaged version from ComfyOrg. You can find all the model files we’ll use in this example in the [HiDream-I1\_ComfyUI](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/) repository.

Before starting, please update your ComfyUI version to ensure it’s at least after this [commit](https://github.com/comfyanonymous/ComfyUI/commit/9ad792f92706e2179c58b2e5348164acafa69288) to make sure your ComfyUI has native support for HiDream

## [​](http://docs.comfy.org#hidream-i1-workflow) HiDream-I1 Workflow

The model requirements for different ComfyUI native HiDream-I1 workflows are basically the same, with only the [diffusion models](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/tree/main/split_files/diffusion_models) files being different.

If you don’t know which version to choose, please refer to the following suggestions:

- **HiDream-I1-Full** can generate the highest quality images
- **HiDream-I1-Dev** balances high-quality image generation with speed
- **HiDream-I1-Fast** can generate images in just 16 steps, suitable for scenarios requiring real-time iteration

For the **dev** and **fast** versions, negative prompts are not needed, so please set the `cfg` parameter to `1.0` during sampling. We have noted the corresponding parameter settings in the relevant workflows.

The full versions of all three versions require a lot of VRAM - you may need more than 27GB of VRAM to run them smoothly. In the corresponding workflow tutorials, we will use the **fp8** version as a demonstration example to ensure that most users can run it smoothly. However, we will still provide download links for different versions of the model in the corresponding examples, and you can choose the appropriate file based on your VRAM situation.

### [​](http://docs.comfy.org#model-installation) Model Installation

The following model files are common files that we will use. Please click on the corresponding links to download and save them according to the model file save location. We will guide you to download the corresponding **diffusion models** in the corresponding workflows.

**text\_encoders**：

- [clip\_l\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_l_hidream.safetensors)
- [clip\_g\_hidream.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/clip_g_hidream.safetensors)
- [t5xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/t5xxl_fp8_e4m3fn_scaled.safetensors) This model has been used in many workflows, you may have already downloaded this file.
- [llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/text_encoders/llama_3.1_8b_instruct_fp8_scaled.safetensors)

**VAE**

- [ae.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/blob/main/split_files/vae/ae.safetensors) This is Flux’s VAE model, if you have used Flux’s workflow before, you may have already downloaded this file.

**diffusion models** We will guide you to download the corresponding model files in the corresponding workflows.

Model file save location

```plaintext
📂 ComfyUI/
├── 📂 models/
│   ├── 📂 text_encoders/
│   │   ├─── clip_l_hidream.safetensors
│   │   ├─── clip_g_hidream.safetensors
│   │   ├─── t5xxl_fp8_e4m3fn_scaled.safetensors
│   │   └─── llama_3.1_8b_instruct_fp8_scaled.safetensors
│   └── 📂 vae/
│   │   └── ae.safetensors
│   └── 📂 diffusion_models/
│       └── ...               # We will guide you to install in the corresponding version workflow       
```

### [​](http://docs.comfy.org#hidream-i1-full-version-workflow) HiDream-I1 Full Version Workflow

#### [​](http://docs.comfy.org#1-model-file-download) 1. Model File Download

Please select the appropriate version based on your hardware. Click the link and download the corresponding model file to save it to the `ComfyUI/models/diffusion_models/` folder.

- FP8 version: [hidream\_i1\_full\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp8.safetensors?download=true) requires more than 16GB of VRAM
- Full version: [hidream\_i1\_full\_f16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_full_fp16.safetensors?download=true) requires more than 27GB of VRAM

#### [​](http://docs.comfy.org#2-workflow-file-download) 2. Workflow File Download

Please download the image below and drag it into ComfyUI to load the corresponding workflow

#### [​](http://docs.comfy.org#3-complete-the-workflow-step-by-step) 3. Complete the Workflow Step by Step

Complete the workflow execution step by step

1. Make sure the `Load Diffusion Model` node is using the `hidream_i1_full_fp8.safetensors` file
2. Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly
   
   - clip\_l\_hidream.safetensors
   - clip\_g\_hidream.safetensors
   - t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   - llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node is using the `ae.safetensors` file
4. For the **full** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `3.0`
5. For the `Ksampler` node, you need to make the following settings
   
   - Set `steps` to `50`
   - Set `cfg` to `5.0`
   - (Optional) Set `sampler` to `lcm`
   - (Optional) Set `scheduler` to `normal`
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

### [​](http://docs.comfy.org#hidream-i1-dev-version-workflow) HiDream-I1 Dev Version Workflow

#### [​](http://docs.comfy.org#1-model-file-download-2) 1. Model File Download

Please select the appropriate version based on your hardware, click the link and download the corresponding model file to save to the `ComfyUI/models/diffusion_models/` folder.

- FP8 version: [hidream\_i1\_dev\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_fp8.safetensors?download=true) requires more than 16GB of VRAM
- Full version: [hidream\_i1\_dev\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_dev_bf16.safetensors?download=true) requires more than 27GB of VRAM

#### [​](http://docs.comfy.org#2-workflow-file-download-2) 2. Workflow File Download

Please download the image below and drag it into ComfyUI to load the corresponding workflow

#### [​](http://docs.comfy.org#3-complete-the-workflow-step-by-step-2) 3. Complete the Workflow Step by Step

Complete the workflow execution step by step

1. Make sure the `Load Diffusion Model` node is using the `hidream_i1_dev_fp8.safetensors` file
2. Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly
   
   - clip\_l\_hidream.safetensors
   - clip\_g\_hidream.safetensors
   - t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   - llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node is using the `ae.safetensors` file
4. For the **dev** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `6.0`
5. For the `Ksampler` node, you need to make the following settings
   
   - Set `steps` to `28`
   - (Important) Set `cfg` to `1.0`
   - (Optional) Set `sampler` to `lcm`
   - (Optional) Set `scheduler` to `normal`
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

### [​](http://docs.comfy.org#hidream-i1-fast-version-workflow) HiDream-I1 Fast Version Workflow

#### [​](http://docs.comfy.org#1-model-file-download-3) 1. Model File Download

Please select the appropriate version based on your hardware, click the link and download the corresponding model file to save to the `ComfyUI/models/diffusion_models/` folder.

- FP8 version: [hidream\_i1\_fast\_fp8.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_fp8.safetensors?download=true) requires more than 16GB of VRAM
- Full version: [hidream\_i1\_fast\_bf16.safetensors](https://huggingface.co/Comfy-Org/HiDream-I1_ComfyUI/resolve/main/split_files/diffusion_models/hidream_i1_fast_fp8.safetensors?download=true) requires more than 27GB of VRAM

#### [​](http://docs.comfy.org#2-workflow-file-download-3) 2. Workflow File Download

Please download the image below and drag it into ComfyUI to load the corresponding workflow

#### [​](http://docs.comfy.org#3-complete-the-workflow-step-by-step-3) 3. Complete the Workflow Step by Step

Complete the workflow execution step by step

1. Make sure the `Load Diffusion Model` node is using the `hidream_i1_fast_fp8.safetensors` file
2. Make sure the four corresponding text encoders in `QuadrupleCLIPLoader` are loaded correctly
   
   - clip\_l\_hidream.safetensors
   - clip\_g\_hidream.safetensors
   - t5xxl\_fp8\_e4m3fn\_scaled.safetensors
   - llama\_3.1\_8b\_instruct\_fp8\_scaled.safetensors
3. Make sure the `Load VAE` node is using the `ae.safetensors` file
4. For the **fast** version, you need to set the `shift` parameter in `ModelSamplingSD3` to `3.0`
5. For the `Ksampler` node, you need to make the following settings
   
   - Set `steps` to `16`
   - (Important) Set `cfg` to `1.0`
   - (Optional) Set `sampler` to `lcm`
   - (Optional) Set `scheduler` to `normal`
6. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute the image generation

## [​](http://docs.comfy.org#other-related-resources) Other Related Resources

### [​](http://docs.comfy.org#gguf-version-models) GGUF Version Models

- [HiDream-I1-Full-gguf](https://huggingface.co/city96/HiDream-I1-Full-gguf)
- [HiDream-I1-Dev-gguf](https://huggingface.co/city96/HiDream-I1-Dev-gguf)

You need to use the “Unet Loader (GGUF)” node in City96’s [ComfyUI-GGUF](https://github.com/city96/ComfyUI-GGUF) to replace the “Load Diffusion Model” node.

### [​](http://docs.comfy.org#nf4-version-models) NF4 Version Models

- [HiDream-I1-nf4](https://github.com/hykilpikonna/HiDream-I1-nf4)
- Use the [ComfyUI-HiDream-Sampler](https://github.com/SanDiegoDude/ComfyUI-HiDream-Sampler) node to use the NF4 version model.

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/image/hidream/hidream-i1.mdx)

[Previous](http://docs.comfy.org/tutorials/flux/flux-1-controlnet)

[HiDream-e1This guide will help you understand and complete the ComfyUI native HiDream-I1 text-to-image workflow example  
\
Next](http://docs.comfy.org/tutorials/image/hidream/hidream-e1)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Model Features](http://docs.comfy.org#model-features)
- [About This Workflow Example](http://docs.comfy.org#about-this-workflow-example)
- [HiDream-I1 Workflow](http://docs.comfy.org#hidream-i1-workflow)
- [Model Installation](http://docs.comfy.org#model-installation)
- [HiDream-I1 Full Version Workflow](http://docs.comfy.org#hidream-i1-full-version-workflow)
- [1. Model File Download](http://docs.comfy.org#1-model-file-download)
- [2. Workflow File Download](http://docs.comfy.org#2-workflow-file-download)
- [3. Complete the Workflow Step by Step](http://docs.comfy.org#3-complete-the-workflow-step-by-step)
- [HiDream-I1 Dev Version Workflow](http://docs.comfy.org#hidream-i1-dev-version-workflow)
- [1. Model File Download](http://docs.comfy.org#1-model-file-download-2)
- [2. Workflow File Download](http://docs.comfy.org#2-workflow-file-download-2)
- [3. Complete the Workflow Step by Step](http://docs.comfy.org#3-complete-the-workflow-step-by-step-2)
- [HiDream-I1 Fast Version Workflow](http://docs.comfy.org#hidream-i1-fast-version-workflow)
- [1. Model File Download](http://docs.comfy.org#1-model-file-download-3)
- [2. Workflow File Download](http://docs.comfy.org#2-workflow-file-download-3)
- [3. Complete the Workflow Step by Step](http://docs.comfy.org#3-complete-the-workflow-step-by-step-3)
- [Other Related Resources](http://docs.comfy.org#other-related-resources)
- [GGUF Version Models](http://docs.comfy.org#gguf-version-models)
- [NF4 Version Models](http://docs.comfy.org#nf4-version-models)

<!-- END Get_Started/tutorials/image/hidream/hidream-i1.md -->


<!-- BEGIN Get_Started/tutorials/video/hunyuan-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
  
  - [LTX-Video](http://docs.comfy.org/tutorials/video/ltxv)
  - [Hunyuan Video](http://docs.comfy.org/tutorials/video/hunyuan-video)
  - Wan Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Hunyuan Video Examples

# ComfyUI Hunyuan Video Examples

This guide shows how to use Hunyuan Text-to-Video and Image-to-Video workflows in ComfyUI

Hunyuan Video series is developed and open-sourced by [Tencent](https://huggingface.co/tencent), featuring a hybrid architecture that supports both [Text-to-Video](https://github.com/Tencent/HunyuanVideo) and [Image-to-Video](https://github.com/Tencent/HunyuanVideo-I2V) generation with a parameter scale of 13B.

Technical features:

- **Core Architecture:** Uses a DiT (Diffusion Transformer) architecture similar to Sora, effectively fusing text, image, and motion information to improve consistency, quality, and alignment between generated video frames. A unified full-attention mechanism enables multi-view camera transitions while ensuring subject consistency.
- **3D VAE:** The custom 3D VAE compresses videos into a compact latent space, making image-to-video generation more efficient.
- **Superior Image-Video-Text Alignment:** Utilizing MLLM text encoders that excel in both image and video generation, better following text instructions, capturing details, and performing complex reasoning.

You can learn more through the official repositories: [Hunyuan Video](https://github.com/Tencent/HunyuanVideo) and [Hunyuan Video-I2V](https://github.com/Tencent/HunyuanVideo-I2V).

This guide will walk you through setting up both **Text-to-Video** and **Image-to-Video** workflows in ComfyUI.

The workflow images in this tutorial contain metadata with model download information.

Simply drag them into ComfyUI or use the menu `Workflows` -&gt; `Open (ctrl+o)` to load the corresponding workflow, which will prompt you to download the required models.

Alternatively, this guide provides direct model links if automatic downloads fail or you are not using the Desktop version. All models are available [here](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/tree/main/split_files) for download.

## [​](http://docs.comfy.org#shared-models-for-all-workflows) Shared Models for All Workflows

The following models are used in both Text-to-Video and Image-to-Video workflows. Please download and save them to the specified directories:

- [clip\_l.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/clip_l.safetensors?download=true)
- [llava\_llama3\_fp8\_scaled.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/text_encoders/llava_llama3_fp8_scaled.safetensors?download=true)
- [hunyuan\_video\_vae\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/vae/hunyuan_video_vae_bf16.safetensors?download=true)

Storage location:

```plaintext
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors
│   │   └── llava_llama3_fp8_scaled.safetensors
│   ├── vae/
│   │   └── hunyuan_video_vae_bf16.safetensors
```

## [​](http://docs.comfy.org#hunyuan-text-to-video-workflow) Hunyuan Text-to-Video Workflow

Hunyuan Text-to-Video was open-sourced in December 2024, supporting 5-second short video generation through natural language descriptions in both Chinese and English.

### [​](http://docs.comfy.org#1-workflow) 1. Workflow

Download the image below and drag it into ComfyUI to load the workflow:

### [​](http://docs.comfy.org#2-manual-models-installation) 2. Manual Models Installation

Download [hunyuan\_video\_t2v\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_t2v_720p_bf16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models` folder.

Ensure you have all these model files in the correct locations:

```plaintext
ComfyUI/
├── models/
│   ├── text_encoders/
│   │   ├── clip_l.safetensors                       // Shared model
│   │   └── llava_llama3_fp8_scaled.safetensors      // Shared model
│   ├── vae/
│   │   └── hunyuan_video_vae_bf16.safetensors       // Shared model
│   └── diffusion_models/
│       └── hunyuan_video_t2v_720p_bf16.safetensors  // T2V model
```

### [​](http://docs.comfy.org#3-steps-to-run-the-workflow) 3. Steps to Run the Workflow

1. Ensure the `DualCLIPLoader` node has loaded these models:
   
   - clip\_name1: clip\_l.safetensors
   - clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. Ensure the `Load Diffusion Model` node has loaded `hunyuan_video_t2v_720p_bf16.safetensors`
3. Ensure the `Load VAE` node has loaded `hunyuan_video_vae_bf16.safetensors`
4. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

When the `length` parameter in the `EmptyHunyuanLatentVideo` node is set to 1, the model can generate a static image.

## [​](http://docs.comfy.org#hunyuan-image-to-video-workflow) Hunyuan Image-to-Video Workflow

Hunyuan Image-to-Video model was open-sourced on March 6, 2025, based on the HunyuanVideo framework. It transforms static images into smooth, high-quality videos and also provides LoRA training code to customize special video effects like hair growth, object transformation, etc.

Currently, the Hunyuan Image-to-Video model has two versions:

- v1 “concat”: Better motion fluidity but less adherence to the image guidance
- v2 “replace”: Updated the day after v1, with better image guidance but seemingly less dynamic compared to v1

v1 “concat”

v2 “replace”

### [​](http://docs.comfy.org#shared-model-for-v1-and-v2-versions) Shared Model for v1 and v2 Versions

Download the following file and save it to the `ComfyUI/models/clip_vision` directory:

- [llava\_llama3\_vision.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/clip_vision/llava_llama3_vision.safetensors?download=true)

### [​](http://docs.comfy.org#v1-%E2%80%9Cconcat%E2%80%9D-image-to-video-workflow) V1 “concat” Image-to-Video Workflow

#### [​](http://docs.comfy.org#1-workflow-and-asset) 1. Workflow and Asset

Download the workflow image below and drag it into ComfyUI to load the workflow:

Download the image below, which we’ll use as the starting frame for the image-to-video generation:

#### [​](http://docs.comfy.org#2-related-models-manual-installation) 2. Related models manual installation

- [hunyuan\_video\_image\_to\_video\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_image_to_video_720p_bf16.safetensors?download=true)

Ensure you have all these model files in the correct locations:

```plaintext
ComfyUI/
├── models/
│   ├── clip_vision/
│   │   └── llava_llama3_vision.safetensors                     // I2V shared model
│   ├── text_encoders/
│   │   ├── clip_l.safetensors                                  // Shared model
│   │   └── llava_llama3_fp8_scaled.safetensors                 // Shared model
│   ├── vae/
│   │   └── hunyuan_video_vae_bf16.safetensors                  // Shared model
│   └── diffusion_models/
│       └── hunyuan_video_image_to_video_720p_bf16.safetensors  // I2V v1 "concat" version model
```

#### [​](http://docs.comfy.org#3-steps-to-run-the-workflow-2) 3. Steps to Run the Workflow

1. Ensure that `DualCLIPLoader` has loaded these models:
   
   - clip\_name1: clip\_l.safetensors
   - clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. Ensure that `Load CLIP Vision` has loaded `llava_llama3_vision.safetensors`
3. Ensure that `Load Image Model` has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`
4. Ensure that `Load VAE` has loaded `vae_name: hunyuan_video_vae_bf16.safetensors`
5. Ensure that `Load Diffusion Model` has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`
6. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

### [​](http://docs.comfy.org#v2-%E2%80%9Creplace%E2%80%9D-image-to-video-workflow) v2 “replace” Image-to-Video Workflow

The v2 workflow is essentially the same as the v1 workflow. You just need to download the **replace** model and use it in the `Load Diffusion Model` node.

#### [​](http://docs.comfy.org#1-workflow-and-asset-2) 1. Workflow and Asset

Download the workflow image below and drag it into ComfyUI to load the workflow:

Download the image below, which we’ll use as the starting frame for the image-to-video generation:

#### [​](http://docs.comfy.org#2-related-models-manual-installation-2) 2. Related models manual installation

- [hunyuan\_video\_v2\_replace\_image\_to\_video\_720p\_bf16.safetensors](https://huggingface.co/Comfy-Org/HunyuanVideo_repackaged/resolve/main/split_files/diffusion_models/hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors?download=true)

Ensure you have all these model files in the correct locations:

```plaintext
ComfyUI/
├── models/
│   ├── clip_vision/
│   │   └── llava_llama3_vision.safetensors                                // I2V shared model
│   ├── text_encoders/
│   │   ├── clip_l.safetensors                                             // Shared model
│   │   └── llava_llama3_fp8_scaled.safetensors                            // Shared model
│   ├── vae/
│   │   └── hunyuan_video_vae_bf16.safetensors                             // Shared model
│   └── diffusion_models/
│       └── hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors  // V2 "replace" version model
```

#### [​](http://docs.comfy.org#3-steps-to-run-the-workflow-3) 3. Steps to Run the Workflow

1. Ensure the `DualCLIPLoader` node has loaded these models:
   
   - clip\_name1: clip\_l.safetensors
   - clip\_name2: llava\_llama3\_fp8\_scaled.safetensors
2. Ensure the `Load CLIP Vision` node has loaded `llava_llama3_vision.safetensors`
3. Ensure the `Load Image Model` node has loaded `hunyuan_video_image_to_video_720p_bf16.safetensors`
4. Ensure the `Load VAE` node has loaded `hunyuan_video_vae_bf16.safetensors`
5. Ensure the `Load Diffusion Model` node has loaded `hunyuan_video_v2_replace_image_to_video_720p_bf16.safetensors`
6. Click the `Queue` button or use the shortcut `Ctrl(cmd) + Enter` to run the workflow

## [​](http://docs.comfy.org#try-it-yourself) Try it yourself

Here are some images and prompts we provide. Based on that content or make an adjustment to create your own video.

```plaintext
Futuristic robot dancing ballet, dynamic motion, fast motion, fast shot, moving scene
```

* * *

```plaintext
Samurai waving sword and hitting the camera. camera angle movement, zoom in, fast scene, super fast, dynamic
```

* * *

```plaintext
flying car fastly moving and flying through the city
```

* * *

```plaintext
cyberpunk car race in night city, dynamic, super fast, fast shot
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/video/hunyuan-video.mdx)

[Previous](http://docs.comfy.org/tutorials/video/ltxv)

[Wan VideoThis guide demonstrates how to generate videos with first and last frames using Wan2.1 Video in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/video/wan/wan-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Shared Models for All Workflows](http://docs.comfy.org#shared-models-for-all-workflows)
- [Hunyuan Text-to-Video Workflow](http://docs.comfy.org#hunyuan-text-to-video-workflow)
- [1. Workflow](http://docs.comfy.org#1-workflow)
- [2. Manual Models Installation](http://docs.comfy.org#2-manual-models-installation)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow)
- [Hunyuan Image-to-Video Workflow](http://docs.comfy.org#hunyuan-image-to-video-workflow)
- [Shared Model for v1 and v2 Versions](http://docs.comfy.org#shared-model-for-v1-and-v2-versions)
- [V1 “concat” Image-to-Video Workflow](http://docs.comfy.org#v1-%E2%80%9Cconcat%E2%80%9D-image-to-video-workflow)
- [1. Workflow and Asset](http://docs.comfy.org#1-workflow-and-asset)
- [2. Related models manual installation](http://docs.comfy.org#2-related-models-manual-installation)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow-2)
- [v2 “replace” Image-to-Video Workflow](http://docs.comfy.org#v2-%E2%80%9Creplace%E2%80%9D-image-to-video-workflow)
- [1. Workflow and Asset](http://docs.comfy.org#1-workflow-and-asset-2)
- [2. Related models manual installation](http://docs.comfy.org#2-related-models-manual-installation-2)
- [3. Steps to Run the Workflow](http://docs.comfy.org#3-steps-to-run-the-workflow-3)
- [Try it yourself](http://docs.comfy.org#try-it-yourself)

<!-- END Get_Started/tutorials/video/hunyuan-video.md -->


<!-- BEGIN Get_Started/tutorials/video/ltxv.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
  
  - [LTX-Video](http://docs.comfy.org/tutorials/video/ltxv)
  - [Hunyuan Video](http://docs.comfy.org/tutorials/video/hunyuan-video)
  - Wan Video
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

LTX-Video

# LTX-Video

[LTX-Video](https://huggingface.co/Lightricks/LTX-Video) is a very efficient video model by lightricks. The important thing with this model is to give it long descriptive prompts.

## [​](http://docs.comfy.org#multi-frame-control) Multi Frame Control

Allows you to control the video with a series of images. You can download the input images: [starting frame](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house1.png) and [ending frame](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/multi-frame/house2.png).

Drag the video directly into ComfyUI to run the workflow.

## [​](http://docs.comfy.org#image-to-video) Image to Video

Allows you to control the video with a first [frame image](https://raw.githubusercontent.com/Comfy-Org/example_workflows/refs/heads/main/ltxv/i2v/girl1.png).

Drag the video directly into ComfyUI to run the workflow.

## [​](http://docs.comfy.org#text-to-video) Text to Video

Drag the video directly into ComfyUI to run the workflow.

## [​](http://docs.comfy.org#requirements) Requirements

Download the following models and place them in the locations specified below:

- [ltx-video-2b-v0.9.5.safetensors](https://huggingface.co/Lightricks/LTX-Video/resolve/main/ltx-video-2b-v0.9.5.safetensors?download=true)
- [t5xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/mochi_preview_repackaged/resolve/main/split_files/text_encoders/t5xxl_fp16.safetensors?download=true)

```plaintext
├── checkpoints/
│   └── ltx-video-2b-v0.9.5.safetensors
└── text_encoders/
    └── t5xxl_fp16.safetensors
```

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/video/ltxv.mdx)

[Previous](http://docs.comfy.org/tutorials/3d/hunyuan3D-2)

[Hunyuan VideoThis guide shows how to use Hunyuan Text-to-Video and Image-to-Video workflows in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/video/hunyuan-video)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Multi Frame Control](http://docs.comfy.org#multi-frame-control)
- [Image to Video](http://docs.comfy.org#image-to-video)
- [Text to Video](http://docs.comfy.org#text-to-video)
- [Requirements](http://docs.comfy.org#requirements)

<!-- END Get_Started/tutorials/video/ltxv.md -->


<!-- BEGIN Get_Started/tutorials/video/wan/fun-control.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
  
  - [LTX-Video](http://docs.comfy.org/tutorials/video/ltxv)
  - [Hunyuan Video](http://docs.comfy.org/tutorials/video/hunyuan-video)
  - Wan Video
    
    - [Wan Video](http://docs.comfy.org/tutorials/video/wan/wan-video)
    - [Wan2.1 Fun Control](http://docs.comfy.org/tutorials/video/wan/fun-control)
    - [Wan2.1 Fun InP](http://docs.comfy.org/tutorials/video/wan/fun-inp)
    - [First-Last Frame](http://docs.comfy.org/tutorials/video/wan/wan-flf)
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Wan2.1 Fun Control Video Examples

# ComfyUI Wan2.1 Fun Control Video Examples

This guide demonstrates how to use Wan2.1 Fun Control in ComfyUI to generate videos with control videos

## [​](http://docs.comfy.org#about-wan2-1-fun-control) About Wan2.1-Fun-Control

**Wan2.1-Fun-Control** is an open-source video generation and control project developed by Alibaba team. It introduces innovative Control Codes mechanisms combined with deep learning and multimodal conditional inputs to generate high-quality videos that conform to preset control conditions. The project focuses on precisely guiding generated video content through multimodal control conditions.

Currently, the Fun Control model supports various control conditions, including **Canny (line art), Depth, OpenPose (human posture), MLSD (geometric edges), and trajectory control.** The model also supports multi-resolution video prediction with options for 512, 768, and 1024 resolutions at 16 frames per second, generating videos up to 81 frames (approximately 5 seconds) in length.

Model versions:

- **1.3B** Lightweight: Suitable for local deployment and quick inference with **lower VRAM requirements**
- **14B** High-performance: Model size reaches 32GB+, offering better results but **requiring higher VRAM**

Here are the relevant code repositories:

- [Wan2.1-Fun-1.3B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Control)
- [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control)
- Code repository: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)

ComfyUI now **natively supports** the Wan2.1 Fun Control model. Before starting this tutorial, please update your ComfyUI to ensure you’re using a version after [this commit](https://github.com/comfyanonymous/ComfyUI/commit/3661c833bcc41b788a7c9f0e7bc48524f8ee5f82).

In this guide, we’ll provide two workflows:

1. A workflow using only native Comfy Core nodes
2. A workflow using custom nodes

Due to current limitations in native nodes for video support, the native-only workflow ensures users can complete the process without installing custom nodes. However, we’ve found that providing a good user experience for video generation is challenging without custom nodes, so we’re providing both workflow versions in this guide.

## [​](http://docs.comfy.org#model-installation) Model Installation

You only need to install these models once. The workflow images also contain model download information, so you can choose your preferred download method.

The following models can be found at [Wan\_2.1\_ComfyUI\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) and [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334)

Click the corresponding links to download. If you’ve used Wan-related workflows before, you only need to download the **Diffusion models**.

**Diffusion models** - choose 1.3B or 14B. The 14B version has a larger file size (32GB) and higher VRAM requirements:

- [wan2.1\_fun\_control\_1.3B\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_control_1.3B_bf16.safetensors?download=true)
- [Wan2.1-Fun-14B-Control](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Control/blob/main/diffusion_pytorch_model.safetensors?download=true): Rename to `Wan2.1-Fun-14B-Control.safetensors` after downloading

**Text encoders** - choose one of the following models (fp16 precision has a larger size and higher performance requirements):

- [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
- [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

- [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

- [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

File storage location:

```plaintext
📂 ComfyUI/
├── 📂 models/
│   ├── 📂 diffusion_models/
│   │   └── wan2.1_fun_control_1.3B_bf16.safetensors
│   ├── 📂 text_encoders/
│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors
│   └── 📂 vae/
│   │   └── wan_2.1_vae.safetensors
│   └── 📂 clip_vision/
│       └──  clip_vision_h.safetensors                 
```

## [​](http://docs.comfy.org#comfyui-native-workflow) ComfyUI Native Workflow

In this workflow, we use videos converted to **WebP format** since the `Load Image` node doesn’t currently support mp4 format. We also use **Canny Edge** to preprocess the original video. Because many users encounter installation failures and environment issues when installing custom nodes, this version of the workflow uses only native nodes to ensure a smoother experience.

Thanks to our powerful ComfyUI authors who provide feature-rich nodes. If you want to directly check the related version, see [Workflow Using Custom Nodes](http://docs.comfy.org/_sites/docs.comfy.org/tutorials/video/wan/fun-control#workflow-using-custom-nodes).

### [​](http://docs.comfy.org#1-workflow-file-download) 1. Workflow File Download

#### [​](http://docs.comfy.org#1-1-workflow-file) 1.1 Workflow File

Download the image below and drag it into ComfyUI to load the workflow:

#### [​](http://docs.comfy.org#1-2-input-images-and-videos-download) 1.2 Input Images and Videos Download

Please download the following image and video for input:

### [​](http://docs.comfy.org#2-complete-the-workflow-step-by-step) 2. Complete the Workflow Step by Step

1. Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_control_1.3B_bf16.safetensors`
2. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
4. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
5. Upload the starting frame to the `Load Image` node (renamed to `Start_image`)
6. Upload the control video to the second `Load Image` node. Note: This node currently doesn’t support mp4, only WebP videos
7. (Optional) Modify the prompt (both English and Chinese are supported)
8. (Optional) Adjust the video size in `WanFunControlToVideo`, avoiding overly large dimensions
9. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

### [​](http://docs.comfy.org#3-usage-notes) 3. Usage Notes

- Since we need to input the same number of frames as the control video into the `WanFunControlToVideo` node, if the specified frame count exceeds the actual control video frames, the excess frames may display scenes not conforming to control conditions. We’ll address this issue in the [Workflow Using Custom Nodes](http://docs.comfy.org/_sites/docs.comfy.org/tutorials/video/wan/fun-control#workflow-using-custom-nodes)
- Avoid setting overly large dimensions, as this can make the sampling process very time-consuming. Try generating smaller images first, then upscale
- Use your imagination to build upon this workflow by adding text-to-image or other types of workflows to achieve direct text-to-video generation or style transfer
- Use tools like [ComfyUI-comfyui\_controlnet\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux) for richer control options

## [​](http://docs.comfy.org#workflow-using-custom-nodes) Workflow Using Custom Nodes

We’ll need to install the following two custom nodes:

- [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)
- [ComfyUI-comfyui\_controlnet\_aux](https://github.com/Fannovel16/comfyui_controlnet_aux)

You can use [ComfyUI Manager](https://github.com/Comfy-Org/ComfyUI-Manager) to install missing nodes or follow the installation instructions for each custom node package.

### [​](http://docs.comfy.org#1-workflow-file-download-2) 1. Workflow File Download

#### [​](http://docs.comfy.org#1-1-workflow-file-2) 1.1 Workflow File

Download the image below and drag it into ComfyUI to load the workflow:

Due to the large size of video files, you can also click [here](https://raw.githubusercontent.com/Comfy-Org/example_workflows/main/wan2.1_fun_control/wan2.1_fun_control_use_custom_nodes.json) to download the workflow file in JSON format.

#### [​](http://docs.comfy.org#1-2-input-images-and-videos-download-2) 1.2 Input Images and Videos Download

Please download the following image and video for input:

### [​](http://docs.comfy.org#2-complete-the-workflow-step-by-step-2) 2. Complete the Workflow Step by Step

> The model part is essentially the same. If you’ve already experienced the native-only workflow, you can directly upload the corresponding images and run it.

01. Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_control_1.3B_bf16.safetensors`
02. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
03. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
04. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
05. Upload the starting frame to the `Load Image` node
06. Upload an mp4 format video to the `Load Video(Upload)` custom node. Note that the workflow has adjusted the default `frame_load_cap`
07. For the current image, the `DWPose Estimator` only uses the `detect_face` option
08. (Optional) Modify the prompt (both English and Chinese are supported)
09. (Optional) Adjust the video size in `WanFunControlToVideo`, avoiding overly large dimensions
10. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

### [​](http://docs.comfy.org#3-workflow-notes) 3. Workflow Notes

Thanks to the ComfyUI community authors for their custom node packages:

- This example uses `Load Video(Upload)` to support mp4 videos
- The `video_info` obtained from `Load Video(Upload)` allows us to maintain the same `fps` for the output video
- You can replace `DWPose Estimator` with other preprocessors from the `ComfyUI-comfyui_controlnet_aux` node package
- Prompts support multiple languages

## [​](http://docs.comfy.org#usage-tips) Usage Tips

- A useful tip is that you can combine multiple image preprocessing techniques and then use the `Image Blend` node to achieve the goal of applying multiple control methods simultaneously.
- You can use the `Video Combine` node from `ComfyUI-VideoHelperSuite` to save videos in mp4 format
- We use `SaveAnimatedWEBP` because we currently don’t support embedding workflow into **mp4** and some other custom nodes may not support embedding workflow too. To preserve the workflow in the video, we choose `SaveAnimatedWEBP` node.
- In the `WanFunControlToVideo` node, `control_video` is not mandatory, so sometimes you can skip using a control video, first generate a very small video size like 320x320, and then use them as control video input to achieve consistent results.
- [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)
- [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/video/wan/fun-control.mdx)

[Previous](http://docs.comfy.org/tutorials/video/wan/wan-video)

[Wan2.1 Fun InPThis guide demonstrates how to use Wan2.1 Fun InP in ComfyUI to generate videos with first and last frame control  
\
Next](http://docs.comfy.org/tutorials/video/wan/fun-inp)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [About Wan2.1-Fun-Control](http://docs.comfy.org#about-wan2-1-fun-control)
- [Model Installation](http://docs.comfy.org#model-installation)
- [ComfyUI Native Workflow](http://docs.comfy.org#comfyui-native-workflow)
- [1. Workflow File Download](http://docs.comfy.org#1-workflow-file-download)
- [1.1 Workflow File](http://docs.comfy.org#1-1-workflow-file)
- [1.2 Input Images and Videos Download](http://docs.comfy.org#1-2-input-images-and-videos-download)
- [2. Complete the Workflow Step by Step](http://docs.comfy.org#2-complete-the-workflow-step-by-step)
- [3. Usage Notes](http://docs.comfy.org#3-usage-notes)
- [Workflow Using Custom Nodes](http://docs.comfy.org#workflow-using-custom-nodes)
- [1. Workflow File Download](http://docs.comfy.org#1-workflow-file-download-2)
- [1.1 Workflow File](http://docs.comfy.org#1-1-workflow-file-2)
- [1.2 Input Images and Videos Download](http://docs.comfy.org#1-2-input-images-and-videos-download-2)
- [2. Complete the Workflow Step by Step](http://docs.comfy.org#2-complete-the-workflow-step-by-step-2)
- [3. Workflow Notes](http://docs.comfy.org#3-workflow-notes)
- [Usage Tips](http://docs.comfy.org#usage-tips)

<!-- END Get_Started/tutorials/video/wan/fun-control.md -->


<!-- BEGIN Get_Started/tutorials/video/wan/fun-inp.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
  
  - [LTX-Video](http://docs.comfy.org/tutorials/video/ltxv)
  - [Hunyuan Video](http://docs.comfy.org/tutorials/video/hunyuan-video)
  - Wan Video
    
    - [Wan Video](http://docs.comfy.org/tutorials/video/wan/wan-video)
    - [Wan2.1 Fun Control](http://docs.comfy.org/tutorials/video/wan/fun-control)
    - [Wan2.1 Fun InP](http://docs.comfy.org/tutorials/video/wan/fun-inp)
    - [First-Last Frame](http://docs.comfy.org/tutorials/video/wan/wan-flf)
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Wan2.1 Fun InP Video Examples

# ComfyUI Wan2.1 Fun InP Video Examples

This guide demonstrates how to use Wan2.1 Fun InP in ComfyUI to generate videos with first and last frame control

## [​](http://docs.comfy.org#about-wan2-1-fun-inp) About Wan2.1-Fun-InP

**Wan-Fun InP** is an open-source video generation model released by Alibaba, part of the Wan2.1-Fun series, focusing on generating videos from images with first and last frame control.

**Key features**:

- **First and last frame control**: Supports inputting both first and last frame images to generate transitional video between them, enhancing video coherence and creative freedom. Compared to earlier community versions, Alibaba’s official model produces more stable and significantly higher quality results.
- **Multi-resolution support**: Supports generating videos at 512×512, 768×768, 1024×1024 and other resolutions to accommodate different scenario requirements.

**Model versions**:

- **1.3B** Lightweight: Suitable for local deployment and quick inference with **lower VRAM requirements**
- **14B** High-performance: Model size reaches 32GB+, offering better results but requiring **higher VRAM**

Below are the relevant model weights and code repositories:

- [Wan2.1-Fun-1.3B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-1.3B-Input)
- [Wan2.1-Fun-14B-Input](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-Input)
- Code repository: [VideoX-Fun](https://github.com/aigc-apps/VideoX-Fun)

Currently, ComfyUI natively supports the Wan2.1 Fun InP model. Before starting this tutorial, please update your ComfyUI to ensure your version is after [this commit](https://github.com/comfyanonymous/ComfyUI/commit/0a1f8869c9998bbfcfeb2e97aa96a6d3e0a2b5df).

## [​](http://docs.comfy.org#wan2-1-fun-inp-workflow) Wan2.1 Fun InP Workflow

Download the image below and drag it into ComfyUI to load the workflow:

### [​](http://docs.comfy.org#1-workflow-file-download) 1. Workflow File Download

### [​](http://docs.comfy.org#2-manual-model-installation) 2. Manual Model Installation

If automatic model downloading is ineffective, please download the models manually and save them to the corresponding folders.

The following models can be found at [Wan\_2.1\_ComfyUI\_repackaged](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged) and [Wan2.1-Fun](https://huggingface.co/collections/alibaba-pai/wan21-fun-67e4fb3b76ca01241eb7e334)

**Diffusion models** - choose 1.3B or 14B. The 14B version has a larger file size (32GB) and higher VRAM requirements:

- [wan2.1\_fun\_inp\_1.3B\_bf16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_fun_inp_1.3B_bf16.safetensors?download=true)
- [Wan2.1-Fun-14B-InP](https://huggingface.co/alibaba-pai/Wan2.1-Fun-14B-InP/resolve/main/diffusion_pytorch_model.safetensors?download=true): Rename to `Wan2.1-Fun-14B-InP.safetensors` after downloading

**Text encoders** - choose one of the following models (fp16 precision has a larger size and higher performance requirements):

- [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
- [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

- [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

- [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

File storage location:

```plaintext
📂 ComfyUI/
├── 📂 models/
│   ├── 📂 diffusion_models/
│   │   └── wan2.1_fun_inp_1.3B_bf16.safetensors
│   ├── 📂 text_encoders/
│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors
│   └── 📂 vae/
│   │   └── wan_2.1_vae.safetensors
│   └── 📂 clip_vision/
│       └──  clip_vision_h.safetensors                 
```

### [​](http://docs.comfy.org#3-complete-the-workflow-step-by-step) 3. Complete the Workflow Step by Step

1. Ensure the `Load Diffusion Model` node has loaded `wan2.1_fun_inp_1.3B_bf16.safetensors`
2. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
4. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
5. Upload the starting frame to the `Load Image` node (renamed to `Start_image`)
6. Upload the ending frame to the second `Load Image` node
7. (Optional) Modify the prompt (both English and Chinese are supported)
8. (Optional) Adjust the video size in `WanFunInpaintToVideo`, avoiding overly large dimensions
9. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

### [​](http://docs.comfy.org#4-workflow-notes) 4. Workflow Notes

Please make sure to use the correct model, as `wan2.1_fun_inp_1.3B_bf16.safetensors` and `wan2.1_fun_control_1.3B_bf16.safetensors` are stored in the same folder and have very similar names. Ensure you’re using the right model.

- When using Wan Fun InP, you may need to frequently modify prompts to ensure the accuracy of the corresponding scene transitions.

## [​](http://docs.comfy.org#other-wan2-1-fun-inp-or-video-related-custom-node-packages) Other Wan2.1 Fun InP or video-related custom node packages

- [ComfyUI-VideoHelperSuite](https://github.com/Kosinkadink/ComfyUI-VideoHelperSuite)
- [ComfyUI-WanVideoWrapper](https://github.com/kijai/ComfyUI-WanVideoWrapper)
- [ComfyUI-KJNodes](https://github.com/kijai/ComfyUI-KJNodes)

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/video/wan/fun-inp.mdx)

[Previous](http://docs.comfy.org/tutorials/video/wan/fun-control)

[First-Last FrameThis guide explains how to complete Wan2.1 FLF2V video generation examples in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/video/wan/wan-flf)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [About Wan2.1-Fun-InP](http://docs.comfy.org#about-wan2-1-fun-inp)
- [Wan2.1 Fun InP Workflow](http://docs.comfy.org#wan2-1-fun-inp-workflow)
- [1. Workflow File Download](http://docs.comfy.org#1-workflow-file-download)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation)
- [3. Complete the Workflow Step by Step](http://docs.comfy.org#3-complete-the-workflow-step-by-step)
- [4. Workflow Notes](http://docs.comfy.org#4-workflow-notes)
- [Other Wan2.1 Fun InP or video-related custom node packages](http://docs.comfy.org#other-wan2-1-fun-inp-or-video-related-custom-node-packages)

<!-- END Get_Started/tutorials/video/wan/fun-inp.md -->


<!-- BEGIN Get_Started/tutorials/video/wan/wan-flf.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
  
  - [LTX-Video](http://docs.comfy.org/tutorials/video/ltxv)
  - [Hunyuan Video](http://docs.comfy.org/tutorials/video/hunyuan-video)
  - Wan Video
    
    - [Wan Video](http://docs.comfy.org/tutorials/video/wan/wan-video)
    - [Wan2.1 Fun Control](http://docs.comfy.org/tutorials/video/wan/fun-control)
    - [Wan2.1 Fun InP](http://docs.comfy.org/tutorials/video/wan/fun-inp)
    - [First-Last Frame](http://docs.comfy.org/tutorials/video/wan/wan-flf)
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Wan2.1 FLF2V Native Example

# ComfyUI Wan2.1 FLF2V Native Example

This guide explains how to complete Wan2.1 FLF2V video generation examples in ComfyUI

Wan FLF2V (First-Last Frame Video Generation) is an open-source video generation model developed by the Alibaba Tongyi Wanxiang team. Its open-source license is [Apache 2.0](https://github.com/Wan-Video/Wan2.1?tab=Apache-2.0-1-ov-file). Users only need to provide two images as the starting and ending frames, and the model automatically generates intermediate transition frames, outputting a logically coherent and naturally flowing 720p high-definition video.

**Core Technical Highlights**

1. **Precise First-Last Frame Control**: The matching rate of first and last frames reaches 98%, defining video boundaries through starting and ending scenes, intelligently filling intermediate dynamic changes to achieve scene transitions and object morphing effects.
2. **Stable and Smooth Video Generation**: Using CLIP semantic features and cross-attention mechanisms, the video jitter rate is reduced by 37% compared to similar models, ensuring natural and smooth transitions.
3. **Multi-functional Creative Capabilities**: Supports dynamic embedding of Chinese and English subtitles, generation of anime/realistic/fantasy and other styles, adapting to different creative needs.
4. **720p HD Output**: Directly generates 1280×720 resolution videos without post-processing, suitable for social media and commercial applications.
5. **Open-source Ecosystem Support**: Model weights, code, and training framework are fully open-sourced, supporting deployment on mainstream AI platforms.

**Technical Principles and Architecture**

1. **DiT Architecture**: Based on diffusion models and Diffusion Transformer architecture, combined with Full Attention mechanism to optimize spatiotemporal dependency modeling, ensuring video coherence.
2. **3D Causal Variational Encoder**: Wan-VAE technology compresses HD frames to 1/128 size while retaining subtle dynamic details, significantly reducing memory requirements.
3. **Three-stage Training Strategy**: Starting from 480P resolution pre-training, gradually upgrading to 720P, balancing generation quality and computational efficiency through phased optimization.

**Related Links**

- **GitHub Repository**: [GitHub](https://github.com/Wan-Video/Wan2.1)
- **Hugging Face Model Page**: [Hugging Face](https://huggingface.co/Wan-AI/Wan2.1-FLF2V-14B-720P)
- **ModelScope Community**: [ModelScope](https://www.modelscope.cn/models/Wan-AI/Wan2.1-FLF2V-14B-720P)

## [​](http://docs.comfy.org#wan2-1-flf2v-720p-comfyui-native-workflow-example) Wan2.1 FLF2V 720P ComfyUI Native Workflow Example

### [​](http://docs.comfy.org#1-download-workflow-files-and-related-input-files) 1. Download Workflow Files and Related Input Files

Since this model is trained on high-resolution images, using smaller sizes may not yield good results. In the example, we use a size of 720 * 1280, which may cause users with lower VRAM hard to run smoothly and will take a long time to generate. If needed, please modify the video generation size at the beginning.

Please download the WebP file below, and drag it into ComfyUI to load the corresponding workflow. The workflow has embedded the corresponding model download file information.

Please download the two images below, which we will use as the starting and ending frames of the video

### [​](http://docs.comfy.org#2-manual-model-installation) 2. Manual Model Installation

If corresponding

All models involved in this guide can be found [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files).

**diffusion\_models** Choose one version based on your hardware conditions

- FP16:[wan2.1\_flf2v\_720p\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_flf2v_720p_14B_fp16.safetensors?download=true)
- FP8:[wan2.1\_flf2v\_720p\_14B\_fp8\_e4m3fn.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/blob/main/split_files/diffusion_models/wan2.1_flf2v_720p_14B_fp8_e4m3fn.safetensors)

If you have previously tried Wan Video related workflows, you may already have the following files.

Choose one version from **Text encoders** for download,

- [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
- [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

- [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

- [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

File Storage Location

```plaintext
ComfyUI/
├── models/
│   ├── diffusion_models/
│   │   └─── wan2.1_flf2v_720p_14B_fp16.safetensors           # or FP8 version
│   ├── text_encoders/
│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors           # or your chosen version
│   ├── vae/
│   │   └──  wan_2.1_vae.safetensors
│   └── clip_vision/
│       └──  clip_vision_h.safetensors   
```

### [​](http://docs.comfy.org#3-complete-workflow-execution-step-by-step) 3. Complete Workflow Execution Step by Step

1. Ensure the `Load Diffusion Model` node has loaded `wan2.1_flf2v_720p_14B_fp16.safetensors` or `wan2.1_flf2v_720p_14B_fp8_e4m3fn.safetensors`
2. Ensure the `Load CLIP` node has loaded `umt5_xxl_fp8_e4m3fn_scaled.safetensors`
3. Ensure the `Load VAE` node has loaded `wan_2.1_vae.safetensors`
4. Ensure the `Load CLIP Vision` node has loaded `clip_vision_h.safetensors`
5. Upload the starting frame to the `Start_image` node
6. Upload the ending frame to the `End_image` node
7. (Optional) Modify the positive and negative prompts, both Chinese and English are supported
8. (**Important**) In `WanFirstLastFrameToVideo`, modify the corresponding video size. We default to using a size of 720 * 1280 to achieve better results for the generated video, but this may cause issues running smoothly on lower memory. You can initially try adjusting it to a size such as 480 * 854 to ensure smooth operation, and then adjust it back to 720 * 1280 when you need to generate larger-sized videos to ensure quality results.
9. Click the `Run` button, or use the shortcut `Ctrl(cmd) + Enter` to execute video generation

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/video/wan/wan-flf.mdx)

[Previous](http://docs.comfy.org/tutorials/video/wan/fun-inp)

[ACE-Step Music GenerationThis guide will help you create dynamic music using the ACE-Step model in ComfyUI  
\
Next](http://docs.comfy.org/tutorials/audio/ace-step/ace-step-v1)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Wan2.1 FLF2V 720P ComfyUI Native Workflow Example](http://docs.comfy.org#wan2-1-flf2v-720p-comfyui-native-workflow-example)
- [1. Download Workflow Files and Related Input Files](http://docs.comfy.org#1-download-workflow-files-and-related-input-files)
- [2. Manual Model Installation](http://docs.comfy.org#2-manual-model-installation)
- [3. Complete Workflow Execution Step by Step](http://docs.comfy.org#3-complete-workflow-execution-step-by-step)

<!-- END Get_Started/tutorials/video/wan/wan-flf.md -->


<!-- BEGIN Get_Started/tutorials/video/wan/wan-video.md -->

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

Search or ask...

Get Started

##### Get Started

- [Introduction](http://docs.comfy.org/get_started/introduction)
- Installation
- [First Image Generation](http://docs.comfy.org/get_started/first_generation)

##### Interface

- [Interface Overview](http://docs.comfy.org/interface/overview)
- [Account Management](http://docs.comfy.org/interface/user)
- [Credits Management](http://docs.comfy.org/interface/credits)
- [Workflow](http://docs.comfy.org/essentials/core-concepts/workflow)
- [Nodes](http://docs.comfy.org/essentials/core-concepts/nodes)
- [Properties](http://docs.comfy.org/essentials/core-concepts/properties)
- [Links](http://docs.comfy.org/essentials/core-concepts/links)
- [Mask Editor](http://docs.comfy.org/interface/maskeditor)
- [Models](http://docs.comfy.org/essentials/core-concepts/models)
- [Dependencies](http://docs.comfy.org/essentials/core-concepts/dependencies)
- [Shortcuts](http://docs.comfy.org/interface/shortcuts)

##### Tutorials

- Basic Examples
- ControlNet
- Flux
- Image
- 3D
- Video
  
  - [LTX-Video](http://docs.comfy.org/tutorials/video/ltxv)
  - [Hunyuan Video](http://docs.comfy.org/tutorials/video/hunyuan-video)
  - Wan Video
    
    - [Wan Video](http://docs.comfy.org/tutorials/video/wan/wan-video)
    - [Wan2.1 Fun Control](http://docs.comfy.org/tutorials/video/wan/fun-control)
    - [Wan2.1 Fun InP](http://docs.comfy.org/tutorials/video/wan/fun-inp)
    - [First-Last Frame](http://docs.comfy.org/tutorials/video/wan/wan-flf)
- Audio
- API Nodes

##### Community

- [Contributing](http://docs.comfy.org/community/contributing)

<!--THE END-->

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
  
  ![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)
  
  English

[ComfyUI home page![logo](https://mintlify.s3.us-west-1.amazonaws.com/dripart/logo.png)](http://docs.comfy.org/)

![US](https://purecatamphetamine.github.io/country-flag-icons/1x1/US.svg)

English

Search or ask...

- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)
- [comfyanonymous/ComfyUI](https://github.com/comfyanonymous/ComfyUI)

Search...

Navigation

ComfyUI Wan2.1 Video Examples

# ComfyUI Wan2.1 Video Examples

This guide demonstrates how to generate videos with first and last frames using Wan2.1 Video in ComfyUI

Wan2.1 Video series is a video generation model open-sourced by Alibaba in February 2025 under the [Apache 2.0 license](https://github.com/Wan-Video/Wan2.1?tab=Apache-2.0-1-ov-file). It offers two versions:

- 14B (14 billion parameters)
- 1.3B (1.3 billion parameters) Covering multiple tasks including text-to-video (T2V) and image-to-video (I2V). The model not only outperforms existing open-source models in performance but more importantly, its lightweight version requires only 8GB of VRAM to run, significantly lowering the barrier to entry.

<!--THE END-->

- [Wan2.1 Code Repository](https://github.com/Wan-Video/Wan2.1)
- [Wan2.1 Model Repository](https://huggingface.co/Wan-AI)

## [​](http://docs.comfy.org#wan2-1-comfyui-native-workflow-examples) Wan2.1 ComfyUI Native Workflow Examples

Please update ComfyUI to the latest version before starting the examples to make sure you have native Wan Video support.

## [​](http://docs.comfy.org#model-installation) Model Installation

All models mentioned in this guide can be found [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files). Below are the common models you’ll need for the examples in this guide, which you can download in advance:

Choose one version from **Text encoders** to download:

- [umt5\_xxl\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp16.safetensors?download=true)
- [umt5\_xxl\_fp8\_e4m3fn\_scaled.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/text_encoders/umt5_xxl_fp8_e4m3fn_scaled.safetensors?download=true)

**VAE**

- [wan\_2.1\_vae.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

**CLIP Vision**

- [clip\_vision\_h.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/clip_vision/clip_vision_h.safetensors?download=true)

File storage locations:

```plaintext
ComfyUI/
├── models/
│   ├── diffusion_models/
│   ├── ...                  # Let's download the models in the corresponding workflow
│   ├── text_encoders/
│   │   └─── umt5_xxl_fp8_e4m3fn_scaled.safetensors
│   └── vae/
│   │   └──  wan_2.1_vae.safetensors
│   └── clip_vision/
│       └──  clip_vision_h.safetensors   
```

For diffusion models, we’ll use the fp16 precision models in this guide because we’ve found that they perform better than the bf16 versions. If you need other precision versions, please visit [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models) to download them.

## [​](http://docs.comfy.org#wan2-1-text-to-video-workflow) Wan2.1 Text-to-Video Workflow

Before starting the workflow, please download [wan2.1\_t2v\_1.3B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_t2v_1.3B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.

> If you need other t2v precision versions, please visit [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/tree/main/split_files/diffusion_models) to download them.

### [​](http://docs.comfy.org#1-workflow-file-download) 1. Workflow File Download

Download the file below and drag it into ComfyUI to load the corresponding workflow:

### [​](http://docs.comfy.org#2-complete-the-workflow-step-by-step) 2. Complete the Workflow Step by Step

1. Make sure the `Load Diffusion Model` node has loaded the `wan2.1_t2v_1.3B_fp16.safetensors` model
2. Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model
3. Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model
4. (Optional) You can modify the video dimensions in the `EmptyHunyuanLatentVideo` node if needed
5. (Optional) If you need to modify the prompts (positive and negative), make changes in the `CLIP Text Encoder` node at number `5`
6. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation

## [​](http://docs.comfy.org#wan2-1-image-to-video-workflow) Wan2.1 Image-to-Video Workflow

**Since Wan Video separates the 480P and 720P models**, we’ll need to provide examples for both resolutions in this guide. In addition to using different models, they also have slight parameter differences.

### [​](http://docs.comfy.org#480p-version) 480P Version

#### [​](http://docs.comfy.org#1-workflow-and-input-image) 1. Workflow and Input Image

Download the image below and drag it into ComfyUI to load the corresponding workflow:

We’ll use the following image as input:

#### [​](http://docs.comfy.org#2-model-download) 2. Model Download

Please download [wan2.1\_i2v\_480p\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_480p_14B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.

#### [​](http://docs.comfy.org#3-complete-the-workflow-step-by-step) 3. Complete the Workflow Step by Step

1. Make sure the `Load Diffusion Model` node has loaded the `wan2.1_i2v_480p_14B_fp16.safetensors` model
2. Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model
3. Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model
4. Make sure the `Load CLIP Vision` node has loaded the `clip_vision_h.safetensors` model
5. Upload the provided input image in the `Load Image` node
6. (Optional) Enter the video description content you want to generate in the `CLIP Text Encoder` node
7. (Optional) You can modify the video dimensions in the `WanImageToVideo` node if needed
8. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation

### [​](http://docs.comfy.org#720p-version) 720P Version

#### [​](http://docs.comfy.org#1-workflow-and-input-image-2) 1. Workflow and Input Image

Download the image below and drag it into ComfyUI to load the corresponding workflow:

We’ll use the following image as input:

#### [​](http://docs.comfy.org#2-model-download-2) 2. Model Download

Please download [wan2.1\_i2v\_720p\_14B\_fp16.safetensors](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/diffusion_models/wan2.1_i2v_720p_14B_fp16.safetensors?download=true) and save it to the `ComfyUI/models/diffusion_models/` directory.

#### [​](http://docs.comfy.org#3-complete-the-workflow-step-by-step-2) 3. Complete the Workflow Step by Step

1. Make sure the `Load Diffusion Model` node has loaded the `wan2.1_i2v_720p_14B_fp16.safetensors` model
2. Make sure the `Load CLIP` node has loaded the `umt5_xxl_fp8_e4m3fn_scaled.safetensors` model
3. Make sure the `Load VAE` node has loaded the `wan_2.1_vae.safetensors` model
4. Make sure the `Load CLIP Vision` node has loaded the `clip_vision_h.safetensors` model
5. Upload the provided input image in the `Load Image` node
6. (Optional) Enter the video description content you want to generate in the `CLIP Text Encoder` node
7. (Optional) You can modify the video dimensions in the `WanImageToVideo` node if needed
8. Click the `Run` button or use the shortcut `Ctrl(cmd) + Enter` to execute the video generation

Was this page helpful?

YesNo

[Suggest edits](https://github.com/comfy-org/docs/edit/main/tutorials/video/wan/wan-video.mdx)

[Previous](http://docs.comfy.org/tutorials/video/hunyuan-video)

[Wan2.1 Fun ControlThis guide demonstrates how to use Wan2.1 Fun Control in ComfyUI to generate videos with control videos  
\
Next](http://docs.comfy.org/tutorials/video/wan/fun-control)

[github](https://github.com/comfyanonymous/ComfyUI/)

[Powered by Mintlify](https://mintlify.com/preview-request?utm_campaign=poweredBy&utm_medium=referral&utm_source=docs.comfy.org)

On this page

- [Wan2.1 ComfyUI Native Workflow Examples](http://docs.comfy.org#wan2-1-comfyui-native-workflow-examples)
- [Model Installation](http://docs.comfy.org#model-installation)
- [Wan2.1 Text-to-Video Workflow](http://docs.comfy.org#wan2-1-text-to-video-workflow)
- [1. Workflow File Download](http://docs.comfy.org#1-workflow-file-download)
- [2. Complete the Workflow Step by Step](http://docs.comfy.org#2-complete-the-workflow-step-by-step)
- [Wan2.1 Image-to-Video Workflow](http://docs.comfy.org#wan2-1-image-to-video-workflow)
- [480P Version](http://docs.comfy.org#480p-version)
- [1. Workflow and Input Image](http://docs.comfy.org#1-workflow-and-input-image)
- [2. Model Download](http://docs.comfy.org#2-model-download)
- [3. Complete the Workflow Step by Step](http://docs.comfy.org#3-complete-the-workflow-step-by-step)
- [720P Version](http://docs.comfy.org#720p-version)
- [1. Workflow and Input Image](http://docs.comfy.org#1-workflow-and-input-image-2)
- [2. Model Download](http://docs.comfy.org#2-model-download-2)
- [3. Complete the Workflow Step by Step](http://docs.comfy.org#3-complete-the-workflow-step-by-step-2)

<!-- END Get_Started/tutorials/video/wan/wan-video.md -->
